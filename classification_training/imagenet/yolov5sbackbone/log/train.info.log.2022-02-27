2022-02-26 08:24:45 - network: yolov5sbackbone
2022-02-26 08:24:45 - num_classes: 1000
2022-02-26 08:24:45 - input_image_size: 256
2022-02-26 08:24:45 - scale: 1.1428571428571428
2022-02-26 08:24:45 - trained_model_path: 
2022-02-26 08:24:45 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-26 08:24:45 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7ff65f1c5be0>
2022-02-26 08:24:45 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7ff65f1c5eb0>
2022-02-26 08:24:45 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7ff65f1c5ee0>
2022-02-26 08:24:45 - seed: 0
2022-02-26 08:24:45 - batch_size: 256
2022-02-26 08:24:45 - num_workers: 16
2022-02-26 08:24:45 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-26 08:24:45 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-26 08:24:45 - epochs: 100
2022-02-26 08:24:45 - print_interval: 100
2022-02-26 08:24:45 - distributed: True
2022-02-26 08:24:45 - sync_bn: False
2022-02-26 08:24:45 - apex: True
2022-02-26 08:24:45 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-26 08:24:45 - gpus_num: 2
2022-02-26 08:24:45 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7ff6500d12b0>
2022-02-26 08:24:50 - --------------------parameters--------------------
2022-02-26 08:24:50 - name: conv.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: conv.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: conv.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.2.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.2.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.2.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.4.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.4.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.4.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: middle_layers.6.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.6.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: middle_layers.6.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: sppf.conv1.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: sppf.conv1.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: sppf.conv1.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: sppf.conv2.layer.0.weight, grad: True
2022-02-26 08:24:50 - name: sppf.conv2.layer.1.weight, grad: True
2022-02-26 08:24:50 - name: sppf.conv2.layer.1.bias, grad: True
2022-02-26 08:24:50 - name: fc.weight, grad: True
2022-02-26 08:24:50 - name: fc.bias, grad: True
2022-02-26 08:24:50 - --------------------buffers--------------------
2022-02-26 08:24:50 - name: conv.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: conv.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: conv.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.2.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.2.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.2.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.4.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.4.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.4.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: middle_layers.6.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: middle_layers.6.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: middle_layers.6.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: sppf.conv1.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: sppf.conv1.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: sppf.conv1.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - name: sppf.conv2.layer.1.running_mean, grad: False
2022-02-26 08:24:50 - name: sppf.conv2.layer.1.running_var, grad: False
2022-02-26 08:24:50 - name: sppf.conv2.layer.1.num_batches_tracked, grad: False
2022-02-26 08:24:50 - epoch 001 lr: 0.1
2022-02-26 08:25:30 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9181
2022-02-26 08:26:03 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7445
2022-02-26 08:26:37 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6484
2022-02-26 08:27:11 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5095
2022-02-26 08:27:45 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.3608
2022-02-26 08:28:20 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.1904
2022-02-26 08:28:53 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1435
2022-02-26 08:29:28 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.8732
2022-02-26 08:30:01 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.8269
2022-02-26 08:30:35 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.7905
2022-02-26 08:31:08 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7359
2022-02-26 08:31:43 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.3939
2022-02-26 08:32:17 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.4114
2022-02-26 08:32:52 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.3383
2022-02-26 08:33:26 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.2343
2022-02-26 08:34:00 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.3759
2022-02-26 08:34:34 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.0902
2022-02-26 08:35:09 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.0367
2022-02-26 08:35:43 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.0028
2022-02-26 08:36:16 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.9798
2022-02-26 08:36:51 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.9785
2022-02-26 08:37:26 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.9291
2022-02-26 08:38:00 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.6997
2022-02-26 08:38:35 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.6125
2022-02-26 08:39:08 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.8808
2022-02-26 08:39:43 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.9829
2022-02-26 08:40:17 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.9703
2022-02-26 08:40:52 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.6047
2022-02-26 08:41:26 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.5279
2022-02-26 08:42:01 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.5671
2022-02-26 08:42:35 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.7267
2022-02-26 08:43:10 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.6312
2022-02-26 08:43:44 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.3518
2022-02-26 08:44:19 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.4270
2022-02-26 08:44:53 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.5515
2022-02-26 08:45:26 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.4570
2022-02-26 08:46:01 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.6141
2022-02-26 08:46:35 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.4456
2022-02-26 08:47:10 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.4989
2022-02-26 08:47:45 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.3281
2022-02-26 08:48:19 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.4773
2022-02-26 08:48:54 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.2878
2022-02-26 08:49:27 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.2255
2022-02-26 08:50:03 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.0355
2022-02-26 08:50:36 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.4050
2022-02-26 08:51:12 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.4230
2022-02-26 08:51:45 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.1677
2022-02-26 08:52:20 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.3802
2022-02-26 08:52:54 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.2710
2022-02-26 08:53:28 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.9640
2022-02-26 08:53:28 - train: epoch 001, train_loss: 5.0014
2022-02-26 08:54:47 - eval: epoch: 001, acc1: 21.358%, acc5: 43.716%, test_loss: 3.9415, per_image_load_time: 1.906ms, per_image_inference_time: 0.302ms
2022-02-26 08:54:47 - until epoch: 001, best_acc1: 21.358%
2022-02-26 08:54:47 - epoch 002 lr: 0.1
2022-02-26 08:55:27 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2458
2022-02-26 08:56:01 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.9294
2022-02-26 08:56:36 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.2404
2022-02-26 08:57:09 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2209
2022-02-26 08:57:44 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.8762
2022-02-26 08:58:18 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.9156
2022-02-26 08:58:53 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.1914
2022-02-26 08:59:29 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.7851
2022-02-26 09:00:02 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.7323
2022-02-26 09:00:36 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.2002
2022-02-26 09:01:11 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.0689
2022-02-26 09:01:45 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.9576
2022-02-26 09:02:20 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9222
2022-02-26 09:02:54 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.0658
2022-02-26 09:03:28 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.1805
2022-02-26 09:04:03 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8999
2022-02-26 09:04:36 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9161
2022-02-26 09:05:11 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9846
2022-02-26 09:05:45 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8792
2022-02-26 09:06:20 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6168
2022-02-26 09:06:54 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.8260
2022-02-26 09:07:29 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6286
2022-02-26 09:08:03 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.8466
2022-02-26 09:08:37 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6910
2022-02-26 09:09:11 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.7700
2022-02-26 09:09:46 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.7686
2022-02-26 09:10:21 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.9903
2022-02-26 09:10:54 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.9125
2022-02-26 09:11:29 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6489
2022-02-26 09:12:04 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.7256
2022-02-26 09:12:39 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6818
2022-02-26 09:13:12 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7378
2022-02-26 09:13:47 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6201
2022-02-26 09:14:22 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8379
2022-02-26 09:14:56 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6269
2022-02-26 09:15:31 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.8269
2022-02-26 09:16:05 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.8345
2022-02-26 09:16:40 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.5804
2022-02-26 09:17:15 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.7717
2022-02-26 09:17:49 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.6622
2022-02-26 09:18:23 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.7764
2022-02-26 09:18:58 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.6554
2022-02-26 09:19:33 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5550
2022-02-26 09:20:08 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.6167
2022-02-26 09:20:42 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5381
2022-02-26 09:21:17 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.4840
2022-02-26 09:21:50 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5643
2022-02-26 09:22:26 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.6139
2022-02-26 09:23:01 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4707
2022-02-26 09:23:35 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.5783
2022-02-26 09:23:36 - train: epoch 002, train_loss: 3.8334
2022-02-26 09:24:56 - eval: epoch: 002, acc1: 29.420%, acc5: 54.306%, test_loss: 3.3658, per_image_load_time: 2.823ms, per_image_inference_time: 0.296ms
2022-02-26 09:24:56 - until epoch: 002, best_acc1: 29.420%
2022-02-26 09:24:56 - epoch 003 lr: 0.1
2022-02-26 09:25:36 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5511
2022-02-26 09:26:10 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.6488
2022-02-26 09:26:43 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.5624
2022-02-26 09:27:17 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.5473
2022-02-26 09:27:51 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.7374
2022-02-26 09:28:26 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.4529
2022-02-26 09:29:00 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.7833
2022-02-26 09:29:34 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5796
2022-02-26 09:30:08 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.4685
2022-02-26 09:30:44 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4753
2022-02-26 09:31:18 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.3988
2022-02-26 09:31:51 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.5710
2022-02-26 09:32:27 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.5134
2022-02-26 09:33:00 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.4635
2022-02-26 09:33:34 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.7298
2022-02-26 09:34:09 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.5169
2022-02-26 09:34:44 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.3079
2022-02-26 09:35:19 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.3168
2022-02-26 09:35:52 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.5599
2022-02-26 09:36:28 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.7688
2022-02-26 09:37:03 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.6325
2022-02-26 09:37:36 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.8745
2022-02-26 09:38:11 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.4774
2022-02-26 09:38:45 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.4221
2022-02-26 09:39:19 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.5461
2022-02-26 09:39:54 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.4067
2022-02-26 09:40:29 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.7453
2022-02-26 09:41:03 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.3347
2022-02-26 09:41:37 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.4221
2022-02-26 09:42:12 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.6073
2022-02-26 09:42:48 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.5480
2022-02-26 09:43:21 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.4496
2022-02-26 09:43:56 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3314
2022-02-26 09:44:30 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6049
2022-02-26 09:45:05 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.1766
2022-02-26 09:45:40 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.4083
2022-02-26 09:46:15 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.4132
2022-02-26 09:46:50 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.4812
2022-02-26 09:47:24 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.5422
2022-02-26 09:47:58 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.3100
2022-02-26 09:48:34 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.4045
2022-02-26 09:49:08 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.3405
2022-02-26 09:49:42 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.1627
2022-02-26 09:50:17 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.2017
2022-02-26 09:50:51 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.4982
2022-02-26 09:51:26 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.3454
2022-02-26 09:52:01 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.2630
2022-02-26 09:52:36 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.4942
2022-02-26 09:53:11 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.4937
2022-02-26 09:53:44 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.4542
2022-02-26 09:53:45 - train: epoch 003, train_loss: 3.4678
2022-02-26 09:55:03 - eval: epoch: 003, acc1: 34.110%, acc5: 59.216%, test_loss: 3.1088, per_image_load_time: 2.727ms, per_image_inference_time: 0.308ms
2022-02-26 09:55:03 - until epoch: 003, best_acc1: 34.110%
2022-02-26 09:55:03 - epoch 004 lr: 0.1
2022-02-26 09:55:42 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.3977
2022-02-26 09:56:17 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.3211
2022-02-26 09:56:50 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.2686
2022-02-26 09:57:24 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.1841
2022-02-26 09:57:58 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.2365
2022-02-26 09:58:32 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.5682
2022-02-26 09:59:06 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.3941
2022-02-26 09:59:40 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.1215
2022-02-26 10:00:14 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 3.1356
2022-02-26 10:00:48 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.4267
2022-02-26 10:01:21 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.2967
2022-02-26 10:01:56 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.1671
2022-02-26 10:02:30 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.1029
2022-02-26 10:03:04 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.2233
2022-02-26 10:03:38 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.4287
2022-02-26 10:04:12 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.2243
2022-02-26 10:04:47 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.4032
2022-02-26 10:05:20 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.4976
2022-02-26 10:05:55 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.3759
2022-02-26 10:06:30 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.3270
2022-02-26 10:07:03 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.3858
2022-02-26 10:07:37 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.3156
2022-02-26 10:08:11 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.2410
2022-02-26 10:08:45 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.0386
2022-02-26 10:09:19 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.1571
2022-02-26 10:09:54 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.3292
2022-02-26 10:10:27 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.2197
2022-02-26 10:11:02 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.2158
2022-02-26 10:11:35 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.2159
2022-02-26 10:12:10 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.2103
2022-02-26 10:12:44 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.2683
2022-02-26 10:13:18 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.2861
2022-02-26 10:13:53 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.2993
2022-02-26 10:14:27 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.1692
2022-02-26 10:15:01 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.1488
2022-02-26 10:15:36 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.9961
2022-02-26 10:16:10 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.2316
2022-02-26 10:16:45 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.1136
2022-02-26 10:17:18 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.1472
2022-02-26 10:17:52 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.9205
2022-02-26 10:18:26 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.1226
2022-02-26 10:19:00 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.2165
2022-02-26 10:19:34 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 3.1072
2022-02-26 10:20:08 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.8840
2022-02-26 10:20:42 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.7147
2022-02-26 10:21:17 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.1326
2022-02-26 10:21:51 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.1030
2022-02-26 10:22:26 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.9891
2022-02-26 10:23:00 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.2032
2022-02-26 10:23:35 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.2206
2022-02-26 10:23:36 - train: epoch 004, train_loss: 3.2601
2022-02-26 10:24:52 - eval: epoch: 004, acc1: 37.364%, acc5: 62.838%, test_loss: 2.9010, per_image_load_time: 2.662ms, per_image_inference_time: 0.309ms
2022-02-26 10:24:52 - until epoch: 004, best_acc1: 37.364%
2022-02-26 10:24:52 - epoch 005 lr: 0.1
2022-02-26 10:25:32 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.2576
2022-02-26 10:26:06 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.1359
2022-02-26 10:26:40 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.3719
2022-02-26 10:27:13 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.1627
2022-02-26 10:27:48 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.9776
2022-02-26 10:28:21 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.1080
2022-02-26 10:28:55 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.1621
2022-02-26 10:29:27 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.4670
2022-02-26 10:30:02 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.1994
2022-02-26 10:30:36 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.1422
2022-02-26 10:31:10 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.2767
2022-02-26 10:31:44 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.0990
2022-02-26 10:32:19 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.0670
2022-02-26 10:32:53 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.2759
2022-02-26 10:33:27 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.0177
2022-02-26 10:34:00 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.9543
2022-02-26 10:34:34 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.9397
2022-02-26 10:35:07 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.3553
2022-02-26 10:35:41 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.8265
2022-02-26 10:36:15 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.0941
2022-02-26 10:36:49 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.9998
2022-02-26 10:37:24 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.1627
2022-02-26 10:37:58 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.0068
2022-02-26 10:38:31 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.1152
2022-02-26 10:39:06 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.1577
2022-02-26 10:39:39 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.3831
2022-02-26 10:40:14 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.2621
2022-02-26 10:40:47 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.1694
2022-02-26 10:41:22 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.9879
2022-02-26 10:41:55 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.1183
2022-02-26 10:42:29 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.1332
2022-02-26 10:43:03 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.2320
2022-02-26 10:43:38 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.9212
2022-02-26 10:44:12 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.9884
2022-02-26 10:44:46 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.1512
2022-02-26 10:45:19 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.0610
2022-02-26 10:45:53 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.0137
2022-02-26 10:46:27 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.9186
2022-02-26 10:47:01 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.3602
2022-02-26 10:47:36 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.1582
2022-02-26 10:48:09 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.0859
2022-02-26 10:48:44 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.1935
2022-02-26 10:49:18 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.0193
2022-02-26 10:49:52 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.1454
2022-02-26 10:50:27 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.2326
2022-02-26 10:50:59 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.0685
2022-02-26 10:51:34 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.9420
2022-02-26 10:52:09 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.8404
2022-02-26 10:52:43 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.2096
2022-02-26 10:53:18 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.0206
2022-02-26 10:53:18 - train: epoch 005, train_loss: 3.1297
2022-02-26 10:54:37 - eval: epoch: 005, acc1: 38.088%, acc5: 64.276%, test_loss: 2.8211, per_image_load_time: 2.763ms, per_image_inference_time: 0.265ms
2022-02-26 10:54:37 - until epoch: 005, best_acc1: 38.088%
2022-02-26 10:54:37 - epoch 006 lr: 0.1
2022-02-26 10:55:17 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.0366
2022-02-26 10:55:50 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.1289
2022-02-26 10:56:24 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.8601
2022-02-26 10:56:58 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.0763
2022-02-26 10:57:32 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.1284
2022-02-26 10:58:06 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.0514
2022-02-26 10:58:39 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0325
2022-02-26 10:59:13 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.1685
2022-02-26 10:59:48 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.0109
2022-02-26 11:00:21 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.8154
2022-02-26 11:00:55 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.9080
2022-02-26 11:01:29 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.1901
2022-02-26 11:02:04 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.2494
2022-02-26 11:02:37 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.0585
2022-02-26 11:03:13 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.2351
2022-02-26 11:03:46 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.9624
2022-02-26 11:04:21 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.2043
2022-02-26 11:04:55 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.2023
2022-02-26 11:05:29 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.9228
2022-02-26 11:06:02 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.1514
2022-02-26 11:06:37 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.0835
2022-02-26 11:07:11 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.9679
2022-02-26 11:07:46 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.9331
2022-02-26 11:08:19 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.0711
2022-02-26 11:08:53 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.2689
2022-02-26 11:09:28 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.9871
2022-02-26 11:10:02 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.1971
2022-02-26 11:10:37 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7343
2022-02-26 11:11:11 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.1575
2022-02-26 11:11:47 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.8902
2022-02-26 11:12:20 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.8676
2022-02-26 11:12:55 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.8650
2022-02-26 11:13:29 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.8325
2022-02-26 11:14:04 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.2331
2022-02-26 11:14:39 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.1552
2022-02-26 11:15:13 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.1756
2022-02-26 11:15:49 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.0878
2022-02-26 11:16:23 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.8594
2022-02-26 11:16:58 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.9137
2022-02-26 11:17:32 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.2296
2022-02-26 11:18:08 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.9605
2022-02-26 11:18:42 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.8605
2022-02-26 11:19:16 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.9100
2022-02-26 11:19:51 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.0828
2022-02-26 11:20:26 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.0999
2022-02-26 11:21:01 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.0374
2022-02-26 11:21:36 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.2008
2022-02-26 11:22:11 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.0483
2022-02-26 11:22:46 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.1224
2022-02-26 11:23:21 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.8377
2022-02-26 11:23:22 - train: epoch 006, train_loss: 3.0420
2022-02-26 11:24:40 - eval: epoch: 006, acc1: 39.398%, acc5: 65.396%, test_loss: 2.7619, per_image_load_time: 2.710ms, per_image_inference_time: 0.318ms
2022-02-26 11:24:40 - until epoch: 006, best_acc1: 39.398%
2022-02-26 11:24:40 - epoch 007 lr: 0.1
2022-02-26 11:25:19 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.8613
2022-02-26 11:25:54 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.2940
2022-02-26 11:26:28 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.2789
2022-02-26 11:27:02 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.9020
2022-02-26 11:27:35 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.9016
2022-02-26 11:28:09 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.1577
2022-02-26 11:28:43 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.9971
2022-02-26 11:29:17 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.0083
2022-02-26 11:29:52 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.0561
2022-02-26 11:30:25 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.0967
2022-02-26 11:30:59 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.8543
2022-02-26 11:31:35 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.8867
2022-02-26 11:32:10 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.8979
2022-02-26 11:32:43 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.1323
2022-02-26 11:33:18 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.0702
2022-02-26 11:33:53 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.9256
2022-02-26 11:34:27 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.0806
2022-02-26 11:35:01 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.8432
2022-02-26 11:35:36 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 3.0291
2022-02-26 11:36:11 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.7020
2022-02-26 11:36:45 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.1252
2022-02-26 11:37:20 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.8587
2022-02-26 11:37:55 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.1245
2022-02-26 11:38:30 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.9178
2022-02-26 11:39:03 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.8864
2022-02-26 11:39:38 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.8967
2022-02-26 11:40:13 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.8479
2022-02-26 11:40:49 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.9479
2022-02-26 11:41:22 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.9165
2022-02-26 11:41:57 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.1187
2022-02-26 11:42:32 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.8170
2022-02-26 11:43:06 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.8189
2022-02-26 11:43:41 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.2604
2022-02-26 11:44:16 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.7819
2022-02-26 11:44:50 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.0376
2022-02-26 11:45:26 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.8654
2022-02-26 11:46:00 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.1345
2022-02-26 11:46:35 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.2605
2022-02-26 11:47:09 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.8838
2022-02-26 11:47:43 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.0549
2022-02-26 11:48:18 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.8817
2022-02-26 11:48:53 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.7960
2022-02-26 11:49:28 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.0569
2022-02-26 11:50:03 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.7509
2022-02-26 11:50:37 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.0510
2022-02-26 11:51:13 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.8964
2022-02-26 11:51:48 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.1775
2022-02-26 11:52:24 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.1942
2022-02-26 11:52:59 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.8698
2022-02-26 11:53:33 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.9731
2022-02-26 11:53:34 - train: epoch 007, train_loss: 2.9716
2022-02-26 11:54:50 - eval: epoch: 007, acc1: 40.874%, acc5: 67.174%, test_loss: 2.6630, per_image_load_time: 2.533ms, per_image_inference_time: 0.361ms
2022-02-26 11:54:50 - until epoch: 007, best_acc1: 40.874%
2022-02-26 11:54:50 - epoch 008 lr: 0.1
2022-02-26 11:55:29 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.8158
2022-02-26 11:56:05 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.0526
2022-02-26 11:56:38 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.7784
2022-02-26 11:57:12 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.7700
2022-02-26 11:57:46 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.8141
2022-02-26 11:58:20 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.8171
2022-02-26 11:58:54 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.2128
2022-02-26 11:59:28 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.9450
2022-02-26 12:00:02 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.8710
2022-02-26 12:00:36 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.9442
2022-02-26 12:01:10 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.6525
2022-02-26 12:01:44 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.6304
2022-02-26 12:02:19 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.9758
2022-02-26 12:02:53 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.8429
2022-02-26 12:03:26 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.0489
2022-02-26 12:04:02 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.9513
2022-02-26 12:04:36 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.9030
2022-02-26 12:05:10 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.9941
2022-02-26 12:05:45 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.7518
2022-02-26 12:06:19 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.9896
2022-02-26 12:06:54 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.9828
2022-02-26 12:07:28 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.8957
2022-02-26 12:08:03 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.9340
2022-02-26 12:08:37 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.8640
2022-02-26 12:09:11 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.8811
2022-02-26 12:09:45 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.9697
2022-02-26 12:10:19 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.1238
2022-02-26 12:10:54 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.0852
2022-02-26 12:11:28 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.9391
2022-02-26 12:12:02 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.0571
2022-02-26 12:12:36 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.8584
2022-02-26 12:13:12 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.2263
2022-02-26 12:13:47 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.1261
2022-02-26 12:14:21 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.1424
2022-02-26 12:14:56 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.9824
2022-02-26 12:15:30 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.0734
2022-02-26 12:16:04 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.8282
2022-02-26 12:16:39 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.8805
2022-02-26 12:17:14 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.9872
2022-02-26 12:17:48 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.1694
2022-02-26 12:18:23 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.8787
2022-02-26 12:18:58 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.7590
2022-02-26 12:19:33 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.6251
2022-02-26 12:20:07 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.9166
2022-02-26 12:20:42 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.0192
2022-02-26 12:21:16 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.0493
2022-02-26 12:21:52 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.8640
2022-02-26 12:22:27 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.8721
2022-02-26 12:23:02 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.9706
2022-02-26 12:23:36 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.7139
2022-02-26 12:23:37 - train: epoch 008, train_loss: 2.9209
2022-02-26 12:24:54 - eval: epoch: 008, acc1: 41.608%, acc5: 68.020%, test_loss: 2.6180, per_image_load_time: 2.690ms, per_image_inference_time: 0.298ms
2022-02-26 12:24:54 - until epoch: 008, best_acc1: 41.608%
2022-02-26 12:24:54 - epoch 009 lr: 0.1
2022-02-26 12:25:33 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.5910
2022-02-26 12:26:09 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.8345
2022-02-26 12:26:42 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.6028
2022-02-26 12:27:16 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.0502
2022-02-26 12:27:50 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.7663
2022-02-26 12:28:24 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.9441
2022-02-26 12:28:58 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.8649
2022-02-26 12:29:32 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.7573
2022-02-26 12:30:07 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.8894
2022-02-26 12:30:41 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.7055
2022-02-26 12:31:16 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.2133
2022-02-26 12:31:50 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.0499
2022-02-26 12:32:23 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.1452
2022-02-26 12:32:57 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.6116
2022-02-26 12:33:32 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.7756
2022-02-26 12:34:07 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.9188
2022-02-26 12:34:41 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.9819
2022-02-26 12:35:15 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.8864
2022-02-26 12:35:50 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.7151
2022-02-26 12:36:24 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.6746
2022-02-26 12:36:58 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.9320
2022-02-26 12:37:32 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.0013
2022-02-26 12:38:07 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.6419
2022-02-26 12:38:42 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.8118
2022-02-26 12:39:17 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.6338
2022-02-26 12:39:51 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.9993
2022-02-26 12:40:25 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.8016
2022-02-26 12:41:00 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.9749
2022-02-26 12:41:34 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.5133
2022-02-26 12:42:09 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.8287
2022-02-26 12:42:43 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.9900
2022-02-26 12:43:18 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.9823
2022-02-26 12:43:52 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.9757
2022-02-26 12:44:27 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.0966
2022-02-26 12:45:01 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.0149
2022-02-26 12:45:36 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.7614
2022-02-26 12:46:11 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.0316
2022-02-26 12:46:44 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.0059
2022-02-26 12:47:19 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.5925
2022-02-26 12:47:52 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.0422
2022-02-26 12:48:28 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.9016
2022-02-26 12:49:02 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.6889
2022-02-26 12:49:37 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.9353
2022-02-26 12:50:12 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.8605
2022-02-26 12:50:46 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.9781
2022-02-26 12:51:22 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.9241
2022-02-26 12:51:56 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.8499
2022-02-26 12:52:32 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.0833
2022-02-26 12:53:07 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.8681
2022-02-26 12:53:41 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.8034
2022-02-26 12:53:42 - train: epoch 009, train_loss: 2.8808
2022-02-26 12:55:00 - eval: epoch: 009, acc1: 41.880%, acc5: 68.158%, test_loss: 2.6131, per_image_load_time: 2.715ms, per_image_inference_time: 0.314ms
2022-02-26 12:55:00 - until epoch: 009, best_acc1: 41.880%
2022-02-26 12:55:00 - epoch 010 lr: 0.1
2022-02-26 12:55:39 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.8578
2022-02-26 12:56:14 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.0430
2022-02-26 12:56:48 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.0080
2022-02-26 12:57:22 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.9682
2022-02-26 12:57:56 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.7881
2022-02-26 12:58:30 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.8911
2022-02-26 12:59:04 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.8437
2022-02-26 12:59:38 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.8070
2022-02-26 13:00:12 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.6784
2022-02-26 13:00:48 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.6966
2022-02-26 13:01:21 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.9783
2022-02-26 13:01:55 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.5925
2022-02-26 13:02:31 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.7695
2022-02-26 13:03:05 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.9111
2022-02-26 13:03:40 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.5011
2022-02-26 13:04:14 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.8861
2022-02-26 13:04:48 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.9917
2022-02-26 13:05:22 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.8108
2022-02-26 13:05:57 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.8237
2022-02-26 13:06:32 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.9264
2022-02-26 13:07:06 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.7824
2022-02-26 13:07:41 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.9811
2022-02-26 13:08:16 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 3.0348
2022-02-26 13:08:49 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.0230
2022-02-26 13:09:24 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.8288
2022-02-26 13:09:59 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.9185
2022-02-26 13:10:33 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.7185
2022-02-26 13:11:08 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.9064
2022-02-26 13:11:42 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.8387
2022-02-26 13:12:17 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.8073
2022-02-26 13:12:51 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.9940
2022-02-26 13:13:26 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.8339
2022-02-26 13:14:01 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.9626
2022-02-26 13:14:34 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.0311
2022-02-26 13:15:08 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9953
2022-02-26 13:15:42 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.1425
2022-02-26 13:16:17 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.7547
2022-02-26 13:16:52 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.9590
2022-02-26 13:17:26 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.5015
2022-02-26 13:18:01 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.7455
2022-02-26 13:18:35 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.8361
2022-02-26 13:19:10 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.9235
2022-02-26 13:19:44 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.7743
2022-02-26 13:20:19 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.9263
2022-02-26 13:20:54 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.8553
2022-02-26 13:21:27 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.8761
2022-02-26 13:22:04 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.8273
2022-02-26 13:22:38 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.8219
2022-02-26 13:23:14 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.6053
2022-02-26 13:23:48 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.6293
2022-02-26 13:23:50 - train: epoch 010, train_loss: 2.8456
2022-02-26 13:25:08 - eval: epoch: 010, acc1: 42.956%, acc5: 69.092%, test_loss: 2.5441, per_image_load_time: 2.724ms, per_image_inference_time: 0.306ms
2022-02-26 13:25:08 - until epoch: 010, best_acc1: 42.956%
2022-02-26 13:25:08 - epoch 011 lr: 0.1
2022-02-26 13:25:47 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.7452
2022-02-26 13:26:21 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.8717
2022-02-26 13:26:56 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.6517
2022-02-26 13:27:30 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.9394
2022-02-26 13:28:05 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.7115
2022-02-26 13:28:38 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.8130
2022-02-26 13:29:13 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.7094
2022-02-26 13:29:48 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.7932
2022-02-26 13:30:21 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.9547
2022-02-26 13:30:55 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.7702
2022-02-26 13:31:30 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.8443
2022-02-26 13:32:04 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.0714
2022-02-26 13:32:38 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.9503
2022-02-26 13:33:11 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.7476
2022-02-26 13:33:46 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.6831
2022-02-26 13:34:19 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.7470
2022-02-26 13:34:54 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.9059
2022-02-26 13:35:28 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.6965
2022-02-26 13:36:03 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.7175
2022-02-26 13:36:37 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.9175
2022-02-26 13:37:11 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.7716
2022-02-26 13:37:47 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.8149
2022-02-26 13:38:21 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 3.0440
2022-02-26 13:38:55 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.6439
2022-02-26 13:39:30 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.0609
2022-02-26 13:40:04 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.7462
2022-02-26 13:40:39 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.7612
2022-02-26 13:41:12 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.5120
2022-02-26 13:41:48 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.8729
2022-02-26 13:42:22 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.1016
2022-02-26 13:42:56 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.9104
2022-02-26 13:43:30 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.6851
2022-02-26 13:44:05 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.9682
2022-02-26 13:44:39 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.7902
2022-02-26 13:45:14 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.8383
2022-02-26 13:45:48 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.9697
2022-02-26 13:46:23 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.0221
2022-02-26 13:46:58 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.4335
2022-02-26 13:47:32 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.8160
2022-02-26 13:48:06 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.8443
2022-02-26 13:48:41 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.6279
2022-02-26 13:49:15 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.6899
2022-02-26 13:49:50 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.8077
2022-02-26 13:50:25 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.8318
2022-02-26 13:51:00 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.6763
2022-02-26 13:51:34 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.7488
2022-02-26 13:52:11 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.4438
2022-02-26 13:52:45 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.6923
2022-02-26 13:53:21 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.6360
2022-02-26 13:53:55 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.7372
2022-02-26 13:53:56 - train: epoch 011, train_loss: 2.8177
2022-02-26 13:55:13 - eval: epoch: 011, acc1: 42.712%, acc5: 68.866%, test_loss: 2.5641, per_image_load_time: 2.688ms, per_image_inference_time: 0.300ms
2022-02-26 13:55:14 - until epoch: 011, best_acc1: 42.956%
2022-02-26 13:55:14 - epoch 012 lr: 0.1
2022-02-26 13:55:52 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.6567
2022-02-26 13:56:26 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.7423
2022-02-26 13:57:01 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.6863
2022-02-26 13:57:34 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.8072
2022-02-26 13:58:09 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.0458
2022-02-26 13:58:43 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.6068
2022-02-26 13:59:18 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.6433
2022-02-26 13:59:52 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.8870
2022-02-26 14:00:26 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.9715
2022-02-26 14:01:00 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.5651
2022-02-26 14:01:33 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.1906
2022-02-26 14:02:08 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.6346
2022-02-26 14:02:42 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.6703
2022-02-26 14:03:17 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.7428
2022-02-26 14:03:51 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.6248
2022-02-26 14:04:26 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.8151
2022-02-26 14:04:59 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.7078
2022-02-26 14:05:34 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.7050
2022-02-26 14:06:08 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.8650
2022-02-26 14:06:42 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.8592
2022-02-26 14:07:17 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.8513
2022-02-26 14:07:51 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 3.0258
2022-02-26 14:08:26 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.8091
2022-02-26 14:09:00 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.7985
2022-02-26 14:09:35 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.5148
2022-02-26 14:10:09 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.6572
2022-02-26 14:10:43 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.7594
2022-02-26 14:11:18 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.7020
2022-02-26 14:11:52 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.6174
2022-02-26 14:12:25 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.6641
2022-02-26 14:13:00 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.8949
2022-02-26 14:13:34 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.4674
2022-02-26 14:14:09 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.6387
2022-02-26 14:14:43 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.6818
2022-02-26 14:15:18 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.9524
2022-02-26 14:15:54 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.8946
2022-02-26 14:16:27 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.6698
2022-02-26 14:17:02 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.7053
2022-02-26 14:17:37 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.7612
2022-02-26 14:18:11 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.7189
2022-02-26 14:18:46 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.7227
2022-02-26 14:19:20 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.6779
2022-02-26 14:19:55 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.9011
2022-02-26 14:20:30 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.6743
2022-02-26 14:21:05 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.8392
2022-02-26 14:21:39 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.0747
2022-02-26 14:22:15 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.7065
2022-02-26 14:22:50 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.7860
2022-02-26 14:23:26 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.8112
2022-02-26 14:24:00 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.3892
2022-02-26 14:24:01 - train: epoch 012, train_loss: 2.7951
2022-02-26 14:25:19 - eval: epoch: 012, acc1: 43.692%, acc5: 69.904%, test_loss: 2.5070, per_image_load_time: 2.675ms, per_image_inference_time: 0.308ms
2022-02-26 14:25:19 - until epoch: 012, best_acc1: 43.692%
2022-02-26 14:25:19 - epoch 013 lr: 0.1
2022-02-26 14:25:58 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.5950
2022-02-26 14:26:33 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.7901
2022-02-26 14:27:07 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.7034
2022-02-26 14:27:42 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.6766
2022-02-26 14:28:15 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.6709
2022-02-26 14:28:50 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 3.0482
2022-02-26 14:29:23 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.5887
2022-02-26 14:29:58 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.8898
2022-02-26 14:30:33 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.7293
2022-02-26 14:31:07 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.8750
2022-02-26 14:31:41 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.8328
2022-02-26 14:32:16 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.8451
2022-02-26 14:32:50 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.8754
2022-02-26 14:33:25 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.7267
2022-02-26 14:33:59 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.8325
2022-02-26 14:34:33 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.4832
2022-02-26 14:35:08 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.9286
2022-02-26 14:35:43 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.7601
2022-02-26 14:36:17 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.7594
2022-02-26 14:36:51 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.0198
2022-02-26 14:37:24 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.0190
2022-02-26 14:37:58 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.6780
2022-02-26 14:38:31 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.6443
2022-02-26 14:39:05 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.8192
2022-02-26 14:39:38 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.6701
2022-02-26 14:40:12 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.8436
2022-02-26 14:40:45 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.7100
2022-02-26 14:41:19 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.8391
2022-02-26 14:41:52 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.8033
2022-02-26 14:42:27 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.7763
2022-02-26 14:43:00 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.6217
2022-02-26 14:43:35 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.6987
2022-02-26 14:44:08 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.5489
2022-02-26 14:44:42 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.7822
2022-02-26 14:45:16 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.6579
2022-02-26 14:45:51 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.0040
2022-02-26 14:46:24 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.5646
2022-02-26 14:46:57 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.8700
2022-02-26 14:47:31 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.7668
2022-02-26 14:48:06 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.8156
2022-02-26 14:48:39 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.7004
2022-02-26 14:49:15 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.8111
2022-02-26 14:49:48 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.7270
2022-02-26 14:50:22 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.7208
2022-02-26 14:50:57 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.7911
2022-02-26 14:51:31 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.7260
2022-02-26 14:52:04 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.9131
2022-02-26 14:52:40 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.8504
2022-02-26 14:53:12 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.8513
2022-02-26 14:53:46 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.8450
2022-02-26 14:53:47 - train: epoch 013, train_loss: 2.7754
2022-02-26 14:55:05 - eval: epoch: 013, acc1: 45.038%, acc5: 71.364%, test_loss: 2.4278, per_image_load_time: 2.704ms, per_image_inference_time: 0.305ms
2022-02-26 14:55:05 - until epoch: 013, best_acc1: 45.038%
2022-02-26 14:55:05 - epoch 014 lr: 0.1
2022-02-26 14:55:44 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.5582
2022-02-26 14:56:18 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.8639
2022-02-26 14:56:52 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.5334
2022-02-26 14:57:26 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.5790
2022-02-26 14:57:59 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.6768
2022-02-26 14:58:32 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.8679
2022-02-26 14:59:07 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.7031
2022-02-26 14:59:39 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.7262
2022-02-26 15:00:13 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.8455
2022-02-26 15:00:46 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.8784
2022-02-26 15:01:21 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.7438
2022-02-26 15:01:54 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.7810
2022-02-26 15:02:28 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.7110
2022-02-26 15:03:01 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.9358
2022-02-26 15:03:34 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.9201
2022-02-26 15:04:08 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.6226
2022-02-26 15:04:43 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.9333
2022-02-26 15:05:17 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.8995
2022-02-26 15:05:50 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.5164
2022-02-26 15:06:21 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.7488
2022-02-26 15:06:56 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.8894
2022-02-26 15:07:30 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.7635
2022-02-26 15:08:03 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.7446
2022-02-26 15:08:38 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 3.0420
2022-02-26 15:09:11 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.8176
2022-02-26 15:09:46 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.7335
2022-02-26 15:10:19 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.7273
2022-02-26 15:10:54 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9842
2022-02-26 15:11:28 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.6700
2022-02-26 15:12:01 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.8780
2022-02-26 15:12:34 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.6042
2022-02-26 15:13:08 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.7246
2022-02-26 15:13:42 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.5321
2022-02-26 15:14:17 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.6668
2022-02-26 15:14:50 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.7752
2022-02-26 15:15:24 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.6422
2022-02-26 15:15:58 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.6338
2022-02-26 15:16:33 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.9569
2022-02-26 15:17:06 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.6151
2022-02-26 15:17:41 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.9193
2022-02-26 15:18:14 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.7919
2022-02-26 15:18:48 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.6093
2022-02-26 15:19:21 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.6897
2022-02-26 15:19:56 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.5743
2022-02-26 15:20:29 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.7903
2022-02-26 15:21:04 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.6738
2022-02-26 15:21:38 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.6693
2022-02-26 15:22:13 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.7438
2022-02-26 15:22:48 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.6538
2022-02-26 15:23:21 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.8080
2022-02-26 15:23:22 - train: epoch 014, train_loss: 2.7572
2022-02-26 15:24:39 - eval: epoch: 014, acc1: 44.046%, acc5: 70.240%, test_loss: 2.4873, per_image_load_time: 2.666ms, per_image_inference_time: 0.296ms
2022-02-26 15:24:39 - until epoch: 014, best_acc1: 45.038%
2022-02-26 15:24:39 - epoch 015 lr: 0.1
2022-02-26 15:25:18 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.5591
2022-02-26 15:25:52 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.9916
2022-02-26 15:26:25 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 3.0297
2022-02-26 15:26:59 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.7667
2022-02-26 15:27:33 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.6515
2022-02-26 15:28:07 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.9023
2022-02-26 15:28:41 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.7104
2022-02-26 15:29:14 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.5892
2022-02-26 15:29:48 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.5033
2022-02-26 15:30:22 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.8426
2022-02-26 15:30:56 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.6772
2022-02-26 15:31:29 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.7584
2022-02-26 15:32:01 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.9449
2022-02-26 15:32:36 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.5876
2022-02-26 15:33:10 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.4416
2022-02-26 15:33:43 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.7681
2022-02-26 15:34:17 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.9827
2022-02-26 15:34:51 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.5759
2022-02-26 15:35:25 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.7072
2022-02-26 15:36:00 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.5953
2022-02-26 15:36:33 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.5841
2022-02-26 15:37:07 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.7893
2022-02-26 15:37:41 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.5698
2022-02-26 15:38:14 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.7509
2022-02-26 15:38:47 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.9057
2022-02-26 15:39:21 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.5209
2022-02-26 15:39:55 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.7293
2022-02-26 15:40:30 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.7654
2022-02-26 15:41:04 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.7459
2022-02-26 15:41:39 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.5182
2022-02-26 15:42:13 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.6404
2022-02-26 15:42:48 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.7566
2022-02-26 15:43:21 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.7205
2022-02-26 15:43:55 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.8009
2022-02-26 15:44:28 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.8903
2022-02-26 15:45:01 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.7420
2022-02-26 15:45:36 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.6792
2022-02-26 15:46:10 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.6231
2022-02-26 15:46:44 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.9929
2022-02-26 15:47:18 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.7933
2022-02-26 15:47:53 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.9970
2022-02-26 15:48:26 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.4538
2022-02-26 15:49:00 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.7206
2022-02-26 15:49:35 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.7076
2022-02-26 15:50:08 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.7677
2022-02-26 15:50:43 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.7115
2022-02-26 15:51:15 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.7815
2022-02-26 15:51:50 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.7548
2022-02-26 15:52:24 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.8390
2022-02-26 15:52:58 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.9280
2022-02-26 15:52:59 - train: epoch 015, train_loss: 2.7418
2022-02-26 15:54:16 - eval: epoch: 015, acc1: 45.552%, acc5: 71.244%, test_loss: 2.4114, per_image_load_time: 2.711ms, per_image_inference_time: 0.291ms
2022-02-26 15:54:16 - until epoch: 015, best_acc1: 45.552%
2022-02-26 15:54:16 - epoch 016 lr: 0.1
2022-02-26 15:54:57 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.5883
2022-02-26 15:55:32 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.4964
2022-02-26 15:56:05 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.6692
2022-02-26 15:56:40 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.9572
2022-02-26 15:57:13 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.5303
2022-02-26 15:57:47 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.9135
2022-02-26 15:58:21 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.4667
2022-02-26 15:58:56 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.6868
2022-02-26 15:59:30 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.8591
2022-02-26 16:00:05 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.5822
2022-02-26 16:00:39 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.5568
2022-02-26 16:01:14 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.6056
2022-02-26 16:01:48 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.8246
2022-02-26 16:02:23 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.6086
2022-02-26 16:02:58 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.8517
2022-02-26 16:03:32 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.8768
2022-02-26 16:04:06 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.7023
2022-02-26 16:04:40 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.6792
2022-02-26 16:05:15 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.6775
2022-02-26 16:05:49 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.3083
2022-02-26 16:06:24 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.8204
2022-02-26 16:06:59 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.7917
2022-02-26 16:07:33 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.8735
2022-02-26 16:08:08 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.9094
2022-02-26 16:08:43 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.5756
2022-02-26 16:09:18 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.8744
2022-02-26 16:09:52 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.6834
2022-02-26 16:10:26 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.5576
2022-02-26 16:11:00 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.7406
2022-02-26 16:11:34 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 3.1290
2022-02-26 16:12:09 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.8505
2022-02-26 16:12:43 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.8784
2022-02-26 16:13:18 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.9603
2022-02-26 16:13:52 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.6205
2022-02-26 16:14:27 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.6693
2022-02-26 16:15:02 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.5400
2022-02-26 16:15:36 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.8274
2022-02-26 16:16:11 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.0116
2022-02-26 16:16:45 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.8100
2022-02-26 16:17:19 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.7527
2022-02-26 16:17:52 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.8346
2022-02-26 16:18:27 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.7686
2022-02-26 16:19:01 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.5779
2022-02-26 16:19:37 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.5767
2022-02-26 16:20:11 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.8454
2022-02-26 16:20:47 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.6061
2022-02-26 16:21:21 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.9653
2022-02-26 16:21:56 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.6720
2022-02-26 16:22:30 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.7878
2022-02-26 16:23:03 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.8173
2022-02-26 16:23:04 - train: epoch 016, train_loss: 2.7259
2022-02-26 16:24:22 - eval: epoch: 016, acc1: 44.664%, acc5: 71.132%, test_loss: 2.4314, per_image_load_time: 1.641ms, per_image_inference_time: 0.304ms
2022-02-26 16:24:22 - until epoch: 016, best_acc1: 45.552%
2022-02-26 16:24:22 - epoch 017 lr: 0.1
2022-02-26 16:25:01 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.6017
2022-02-26 16:25:37 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.7933
2022-02-26 16:26:11 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.8907
2022-02-26 16:26:45 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.4622
2022-02-26 16:27:21 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.6786
2022-02-26 16:27:55 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.1236
2022-02-26 16:28:30 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.6953
2022-02-26 16:29:04 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.6205
2022-02-26 16:29:38 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.6603
2022-02-26 16:30:11 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.6320
2022-02-26 16:30:46 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.0382
2022-02-26 16:31:20 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.9610
2022-02-26 16:31:54 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.7242
2022-02-26 16:32:30 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.7052
2022-02-26 16:33:04 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.4491
2022-02-26 16:33:39 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.5338
2022-02-26 16:34:14 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.6773
2022-02-26 16:34:48 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.9148
2022-02-26 16:35:22 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.5529
2022-02-26 16:35:57 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.8087
2022-02-26 16:36:31 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.6709
2022-02-26 16:37:04 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.5935
2022-02-26 16:37:39 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.8108
2022-02-26 16:38:14 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.6362
2022-02-26 16:38:49 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.9450
2022-02-26 16:39:24 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.6285
2022-02-26 16:39:58 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.7036
2022-02-26 16:40:32 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.9218
2022-02-26 16:41:06 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.9575
2022-02-26 16:41:40 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.4934
2022-02-26 16:42:15 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.0062
2022-02-26 16:42:47 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.6259
2022-02-26 16:43:21 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.7795
2022-02-26 16:43:54 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.5743
2022-02-26 16:44:29 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.7624
2022-02-26 16:45:03 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.8927
2022-02-26 16:45:37 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.7069
2022-02-26 16:46:10 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 3.0031
2022-02-26 16:46:44 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.7071
2022-02-26 16:47:19 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.4627
2022-02-26 16:47:52 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.7370
2022-02-26 16:48:27 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.6555
2022-02-26 16:49:00 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.6480
2022-02-26 16:49:33 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.6990
2022-02-26 16:50:07 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.8839
2022-02-26 16:50:42 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.7344
2022-02-26 16:51:16 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.7293
2022-02-26 16:51:50 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.8072
2022-02-26 16:52:25 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.4489
2022-02-26 16:52:58 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.6033
2022-02-26 16:52:59 - train: epoch 017, train_loss: 2.7130
2022-02-26 16:54:16 - eval: epoch: 017, acc1: 45.714%, acc5: 71.978%, test_loss: 2.3897, per_image_load_time: 2.674ms, per_image_inference_time: 0.333ms
2022-02-26 16:54:16 - until epoch: 017, best_acc1: 45.714%
2022-02-26 16:54:16 - epoch 018 lr: 0.1
2022-02-26 16:54:56 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.6234
2022-02-26 16:55:30 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.7709
2022-02-26 16:56:05 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.9316
2022-02-26 16:56:37 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.7502
2022-02-26 16:57:11 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.7330
2022-02-26 16:57:45 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.9529
2022-02-26 16:58:19 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.5156
2022-02-26 16:58:52 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.6937
2022-02-26 16:59:26 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.6778
2022-02-26 17:00:00 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.5252
2022-02-26 17:00:34 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.0988
2022-02-26 17:01:09 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.6434
2022-02-26 17:01:44 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.9254
2022-02-26 17:02:19 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.7398
2022-02-26 17:02:53 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.9737
2022-02-26 17:03:25 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.6721
2022-02-26 17:03:59 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.4304
2022-02-26 17:04:35 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.4866
2022-02-26 17:05:09 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.7638
2022-02-26 17:05:42 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.0879
2022-02-26 17:06:19 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.9679
2022-02-26 17:06:54 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.7640
2022-02-26 17:07:29 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.7524
2022-02-26 17:08:03 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.5834
2022-02-26 17:08:36 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.4319
2022-02-26 17:09:08 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.5730
2022-02-26 17:09:42 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.8760
2022-02-26 17:10:16 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.5037
2022-02-26 17:10:50 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.7458
2022-02-26 17:11:24 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.6011
2022-02-26 17:11:58 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.1114
2022-02-26 17:12:32 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.5185
2022-02-26 17:13:07 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.6435
2022-02-26 17:13:40 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.6363
2022-02-26 17:14:14 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.8527
2022-02-26 17:14:48 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.8017
2022-02-26 17:15:21 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.1806
2022-02-26 17:15:56 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.7668
2022-02-26 17:16:29 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.7725
2022-02-26 17:17:04 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.6273
2022-02-26 17:17:37 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.6199
2022-02-26 17:18:11 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.7433
2022-02-26 17:18:45 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.3947
2022-02-26 17:19:19 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.8125
2022-02-26 17:19:53 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.7568
2022-02-26 17:20:28 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.5597
2022-02-26 17:21:02 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.8023
2022-02-26 17:21:35 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.8717
2022-02-26 17:22:09 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.7315
2022-02-26 17:22:43 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.8998
2022-02-26 17:22:45 - train: epoch 018, train_loss: 2.6993
2022-02-26 17:24:01 - eval: epoch: 018, acc1: 45.506%, acc5: 71.866%, test_loss: 2.4041, per_image_load_time: 2.586ms, per_image_inference_time: 0.347ms
2022-02-26 17:24:01 - until epoch: 018, best_acc1: 45.714%
2022-02-26 17:24:01 - epoch 019 lr: 0.1
2022-02-26 17:24:41 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.4891
2022-02-26 17:25:13 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.8725
2022-02-26 17:25:47 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.9787
2022-02-26 17:26:20 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.5867
2022-02-26 17:26:54 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.6277
2022-02-26 17:27:27 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.6088
2022-02-26 17:28:01 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.4591
2022-02-26 17:28:34 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.8906
2022-02-26 17:29:07 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.7514
2022-02-26 17:29:41 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.8479
2022-02-26 17:30:15 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.4340
2022-02-26 17:30:49 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.7530
2022-02-26 17:31:23 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.8260
2022-02-26 17:31:56 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.6741
2022-02-26 17:32:32 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.1094
2022-02-26 17:33:04 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.6294
2022-02-26 17:33:39 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.7307
2022-02-26 17:34:12 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.5800
2022-02-26 17:34:45 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.8637
2022-02-26 17:35:19 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.5912
2022-02-26 17:35:53 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.7297
2022-02-26 17:36:26 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.6229
2022-02-26 17:37:01 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.7115
2022-02-26 17:37:34 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.8965
2022-02-26 17:38:08 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.6456
2022-02-26 17:38:42 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.7111
2022-02-26 17:39:15 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.9317
2022-02-26 17:39:50 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.6261
2022-02-26 17:40:24 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.6950
2022-02-26 17:40:56 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.9735
2022-02-26 17:41:30 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.7048
2022-02-26 17:42:03 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.4172
2022-02-26 17:42:38 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.6007
2022-02-26 17:43:11 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.6973
2022-02-26 17:43:45 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.7092
2022-02-26 17:44:19 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.6242
2022-02-26 17:44:53 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.7687
2022-02-26 17:45:26 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.7583
2022-02-26 17:46:00 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.5806
2022-02-26 17:46:34 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.6095
2022-02-26 17:47:07 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.8182
2022-02-26 17:47:40 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.6775
2022-02-26 17:48:15 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.6053
2022-02-26 17:48:49 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.7653
2022-02-26 17:49:22 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.0414
2022-02-26 17:49:56 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.5954
2022-02-26 17:50:31 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.5976
2022-02-26 17:51:05 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.7547
2022-02-26 17:51:39 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.5887
2022-02-26 17:52:12 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.6088
2022-02-26 17:52:13 - train: epoch 019, train_loss: 2.6934
2022-02-26 17:53:30 - eval: epoch: 019, acc1: 45.404%, acc5: 71.496%, test_loss: 2.4042, per_image_load_time: 2.630ms, per_image_inference_time: 0.340ms
2022-02-26 17:53:30 - until epoch: 019, best_acc1: 45.714%
2022-02-26 17:53:30 - epoch 020 lr: 0.1
2022-02-26 17:54:08 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.8452
2022-02-26 17:54:42 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.4714
2022-02-26 17:55:16 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.8774
2022-02-26 17:55:49 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.3834
2022-02-26 17:56:24 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.6356
2022-02-26 17:56:57 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.9131
2022-02-26 17:57:32 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.5952
2022-02-26 17:58:05 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.7742
2022-02-26 17:58:39 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.0956
2022-02-26 17:59:12 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.6507
2022-02-26 17:59:47 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.6534
2022-02-26 18:00:19 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.5300
2022-02-26 18:00:52 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.5877
2022-02-26 18:01:26 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.7737
2022-02-26 18:02:00 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.7265
2022-02-26 18:02:34 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.5007
2022-02-26 18:03:08 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.4321
2022-02-26 18:03:42 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.7453
2022-02-26 18:04:16 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.5125
2022-02-26 18:04:50 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.6945
2022-02-26 18:05:23 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.9189
2022-02-26 18:05:57 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.6097
2022-02-26 18:06:30 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.6431
2022-02-26 18:07:04 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.8392
2022-02-26 18:07:37 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.6537
2022-02-26 18:08:11 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.4400
2022-02-26 18:08:45 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.6572
2022-02-26 18:09:19 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.7527
2022-02-26 18:09:53 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.8194
2022-02-26 18:10:27 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.5274
2022-02-26 18:11:01 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.7493
2022-02-26 18:11:35 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.9222
2022-02-26 18:12:09 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.4875
2022-02-26 18:12:44 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.6745
2022-02-26 18:13:16 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.6277
2022-02-26 18:13:50 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.6169
2022-02-26 18:14:24 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.5144
2022-02-26 18:14:59 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.5554
2022-02-26 18:15:32 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.6765
2022-02-26 18:16:06 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.5820
2022-02-26 18:16:40 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.5768
2022-02-26 18:17:15 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.5596
2022-02-26 18:17:48 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.6971
2022-02-26 18:18:23 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.6684
2022-02-26 18:18:56 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.6747
2022-02-26 18:19:29 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.7187
2022-02-26 18:20:03 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.4787
2022-02-26 18:20:36 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.7103
2022-02-26 18:21:12 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.7718
2022-02-26 18:21:46 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.6488
2022-02-26 18:21:47 - train: epoch 020, train_loss: 2.6796
2022-02-26 18:23:03 - eval: epoch: 020, acc1: 46.418%, acc5: 72.388%, test_loss: 2.3515, per_image_load_time: 2.130ms, per_image_inference_time: 0.343ms
2022-02-26 18:23:03 - until epoch: 020, best_acc1: 46.418%
2022-02-26 18:23:03 - epoch 021 lr: 0.1
2022-02-26 18:23:42 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.4909
2022-02-26 18:24:15 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.7159
2022-02-26 18:24:49 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.3600
2022-02-26 18:25:24 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.6728
2022-02-26 18:25:56 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.5184
2022-02-26 18:26:29 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.5512
2022-02-26 18:27:02 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.5280
2022-02-26 18:27:36 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.7856
2022-02-26 18:28:09 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.7821
2022-02-26 18:28:43 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.5129
2022-02-26 18:29:17 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.5138
2022-02-26 18:29:51 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.6471
2022-02-26 18:30:25 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.6197
2022-02-26 18:30:59 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.6670
2022-02-26 18:31:33 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.4803
2022-02-26 18:32:06 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.6291
2022-02-26 18:32:39 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.6197
2022-02-26 18:33:13 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.5154
2022-02-26 18:33:47 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.7243
2022-02-26 18:34:20 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.7839
2022-02-26 18:34:53 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.4487
2022-02-26 18:35:27 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.7623
2022-02-26 18:36:02 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.5255
2022-02-26 18:36:36 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.5730
2022-02-26 18:37:10 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.6467
2022-02-26 18:37:44 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.8056
2022-02-26 18:38:18 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.5333
2022-02-26 18:38:51 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.7760
2022-02-26 18:39:23 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.6029
2022-02-26 18:39:57 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.8866
2022-02-26 18:40:30 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.7054
2022-02-26 18:41:05 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.6916
2022-02-26 18:41:38 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.9114
2022-02-26 18:42:12 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.7578
2022-02-26 18:42:46 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.5225
2022-02-26 18:43:20 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.6486
2022-02-26 18:43:54 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.6766
2022-02-26 18:44:28 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.5410
2022-02-26 18:45:02 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.4683
2022-02-26 18:45:35 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.9063
2022-02-26 18:46:09 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.5293
2022-02-26 18:46:42 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.6195
2022-02-26 18:47:16 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.6661
2022-02-26 18:47:50 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.7393
2022-02-26 18:48:23 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.7628
2022-02-26 18:48:59 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.6465
2022-02-26 18:49:32 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.6895
2022-02-26 18:50:06 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.7888
2022-02-26 18:50:40 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.5332
2022-02-26 18:51:14 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.4682
2022-02-26 18:51:16 - train: epoch 021, train_loss: 2.6717
2022-02-26 18:52:30 - eval: epoch: 021, acc1: 45.844%, acc5: 71.992%, test_loss: 2.3861, per_image_load_time: 1.514ms, per_image_inference_time: 0.327ms
2022-02-26 18:52:30 - until epoch: 021, best_acc1: 46.418%
2022-02-26 18:52:30 - epoch 022 lr: 0.1
2022-02-26 18:53:09 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.4458
2022-02-26 18:53:43 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.4613
2022-02-26 18:54:17 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.2680
2022-02-26 18:54:51 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.5265
2022-02-26 18:55:25 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.6667
2022-02-26 18:55:58 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.7026
2022-02-26 18:56:33 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.7359
2022-02-26 18:57:06 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.7223
2022-02-26 18:57:41 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.7765
2022-02-26 18:58:13 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.6784
2022-02-26 18:58:47 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.7933
2022-02-26 18:59:21 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.4273
2022-02-26 18:59:55 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.7397
2022-02-26 19:00:28 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.7382
2022-02-26 19:01:03 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.6022
2022-02-26 19:01:37 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.5166
2022-02-26 19:02:11 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.4420
2022-02-26 19:02:44 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.9319
2022-02-26 19:03:18 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.5693
2022-02-26 19:03:52 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.6636
2022-02-26 19:04:26 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.6354
2022-02-26 19:04:58 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.4044
2022-02-26 19:05:32 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.7871
2022-02-26 19:06:06 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.5389
2022-02-26 19:06:40 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.6395
2022-02-26 19:07:14 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.4430
2022-02-26 19:07:49 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.4836
2022-02-26 19:08:23 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.1013
2022-02-26 19:08:56 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.4805
2022-02-26 19:09:31 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.8299
2022-02-26 19:10:05 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.7282
2022-02-26 19:10:39 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.7923
2022-02-26 19:11:12 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.6200
2022-02-26 19:11:45 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.5189
2022-02-26 19:12:19 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.8441
2022-02-26 19:12:54 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.7148
2022-02-26 19:13:27 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.7348
2022-02-26 19:14:01 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.8394
2022-02-26 19:14:36 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.6875
2022-02-26 19:15:09 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.7369
2022-02-26 19:15:44 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.5604
2022-02-26 19:16:17 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.6670
2022-02-26 19:16:51 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.8433
2022-02-26 19:17:26 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.6991
2022-02-26 19:17:59 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.6365
2022-02-26 19:18:32 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.8080
2022-02-26 19:19:06 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.7030
2022-02-26 19:19:41 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.3818
2022-02-26 19:20:16 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.4866
2022-02-26 19:20:49 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.5752
2022-02-26 19:20:51 - train: epoch 022, train_loss: 2.6662
2022-02-26 19:22:08 - eval: epoch: 022, acc1: 45.402%, acc5: 71.538%, test_loss: 2.4106, per_image_load_time: 2.648ms, per_image_inference_time: 0.333ms
2022-02-26 19:22:08 - until epoch: 022, best_acc1: 46.418%
2022-02-26 19:22:08 - epoch 023 lr: 0.1
2022-02-26 19:22:47 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.5606
2022-02-26 19:23:20 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.3256
2022-02-26 19:23:54 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.5887
2022-02-26 19:24:27 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.6977
2022-02-26 19:25:01 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.6882
2022-02-26 19:25:34 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.5995
2022-02-26 19:26:08 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.4538
2022-02-26 19:26:42 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.7100
2022-02-26 19:27:15 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.7031
2022-02-26 19:27:50 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.4136
2022-02-26 19:28:24 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.8024
2022-02-26 19:28:58 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.4972
2022-02-26 19:29:31 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.6500
2022-02-26 19:30:06 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.6279
2022-02-26 19:30:38 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.5926
2022-02-26 19:31:12 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.5286
2022-02-26 19:31:46 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.8906
2022-02-26 19:32:20 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.6342
2022-02-26 19:32:53 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.7411
2022-02-26 19:33:28 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.2601
2022-02-26 19:34:02 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.8155
2022-02-26 19:34:36 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.3383
2022-02-26 19:35:10 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.5153
2022-02-26 19:35:44 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.7193
2022-02-26 19:36:18 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.7632
2022-02-26 19:36:51 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.6515
2022-02-26 19:37:24 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.5343
2022-02-26 19:37:58 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.6716
2022-02-26 19:38:32 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.8031
2022-02-26 19:39:05 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.7840
2022-02-26 19:39:40 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.7262
2022-02-26 19:40:15 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.8393
2022-02-26 19:40:47 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.8306
2022-02-26 19:41:22 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.9194
2022-02-26 19:41:56 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.5442
2022-02-26 19:42:30 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.5279
2022-02-26 19:43:04 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.7185
2022-02-26 19:43:37 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.5980
2022-02-26 19:44:10 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.6729
2022-02-26 19:44:44 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.6578
2022-02-26 19:45:18 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.5806
2022-02-26 19:45:52 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.5026
2022-02-26 19:46:26 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.4992
2022-02-26 19:47:00 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.4615
2022-02-26 19:47:35 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.5299
2022-02-26 19:48:09 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.7523
2022-02-26 19:48:45 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.5022
2022-02-26 19:49:17 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.5339
2022-02-26 19:49:52 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.6381
2022-02-26 19:50:25 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.6747
2022-02-26 19:50:26 - train: epoch 023, train_loss: 2.6584
2022-02-26 19:51:43 - eval: epoch: 023, acc1: 46.700%, acc5: 72.548%, test_loss: 2.3342, per_image_load_time: 2.257ms, per_image_inference_time: 0.346ms
2022-02-26 19:51:43 - until epoch: 023, best_acc1: 46.700%
2022-02-26 23:57:43 - epoch 024 lr: 0.1
2022-02-26 23:58:21 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.5244
2022-02-26 23:58:55 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.6728
2022-02-26 23:59:29 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.5877
2022-02-27 00:00:03 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.7490
2022-02-27 00:00:36 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.7402
2022-02-27 00:01:10 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.5877
2022-02-27 00:01:44 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.4387
2022-02-27 00:02:18 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.4648
2022-02-27 00:02:51 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.8130
2022-02-27 00:03:24 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.6752
2022-02-27 00:03:57 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.3554
2022-02-27 00:04:30 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.5571
2022-02-27 00:05:03 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.7114
2022-02-27 00:05:38 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.6481
2022-02-27 00:06:12 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.7159
2022-02-27 00:06:45 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.5036
2022-02-27 00:07:19 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.5985
2022-02-27 00:07:54 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.8001
2022-02-27 00:08:26 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.3024
2022-02-27 00:09:00 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.7364
2022-02-27 00:09:34 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.4488
2022-02-27 00:10:08 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.5669
2022-02-27 00:10:41 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.5537
2022-02-27 00:11:15 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.8013
2022-02-27 00:11:49 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.6165
2022-02-27 00:12:23 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.6975
2022-02-27 00:12:57 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.7662
2022-02-27 00:13:30 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.7867
2022-02-27 00:14:05 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.6819
2022-02-27 00:14:39 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.5836
2022-02-27 00:15:12 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.4555
2022-02-27 00:15:47 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.9084
2022-02-27 00:16:20 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.5090
2022-02-27 00:16:52 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.6089
2022-02-27 00:17:28 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.5338
2022-02-27 00:18:01 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.5527
2022-02-27 00:18:35 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.5935
2022-02-27 00:19:09 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.7508
2022-02-27 00:19:43 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.7688
2022-02-27 00:20:17 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.5868
2022-02-27 00:20:51 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.5638
2022-02-27 00:21:25 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.5236
2022-02-27 00:21:59 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.6212
2022-02-27 00:22:33 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.6451
2022-02-27 00:23:06 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.4962
2022-02-27 00:23:41 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.8134
2022-02-27 00:24:14 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.6226
2022-02-27 00:24:49 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.4366
2022-02-27 00:25:24 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.7394
2022-02-27 00:25:58 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.6857
2022-02-27 00:25:59 - train: epoch 024, train_loss: 2.6523
2022-02-27 00:27:15 - eval: epoch: 024, acc1: 47.034%, acc5: 72.902%, test_loss: 2.3206, per_image_load_time: 1.057ms, per_image_inference_time: 0.301ms
2022-02-27 00:27:15 - until epoch: 024, best_acc1: 47.034%
2022-02-27 00:27:15 - epoch 025 lr: 0.1
2022-02-27 00:27:54 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.7811
2022-02-27 00:28:28 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.3830
2022-02-27 00:29:01 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.8392
2022-02-27 00:29:34 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.8884
2022-02-27 00:30:07 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.4133
2022-02-27 00:30:40 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.6884
2022-02-27 00:31:14 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.6871
2022-02-27 00:31:49 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.6175
2022-02-27 00:32:21 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.4896
2022-02-27 00:32:56 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.4301
2022-02-27 00:33:30 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.5390
2022-02-27 00:34:04 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.9204
2022-02-27 00:34:38 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.5301
2022-02-27 00:35:11 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.6228
2022-02-27 00:35:43 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.5953
2022-02-27 00:36:17 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.4631
2022-02-27 00:36:50 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.6552
2022-02-27 00:37:25 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.5680
2022-02-27 00:37:58 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.5159
2022-02-27 00:38:33 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.6323
2022-02-27 00:39:06 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.4362
2022-02-27 00:39:41 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.5583
2022-02-27 00:40:15 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.5433
2022-02-27 00:40:49 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.4496
2022-02-27 00:41:22 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.6593
2022-02-27 00:41:56 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.6498
2022-02-27 00:42:30 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.8415
2022-02-27 00:43:02 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.5524
2022-02-27 00:43:37 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.5211
2022-02-27 00:44:10 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.8386
2022-02-27 00:44:44 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.5825
2022-02-27 00:45:17 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.6308
2022-02-27 00:45:52 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.8876
2022-02-27 00:46:25 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4932
2022-02-27 00:46:59 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.4762
2022-02-27 00:47:32 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.7498
2022-02-27 00:48:07 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.6219
2022-02-27 00:48:39 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.8701
2022-02-27 00:49:12 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.9920
2022-02-27 00:49:46 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.5638
2022-02-27 00:50:20 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.9356
2022-02-27 00:50:54 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.5726
2022-02-27 00:51:27 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.5982
2022-02-27 00:52:01 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.4477
2022-02-27 00:52:36 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.6233
2022-02-27 00:53:11 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.5048
2022-02-27 00:53:45 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.6913
2022-02-27 00:54:20 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.5636
2022-02-27 00:54:53 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.7859
2022-02-27 00:55:25 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.9386
2022-02-27 00:55:26 - train: epoch 025, train_loss: 2.6438
2022-02-27 00:56:42 - eval: epoch: 025, acc1: 47.028%, acc5: 73.190%, test_loss: 2.3093, per_image_load_time: 1.738ms, per_image_inference_time: 0.313ms
2022-02-27 00:56:42 - until epoch: 025, best_acc1: 47.034%
2022-02-27 00:56:42 - epoch 026 lr: 0.1
2022-02-27 00:57:20 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.6520
2022-02-27 00:57:54 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.2769
2022-02-27 00:58:27 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.6015
2022-02-27 00:59:01 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.5808
2022-02-27 00:59:35 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.8647
2022-02-27 01:00:08 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.7319
2022-02-27 01:00:42 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.6179
2022-02-27 01:01:16 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.5855
2022-02-27 01:01:48 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.8159
2022-02-27 01:02:21 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.6030
2022-02-27 01:02:55 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.8014
2022-02-27 01:03:29 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.8310
2022-02-27 01:04:03 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.5896
2022-02-27 01:04:37 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.7431
2022-02-27 01:05:11 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.5102
2022-02-27 01:05:44 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.6599
2022-02-27 01:06:19 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.7323
2022-02-27 01:06:53 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.8652
2022-02-27 01:07:27 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6981
2022-02-27 01:08:00 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.7508
2022-02-27 01:08:32 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.8021
2022-02-27 01:09:07 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.6532
2022-02-27 01:09:40 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.7421
2022-02-27 01:10:15 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6977
2022-02-27 01:10:48 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.8699
2022-02-27 01:11:23 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.6394
2022-02-27 01:11:56 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.7542
2022-02-27 01:12:30 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.3976
2022-02-27 01:13:04 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.6818
2022-02-27 01:13:38 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.5355
2022-02-27 01:14:12 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.5395
2022-02-27 01:14:46 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.6183
2022-02-27 01:15:18 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.6675
2022-02-27 01:15:52 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.8194
2022-02-27 01:16:26 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4962
2022-02-27 01:17:00 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.5978
2022-02-27 01:17:35 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.5801
2022-02-27 01:18:08 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.7382
2022-02-27 01:18:42 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.8254
2022-02-27 01:19:16 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.7551
2022-02-27 01:19:50 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.8578
2022-02-27 01:20:25 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.7874
2022-02-27 01:20:58 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.9783
2022-02-27 01:21:31 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.6775
2022-02-27 01:22:06 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.7952
2022-02-27 01:22:39 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.6120
2022-02-27 01:23:14 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.5110
2022-02-27 01:23:49 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.5696
2022-02-27 01:24:23 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.5664
2022-02-27 01:24:57 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5989
2022-02-27 01:24:58 - train: epoch 026, train_loss: 2.6383
2022-02-27 01:26:13 - eval: epoch: 026, acc1: 46.714%, acc5: 72.710%, test_loss: 2.3419, per_image_load_time: 2.215ms, per_image_inference_time: 0.336ms
2022-02-27 01:26:13 - until epoch: 026, best_acc1: 47.034%
2022-02-27 01:26:13 - epoch 027 lr: 0.1
2022-02-27 01:26:52 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.6983
2022-02-27 01:27:25 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.7902
2022-02-27 01:27:58 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.7396
2022-02-27 01:28:32 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.7422
2022-02-27 01:29:05 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.6504
2022-02-27 01:29:38 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.8070
2022-02-27 01:30:11 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.6473
2022-02-27 01:30:47 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.6350
2022-02-27 01:31:19 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.3517
2022-02-27 01:31:54 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.6720
2022-02-27 01:32:27 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.5723
2022-02-27 01:33:01 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.5957
2022-02-27 01:33:35 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.7014
2022-02-27 01:34:08 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.8712
2022-02-27 01:34:41 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.6762
2022-02-27 01:35:15 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.7827
2022-02-27 01:35:49 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.5937
2022-02-27 01:36:23 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.7445
2022-02-27 01:36:56 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.7574
2022-02-27 01:37:30 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.6517
2022-02-27 01:38:04 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.7188
2022-02-27 01:38:39 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.7161
2022-02-27 01:39:12 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.8938
2022-02-27 01:39:47 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.7068
2022-02-27 01:40:20 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.6081
2022-02-27 01:40:54 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6835
2022-02-27 01:41:26 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.6116
2022-02-27 01:42:01 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.5391
2022-02-27 01:42:34 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.7080
2022-02-27 01:43:09 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.5705
2022-02-27 01:43:43 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.5578
2022-02-27 01:44:17 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.3588
2022-02-27 01:44:50 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.5363
2022-02-27 01:45:25 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.6261
2022-02-27 01:45:59 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.7982
2022-02-27 01:46:33 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.6882
2022-02-27 01:47:06 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.6224
2022-02-27 01:47:39 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.5337
2022-02-27 01:48:14 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.6952
2022-02-27 01:48:48 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.9021
2022-02-27 01:49:22 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.6921
2022-02-27 01:49:56 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.5277
2022-02-27 01:50:30 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.6071
2022-02-27 01:51:05 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.5272
2022-02-27 01:51:38 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.3625
2022-02-27 01:52:14 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.5436
2022-02-27 01:52:47 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.8917
2022-02-27 01:53:22 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.7534
2022-02-27 01:53:56 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.5445
2022-02-27 01:54:28 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.3192
2022-02-27 01:54:29 - train: epoch 027, train_loss: 2.6327
2022-02-27 01:55:46 - eval: epoch: 027, acc1: 47.072%, acc5: 73.376%, test_loss: 2.3107, per_image_load_time: 1.379ms, per_image_inference_time: 0.307ms
2022-02-27 01:55:46 - until epoch: 027, best_acc1: 47.072%
2022-02-27 01:55:46 - epoch 028 lr: 0.1
2022-02-27 01:56:25 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.5136
2022-02-27 01:57:00 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.6539
2022-02-27 01:57:34 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.4649
2022-02-27 01:58:08 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.5960
2022-02-27 01:58:41 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.5917
2022-02-27 01:59:16 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.7482
2022-02-27 01:59:48 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.7723
2022-02-27 02:00:22 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.3910
2022-02-27 02:00:55 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.6442
2022-02-27 02:01:29 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.6978
2022-02-27 02:02:03 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.5428
2022-02-27 02:02:38 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.4324
2022-02-27 02:03:12 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.6788
2022-02-27 02:03:46 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.0661
2022-02-27 02:04:19 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.4166
2022-02-27 02:04:53 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.5370
2022-02-27 02:05:28 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.4478
2022-02-27 02:06:02 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.5693
2022-02-27 02:06:35 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.6605
2022-02-27 02:07:08 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.6662
2022-02-27 02:07:42 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.9278
2022-02-27 02:08:17 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.5247
2022-02-27 02:08:49 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.7804
2022-02-27 02:09:24 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.7366
2022-02-27 02:09:58 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.6093
2022-02-27 02:10:32 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.7160
2022-02-27 02:11:07 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.7350
2022-02-27 02:11:40 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.5837
2022-02-27 02:12:15 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.5271
2022-02-27 02:12:48 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4444
2022-02-27 02:13:22 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.6257
2022-02-27 02:13:55 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.5188
2022-02-27 02:14:29 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.5859
2022-02-27 02:15:04 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.7178
2022-02-27 02:15:37 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.6977
2022-02-27 02:16:12 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.6960
2022-02-27 02:16:46 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.6972
2022-02-27 02:17:20 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.3083
2022-02-27 02:17:54 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.8162
2022-02-27 02:18:29 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.7881
2022-02-27 02:19:02 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.6359
2022-02-27 02:19:35 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.6078
2022-02-27 02:20:08 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.6080
2022-02-27 02:20:43 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.7260
2022-02-27 02:21:18 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.6516
2022-02-27 02:21:50 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.7154
2022-02-27 02:22:26 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.7203
2022-02-27 02:23:00 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.6657
2022-02-27 02:23:36 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.6707
2022-02-27 02:24:09 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.3148
2022-02-27 02:24:10 - train: epoch 028, train_loss: 2.6271
2022-02-27 02:25:26 - eval: epoch: 028, acc1: 46.772%, acc5: 72.866%, test_loss: 2.3352, per_image_load_time: 1.512ms, per_image_inference_time: 0.309ms
2022-02-27 02:25:26 - until epoch: 028, best_acc1: 47.072%
2022-02-27 02:25:26 - epoch 029 lr: 0.1
2022-02-27 02:26:04 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.5512
2022-02-27 02:26:38 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4256
2022-02-27 02:27:11 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.6257
2022-02-27 02:27:44 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.6243
2022-02-27 02:28:18 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.8011
2022-02-27 02:28:52 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.6041
2022-02-27 02:29:25 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.4704
2022-02-27 02:29:59 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.5441
2022-02-27 02:30:33 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.6281
2022-02-27 02:31:07 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.3554
2022-02-27 02:31:41 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.5580
2022-02-27 02:32:15 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.5355
2022-02-27 02:32:48 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.5941
2022-02-27 02:33:22 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.7145
2022-02-27 02:33:55 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.6838
2022-02-27 02:34:29 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.6841
2022-02-27 02:35:03 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.7361
2022-02-27 02:35:37 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4998
2022-02-27 02:36:11 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.4399
2022-02-27 02:36:45 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.4787
2022-02-27 02:37:19 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.6414
2022-02-27 02:37:53 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.8475
2022-02-27 02:38:27 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.5379
2022-02-27 02:39:01 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.6726
2022-02-27 02:39:33 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.5173
2022-02-27 02:40:07 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.8025
2022-02-27 02:40:41 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.5038
2022-02-27 02:41:14 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.6128
2022-02-27 02:41:49 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.4023
2022-02-27 02:42:23 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.6366
2022-02-27 02:42:56 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.5850
2022-02-27 02:43:31 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.7157
2022-02-27 02:44:04 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.4779
2022-02-27 02:44:38 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.4682
2022-02-27 02:45:13 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.7462
2022-02-27 02:45:45 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.6817
2022-02-27 02:46:19 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.6201
2022-02-27 02:46:52 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.6423
2022-02-27 02:47:27 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.4137
2022-02-27 02:48:00 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.6714
2022-02-27 02:48:35 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.7020
2022-02-27 02:49:09 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.3420
2022-02-27 02:49:42 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.6223
2022-02-27 02:50:17 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.7732
2022-02-27 02:50:51 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.6910
2022-02-27 02:51:26 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.8936
2022-02-27 02:51:59 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.4673
2022-02-27 02:52:32 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.6096
2022-02-27 02:53:07 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.8699
2022-02-27 02:53:40 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.3017
2022-02-27 02:53:41 - train: epoch 029, train_loss: 2.6234
2022-02-27 02:54:57 - eval: epoch: 029, acc1: 47.880%, acc5: 73.374%, test_loss: 2.2858, per_image_load_time: 2.584ms, per_image_inference_time: 0.334ms
2022-02-27 02:54:57 - until epoch: 029, best_acc1: 47.880%
2022-02-27 02:54:57 - epoch 030 lr: 0.1
2022-02-27 02:55:36 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.6795
2022-02-27 02:56:10 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.5511
2022-02-27 02:56:43 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.6412
2022-02-27 02:57:17 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.5666
2022-02-27 02:57:51 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.6867
2022-02-27 02:58:24 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.7456
2022-02-27 02:58:57 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.7774
2022-02-27 02:59:31 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.5564
2022-02-27 03:00:05 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.6865
2022-02-27 03:00:39 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.5985
2022-02-27 03:01:11 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.5898
2022-02-27 03:01:46 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.5905
2022-02-27 03:02:20 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.5567
2022-02-27 03:02:54 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.3960
2022-02-27 03:03:28 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.4973
2022-02-27 03:04:02 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.5898
2022-02-27 03:04:36 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.9388
2022-02-27 03:05:09 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.5905
2022-02-27 03:05:42 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.5952
2022-02-27 03:06:16 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.5267
2022-02-27 03:06:50 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.7561
2022-02-27 03:07:23 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.6672
2022-02-27 03:07:58 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.5628
2022-02-27 03:08:31 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.6198
2022-02-27 03:09:05 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.6982
2022-02-27 03:09:40 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.6616
2022-02-27 03:10:14 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.5081
2022-02-27 03:10:48 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.5697
2022-02-27 03:11:21 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.3615
2022-02-27 03:11:55 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.6403
2022-02-27 03:12:28 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.5729
2022-02-27 03:13:03 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.5556
2022-02-27 03:13:36 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.7656
2022-02-27 03:14:11 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.5787
2022-02-27 03:14:44 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4624
2022-02-27 03:15:19 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.6530
2022-02-27 03:15:52 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.5914
2022-02-27 03:16:27 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.6133
2022-02-27 03:17:01 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.5250
2022-02-27 03:17:35 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.5696
2022-02-27 03:18:08 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.3547
2022-02-27 03:18:41 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.8085
2022-02-27 03:19:16 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.7015
2022-02-27 03:19:49 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.6655
2022-02-27 03:20:23 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.7450
2022-02-27 03:20:57 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.4840
2022-02-27 03:21:32 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.5564
2022-02-27 03:22:06 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.5608
2022-02-27 03:22:42 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.8111
2022-02-27 03:23:15 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.9162
2022-02-27 03:23:16 - train: epoch 030, train_loss: 2.6198
2022-02-27 03:24:30 - eval: epoch: 030, acc1: 46.678%, acc5: 72.844%, test_loss: 2.3302, per_image_load_time: 1.869ms, per_image_inference_time: 0.331ms
2022-02-27 03:24:30 - until epoch: 030, best_acc1: 47.880%
2022-02-27 03:24:30 - epoch 031 lr: 0.010000000000000002
2022-02-27 03:25:09 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.4282
2022-02-27 03:25:43 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.2604
2022-02-27 03:26:15 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.2882
2022-02-27 03:26:48 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.2273
2022-02-27 03:27:22 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.3264
2022-02-27 03:27:57 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.2139
2022-02-27 03:28:30 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.2784
2022-02-27 03:29:05 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.2327
2022-02-27 03:29:37 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.2793
2022-02-27 03:30:12 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.4806
2022-02-27 03:30:45 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.3427
2022-02-27 03:31:19 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.3296
2022-02-27 03:31:52 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.9965
2022-02-27 03:32:26 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8453
2022-02-27 03:33:00 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.3052
2022-02-27 03:33:34 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.8988
2022-02-27 03:34:08 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.9750
2022-02-27 03:34:42 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.0795
2022-02-27 03:35:16 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.0662
2022-02-27 03:35:49 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.1660
2022-02-27 03:36:24 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.0477
2022-02-27 03:36:57 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.0287
2022-02-27 03:37:31 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.9454
2022-02-27 03:38:03 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.1608
2022-02-27 03:38:38 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.1218
2022-02-27 03:39:12 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.0413
2022-02-27 03:39:46 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.2966
2022-02-27 03:40:20 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.4929
2022-02-27 03:40:54 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.1769
2022-02-27 03:41:28 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.5701
2022-02-27 03:42:02 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.1036
2022-02-27 03:42:36 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.3167
2022-02-27 03:43:10 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.1087
2022-02-27 03:43:44 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.3009
2022-02-27 03:44:17 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.3248
2022-02-27 03:44:49 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.1142
2022-02-27 03:45:24 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.9978
2022-02-27 03:45:58 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.0136
2022-02-27 03:46:31 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.9979
2022-02-27 03:47:06 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.0208
2022-02-27 03:47:40 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.2438
2022-02-27 03:48:14 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.1174
2022-02-27 03:48:48 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.0700
2022-02-27 03:49:22 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.1693
2022-02-27 03:49:56 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.1293
2022-02-27 03:50:30 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.2979
2022-02-27 03:51:03 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.9751
2022-02-27 03:51:37 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.9277
2022-02-27 03:52:12 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.0612
2022-02-27 03:52:46 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.1372
2022-02-27 03:52:47 - train: epoch 031, train_loss: 2.1585
2022-02-27 03:54:03 - eval: epoch: 031, acc1: 58.786%, acc5: 81.730%, test_loss: 1.7465, per_image_load_time: 2.458ms, per_image_inference_time: 0.344ms
2022-02-27 03:54:03 - until epoch: 031, best_acc1: 58.786%
2022-02-27 03:54:03 - epoch 032 lr: 0.010000000000000002
2022-02-27 03:54:42 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.9095
2022-02-27 03:55:16 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.9261
2022-02-27 03:55:50 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.9041
2022-02-27 03:56:23 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.0423
2022-02-27 03:56:57 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.9287
2022-02-27 03:57:29 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.0099
2022-02-27 03:58:03 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.0928
2022-02-27 03:58:36 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.0137
2022-02-27 03:59:11 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.2853
2022-02-27 03:59:45 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.0377
2022-02-27 04:00:19 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.0751
2022-02-27 04:00:53 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.0647
2022-02-27 04:01:27 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.1889
2022-02-27 04:02:01 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.3174
2022-02-27 04:02:35 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.9522
2022-02-27 04:03:09 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.2346
2022-02-27 04:03:42 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.0386
2022-02-27 04:04:16 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.3027
2022-02-27 04:04:49 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.8884
2022-02-27 04:05:24 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.0805
2022-02-27 04:05:57 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.8626
2022-02-27 04:06:31 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.8846
2022-02-27 04:07:05 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.9474
2022-02-27 04:07:39 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.1384
2022-02-27 04:08:13 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.0280
2022-02-27 04:08:48 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 2.0518
2022-02-27 04:09:21 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.1177
2022-02-27 04:09:55 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.1879
2022-02-27 04:10:27 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.8895
2022-02-27 04:11:02 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.9167
2022-02-27 04:11:36 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.1038
2022-02-27 04:12:10 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.1520
2022-02-27 04:12:44 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.7588
2022-02-27 04:13:18 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.8271
2022-02-27 04:13:52 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.9036
2022-02-27 04:14:26 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.1782
2022-02-27 04:15:00 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.0171
2022-02-27 04:15:34 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.8253
2022-02-27 04:16:07 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.9908
2022-02-27 04:16:40 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.9758
2022-02-27 04:17:15 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.9713
2022-02-27 04:17:49 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.9036
2022-02-27 04:18:22 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.4176
2022-02-27 04:18:57 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.9163
2022-02-27 04:19:31 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.9903
2022-02-27 04:20:05 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.8940
2022-02-27 04:20:40 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.2492
2022-02-27 04:21:15 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.9181
2022-02-27 04:21:49 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.9387
2022-02-27 04:22:23 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.9478
2022-02-27 04:22:24 - train: epoch 032, train_loss: 2.0500
2022-02-27 04:23:38 - eval: epoch: 032, acc1: 59.694%, acc5: 82.422%, test_loss: 1.7011, per_image_load_time: 2.479ms, per_image_inference_time: 0.304ms
2022-02-27 04:23:38 - until epoch: 032, best_acc1: 59.694%
2022-02-27 04:23:38 - epoch 033 lr: 0.010000000000000002
2022-02-27 04:24:17 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.7531
2022-02-27 04:24:51 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.2461
2022-02-27 04:25:25 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.0927
2022-02-27 04:25:58 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.7550
2022-02-27 04:26:32 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.0833
2022-02-27 04:27:07 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.9891
2022-02-27 04:27:40 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.1625
2022-02-27 04:28:14 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.0847
2022-02-27 04:28:48 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.0105
2022-02-27 04:29:21 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.0026
2022-02-27 04:29:54 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.9237
2022-02-27 04:30:29 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.1196
2022-02-27 04:31:01 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.9342
2022-02-27 04:31:36 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.0726
2022-02-27 04:32:10 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.1771
2022-02-27 04:32:44 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.2165
2022-02-27 04:33:18 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.8589
2022-02-27 04:33:52 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.1539
2022-02-27 04:34:27 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.8613
2022-02-27 04:35:01 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.8925
2022-02-27 04:35:34 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.9588
2022-02-27 04:36:07 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.8351
2022-02-27 04:36:40 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.0123
2022-02-27 04:37:16 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.0135
2022-02-27 04:37:49 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.1479
2022-02-27 04:38:23 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 2.1237
2022-02-27 04:38:57 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.1338
2022-02-27 04:39:31 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.1471
2022-02-27 04:40:05 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.1787
2022-02-27 04:40:39 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.8428
2022-02-27 04:41:13 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.9663
2022-02-27 04:41:47 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.0374
2022-02-27 04:42:20 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.8939
2022-02-27 04:42:53 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.9485
2022-02-27 04:43:27 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.2018
2022-02-27 04:44:02 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.3440
2022-02-27 04:44:35 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.8164
2022-02-27 04:45:09 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.9362
2022-02-27 04:45:43 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.4855
2022-02-27 04:46:17 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.0781
2022-02-27 04:46:51 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.0253
2022-02-27 04:47:25 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.8416
2022-02-27 04:47:59 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.0993
2022-02-27 04:48:33 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0379
2022-02-27 04:49:05 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.0648
2022-02-27 04:49:40 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.9759
2022-02-27 04:50:14 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.0646
2022-02-27 04:50:48 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.2374
2022-02-27 04:51:23 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.7547
2022-02-27 04:51:57 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.0060
2022-02-27 04:51:58 - train: epoch 033, train_loss: 2.0085
2022-02-27 04:53:13 - eval: epoch: 033, acc1: 60.388%, acc5: 82.756%, test_loss: 1.6762, per_image_load_time: 2.105ms, per_image_inference_time: 0.333ms
2022-02-27 04:53:13 - until epoch: 033, best_acc1: 60.388%
2022-02-27 04:53:13 - epoch 034 lr: 0.010000000000000002
2022-02-27 04:53:51 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.8581
2022-02-27 04:54:26 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.0047
2022-02-27 04:54:58 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.7906
2022-02-27 04:55:31 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.8723
2022-02-27 04:56:04 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.9889
2022-02-27 04:56:39 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.0960
2022-02-27 04:57:12 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.8523
2022-02-27 04:57:46 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.1211
2022-02-27 04:58:19 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.0203
2022-02-27 04:58:53 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.8544
2022-02-27 04:59:26 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.9346
2022-02-27 05:00:01 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.1190
2022-02-27 05:00:34 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.9855
2022-02-27 05:01:09 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.0200
2022-02-27 05:01:43 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.0593
2022-02-27 05:02:16 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.8112
2022-02-27 05:02:50 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.9094
2022-02-27 05:03:24 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1914
2022-02-27 05:03:58 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.2534
2022-02-27 05:04:32 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.9346
2022-02-27 05:05:07 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.2272
2022-02-27 05:05:41 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7840
2022-02-27 05:06:14 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.0418
2022-02-27 05:06:49 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.9710
2022-02-27 05:07:23 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.0408
2022-02-27 05:07:58 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.8796
2022-02-27 05:08:31 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.2264
2022-02-27 05:09:04 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.8408
2022-02-27 05:09:38 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 2.0241
2022-02-27 05:10:12 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 2.1162
2022-02-27 05:10:46 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9749
2022-02-27 05:11:21 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.9792
2022-02-27 05:11:55 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.7831
2022-02-27 05:12:29 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.9791
2022-02-27 05:13:03 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.7490
2022-02-27 05:13:37 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.8661
2022-02-27 05:14:10 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.9678
2022-02-27 05:14:44 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.9265
2022-02-27 05:15:17 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.1905
2022-02-27 05:15:52 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.8714
2022-02-27 05:16:25 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.0344
2022-02-27 05:16:59 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.0432
2022-02-27 05:17:34 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.0867
2022-02-27 05:18:08 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.9276
2022-02-27 05:18:43 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.0993
2022-02-27 05:19:17 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.1898
2022-02-27 05:19:51 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.9648
2022-02-27 05:20:26 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.0029
2022-02-27 05:21:00 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.9811
2022-02-27 05:21:33 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.8144
2022-02-27 05:21:34 - train: epoch 034, train_loss: 1.9904
2022-02-27 05:22:50 - eval: epoch: 034, acc1: 60.596%, acc5: 82.996%, test_loss: 1.6573, per_image_load_time: 2.394ms, per_image_inference_time: 0.301ms
2022-02-27 05:22:50 - until epoch: 034, best_acc1: 60.596%
2022-02-27 05:22:50 - epoch 035 lr: 0.010000000000000002
2022-02-27 05:23:29 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.8497
2022-02-27 05:24:04 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.7871
2022-02-27 05:24:37 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.0687
2022-02-27 05:25:10 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.0508
2022-02-27 05:25:44 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 2.0245
2022-02-27 05:26:18 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.0578
2022-02-27 05:26:52 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.0928
2022-02-27 05:27:26 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.0045
2022-02-27 05:27:59 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.1873
2022-02-27 05:28:32 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.9989
2022-02-27 05:29:06 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.2272
2022-02-27 05:29:40 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.8738
2022-02-27 05:30:14 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9883
2022-02-27 05:30:48 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.0777
2022-02-27 05:31:21 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.0014
2022-02-27 05:31:56 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.9247
2022-02-27 05:32:30 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.8381
2022-02-27 05:33:04 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.0756
2022-02-27 05:33:38 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.9368
2022-02-27 05:34:11 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.1488
2022-02-27 05:34:44 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.9439
2022-02-27 05:35:18 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.9104
2022-02-27 05:35:53 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.0815
2022-02-27 05:36:26 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.1386
2022-02-27 05:37:00 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.0318
2022-02-27 05:37:35 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.0194
2022-02-27 05:38:08 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.0385
2022-02-27 05:38:42 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.0040
2022-02-27 05:39:17 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.9525
2022-02-27 05:39:50 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.8935
2022-02-27 05:40:24 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.1374
2022-02-27 05:40:57 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.8749
2022-02-27 05:41:30 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.8488
2022-02-27 05:42:05 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.0653
2022-02-27 05:42:39 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.7811
2022-02-27 05:43:12 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.9984
2022-02-27 05:43:46 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.8123
2022-02-27 05:44:20 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.9087
2022-02-27 05:44:55 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.0780
2022-02-27 05:45:29 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.7035
2022-02-27 05:46:03 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.9709
2022-02-27 05:46:37 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.6455
2022-02-27 05:47:10 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.1378
2022-02-27 05:47:44 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.2731
2022-02-27 05:48:18 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.0292
2022-02-27 05:48:52 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.1524
2022-02-27 05:49:26 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.1770
2022-02-27 05:50:01 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.9841
2022-02-27 05:50:36 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.0039
2022-02-27 05:51:10 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.9427
2022-02-27 05:51:11 - train: epoch 035, train_loss: 1.9705
2022-02-27 05:52:28 - eval: epoch: 035, acc1: 60.666%, acc5: 83.048%, test_loss: 1.6503, per_image_load_time: 2.677ms, per_image_inference_time: 0.300ms
2022-02-27 05:52:28 - until epoch: 035, best_acc1: 60.666%
2022-02-27 05:52:28 - epoch 036 lr: 0.010000000000000002
2022-02-27 05:53:07 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.1400
2022-02-27 05:53:40 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.8542
2022-02-27 05:54:14 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.9195
2022-02-27 05:54:47 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.9204
2022-02-27 05:55:20 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.8378
2022-02-27 05:55:55 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.8507
2022-02-27 05:56:29 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.7862
2022-02-27 05:57:02 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.8139
2022-02-27 05:57:36 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.9979
2022-02-27 05:58:09 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.8461
2022-02-27 05:58:44 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.0666
2022-02-27 05:59:18 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.8882
2022-02-27 05:59:52 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.0460
2022-02-27 06:00:25 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.9639
2022-02-27 06:00:58 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.9413
2022-02-27 06:01:32 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.9610
2022-02-27 06:02:06 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.7652
2022-02-27 06:02:40 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.7811
2022-02-27 06:03:14 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.9190
2022-02-27 06:03:47 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.8348
2022-02-27 06:04:22 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 2.0144
2022-02-27 06:04:55 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.9585
2022-02-27 06:05:29 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.9150
2022-02-27 06:06:03 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.1488
2022-02-27 06:06:37 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.7027
2022-02-27 06:07:10 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.1055
2022-02-27 06:07:44 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.9942
2022-02-27 06:08:18 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.8025
2022-02-27 06:08:52 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.8029
2022-02-27 06:09:25 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.9668
2022-02-27 06:10:00 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.0110
2022-02-27 06:10:33 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.9161
2022-02-27 06:11:08 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.8660
2022-02-27 06:11:42 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.7993
2022-02-27 06:12:16 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.4757
2022-02-27 06:12:49 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.3023
2022-02-27 06:13:23 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.9432
2022-02-27 06:13:56 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.9715
2022-02-27 06:14:30 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.0831
2022-02-27 06:15:04 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.1570
2022-02-27 06:15:39 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.9051
2022-02-27 06:16:12 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.9524
2022-02-27 06:16:46 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.7866
2022-02-27 06:17:20 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.0330
2022-02-27 06:17:55 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.9741
2022-02-27 06:18:29 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.9883
2022-02-27 06:19:04 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.8833
2022-02-27 06:19:37 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.7965
2022-02-27 06:20:11 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.7623
2022-02-27 06:20:44 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.7614
2022-02-27 06:20:45 - train: epoch 036, train_loss: 1.9626
2022-02-27 06:22:02 - eval: epoch: 036, acc1: 60.656%, acc5: 83.180%, test_loss: 1.6470, per_image_load_time: 2.638ms, per_image_inference_time: 0.293ms
2022-02-27 06:22:02 - until epoch: 036, best_acc1: 60.666%
2022-02-27 06:22:02 - epoch 037 lr: 0.010000000000000002
2022-02-27 06:22:41 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.8756
2022-02-27 06:23:14 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.8793
2022-02-27 06:23:48 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.8332
2022-02-27 06:24:22 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.8691
2022-02-27 06:24:56 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.8300
2022-02-27 06:25:31 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.9572
2022-02-27 06:26:03 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.9481
2022-02-27 06:26:37 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.8558
2022-02-27 06:27:09 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.1946
2022-02-27 06:27:43 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.8858
2022-02-27 06:28:17 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.0228
2022-02-27 06:28:51 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.0770
2022-02-27 06:29:25 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.8842
2022-02-27 06:29:59 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.0986
2022-02-27 06:30:32 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.7704
2022-02-27 06:31:06 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.7111
2022-02-27 06:31:41 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.9258
2022-02-27 06:32:14 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.0267
2022-02-27 06:32:47 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.9307
2022-02-27 06:33:19 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.1389
2022-02-27 06:33:53 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.0509
2022-02-27 06:34:27 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.6287
2022-02-27 06:35:01 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.9010
2022-02-27 06:35:35 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.9483
2022-02-27 06:36:09 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.9979
2022-02-27 06:36:43 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.0510
2022-02-27 06:37:17 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.2253
2022-02-27 06:37:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.9495
2022-02-27 06:38:24 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.9292
2022-02-27 06:38:57 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.0832
2022-02-27 06:39:30 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.7910
2022-02-27 06:40:04 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.9649
2022-02-27 06:40:38 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.9003
2022-02-27 06:41:12 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.8321
2022-02-27 06:41:46 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.0830
2022-02-27 06:42:20 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.1332
2022-02-27 06:42:54 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.9774
2022-02-27 06:43:28 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.9812
2022-02-27 06:44:01 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.8598
2022-02-27 06:44:36 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 2.0375
2022-02-27 06:45:10 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.9199
2022-02-27 06:45:43 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.9173
2022-02-27 06:46:17 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.0003
2022-02-27 06:46:50 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.9351
2022-02-27 06:47:25 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.8033
2022-02-27 06:47:59 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.8862
2022-02-27 06:48:34 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.9359
2022-02-27 06:49:08 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.7909
2022-02-27 06:49:43 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.7757
2022-02-27 06:50:17 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.8570
2022-02-27 06:50:18 - train: epoch 037, train_loss: 1.9586
2022-02-27 06:51:34 - eval: epoch: 037, acc1: 61.136%, acc5: 83.328%, test_loss: 1.6362, per_image_load_time: 2.486ms, per_image_inference_time: 0.350ms
2022-02-27 06:51:34 - until epoch: 037, best_acc1: 61.136%
