2022-02-28 08:14:02 - epoch 038 lr: 0.010000000000000002
2022-02-28 08:14:40 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.7739
2022-02-28 08:15:13 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.9074
2022-02-28 08:15:46 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.8553
2022-02-28 08:16:20 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.6291
2022-02-28 08:16:53 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.7436
2022-02-28 08:17:26 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.1064
2022-02-28 08:17:59 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.6345
2022-02-28 08:18:32 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.8272
2022-02-28 08:19:05 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.0244
2022-02-28 08:19:38 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.0137
2022-02-28 08:20:11 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 2.1290
2022-02-28 08:20:44 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.9404
2022-02-28 08:21:18 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.9623
2022-02-28 08:21:51 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7868
2022-02-28 08:22:24 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.7740
2022-02-28 08:22:58 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.0416
2022-02-28 08:23:31 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.0977
2022-02-28 08:24:04 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.9737
2022-02-28 08:24:38 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.0169
2022-02-28 08:25:10 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.8090
2022-02-28 08:25:43 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.7764
2022-02-28 08:26:17 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.6892
2022-02-28 08:26:50 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.1065
2022-02-28 08:27:23 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.0988
2022-02-28 08:27:56 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.7769
2022-02-28 08:28:30 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.9708
2022-02-28 08:29:03 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 2.0805
2022-02-28 08:29:36 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.0713
2022-02-28 08:30:09 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.9463
2022-02-28 08:30:43 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.8277
2022-02-28 08:31:16 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.9648
2022-02-28 08:31:49 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.6131
2022-02-28 08:32:22 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.6565
2022-02-28 08:32:56 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.9946
2022-02-28 08:33:30 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.8225
2022-02-28 08:34:02 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.0249
2022-02-28 08:34:36 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.8455
2022-02-28 08:35:09 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.0938
2022-02-28 08:35:43 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.8306
2022-02-28 08:36:16 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.9227
2022-02-28 08:36:49 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.9553
2022-02-28 08:37:22 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.8266
2022-02-28 08:37:56 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.0009
2022-02-28 08:38:30 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.9087
2022-02-28 08:39:02 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.9927
2022-02-28 08:39:36 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.1154
2022-02-28 08:40:09 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 2.0118
2022-02-28 08:40:43 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.7838
2022-02-28 08:41:16 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.7332
2022-02-28 08:41:49 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.6628
2022-02-28 08:41:49 - train: epoch 038, train_loss: 1.9557
2022-02-28 08:43:04 - eval: epoch: 038, acc1: 60.832%, acc5: 83.146%, test_loss: 1.6481, per_image_load_time: 1.114ms, per_image_inference_time: 0.371ms
2022-02-28 08:43:04 - until epoch: 038, best_acc1: 61.136%
2022-02-28 08:43:04 - epoch 039 lr: 0.010000000000000002
2022-02-28 08:43:42 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.9871
2022-02-28 08:44:15 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.0188
2022-02-28 08:44:49 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.7292
2022-02-28 08:45:22 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.0638
2022-02-28 08:45:55 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.0231
2022-02-28 08:46:29 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.8232
2022-02-28 08:47:02 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.1394
2022-02-28 08:47:35 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.9180
2022-02-28 08:48:08 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.0352
2022-02-28 08:48:42 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.8632
2022-02-28 08:49:15 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.0560
2022-02-28 08:49:49 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.0205
2022-02-28 08:50:22 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.0752
2022-02-28 08:50:55 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.8923
2022-02-28 08:51:28 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.8619
2022-02-28 08:52:02 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7946
2022-02-28 08:52:35 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.9624
2022-02-28 08:53:09 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.9060
2022-02-28 08:53:42 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.7171
2022-02-28 08:54:16 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.9804
2022-02-28 08:54:49 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.0621
2022-02-28 08:55:22 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.9122
2022-02-28 08:55:55 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.1557
2022-02-28 08:56:29 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.1032
2022-02-28 08:57:02 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.8939
2022-02-28 08:57:35 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.9712
2022-02-28 08:58:09 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.9620
2022-02-28 08:58:43 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.8444
2022-02-28 08:59:16 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.7669
2022-02-28 08:59:50 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.0638
2022-02-28 09:00:22 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.9364
2022-02-28 09:00:56 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.0069
2022-02-28 09:01:30 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.8923
2022-02-28 09:02:03 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.9523
2022-02-28 09:02:37 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.2449
2022-02-28 09:03:10 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.9676
2022-02-28 09:03:43 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.8593
2022-02-28 09:04:17 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.7015
2022-02-28 09:04:50 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.1462
2022-02-28 09:05:24 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.7907
2022-02-28 09:05:57 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.9959
2022-02-28 09:06:31 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.9329
2022-02-28 09:07:04 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.8315
2022-02-28 09:07:37 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.9027
2022-02-28 09:08:11 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.8536
2022-02-28 09:08:44 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.0085
2022-02-28 09:09:19 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 2.2182
2022-02-28 09:09:52 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.8632
2022-02-28 09:10:26 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.9097
2022-02-28 09:10:58 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.9494
2022-02-28 09:10:59 - train: epoch 039, train_loss: 1.9518
2022-02-28 09:12:13 - eval: epoch: 039, acc1: 60.800%, acc5: 83.426%, test_loss: 1.6340, per_image_load_time: 2.501ms, per_image_inference_time: 0.376ms
2022-02-28 09:12:13 - until epoch: 039, best_acc1: 61.136%
2022-02-28 09:12:13 - epoch 040 lr: 0.010000000000000002
2022-02-28 09:12:52 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.2569
2022-02-28 09:13:25 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.9144
2022-02-28 09:13:58 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.0079
2022-02-28 09:14:31 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.7074
2022-02-28 09:15:04 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.8180
2022-02-28 09:15:37 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.0395
2022-02-28 09:16:10 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.9220
2022-02-28 09:16:44 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.9611
2022-02-28 09:17:17 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.0125
2022-02-28 09:17:50 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.6860
2022-02-28 09:18:23 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.7413
2022-02-28 09:18:57 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.9888
2022-02-28 09:19:29 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.7427
2022-02-28 09:20:03 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.8820
2022-02-28 09:20:35 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.0100
2022-02-28 09:21:09 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.9126
2022-02-28 09:21:42 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.8958
2022-02-28 09:22:15 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.7331
2022-02-28 09:22:49 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.7621
2022-02-28 09:23:22 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.9225
2022-02-28 09:23:55 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.7786
2022-02-28 09:24:29 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.7480
2022-02-28 09:25:01 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.0247
2022-02-28 09:25:35 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.9777
2022-02-28 09:26:09 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.2220
2022-02-28 09:26:42 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.8742
2022-02-28 09:27:15 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.0264
2022-02-28 09:27:48 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.9305
2022-02-28 09:28:22 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.0215
2022-02-28 09:28:55 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.8184
2022-02-28 09:29:29 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.0082
2022-02-28 09:30:02 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.3383
2022-02-28 09:30:36 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.0672
2022-02-28 09:31:10 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.9613
2022-02-28 09:31:43 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.8744
2022-02-28 09:32:16 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.8455
2022-02-28 09:32:49 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.8621
2022-02-28 09:33:22 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.8668
2022-02-28 09:33:56 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.7989
2022-02-28 09:34:29 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.9845
2022-02-28 09:35:02 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.8885
2022-02-28 09:35:35 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 2.1033
2022-02-28 09:36:08 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.8243
2022-02-28 09:36:41 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.8438
2022-02-28 09:37:16 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.7224
2022-02-28 09:37:49 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.1596
2022-02-28 09:38:22 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.0579
2022-02-28 09:38:56 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.9012
2022-02-28 09:39:30 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.9550
2022-02-28 09:40:03 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.9545
2022-02-28 09:40:04 - train: epoch 040, train_loss: 1.9521
2022-02-28 09:41:18 - eval: epoch: 040, acc1: 60.628%, acc5: 83.208%, test_loss: 1.6487, per_image_load_time: 2.488ms, per_image_inference_time: 0.383ms
2022-02-28 09:41:18 - until epoch: 040, best_acc1: 61.136%
2022-02-28 09:41:18 - epoch 041 lr: 0.010000000000000002
2022-02-28 09:41:56 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.1205
2022-02-28 09:42:29 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.2946
2022-02-28 09:43:02 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.8548
2022-02-28 09:43:34 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.9587
2022-02-28 09:44:08 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.8037
2022-02-28 09:44:40 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.0147
2022-02-28 09:45:14 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.9524
2022-02-28 09:45:47 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.0117
2022-02-28 09:46:21 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.7618
2022-02-28 09:46:54 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.2101
2022-02-28 09:47:28 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.8971
2022-02-28 09:47:59 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.7638
2022-02-28 09:48:32 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.7701
2022-02-28 09:49:06 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.0460
2022-02-28 09:49:38 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.2414
2022-02-28 09:50:11 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.9290
2022-02-28 09:50:44 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.0057
2022-02-28 09:51:17 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.8169
2022-02-28 09:51:51 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.0101
2022-02-28 09:52:23 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.7661
2022-02-28 09:52:56 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.6248
2022-02-28 09:53:29 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.9328
2022-02-28 09:54:03 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.8088
2022-02-28 09:54:35 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.2095
2022-02-28 09:55:08 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.8065
2022-02-28 09:55:41 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.0107
2022-02-28 09:56:14 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.2645
2022-02-28 09:56:46 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.9287
2022-02-28 09:57:19 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.8457
2022-02-28 09:57:51 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.0314
2022-02-28 09:58:25 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.9238
2022-02-28 09:58:58 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.9206
2022-02-28 09:59:31 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.8192
2022-02-28 10:00:04 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.7500
2022-02-28 10:00:37 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.8046
2022-02-28 10:01:09 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.9393
2022-02-28 10:01:43 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.0469
2022-02-28 10:02:15 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.9215
2022-02-28 10:02:48 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.8792
2022-02-28 10:03:21 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.7229
2022-02-28 10:03:54 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.2145
2022-02-28 10:04:26 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.8224
2022-02-28 10:04:59 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.7989
2022-02-28 10:05:32 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.7214
2022-02-28 10:06:06 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.9835
2022-02-28 10:06:38 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.1357
2022-02-28 10:07:11 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.0677
2022-02-28 10:07:44 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.8505
2022-02-28 10:08:18 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.9771
2022-02-28 10:08:51 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.0937
2022-02-28 10:08:52 - train: epoch 041, train_loss: 1.9511
2022-02-28 10:10:05 - eval: epoch: 041, acc1: 60.564%, acc5: 83.240%, test_loss: 1.6460, per_image_load_time: 2.025ms, per_image_inference_time: 0.352ms
2022-02-28 10:10:05 - until epoch: 041, best_acc1: 61.136%
2022-02-28 10:10:05 - epoch 042 lr: 0.010000000000000002
2022-02-28 10:10:44 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.6960
2022-02-28 10:11:16 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.7583
2022-02-28 10:11:49 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.1539
2022-02-28 10:12:22 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.7862
2022-02-28 10:12:54 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.7220
2022-02-28 10:13:27 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.7545
2022-02-28 10:13:59 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.0658
2022-02-28 10:14:32 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 2.0264
2022-02-28 10:15:06 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.0072
2022-02-28 10:15:38 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.2366
2022-02-28 10:16:12 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.7870
2022-02-28 10:16:43 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.8749
2022-02-28 10:17:17 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9364
2022-02-28 10:17:49 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.0182
2022-02-28 10:18:22 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.8631
2022-02-28 10:18:54 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.9424
2022-02-28 10:19:27 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.0605
2022-02-28 10:20:00 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.8740
2022-02-28 10:20:34 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.1720
2022-02-28 10:21:06 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.0351
2022-02-28 10:21:40 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.6622
2022-02-28 10:22:13 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.8824
2022-02-28 10:22:47 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.9127
2022-02-28 10:23:19 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.0505
2022-02-28 10:23:53 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.9735
2022-02-28 10:24:27 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.1577
2022-02-28 10:25:00 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.8195
2022-02-28 10:25:33 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.9564
2022-02-28 10:26:07 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.8711
2022-02-28 10:26:41 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.9596
2022-02-28 10:27:13 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.0757
2022-02-28 10:27:45 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.9190
2022-02-28 10:28:18 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.2449
2022-02-28 10:28:51 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.6491
2022-02-28 10:29:24 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.0923
2022-02-28 10:29:57 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.1779
2022-02-28 10:30:30 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.9211
2022-02-28 10:31:04 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.8017
2022-02-28 10:31:37 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.8358
2022-02-28 10:32:11 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.8021
2022-02-28 10:32:44 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.1252
2022-02-28 10:33:17 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.9968
2022-02-28 10:33:50 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7397
2022-02-28 10:34:23 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.7904
2022-02-28 10:34:57 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.8544
2022-02-28 10:35:30 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.1970
2022-02-28 10:36:03 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.9176
2022-02-28 10:36:36 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.9635
2022-02-28 10:37:11 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.8464
2022-02-28 10:37:43 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.8719
2022-02-28 10:37:44 - train: epoch 042, train_loss: 1.9513
2022-02-28 10:38:59 - eval: epoch: 042, acc1: 60.802%, acc5: 83.338%, test_loss: 1.6445, per_image_load_time: 2.627ms, per_image_inference_time: 0.329ms
2022-02-28 10:38:59 - until epoch: 042, best_acc1: 61.136%
2022-02-28 10:38:59 - epoch 043 lr: 0.010000000000000002
2022-02-28 10:39:39 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.1423
2022-02-28 10:40:11 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.1135
2022-02-28 10:40:43 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.7311
2022-02-28 10:41:16 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.8312
2022-02-28 10:41:49 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 2.0438
2022-02-28 10:42:22 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.9035
2022-02-28 10:42:54 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.9752
2022-02-28 10:43:26 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.9233
2022-02-28 10:43:59 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.8235
2022-02-28 10:44:32 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.9914
2022-02-28 10:45:05 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 2.1925
2022-02-28 10:45:38 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.7626
2022-02-28 10:46:10 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.1208
2022-02-28 10:46:43 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.7371
2022-02-28 10:47:15 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.8923
2022-02-28 10:47:48 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.9697
2022-02-28 10:48:20 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.0974
2022-02-28 10:48:53 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.0363
2022-02-28 10:49:25 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.9182
2022-02-28 10:49:58 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.9396
2022-02-28 10:50:30 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.9872
2022-02-28 10:51:03 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.1979
2022-02-28 10:51:35 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.1307
2022-02-28 10:52:08 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.8940
2022-02-28 10:52:41 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.1468
2022-02-28 10:53:14 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.8222
2022-02-28 10:53:46 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.0649
2022-02-28 10:54:19 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.7423
2022-02-28 10:54:52 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7947
2022-02-28 10:55:25 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.1459
2022-02-28 10:55:57 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.1003
2022-02-28 10:56:31 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.9377
2022-02-28 10:57:02 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.8251
2022-02-28 10:57:35 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 2.2314
2022-02-28 10:58:08 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.1436
2022-02-28 10:58:41 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.1692
2022-02-28 10:59:13 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.8051
2022-02-28 10:59:46 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.0491
2022-02-28 11:00:19 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.7758
2022-02-28 11:00:51 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.0269
2022-02-28 11:01:24 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.1554
2022-02-28 11:01:56 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.9373
2022-02-28 11:02:29 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.0474
2022-02-28 11:03:02 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.0686
2022-02-28 11:03:34 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.9257
2022-02-28 11:04:07 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.9059
2022-02-28 11:04:40 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.1686
2022-02-28 11:05:13 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.0171
2022-02-28 11:05:45 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.0484
2022-02-28 11:06:17 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 2.1130
2022-02-28 11:06:18 - train: epoch 043, train_loss: 1.9503
2022-02-28 11:07:31 - eval: epoch: 043, acc1: 60.504%, acc5: 83.116%, test_loss: 1.6522, per_image_load_time: 2.323ms, per_image_inference_time: 0.381ms
2022-02-28 11:07:31 - until epoch: 043, best_acc1: 61.136%
2022-02-28 11:07:31 - epoch 044 lr: 0.010000000000000002
2022-02-28 11:08:09 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.7928
2022-02-28 11:08:43 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.0751
2022-02-28 11:09:15 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.8755
2022-02-28 11:09:49 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.6200
2022-02-28 11:10:21 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.9406
2022-02-28 11:10:54 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.9510
2022-02-28 11:11:27 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.8591
2022-02-28 11:12:00 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 2.1005
2022-02-28 11:12:33 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.7925
2022-02-28 11:13:06 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.9294
2022-02-28 11:13:38 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.6137
2022-02-28 11:14:12 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.8756
2022-02-28 11:14:46 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.1476
2022-02-28 11:15:19 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.9983
2022-02-28 11:15:51 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.9919
2022-02-28 11:16:24 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.7834
2022-02-28 11:16:57 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.9875
2022-02-28 11:17:31 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.9052
2022-02-28 11:18:03 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.0009
2022-02-28 11:18:37 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.8799
2022-02-28 11:19:10 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.1152
2022-02-28 11:19:43 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.0732
2022-02-28 11:20:17 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.9504
2022-02-28 11:20:50 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.9110
2022-02-28 11:21:22 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.9782
2022-02-28 11:21:55 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.1421
2022-02-28 11:22:28 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9555
2022-02-28 11:23:01 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.7345
2022-02-28 11:23:34 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.9113
2022-02-28 11:24:08 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.7025
2022-02-28 11:24:40 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.9595
2022-02-28 11:25:13 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.8729
2022-02-28 11:25:46 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.8990
2022-02-28 11:26:20 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.0254
2022-02-28 11:26:53 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.8721
2022-02-28 11:27:26 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.0014
2022-02-28 11:27:58 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.8268
2022-02-28 11:28:32 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.8178
2022-02-28 11:29:05 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.9378
2022-02-28 11:29:39 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.2123
2022-02-28 11:30:11 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.9576
2022-02-28 11:30:44 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.3624
2022-02-28 11:31:16 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.8886
2022-02-28 11:31:49 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.0780
2022-02-28 11:32:21 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.1732
2022-02-28 11:32:54 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.9655
2022-02-28 11:33:26 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.0680
2022-02-28 11:34:01 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.0346
2022-02-28 11:34:34 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.7208
2022-02-28 11:35:07 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.0026
2022-02-28 11:35:07 - train: epoch 044, train_loss: 1.9508
2022-02-28 11:36:21 - eval: epoch: 044, acc1: 60.738%, acc5: 83.398%, test_loss: 1.6471, per_image_load_time: 2.363ms, per_image_inference_time: 0.374ms
2022-02-28 11:36:21 - until epoch: 044, best_acc1: 61.136%
2022-02-28 11:36:21 - epoch 045 lr: 0.010000000000000002
2022-02-28 11:36:59 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.9089
2022-02-28 11:37:33 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.8151
2022-02-28 11:38:06 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.8024
2022-02-28 11:38:39 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.9129
2022-02-28 11:39:12 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.2024
2022-02-28 11:39:46 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.0169
2022-02-28 11:40:19 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.6078
2022-02-28 11:40:51 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.9628
2022-02-28 11:41:25 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.8839
2022-02-28 11:41:58 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.8998
2022-02-28 11:42:29 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.0998
2022-02-28 11:43:03 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.8473
2022-02-28 11:43:37 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9612
2022-02-28 11:44:10 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.8954
2022-02-28 11:44:43 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.9444
2022-02-28 11:45:16 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.8809
2022-02-28 11:45:49 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.8133
2022-02-28 11:46:21 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.9258
2022-02-28 11:46:54 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.9385
2022-02-28 11:47:28 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.0907
2022-02-28 11:48:00 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.0817
2022-02-28 11:48:34 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.9995
2022-02-28 11:49:07 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.7824
2022-02-28 11:49:39 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.9468
2022-02-28 11:50:12 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.9523
2022-02-28 11:50:45 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.9616
2022-02-28 11:51:18 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.8030
2022-02-28 11:51:51 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.9977
2022-02-28 11:52:24 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.0565
2022-02-28 11:52:57 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.1429
2022-02-28 11:53:31 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.9148
2022-02-28 11:54:04 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.1051
2022-02-28 11:54:37 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.1381
2022-02-28 11:55:10 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.8016
2022-02-28 11:55:43 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.1446
2022-02-28 11:56:16 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.9657
2022-02-28 11:56:50 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.9580
2022-02-28 11:57:23 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.9115
2022-02-28 11:57:56 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.2014
2022-02-28 11:58:29 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.0429
2022-02-28 11:59:03 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.9562
2022-02-28 11:59:35 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.1467
2022-02-28 12:00:08 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.9670
2022-02-28 12:00:41 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.0275
2022-02-28 12:01:15 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.9015
2022-02-28 12:01:47 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.0591
2022-02-28 12:02:21 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.8329
2022-02-28 12:02:53 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.7753
2022-02-28 12:03:28 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.9898
2022-02-28 12:04:00 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.6925
2022-02-28 12:04:01 - train: epoch 045, train_loss: 1.9490
2022-02-28 12:05:15 - eval: epoch: 045, acc1: 60.620%, acc5: 83.280%, test_loss: 1.6424, per_image_load_time: 2.138ms, per_image_inference_time: 0.354ms
2022-02-28 12:05:15 - until epoch: 045, best_acc1: 61.136%
2022-02-28 12:05:15 - epoch 046 lr: 0.010000000000000002
2022-02-28 12:05:53 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.8112
2022-02-28 12:06:26 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.7758
2022-02-28 12:06:58 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.8639
2022-02-28 12:07:32 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.9504
2022-02-28 12:08:04 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.7866
2022-02-28 12:08:37 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.0399
2022-02-28 12:09:10 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.8070
2022-02-28 12:09:44 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.0296
2022-02-28 12:10:16 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.9282
2022-02-28 12:10:49 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6338
2022-02-28 12:11:21 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8941
2022-02-28 12:11:55 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.8641
2022-02-28 12:12:28 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.8956
2022-02-28 12:13:01 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.0749
2022-02-28 12:13:34 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.7954
2022-02-28 12:14:07 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.1168
2022-02-28 12:14:40 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.1253
2022-02-28 12:15:14 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.1145
2022-02-28 12:15:46 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.9550
2022-02-28 12:16:19 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.7974
2022-02-28 12:16:51 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.2383
2022-02-28 12:17:25 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.8590
2022-02-28 12:17:58 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.0113
2022-02-28 12:18:32 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.8839
2022-02-28 12:19:04 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.9020
2022-02-28 12:19:38 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.2729
2022-02-28 12:20:11 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.8332
2022-02-28 12:20:44 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.7940
2022-02-28 12:21:17 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.0546
2022-02-28 12:21:50 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.7689
2022-02-28 12:22:23 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.7723
2022-02-28 12:22:57 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.9571
2022-02-28 12:23:29 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.9485
2022-02-28 12:24:03 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.9092
2022-02-28 12:24:36 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.2072
2022-02-28 12:25:09 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 2.0343
2022-02-28 12:25:42 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.8223
2022-02-28 12:26:15 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.9204
2022-02-28 12:26:48 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.8375
2022-02-28 12:27:22 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.8557
2022-02-28 12:27:55 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.0755
2022-02-28 12:28:28 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.8851
2022-02-28 12:29:01 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.1909
2022-02-28 12:29:34 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.0544
2022-02-28 12:30:08 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.7693
2022-02-28 12:30:40 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9532
2022-02-28 12:31:14 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.9807
2022-02-28 12:31:47 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.7107
2022-02-28 12:32:21 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.2555
2022-02-28 12:32:54 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.8379
2022-02-28 12:32:55 - train: epoch 046, train_loss: 1.9517
2022-02-28 12:34:09 - eval: epoch: 046, acc1: 60.640%, acc5: 83.196%, test_loss: 1.6478, per_image_load_time: 1.163ms, per_image_inference_time: 0.379ms
2022-02-28 12:34:09 - until epoch: 046, best_acc1: 61.136%
2022-02-28 12:34:09 - epoch 047 lr: 0.010000000000000002
2022-02-28 12:34:48 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.0080
2022-02-28 12:35:20 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.1233
2022-02-28 12:35:53 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.9331
2022-02-28 12:36:25 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.6479
2022-02-28 12:36:59 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.7493
2022-02-28 12:37:32 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.0028
2022-02-28 12:38:05 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.8968
2022-02-28 12:38:38 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.9650
2022-02-28 12:39:10 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.1824
2022-02-28 12:39:43 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.9592
2022-02-28 12:40:17 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.0252
2022-02-28 12:40:49 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.9342
2022-02-28 12:41:22 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.9784
2022-02-28 12:41:54 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.1188
2022-02-28 12:42:27 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.8327
2022-02-28 12:42:58 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.9443
2022-02-28 12:43:31 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.8321
2022-02-28 12:44:04 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.3134
2022-02-28 12:44:37 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.7664
2022-02-28 12:45:09 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.1264
2022-02-28 12:45:41 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.9661
2022-02-28 12:46:14 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.0388
2022-02-28 12:46:46 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.9461
2022-02-28 12:47:19 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.7148
2022-02-28 12:47:51 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.9326
2022-02-28 12:48:22 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.1782
2022-02-28 12:48:55 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.7584
2022-02-28 12:49:27 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.9184
2022-02-28 12:49:58 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 2.0198
2022-02-28 12:50:31 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.0865
2022-02-28 12:51:03 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 2.0053
2022-02-28 12:51:36 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.0482
2022-02-28 12:52:07 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.8217
2022-02-28 12:52:40 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.9652
2022-02-28 12:53:12 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.1195
2022-02-28 12:53:45 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.8632
2022-02-28 12:54:16 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.8593
2022-02-28 12:54:49 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 2.0424
2022-02-28 12:55:21 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.9411
2022-02-28 12:55:54 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.9048
2022-02-28 12:56:26 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.0925
2022-02-28 12:56:58 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.9215
2022-02-28 12:57:31 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.8983
2022-02-28 12:58:04 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.9294
2022-02-28 12:58:36 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.9512
2022-02-28 12:59:08 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.8785
2022-02-28 12:59:40 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.0891
2022-02-28 13:00:13 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.9331
2022-02-28 13:00:45 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.1120
2022-02-28 13:01:17 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.9641
2022-02-28 13:01:18 - train: epoch 047, train_loss: 1.9477
2022-02-28 13:02:30 - eval: epoch: 047, acc1: 60.760%, acc5: 83.196%, test_loss: 1.6470, per_image_load_time: 2.409ms, per_image_inference_time: 0.357ms
2022-02-28 13:02:31 - until epoch: 047, best_acc1: 61.136%
2022-02-28 13:02:31 - epoch 048 lr: 0.010000000000000002
2022-02-28 13:03:09 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9511
2022-02-28 13:03:41 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.2177
2022-02-28 13:04:14 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.8440
2022-02-28 13:04:46 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.8724
2022-02-28 13:05:19 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.7781
2022-02-28 13:05:51 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.0866
2022-02-28 13:06:23 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.7967
2022-02-28 13:06:56 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.9824
2022-02-28 13:07:28 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.1659
2022-02-28 13:08:01 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.1088
2022-02-28 13:08:34 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.1487
2022-02-28 13:09:07 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.0389
2022-02-28 13:09:40 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.9168
2022-02-28 13:10:13 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.9912
2022-02-28 13:10:46 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.1402
2022-02-28 13:11:19 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.9527
2022-02-28 13:11:52 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8360
2022-02-28 13:12:25 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.8363
2022-02-28 13:12:58 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.9467
2022-02-28 13:13:30 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.9963
2022-02-28 13:14:03 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.0676
2022-02-28 13:14:36 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.1328
2022-02-28 13:15:09 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.7001
2022-02-28 13:15:42 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.1278
2022-02-28 13:16:14 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.9256
2022-02-28 13:16:47 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.0515
2022-02-28 13:17:19 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.9231
2022-02-28 13:17:52 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.7941
2022-02-28 13:18:24 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.0405
2022-02-28 13:18:57 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.8868
2022-02-28 13:19:30 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 2.2199
2022-02-28 13:20:03 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.8357
2022-02-28 13:20:36 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.1121
2022-02-28 13:21:08 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.8069
2022-02-28 13:21:42 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0661
2022-02-28 13:22:14 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.1289
2022-02-28 13:22:47 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.9889
2022-02-28 13:23:19 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.9832
2022-02-28 13:23:53 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.9279
2022-02-28 13:24:25 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.7073
2022-02-28 13:24:58 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.0806
2022-02-28 13:25:30 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.9375
2022-02-28 13:26:03 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.7472
2022-02-28 13:26:36 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.8721
2022-02-28 13:27:10 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.8827
2022-02-28 13:27:42 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.8124
2022-02-28 13:28:16 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.0318
2022-02-28 13:28:48 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.1651
2022-02-28 13:29:23 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.0183
2022-02-28 13:29:54 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.9052
2022-02-28 13:29:55 - train: epoch 048, train_loss: 1.9471
2022-02-28 13:31:08 - eval: epoch: 048, acc1: 60.494%, acc5: 83.288%, test_loss: 1.6499, per_image_load_time: 2.353ms, per_image_inference_time: 0.371ms
2022-02-28 13:31:09 - until epoch: 048, best_acc1: 61.136%
2022-02-28 13:31:09 - epoch 049 lr: 0.010000000000000002
2022-02-28 13:31:47 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.1974
2022-02-28 13:32:19 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.7163
2022-02-28 13:32:52 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.1356
2022-02-28 13:33:25 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.0420
2022-02-28 13:33:57 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.0715
2022-02-28 13:34:29 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.9763
2022-02-28 13:35:02 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.0023
2022-02-28 13:35:34 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.1055
2022-02-28 13:36:06 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.7510
2022-02-28 13:36:39 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.0416
2022-02-28 13:37:11 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.9261
2022-02-28 13:37:44 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.8407
2022-02-28 13:38:15 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.1063
2022-02-28 13:38:48 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.9706
2022-02-28 13:39:22 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.6787
2022-02-28 13:39:53 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.0553
2022-02-28 13:40:26 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.9739
2022-02-28 13:40:59 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.8849
2022-02-28 13:41:32 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.8940
2022-02-28 13:42:05 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 2.0306
2022-02-28 13:42:38 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.8451
2022-02-28 13:43:11 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.0689
2022-02-28 13:43:44 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.8193
2022-02-28 13:44:16 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.7278
2022-02-28 13:44:49 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.9530
2022-02-28 13:45:23 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.9874
2022-02-28 13:45:54 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.9779
2022-02-28 13:46:28 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.0680
2022-02-28 13:47:00 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 2.0015
2022-02-28 13:47:32 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.9811
2022-02-28 13:48:05 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 2.1061
2022-02-28 13:48:37 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.0945
2022-02-28 13:49:09 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.8886
2022-02-28 13:49:41 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.1666
2022-02-28 13:50:14 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.0364
2022-02-28 13:50:47 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.8756
2022-02-28 13:51:19 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.8508
2022-02-28 13:51:51 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.1011
2022-02-28 13:52:25 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.2229
2022-02-28 13:52:57 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.8653
2022-02-28 13:53:30 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.8342
2022-02-28 13:54:02 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.1264
2022-02-28 13:54:35 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.9495
2022-02-28 13:55:07 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.1813
2022-02-28 13:55:40 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.8812
2022-02-28 13:56:12 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.0279
2022-02-28 13:56:45 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.0745
2022-02-28 13:57:18 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.9379
2022-02-28 13:57:51 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.8710
2022-02-28 13:58:22 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.9691
2022-02-28 13:58:23 - train: epoch 049, train_loss: 1.9442
2022-02-28 13:59:36 - eval: epoch: 049, acc1: 60.828%, acc5: 83.250%, test_loss: 1.6431, per_image_load_time: 2.496ms, per_image_inference_time: 0.345ms
2022-02-28 13:59:36 - until epoch: 049, best_acc1: 61.136%
2022-02-28 13:59:36 - epoch 050 lr: 0.010000000000000002
2022-02-28 14:00:13 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.9202
2022-02-28 14:00:46 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.0501
2022-02-28 14:01:19 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.9050
2022-02-28 14:01:51 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.7626
2022-02-28 14:02:23 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.9304
2022-02-28 14:02:56 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.0243
2022-02-28 14:03:30 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.6513
2022-02-28 14:04:01 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.5529
2022-02-28 14:04:34 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.9530
2022-02-28 14:05:06 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.1694
2022-02-28 14:05:38 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.0067
2022-02-28 14:06:11 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.9389
2022-02-28 14:06:43 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.8702
2022-02-28 14:07:17 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.9732
2022-02-28 14:07:49 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.8057
2022-02-28 14:08:22 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.9450
2022-02-28 14:08:55 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.9457
2022-02-28 14:09:27 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.0086
2022-02-28 14:09:59 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.9434
2022-02-28 14:10:33 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.8003
2022-02-28 14:11:04 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.8118
2022-02-28 14:11:37 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.0871
2022-02-28 14:12:10 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.7465
2022-02-28 14:12:43 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.1039
2022-02-28 14:13:15 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.8026
2022-02-28 14:13:48 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.6790
2022-02-28 14:14:21 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.9372
2022-02-28 14:14:53 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.0242
2022-02-28 14:15:26 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.2560
2022-02-28 14:15:59 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.9554
2022-02-28 14:16:31 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.0041
2022-02-28 14:17:04 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.0929
2022-02-28 14:17:36 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.8900
2022-02-28 14:18:10 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.9821
2022-02-28 14:18:41 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 2.1020
2022-02-28 14:19:15 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.0812
2022-02-28 14:19:46 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.2426
2022-02-28 14:20:20 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.8664
2022-02-28 14:20:52 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.6908
2022-02-28 14:21:24 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.1402
2022-02-28 14:21:57 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.9569
2022-02-28 14:22:30 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.1272
2022-02-28 14:23:03 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.9735
2022-02-28 14:23:36 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.8296
2022-02-28 14:24:08 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.8624
2022-02-28 14:24:41 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.9370
2022-02-28 14:25:14 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.0922
2022-02-28 14:25:47 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.9721
2022-02-28 14:26:20 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.7717
2022-02-28 14:26:52 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.7941
2022-02-28 14:26:53 - train: epoch 050, train_loss: 1.9439
2022-02-28 14:28:06 - eval: epoch: 050, acc1: 60.602%, acc5: 83.344%, test_loss: 1.6513, per_image_load_time: 2.213ms, per_image_inference_time: 0.365ms
2022-02-28 14:28:06 - until epoch: 050, best_acc1: 61.136%
2022-02-28 14:28:06 - epoch 051 lr: 0.010000000000000002
2022-02-28 14:28:43 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.1647
2022-02-28 14:29:17 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.0923
2022-02-28 14:29:48 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.7841
2022-02-28 14:30:21 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.7220
2022-02-28 14:30:54 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.0202
2022-02-28 14:31:26 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.9220
2022-02-28 14:31:59 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.9087
2022-02-28 14:32:32 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.2655
2022-02-28 14:33:04 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.7433
2022-02-28 14:33:36 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.1088
2022-02-28 14:34:09 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.0616
2022-02-28 14:34:42 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.8726
2022-02-28 14:35:15 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.7572
2022-02-28 14:35:49 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.7184
2022-02-28 14:36:21 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.8093
2022-02-28 14:36:54 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.6591
2022-02-28 14:37:27 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.0574
2022-02-28 14:38:00 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.9318
2022-02-28 14:38:32 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.7446
2022-02-28 14:39:06 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.9822
2022-02-28 14:39:38 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.9675
2022-02-28 14:40:12 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.9017
2022-02-28 14:40:44 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.2544
2022-02-28 14:41:16 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.9171
2022-02-28 14:41:50 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.7994
2022-02-28 14:42:23 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.7711
2022-02-28 14:42:56 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.8201
2022-02-28 14:43:29 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.9270
2022-02-28 14:44:02 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.9924
2022-02-28 14:44:35 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.8295
2022-02-28 14:45:08 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.9381
2022-02-28 14:45:41 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.8656
2022-02-28 14:46:13 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.7203
2022-02-28 14:46:45 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.7816
2022-02-28 14:47:18 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.0040
2022-02-28 14:47:50 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.9105
2022-02-28 14:48:24 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.9898
2022-02-28 14:48:56 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 2.0839
2022-02-28 14:49:28 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.0103
2022-02-28 14:50:01 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 2.0829
2022-02-28 14:50:33 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.9011
2022-02-28 14:51:06 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.0965
2022-02-28 14:51:38 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.0484
2022-02-28 14:52:11 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.9677
2022-02-28 14:52:43 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.7291
2022-02-28 14:53:16 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.0624
2022-02-28 14:53:48 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.9056
2022-02-28 14:54:22 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.8276
2022-02-28 14:54:55 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.0442
2022-02-28 14:55:26 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 2.0027
2022-02-28 14:55:28 - train: epoch 051, train_loss: 1.9440
2022-02-28 14:56:40 - eval: epoch: 051, acc1: 60.822%, acc5: 83.360%, test_loss: 1.6432, per_image_load_time: 1.370ms, per_image_inference_time: 0.363ms
2022-02-28 14:56:40 - until epoch: 051, best_acc1: 61.136%
2022-02-28 14:56:40 - epoch 052 lr: 0.010000000000000002
2022-02-28 14:57:18 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.9144
2022-02-28 14:57:49 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.0250
2022-02-28 14:58:22 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.1125
2022-02-28 14:58:54 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.0584
2022-02-28 14:59:27 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.8942
2022-02-28 15:00:00 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.7838
2022-02-28 15:00:32 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.9248
2022-02-28 15:01:04 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.9363
2022-02-28 15:01:36 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.0578
2022-02-28 15:02:10 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.0848
2022-02-28 15:02:42 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.9298
2022-02-28 15:03:14 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.7133
2022-02-28 15:03:47 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.9674
2022-02-28 15:04:19 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.1598
2022-02-28 15:04:52 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.7307
2022-02-28 15:05:25 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.7038
2022-02-28 15:05:57 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.9281
2022-02-28 15:06:30 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.8289
2022-02-28 15:07:02 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.7142
2022-02-28 15:07:35 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.2016
2022-02-28 15:08:07 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.0533
2022-02-28 15:08:40 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.0630
2022-02-28 15:09:13 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.8124
2022-02-28 15:09:45 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.8839
2022-02-28 15:10:17 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.8687
2022-02-28 15:10:51 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.6613
2022-02-28 15:11:23 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.9417
2022-02-28 15:11:55 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.9697
2022-02-28 15:12:28 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.7282
2022-02-28 15:13:01 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.8357
2022-02-28 15:13:33 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.7899
2022-02-28 15:14:06 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.8852
2022-02-28 15:14:38 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.7745
2022-02-28 15:15:11 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.1739
2022-02-28 15:15:43 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.0960
2022-02-28 15:16:16 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.0845
2022-02-28 15:16:49 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.9078
2022-02-28 15:17:22 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.8026
2022-02-28 15:17:53 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.9799
2022-02-28 15:18:26 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.1282
2022-02-28 15:18:59 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.8386
2022-02-28 15:19:32 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.9002
2022-02-28 15:20:04 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 2.0967
2022-02-28 15:20:37 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.9059
2022-02-28 15:21:09 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.7616
2022-02-28 15:21:43 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.8999
2022-02-28 15:22:15 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.0535
2022-02-28 15:22:50 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.8885
2022-02-28 15:23:21 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.0527
2022-02-28 15:23:54 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.9777
2022-02-28 15:23:55 - train: epoch 052, train_loss: 1.9411
2022-02-28 15:25:09 - eval: epoch: 052, acc1: 60.646%, acc5: 83.176%, test_loss: 1.6500, per_image_load_time: 1.209ms, per_image_inference_time: 0.370ms
2022-02-28 15:25:09 - until epoch: 052, best_acc1: 61.136%
2022-02-28 15:25:09 - epoch 053 lr: 0.010000000000000002
2022-02-28 15:25:47 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.2735
2022-02-28 15:26:20 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.9308
2022-02-28 15:26:52 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.2605
2022-02-28 15:27:25 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.2200
2022-02-28 15:27:57 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.0584
2022-02-28 15:28:30 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.9249
2022-02-28 15:29:02 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.9710
2022-02-28 15:29:35 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.8970
2022-02-28 15:30:08 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.9809
2022-02-28 15:30:40 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.8087
2022-02-28 15:31:14 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.7617
2022-02-28 15:31:47 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.6699
2022-02-28 15:32:19 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.0325
2022-02-28 15:32:52 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.1400
2022-02-28 15:33:25 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.9171
2022-02-28 15:33:58 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.1276
2022-02-28 15:34:32 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.9836
2022-02-28 15:35:04 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.1255
2022-02-28 15:35:37 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.7548
2022-02-28 15:36:10 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.9075
2022-02-28 15:36:44 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.0762
2022-02-28 15:37:16 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.8918
2022-02-28 15:37:49 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.9096
2022-02-28 15:38:21 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.6729
2022-02-28 15:38:55 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.2253
2022-02-28 15:39:27 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.0681
2022-02-28 15:40:01 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.9141
2022-02-28 15:40:33 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.0797
2022-02-28 15:41:06 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.9247
2022-02-28 15:41:40 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.7005
2022-02-28 15:42:12 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.1332
2022-02-28 15:42:46 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.1334
2022-02-28 15:43:18 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.0348
2022-02-28 15:43:51 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.8972
2022-02-28 15:44:24 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.8988
2022-02-28 15:44:56 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 2.1096
2022-02-28 15:45:30 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.0592
2022-02-28 15:46:02 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.7537
2022-02-28 15:46:36 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.2637
2022-02-28 15:47:08 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.9337
2022-02-28 15:47:42 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.9663
2022-02-28 15:48:14 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.9757
2022-02-28 15:48:47 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.0770
2022-02-28 15:49:19 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.0609
2022-02-28 15:49:52 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.0205
2022-02-28 15:50:25 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.8427
2022-02-28 15:50:58 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.8671
2022-02-28 15:51:32 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.1943
2022-02-28 15:52:04 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.0210
2022-02-28 15:52:37 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.9995
2022-02-28 15:52:38 - train: epoch 053, train_loss: 1.9393
2022-02-28 15:53:51 - eval: epoch: 053, acc1: 60.788%, acc5: 83.332%, test_loss: 1.6410, per_image_load_time: 1.274ms, per_image_inference_time: 0.374ms
2022-02-28 15:53:51 - until epoch: 053, best_acc1: 61.136%
2022-02-28 15:53:51 - epoch 054 lr: 0.010000000000000002
2022-02-28 15:54:29 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.9429
2022-02-28 15:55:02 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.9744
2022-02-28 15:55:34 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.9824
2022-02-28 15:56:06 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.8718
2022-02-28 15:56:39 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 2.0103
2022-02-28 15:57:11 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.8963
2022-02-28 15:57:44 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.1132
2022-02-28 15:58:17 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.1574
2022-02-28 15:58:50 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.5632
2022-02-28 15:59:23 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.8205
2022-02-28 15:59:56 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.9210
2022-02-28 16:00:29 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.0613
2022-02-28 16:01:02 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 2.1422
2022-02-28 16:01:34 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.0271
2022-02-28 16:02:07 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.9951
2022-02-28 16:02:40 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.7471
2022-02-28 16:03:12 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.8541
2022-02-28 16:03:46 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.9137
2022-02-28 16:04:18 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.0426
2022-02-28 16:04:51 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.9509
2022-02-28 16:05:24 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.9118
2022-02-28 16:05:57 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.9671
2022-02-28 16:06:30 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.6634
2022-02-28 16:07:03 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.0861
2022-02-28 16:07:36 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.9007
2022-02-28 16:08:09 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.8816
2022-02-28 16:08:42 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.8891
2022-02-28 16:09:14 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.0457
2022-02-28 16:09:47 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.7335
2022-02-28 16:10:19 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.0010
2022-02-28 16:10:53 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.9980
2022-02-28 16:11:25 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.0006
2022-02-28 16:11:58 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.0447
2022-02-28 16:12:31 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.7845
2022-02-28 16:13:04 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.0714
2022-02-28 16:13:36 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.8976
2022-02-28 16:14:10 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.9290
2022-02-28 16:14:43 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.9191
2022-02-28 16:15:16 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.8546
2022-02-28 16:15:48 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 2.0323
2022-02-28 16:16:21 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.9337
2022-02-28 16:16:54 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.8585
2022-02-28 16:17:26 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.0485
2022-02-28 16:17:59 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.8156
2022-02-28 16:18:30 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.8418
2022-02-28 16:19:03 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.1084
2022-02-28 16:19:36 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.3513
2022-02-28 16:20:08 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.9636
2022-02-28 16:20:42 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.9303
2022-02-28 16:21:13 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.7898
2022-02-28 16:21:14 - train: epoch 054, train_loss: 1.9364
2022-02-28 16:22:28 - eval: epoch: 054, acc1: 60.672%, acc5: 83.176%, test_loss: 1.6494, per_image_load_time: 1.496ms, per_image_inference_time: 0.360ms
2022-02-28 16:22:28 - until epoch: 054, best_acc1: 61.136%
2022-02-28 16:22:28 - epoch 055 lr: 0.010000000000000002
2022-02-28 16:23:06 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.9096
2022-02-28 16:23:38 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.8911
2022-02-28 16:24:10 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.7286
2022-02-28 16:24:44 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.8570
2022-02-28 16:25:16 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.7062
2022-02-28 16:25:49 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.0852
2022-02-28 16:26:21 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.8711
2022-02-28 16:26:54 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.8627
2022-02-28 16:27:26 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.9988
2022-02-28 16:27:59 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.9470
2022-02-28 16:28:32 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.9371
2022-02-28 16:29:04 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.9200
2022-02-28 16:29:37 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.0990
2022-02-28 16:30:10 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.9844
2022-02-28 16:30:42 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.9126
2022-02-28 16:31:15 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.9564
2022-02-28 16:31:48 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.1318
2022-02-28 16:32:20 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.8145
2022-02-28 16:32:54 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.0965
2022-02-28 16:33:26 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.9559
2022-02-28 16:33:59 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.7406
2022-02-28 16:34:31 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.1398
2022-02-28 16:35:05 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.8291
2022-02-28 16:35:37 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.6958
2022-02-28 16:36:12 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 2.0882
2022-02-28 16:36:44 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.8342
2022-02-28 16:37:17 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.9234
2022-02-28 16:37:50 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.0723
2022-02-28 16:38:22 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.9505
2022-02-28 16:38:54 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.0163
2022-02-28 16:39:27 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.0616
2022-02-28 16:40:01 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.8667
2022-02-28 16:40:33 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.8368
2022-02-28 16:41:06 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.0785
2022-02-28 16:41:39 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.8497
2022-02-28 16:42:12 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.0676
2022-02-28 16:42:45 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.8667
2022-02-28 16:43:19 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 2.0120
2022-02-28 16:43:51 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.1969
2022-02-28 16:44:24 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.9075
2022-02-28 16:44:57 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 2.0572
2022-02-28 16:45:30 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.0274
2022-02-28 16:46:03 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.1156
2022-02-28 16:46:36 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.9477
2022-02-28 16:47:10 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.9472
2022-02-28 16:47:42 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.9074
2022-02-28 16:48:15 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.7765
2022-02-28 16:48:48 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.9490
2022-02-28 16:49:22 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.0677
2022-02-28 16:49:54 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.9936
2022-02-28 16:49:55 - train: epoch 055, train_loss: 1.9359
2022-02-28 16:51:08 - eval: epoch: 055, acc1: 60.938%, acc5: 83.742%, test_loss: 1.6255, per_image_load_time: 1.347ms, per_image_inference_time: 0.366ms
2022-02-28 16:51:09 - until epoch: 055, best_acc1: 61.136%
2022-02-28 16:51:09 - epoch 056 lr: 0.010000000000000002
2022-02-28 16:51:46 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.8103
2022-02-28 16:52:18 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.9257
2022-02-28 16:52:52 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.8629
2022-02-28 16:53:24 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.8957
2022-02-28 16:53:58 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.9210
2022-02-28 16:54:31 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.8449
2022-02-28 16:55:04 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 2.0445
2022-02-28 16:55:37 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.0176
2022-02-28 16:56:09 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.0156
2022-02-28 16:56:43 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.9983
2022-02-28 16:57:16 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.7715
2022-02-28 16:57:49 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.1529
2022-02-28 16:58:22 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.0777
2022-02-28 16:58:55 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.9032
2022-02-28 16:59:28 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.0851
2022-02-28 17:00:01 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.8010
2022-02-28 17:00:34 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.0079
2022-02-28 17:01:06 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.2345
2022-02-28 17:01:40 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.0784
2022-02-28 17:02:13 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.7612
2022-02-28 17:02:45 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.9796
2022-02-28 17:03:18 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.0642
2022-02-28 17:03:51 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.9832
2022-02-28 17:04:25 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.8148
2022-02-28 17:04:57 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.0309
2022-02-28 17:05:29 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.9300
2022-02-28 17:06:03 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.8846
2022-02-28 17:06:35 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.9081
2022-02-28 17:07:07 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.8204
2022-02-28 17:07:40 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.0457
2022-02-28 17:08:13 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.9215
2022-02-28 17:08:46 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.7478
2022-02-28 17:09:19 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.9542
2022-02-28 17:09:52 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.8932
2022-02-28 17:10:24 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 2.0397
2022-02-28 17:10:57 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.8967
2022-02-28 17:11:30 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.0846
2022-02-28 17:12:02 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.0891
2022-02-28 17:12:35 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.4596
2022-02-28 17:13:08 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.9720
2022-02-28 17:13:40 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.0372
2022-02-28 17:14:14 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.0988
2022-02-28 17:14:46 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.8302
2022-02-28 17:15:19 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.0225
2022-02-28 17:15:51 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.8719
2022-02-28 17:16:24 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.9419
2022-02-28 17:16:57 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.0921
2022-02-28 17:17:30 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.0987
2022-02-28 17:18:02 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.0026
2022-02-28 17:18:34 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.9510
2022-02-28 17:18:36 - train: epoch 056, train_loss: 1.9338
2022-02-28 17:19:49 - eval: epoch: 056, acc1: 60.810%, acc5: 83.634%, test_loss: 1.6340, per_image_load_time: 0.858ms, per_image_inference_time: 0.365ms
2022-02-28 17:19:49 - until epoch: 056, best_acc1: 61.136%
2022-02-28 17:19:49 - epoch 057 lr: 0.010000000000000002
2022-02-28 17:20:27 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.8568
2022-02-28 17:21:00 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.0085
2022-02-28 17:21:32 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.9013
2022-02-28 17:22:04 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.8632
2022-02-28 17:22:37 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.7786
2022-02-28 17:23:09 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.0351
2022-02-28 17:23:43 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.7578
2022-02-28 17:24:14 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.8403
2022-02-28 17:24:47 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.8653
2022-02-28 17:25:20 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.7267
2022-02-28 17:25:52 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.9771
2022-02-28 17:26:26 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.9662
2022-02-28 17:26:58 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.9370
2022-02-28 17:27:30 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.8503
2022-02-28 17:28:03 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.8827
2022-02-28 17:28:35 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.0960
2022-02-28 17:29:08 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.9861
2022-02-28 17:29:41 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.9154
2022-02-28 17:30:14 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.9662
2022-02-28 17:30:47 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.9477
2022-02-28 17:31:19 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 2.0257
2022-02-28 17:31:52 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.8839
2022-02-28 17:32:25 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.9112
2022-02-28 17:32:57 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.8641
2022-02-28 17:33:30 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.0919
2022-02-28 17:34:03 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.8066
2022-02-28 17:34:35 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.7118
2022-02-28 17:35:07 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.6416
2022-02-28 17:35:40 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.9830
2022-02-28 17:36:12 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.2615
2022-02-28 17:36:45 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.1112
2022-02-28 17:37:17 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.0546
2022-02-28 17:37:50 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.0076
2022-02-28 17:38:23 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.8600
2022-02-28 17:38:56 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.1751
2022-02-28 17:39:29 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.9468
2022-02-28 17:40:02 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.7480
2022-02-28 17:40:34 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.7686
2022-02-28 17:41:07 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.0997
2022-02-28 17:41:39 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.7428
2022-02-28 17:42:12 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.9322
2022-02-28 17:42:44 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.9380
2022-02-28 17:43:16 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.6044
2022-02-28 17:43:50 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.0280
2022-02-28 17:44:22 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.0178
2022-02-28 17:44:55 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.8030
2022-02-28 17:45:28 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.7275
2022-02-28 17:46:00 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.2329
2022-02-28 17:46:34 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.0769
2022-02-28 17:47:05 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.9291
2022-02-28 17:47:06 - train: epoch 057, train_loss: 1.9294
2022-02-28 17:48:20 - eval: epoch: 057, acc1: 60.954%, acc5: 83.468%, test_loss: 1.6310, per_image_load_time: 2.340ms, per_image_inference_time: 0.373ms
2022-02-28 17:48:20 - until epoch: 057, best_acc1: 61.136%
2022-02-28 17:48:20 - epoch 058 lr: 0.010000000000000002
2022-02-28 17:48:58 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.9838
2022-02-28 17:49:31 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.9520
2022-02-28 17:50:04 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.9727
2022-02-28 17:50:36 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.8983
2022-02-28 17:51:08 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.7638
2022-02-28 17:51:42 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.0157
2022-02-28 17:52:13 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 2.0265
2022-02-28 17:52:47 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.9832
2022-02-28 17:53:19 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.8820
2022-02-28 17:53:51 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.9814
2022-02-28 17:54:24 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.7529
2022-02-28 17:54:57 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 2.0815
2022-02-28 17:55:29 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.9490
2022-02-28 17:56:02 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.2037
2022-02-28 17:56:35 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.9743
2022-02-28 17:57:08 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.7725
2022-02-28 17:57:40 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.0113
2022-02-28 17:58:13 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.0629
2022-02-28 17:58:45 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.0415
2022-02-28 17:59:18 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.0569
2022-02-28 17:59:50 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.9563
2022-02-28 18:00:23 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.8683
2022-02-28 18:00:55 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.9223
2022-02-28 18:01:28 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.0448
2022-02-28 18:02:00 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.9505
2022-02-28 18:02:34 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.9059
2022-02-28 18:03:06 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.9577
2022-02-28 18:03:39 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.7599
2022-02-28 18:04:12 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.8785
2022-02-28 18:04:44 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.1515
2022-02-28 18:05:17 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.8162
2022-02-28 18:05:49 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.7181
2022-02-28 18:06:21 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.8518
2022-02-28 18:06:55 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.7166
2022-02-28 18:07:26 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.8321
2022-02-28 18:07:59 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.9533
2022-02-28 18:08:32 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.9137
2022-02-28 18:09:04 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.0379
2022-02-28 18:09:38 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.8174
2022-02-28 18:10:11 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.0787
2022-02-28 18:10:43 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 2.0140
2022-02-28 18:11:17 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.7576
2022-02-28 18:11:50 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.8836
2022-02-28 18:12:23 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.8724
2022-02-28 18:12:55 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.9124
2022-02-28 18:13:29 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.7169
2022-02-28 18:14:01 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.8121
2022-02-28 18:14:36 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.8914
2022-02-28 18:15:08 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.9077
2022-02-28 18:15:41 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.8746
2022-02-28 18:15:42 - train: epoch 058, train_loss: 1.9303
2022-02-28 18:16:57 - eval: epoch: 058, acc1: 61.062%, acc5: 83.600%, test_loss: 1.6337, per_image_load_time: 2.520ms, per_image_inference_time: 0.344ms
2022-02-28 18:16:57 - until epoch: 058, best_acc1: 61.136%
2022-02-28 18:16:57 - epoch 059 lr: 0.010000000000000002
2022-02-28 18:17:35 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.7981
2022-02-28 18:18:08 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.8169
2022-02-28 18:18:39 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.7518
2022-02-28 18:19:11 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.8904
2022-02-28 18:19:44 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.7935
2022-02-28 18:20:17 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.8218
2022-02-28 18:20:49 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.8187
2022-02-28 18:21:21 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.9616
2022-02-28 18:21:54 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.9680
2022-02-28 18:22:27 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.0642
2022-02-28 18:22:59 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.3490
2022-02-28 18:23:31 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.6289
2022-02-28 18:24:05 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.2960
2022-02-28 18:24:37 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.1203
2022-02-28 18:25:09 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.8909
2022-02-28 18:25:42 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.7726
2022-02-28 18:26:14 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.9734
2022-02-28 18:26:46 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.8985
2022-02-28 18:27:19 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.9005
2022-02-28 18:27:52 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.6277
2022-02-28 18:28:25 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.1819
2022-02-28 18:28:56 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.8396
2022-02-28 18:29:29 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.9482
2022-02-28 18:30:01 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.0244
2022-02-28 18:30:34 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.8163
2022-02-28 18:31:06 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.7415
2022-02-28 18:31:38 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.7918
2022-02-28 18:32:11 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.1425
2022-02-28 18:32:44 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.9054
2022-02-28 18:33:16 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.2207
2022-02-28 18:33:48 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.8349
2022-02-28 18:34:21 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.8911
2022-02-28 18:34:54 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.8898
2022-02-28 18:35:25 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.2365
2022-02-28 18:35:57 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.8864
2022-02-28 18:36:31 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.0618
2022-02-28 18:37:03 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.8302
2022-02-28 18:37:36 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.7463
2022-02-28 18:38:08 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.7013
2022-02-28 18:38:41 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.0379
2022-02-28 18:39:12 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.7275
2022-02-28 18:39:45 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.0422
2022-02-28 18:40:17 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.3055
2022-02-28 18:40:50 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.0792
2022-02-28 18:41:23 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.9764
2022-02-28 18:41:57 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.9433
2022-02-28 18:42:30 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.7647
2022-02-28 18:43:03 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.8885
2022-02-28 18:43:37 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.9733
2022-02-28 18:44:09 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.0566
2022-02-28 18:44:10 - train: epoch 059, train_loss: 1.9256
2022-02-28 18:45:24 - eval: epoch: 059, acc1: 60.980%, acc5: 83.502%, test_loss: 1.6317, per_image_load_time: 1.458ms, per_image_inference_time: 0.354ms
2022-02-28 18:45:24 - until epoch: 059, best_acc1: 61.136%
2022-02-28 18:45:24 - epoch 060 lr: 0.010000000000000002
2022-02-28 18:46:02 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.0419
2022-02-28 18:46:34 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.7848
2022-02-28 18:47:07 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.8460
2022-02-28 18:47:40 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.0248
2022-02-28 18:48:13 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.1215
2022-02-28 18:48:45 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.1239
2022-02-28 18:49:19 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.7486
2022-02-28 18:49:51 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.0260
2022-02-28 18:50:25 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.9141
2022-02-28 18:50:58 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.6021
2022-02-28 18:51:31 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.9402
2022-02-28 18:52:05 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.7883
2022-02-28 18:52:37 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.9406
2022-02-28 18:53:10 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.0756
2022-02-28 18:53:43 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 2.0132
2022-02-28 18:54:16 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 2.0280
2022-02-28 18:54:48 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.9106
2022-02-28 18:55:21 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.9186
2022-02-28 18:55:54 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.0752
2022-02-28 18:56:28 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.8733
2022-02-28 18:57:01 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.1768
2022-02-28 18:57:34 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.9743
2022-02-28 18:58:08 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7838
2022-02-28 18:58:40 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.8729
2022-02-28 18:59:14 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.0602
2022-02-28 18:59:47 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 2.0857
2022-02-28 19:00:20 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.8134
2022-02-28 19:00:53 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.9307
2022-02-28 19:01:26 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.9974
2022-02-28 19:01:58 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.1172
2022-02-28 19:02:32 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.1394
2022-02-28 19:03:04 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.8223
2022-02-28 19:03:38 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.8286
2022-02-28 19:04:10 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 2.0586
2022-02-28 19:04:44 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.0223
2022-02-28 19:05:16 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.0279
2022-02-28 19:05:49 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.7826
2022-02-28 19:06:22 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.0080
2022-02-28 19:06:55 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.9374
2022-02-28 19:07:28 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.9856
2022-02-28 19:08:01 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.0181
2022-02-28 19:08:33 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.8277
2022-02-28 19:09:08 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.8060
2022-02-28 19:09:40 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.0279
2022-02-28 19:10:13 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.1969
2022-02-28 19:10:46 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.8278
2022-02-28 19:11:20 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.8491
2022-02-28 19:11:52 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.8939
2022-02-28 19:12:25 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.8728
2022-02-28 19:12:57 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.0168
2022-02-28 19:12:59 - train: epoch 060, train_loss: 1.9236
2022-02-28 19:14:12 - eval: epoch: 060, acc1: 60.828%, acc5: 83.558%, test_loss: 1.6286, per_image_load_time: 1.909ms, per_image_inference_time: 0.377ms
2022-02-28 19:14:12 - until epoch: 060, best_acc1: 61.136%
2022-02-28 19:14:12 - epoch 061 lr: 0.0010000000000000002
2022-02-28 19:14:49 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.5262
2022-02-28 19:15:21 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.8630
2022-02-28 19:15:53 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.7587
2022-02-28 19:16:26 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.9887
2022-02-28 19:16:58 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.8689
2022-02-28 19:17:31 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.4437
2022-02-28 19:18:03 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.7345
2022-02-28 19:18:37 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.6929
2022-02-28 19:19:09 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.5617
2022-02-28 19:19:43 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.5851
2022-02-28 19:20:17 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.5879
2022-02-28 19:20:49 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.6288
2022-02-28 19:21:23 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.6629
2022-02-28 19:21:55 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.7416
2022-02-28 19:22:29 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.7016
2022-02-28 19:23:01 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.5077
2022-02-28 19:23:35 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.7703
2022-02-28 19:24:08 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.7726
2022-02-28 19:24:41 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.9640
2022-02-28 19:25:14 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.8947
2022-02-28 19:25:47 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.0897
2022-02-28 19:26:20 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.5755
2022-02-28 19:26:52 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.6410
2022-02-28 19:27:25 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.6098
2022-02-28 19:27:59 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.6410
2022-02-28 19:28:31 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.8087
2022-02-28 19:29:04 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.9635
2022-02-28 19:29:39 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.7626
2022-02-28 19:30:13 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.8285
2022-02-28 19:30:45 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.9259
2022-02-28 19:31:17 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.7456
2022-02-28 19:31:49 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.7046
2022-02-28 19:32:21 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.7980
2022-02-28 19:32:54 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.5364
2022-02-28 19:33:27 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.9430
2022-02-28 19:34:00 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.6490
2022-02-28 19:34:33 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.7833
2022-02-28 19:35:06 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.6544
2022-02-28 19:35:40 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.6587
2022-02-28 19:36:13 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.7292
2022-02-28 19:36:46 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.9535
2022-02-28 19:37:20 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.6835
2022-02-28 19:37:52 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.7902
2022-02-28 19:38:26 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.8356
2022-02-28 19:38:58 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.6723
2022-02-28 19:39:33 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.7148
2022-02-28 19:40:04 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.8578
2022-02-28 19:40:37 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.7827
2022-02-28 19:41:10 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.8384
2022-02-28 19:41:42 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 2.0431
2022-02-28 19:41:43 - train: epoch 061, train_loss: 1.7588
2022-02-28 19:42:58 - eval: epoch: 061, acc1: 64.248%, acc5: 85.646%, test_loss: 1.4752, per_image_load_time: 1.829ms, per_image_inference_time: 0.360ms
2022-02-28 19:42:58 - until epoch: 061, best_acc1: 64.248%
2022-02-28 19:42:58 - epoch 062 lr: 0.0010000000000000002
2022-02-28 19:43:36 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.8315
2022-02-28 19:44:08 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.9275
2022-02-28 19:44:41 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.7460
2022-02-28 19:45:13 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.4783
2022-02-28 19:45:46 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.7816
2022-02-28 19:46:19 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.5619
2022-02-28 19:46:51 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.6352
2022-02-28 19:47:24 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.8942
2022-02-28 19:47:57 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.9053
2022-02-28 19:48:30 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.9300
2022-02-28 19:49:02 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.7275
2022-02-28 19:49:37 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.7913
2022-02-28 19:50:09 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.7498
2022-02-28 19:50:42 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.7196
2022-02-28 19:51:14 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.7101
2022-02-28 19:51:48 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.9707
2022-02-28 19:52:20 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.6410
2022-02-28 19:52:53 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.5405
2022-02-28 19:53:26 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.7330
2022-02-28 19:54:00 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.9071
2022-02-28 19:54:32 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.8167
2022-02-28 19:55:05 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.5235
2022-02-28 19:55:38 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.8223
2022-02-28 19:56:11 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.7979
2022-02-28 19:56:43 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.8608
2022-02-28 19:57:16 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.6196
2022-02-28 19:57:47 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.7097
2022-02-28 19:58:19 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.8941
2022-02-28 19:58:51 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.6125
2022-02-28 19:59:23 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 2.0040
2022-02-28 19:59:56 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.7167
2022-02-28 20:00:28 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.5914
2022-02-28 20:01:02 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.8551
2022-02-28 20:01:35 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.7433
2022-02-28 20:02:08 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.7667
2022-02-28 20:02:41 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.8343
2022-02-28 20:03:13 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.7182
2022-02-28 20:03:46 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.7797
2022-02-28 20:04:19 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.6195
2022-02-28 20:04:51 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.4205
2022-02-28 20:05:24 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.6695
2022-02-28 20:05:57 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.5364
2022-02-28 20:06:30 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.6021
2022-02-28 20:07:03 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.6328
2022-02-28 20:07:36 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.6760
2022-02-28 20:08:09 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.6944
2022-02-28 20:08:41 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.6996
2022-02-28 20:09:15 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.6160
2022-02-28 20:09:48 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.6916
2022-02-28 20:10:19 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.7821
2022-02-28 20:10:21 - train: epoch 062, train_loss: 1.7201
2022-02-28 20:11:35 - eval: epoch: 062, acc1: 64.770%, acc5: 85.946%, test_loss: 1.4598, per_image_load_time: 0.975ms, per_image_inference_time: 0.346ms
2022-02-28 20:11:35 - until epoch: 062, best_acc1: 64.770%
2022-02-28 20:11:35 - epoch 063 lr: 0.0010000000000000002
2022-02-28 20:12:12 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.7301
2022-02-28 20:12:45 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.7057
2022-02-28 20:13:17 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.8106
2022-02-28 20:13:49 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.6605
2022-02-28 20:14:22 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.4281
2022-02-28 20:14:54 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.5062
2022-02-28 20:15:27 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.8663
2022-02-28 20:15:59 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.8919
2022-02-28 20:16:31 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.9622
2022-02-28 20:17:04 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 2.0640
2022-02-28 20:17:36 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.4849
2022-02-28 20:18:10 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.8615
2022-02-28 20:18:42 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.4141
2022-02-28 20:19:15 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.9656
2022-02-28 20:19:47 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.5095
2022-02-28 20:20:20 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.5925
2022-02-28 20:20:53 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.4667
2022-02-28 20:21:26 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.6543
2022-02-28 20:21:58 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.8163
2022-02-28 20:22:33 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.3575
2022-02-28 20:23:05 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.6352
2022-02-28 20:23:38 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.9124
2022-02-28 20:24:10 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.8040
2022-02-28 20:24:43 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.7741
2022-02-28 20:25:17 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.7143
2022-02-28 20:25:50 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.5636
2022-02-28 20:26:23 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 2.0794
2022-02-28 20:26:57 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.7917
2022-02-28 20:27:30 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.7276
2022-02-28 20:28:01 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.9079
2022-02-28 20:28:35 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.8855
2022-02-28 20:29:07 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.7634
2022-02-28 20:29:41 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.6478
2022-02-28 20:30:13 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.5823
2022-02-28 20:30:46 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.8845
2022-02-28 20:31:18 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.4646
2022-02-28 20:31:52 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.7773
2022-02-28 20:32:25 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.8963
2022-02-28 20:32:58 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.5278
2022-02-28 20:33:32 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.5707
2022-02-28 20:34:04 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.6579
2022-02-28 20:34:37 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.8600
2022-02-28 20:35:10 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.5977
2022-02-28 20:35:43 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.6477
2022-02-28 20:36:17 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.6705
2022-02-28 20:36:49 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.5305
2022-02-28 20:37:23 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.6302
2022-02-28 20:37:56 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.6772
2022-02-28 20:38:29 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.6404
2022-02-28 20:39:01 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.6559
2022-02-28 20:39:03 - train: epoch 063, train_loss: 1.7065
2022-02-28 20:40:17 - eval: epoch: 063, acc1: 64.848%, acc5: 85.932%, test_loss: 1.4528, per_image_load_time: 2.489ms, per_image_inference_time: 0.366ms
2022-02-28 20:40:17 - until epoch: 063, best_acc1: 64.848%
2022-02-28 20:40:17 - epoch 064 lr: 0.0010000000000000002
2022-02-28 20:40:55 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.6070
2022-02-28 20:41:27 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.7887
2022-02-28 20:42:00 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.7198
2022-02-28 20:42:33 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.4550
2022-02-28 20:43:05 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.5698
2022-02-28 20:43:39 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.8683
2022-02-28 20:44:10 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.8906
2022-02-28 20:44:44 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.5574
2022-02-28 20:45:16 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.3954
2022-02-28 20:45:50 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.7953
2022-02-28 20:46:22 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.7012
2022-02-28 20:46:55 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.5280
2022-02-28 20:47:29 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.6806
2022-02-28 20:48:02 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.8560
2022-02-28 20:48:34 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.8731
2022-02-28 20:49:06 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.7194
2022-02-28 20:49:40 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.4915
2022-02-28 20:50:12 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.5089
2022-02-28 20:50:45 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.7848
2022-02-28 20:51:17 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.5200
2022-02-28 20:51:50 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.7524
2022-02-28 20:52:23 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.8294
2022-02-28 20:52:56 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.8129
2022-02-28 20:53:28 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.7248
2022-02-28 20:54:01 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.8580
2022-02-28 20:54:34 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.7596
2022-02-28 20:55:06 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.5985
2022-02-28 20:55:39 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.7011
2022-02-28 20:56:11 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.8230
2022-02-28 20:56:44 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.5221
2022-02-28 20:57:18 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.5764
2022-02-28 20:57:50 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.7007
2022-02-28 20:58:23 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.6000
2022-02-28 20:58:55 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.7547
2022-02-28 20:59:28 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.6618
2022-02-28 21:00:01 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.7316
2022-02-28 21:00:35 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.5049
2022-02-28 21:01:07 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.6877
2022-02-28 21:01:40 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.7073
2022-02-28 21:02:13 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.6938
2022-02-28 21:02:45 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.7694
2022-02-28 21:03:18 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.5817
2022-02-28 21:03:51 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.8265
2022-02-28 21:04:24 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.4732
2022-02-28 21:04:56 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.6071
2022-02-28 21:05:30 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.8097
2022-02-28 21:06:02 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.8102
2022-02-28 21:06:36 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.6979
2022-02-28 21:07:08 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.8780
2022-02-28 21:07:40 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.4904
2022-02-28 21:07:42 - train: epoch 064, train_loss: 1.6975
2022-02-28 21:08:56 - eval: epoch: 064, acc1: 65.138%, acc5: 86.086%, test_loss: 1.4456, per_image_load_time: 2.494ms, per_image_inference_time: 0.338ms
2022-02-28 21:08:56 - until epoch: 064, best_acc1: 65.138%
2022-02-28 21:08:56 - epoch 065 lr: 0.0010000000000000002
2022-02-28 21:09:33 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.7486
2022-02-28 21:10:07 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.7620
2022-02-28 21:10:39 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.6296
2022-02-28 21:11:12 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.7461
2022-02-28 21:11:43 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.6410
2022-02-28 21:12:17 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.6962
2022-02-28 21:12:49 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.9530
2022-02-28 21:13:22 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.7094
2022-02-28 21:13:54 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.5488
2022-02-28 21:14:28 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.6489
2022-02-28 21:15:00 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.7049
2022-02-28 21:15:33 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.9408
2022-02-28 21:16:05 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.8492
2022-02-28 21:16:39 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.5018
2022-02-28 21:17:12 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.7224
2022-02-28 21:17:45 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.8157
2022-02-28 21:18:18 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.8207
2022-02-28 21:18:52 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.5756
2022-02-28 21:19:24 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.4183
2022-02-28 21:19:57 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.5670
2022-02-28 21:20:30 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.5919
2022-02-28 21:21:03 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.9447
2022-02-28 21:21:37 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.6479
2022-02-28 21:22:09 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.9339
2022-02-28 21:22:43 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.7493
2022-02-28 21:23:16 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.6379
2022-02-28 21:23:49 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.7042
2022-02-28 21:24:22 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.7033
2022-02-28 21:24:56 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.6587
2022-02-28 21:25:29 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.6304
2022-02-28 21:26:02 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.6602
2022-02-28 21:26:36 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.8274
2022-02-28 21:27:10 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.6712
2022-02-28 21:27:43 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.7388
2022-02-28 21:28:17 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.9326
2022-02-28 21:28:49 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.6682
2022-02-28 21:29:22 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.5860
2022-02-28 21:29:54 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.4443
2022-02-28 21:30:27 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 2.0177
2022-02-28 21:30:59 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.6767
2022-02-28 21:31:32 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.7145
2022-02-28 21:32:04 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.8693
2022-02-28 21:32:36 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.7517
2022-02-28 21:33:08 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.7099
2022-02-28 21:33:41 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.5246
2022-02-28 21:34:13 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.8350
2022-02-28 21:34:47 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.7667
2022-02-28 21:35:18 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.4735
2022-02-28 21:35:52 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.5980
2022-02-28 21:36:22 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.7751
2022-02-28 21:36:24 - train: epoch 065, train_loss: 1.6896
2022-02-28 21:37:37 - eval: epoch: 065, acc1: 65.160%, acc5: 86.130%, test_loss: 1.4414, per_image_load_time: 1.138ms, per_image_inference_time: 0.373ms
2022-02-28 21:37:37 - until epoch: 065, best_acc1: 65.160%
2022-02-28 21:37:37 - epoch 066 lr: 0.0010000000000000002
2022-02-28 21:38:14 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.7486
2022-02-28 21:38:47 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.8433
2022-02-28 21:39:20 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.6411
2022-02-28 21:39:51 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.5847
2022-02-28 21:40:24 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.7690
2022-02-28 21:40:56 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.5948
2022-02-28 21:41:29 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.6000
2022-02-28 21:42:01 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.9072
2022-02-28 21:42:34 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.6103
2022-02-28 21:43:07 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.5470
2022-02-28 21:43:39 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.5224
2022-02-28 21:44:12 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.8074
2022-02-28 21:44:45 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.5971
2022-02-28 21:45:18 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.6562
2022-02-28 21:45:51 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.6777
2022-02-28 21:46:26 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.7202
2022-02-28 21:46:59 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.6802
2022-02-28 21:47:32 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.5815
2022-02-28 21:48:05 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.7932
2022-02-28 21:48:39 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.6462
2022-02-28 21:49:11 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.7604
2022-02-28 21:49:44 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.5078
2022-02-28 21:50:16 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.8795
2022-02-28 21:50:50 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.6664
2022-02-28 21:51:22 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.7565
2022-02-28 21:51:54 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.5300
2022-02-28 21:52:27 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.8287
2022-02-28 21:52:59 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.5802
2022-02-28 21:53:32 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.6068
2022-02-28 21:54:05 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.6110
2022-02-28 21:54:38 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.9615
2022-02-28 21:55:10 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.8090
2022-02-28 21:55:43 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.4773
2022-02-28 21:56:15 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.7174
2022-02-28 21:56:48 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.8445
2022-02-28 21:57:20 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.8151
2022-02-28 21:57:53 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.6156
2022-02-28 21:58:25 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.7701
2022-02-28 21:58:58 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.6395
2022-02-28 21:59:29 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.6750
2022-02-28 22:00:02 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.6594
2022-02-28 22:00:35 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.4766
2022-02-28 22:01:08 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.4751
2022-02-28 22:01:39 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.5962
2022-02-28 22:02:12 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.8795
2022-02-28 22:02:45 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.8328
2022-02-28 22:03:19 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.5130
2022-02-28 22:03:53 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.8097
2022-02-28 22:04:26 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.5761
2022-02-28 22:05:00 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.6505
2022-02-28 22:05:02 - train: epoch 066, train_loss: 1.6845
2022-02-28 22:06:16 - eval: epoch: 066, acc1: 65.154%, acc5: 86.154%, test_loss: 1.4385, per_image_load_time: 2.280ms, per_image_inference_time: 0.335ms
2022-02-28 22:06:16 - until epoch: 066, best_acc1: 65.160%
2022-03-01 07:57:46 - epoch 067 lr: 0.0010000000000000002
2022-03-01 07:58:25 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.6233
2022-03-01 07:58:59 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.5961
2022-03-01 07:59:31 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.7576
2022-03-01 08:00:03 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.5413
2022-03-01 08:00:37 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.4261
2022-03-01 08:01:10 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.4609
2022-03-01 08:01:43 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.7078
2022-03-01 08:02:15 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.8801
2022-03-01 08:02:48 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.9261
2022-03-01 08:03:21 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.8107
2022-03-01 08:03:53 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.6410
2022-03-01 08:04:26 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.7499
2022-03-01 08:04:59 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.6910
2022-03-01 08:05:32 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.7664
2022-03-01 08:06:06 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.5529
2022-03-01 08:06:38 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.7639
2022-03-01 08:07:11 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.6448
2022-03-01 08:07:43 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.7857
2022-03-01 08:08:16 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.6973
2022-03-01 08:08:49 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.7820
2022-03-01 08:09:22 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.4938
2022-03-01 08:09:55 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.5629
2022-03-01 08:10:28 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.6132
2022-03-01 08:11:01 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.5568
2022-03-01 08:11:35 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.7411
2022-03-01 08:12:08 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.7377
2022-03-01 08:12:41 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.6038
2022-03-01 08:13:15 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.8348
2022-03-01 08:13:48 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.5955
2022-03-01 08:14:21 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.6220
2022-03-01 08:14:54 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.4851
2022-03-01 08:15:27 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.7415
2022-03-01 08:16:01 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.5440
2022-03-01 08:16:33 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.7989
2022-03-01 08:17:07 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.7189
2022-03-01 08:17:40 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.7275
2022-03-01 08:18:14 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.7940
2022-03-01 08:18:47 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.7586
2022-03-01 08:19:21 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.7506
2022-03-01 08:19:54 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.8013
2022-03-01 08:20:27 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.7061
2022-03-01 08:21:00 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.8879
2022-03-01 08:21:33 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.3959
2022-03-01 08:22:06 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.5437
2022-03-01 08:22:40 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.5204
2022-03-01 08:23:12 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.5869
2022-03-01 08:23:47 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.9112
2022-03-01 08:24:22 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.5905
2022-03-01 08:24:56 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.6342
2022-03-01 08:25:27 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.7151
2022-03-01 08:25:28 - train: epoch 067, train_loss: 1.6778
2022-03-01 08:26:43 - eval: epoch: 067, acc1: 65.242%, acc5: 86.156%, test_loss: 1.4345, per_image_load_time: 1.582ms, per_image_inference_time: 0.374ms
2022-03-01 08:26:43 - until epoch: 067, best_acc1: 65.242%
2022-03-01 08:26:43 - epoch 068 lr: 0.0010000000000000002
2022-03-01 08:27:22 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.9640
2022-03-01 08:27:54 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.7575
2022-03-01 08:28:27 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.6751
2022-03-01 08:28:59 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.7386
2022-03-01 08:29:32 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.6037
2022-03-01 08:30:04 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.5417
2022-03-01 08:30:36 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.7659
2022-03-01 08:31:09 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.5710
2022-03-01 08:31:42 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.6593
2022-03-01 08:32:15 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.6141
2022-03-01 08:32:48 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.7174
2022-03-01 08:33:21 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.5662
2022-03-01 08:33:54 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.7338
2022-03-01 08:34:27 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.6217
2022-03-01 08:35:01 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.7446
2022-03-01 08:35:34 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.5578
2022-03-01 08:36:07 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.8841
2022-03-01 08:36:40 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.7866
2022-03-01 08:37:14 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.5636
2022-03-01 08:37:46 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.8399
2022-03-01 08:38:20 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.8365
2022-03-01 08:38:53 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.6375
2022-03-01 08:39:26 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.8055
2022-03-01 08:40:00 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.7233
2022-03-01 08:40:32 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.7166
2022-03-01 08:41:06 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.6490
2022-03-01 08:41:38 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.6615
2022-03-01 08:42:12 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.7321
2022-03-01 08:42:45 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.8731
2022-03-01 08:43:19 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.7787
2022-03-01 08:43:51 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.6195
2022-03-01 08:44:25 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.8942
2022-03-01 08:44:58 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.7341
2022-03-01 08:45:32 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.6717
2022-03-01 08:46:04 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.6535
2022-03-01 08:46:39 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.6647
2022-03-01 08:47:11 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.6738
2022-03-01 08:47:45 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.5469
2022-03-01 08:48:18 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.8033
2022-03-01 08:48:52 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.6577
2022-03-01 08:49:25 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.4075
2022-03-01 08:49:58 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.6870
2022-03-01 08:50:32 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.5892
2022-03-01 08:51:05 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.6861
2022-03-01 08:51:39 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.8358
2022-03-01 08:52:13 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.5531
2022-03-01 08:52:46 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.8243
2022-03-01 08:53:20 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.8217
2022-03-01 08:53:54 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.6858
2022-03-01 08:54:26 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.6464
2022-03-01 08:54:27 - train: epoch 068, train_loss: 1.6736
2022-03-01 08:55:41 - eval: epoch: 068, acc1: 65.296%, acc5: 86.306%, test_loss: 1.4309, per_image_load_time: 2.229ms, per_image_inference_time: 0.367ms
2022-03-01 08:55:41 - until epoch: 068, best_acc1: 65.296%
2022-03-01 08:55:41 - epoch 069 lr: 0.0010000000000000002
2022-03-01 08:56:19 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.7397
2022-03-01 08:56:51 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.7894
2022-03-01 08:57:24 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.7387
2022-03-01 08:57:56 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.6697
2022-03-01 08:58:29 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.9283
2022-03-01 08:59:02 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.5503
2022-03-01 08:59:36 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.8175
2022-03-01 09:00:08 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.7473
2022-03-01 09:00:41 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.4006
2022-03-01 09:01:14 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.5615
2022-03-01 09:01:48 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.6188
2022-03-01 09:02:21 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.5912
2022-03-01 09:02:54 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.9279
2022-03-01 09:03:27 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.5070
2022-03-01 09:04:00 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.6175
2022-03-01 09:04:33 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.8336
2022-03-01 09:05:06 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.6981
2022-03-01 09:05:39 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.6436
2022-03-01 09:06:13 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.6112
2022-03-01 09:06:45 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.8666
2022-03-01 09:07:19 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.6994
2022-03-01 09:07:51 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.5816
2022-03-01 09:08:25 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.4431
2022-03-01 09:08:57 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.7162
2022-03-01 09:09:30 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.4319
2022-03-01 09:10:04 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.6333
2022-03-01 09:10:36 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.7153
2022-03-01 09:11:09 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.8252
2022-03-01 09:11:42 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.5847
2022-03-01 09:12:16 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.3784
2022-03-01 09:12:49 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.9056
2022-03-01 09:13:21 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.5792
2022-03-01 09:13:53 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.7025
2022-03-01 09:14:26 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.5744
2022-03-01 09:14:59 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.6117
2022-03-01 09:15:32 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.4356
2022-03-01 09:16:06 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.8137
2022-03-01 09:16:39 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.7652
2022-03-01 09:17:11 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.6852
2022-03-01 09:17:45 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.8071
2022-03-01 09:18:18 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.5434
2022-03-01 09:18:51 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.6060
2022-03-01 09:19:24 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.7126
2022-03-01 09:19:58 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.7047
2022-03-01 09:20:30 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.8611
2022-03-01 09:21:04 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.8204
2022-03-01 09:21:37 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.7684
2022-03-01 09:22:11 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.8116
2022-03-01 09:22:44 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.6710
2022-03-01 09:23:17 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.6765
2022-03-01 09:23:18 - train: epoch 069, train_loss: 1.6739
2022-03-01 09:24:32 - eval: epoch: 069, acc1: 65.300%, acc5: 86.344%, test_loss: 1.4298, per_image_load_time: 1.888ms, per_image_inference_time: 0.385ms
2022-03-01 09:24:32 - until epoch: 069, best_acc1: 65.300%
2022-03-01 09:24:32 - epoch 070 lr: 0.0010000000000000002
2022-03-01 09:25:11 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.6172
2022-03-01 09:25:43 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.5577
2022-03-01 09:26:16 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.7567
2022-03-01 09:26:49 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.5107
2022-03-01 09:27:21 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.6269
2022-03-01 09:27:54 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.6069
2022-03-01 09:28:27 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.5824
2022-03-01 09:28:59 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.6474
2022-03-01 09:29:33 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.4037
2022-03-01 09:30:05 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.6027
2022-03-01 09:30:39 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.8838
2022-03-01 09:31:11 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.7028
2022-03-01 09:31:44 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.5867
2022-03-01 09:32:18 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.6043
2022-03-01 09:32:50 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.7323
2022-03-01 09:33:24 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.6918
2022-03-01 09:33:57 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.7666
2022-03-01 09:34:30 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.4841
2022-03-01 09:35:04 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.6822
2022-03-01 09:35:36 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.6403
2022-03-01 09:36:09 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.8110
2022-03-01 09:36:43 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.6709
2022-03-01 09:37:16 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.7790
2022-03-01 09:37:50 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.7319
2022-03-01 09:38:24 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.5922
2022-03-01 09:38:57 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.5628
2022-03-01 09:39:29 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.8806
2022-03-01 09:40:03 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.8943
2022-03-01 09:40:36 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.7944
2022-03-01 09:41:09 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.5780
2022-03-01 09:41:42 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.7187
2022-03-01 09:42:15 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.8310
2022-03-01 09:42:48 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.6342
2022-03-01 09:43:22 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.6223
2022-03-01 09:43:55 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.7519
2022-03-01 09:44:29 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.6140
2022-03-01 09:45:02 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.6024
2022-03-01 09:45:35 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.6561
2022-03-01 09:46:08 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.5141
2022-03-01 09:46:41 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.6004
2022-03-01 09:47:14 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.6204
2022-03-01 09:47:48 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.7375
2022-03-01 09:48:20 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.6783
2022-03-01 09:48:55 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.6380
2022-03-01 09:49:27 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.7777
2022-03-01 09:50:00 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.7308
2022-03-01 09:50:34 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.6335
2022-03-01 09:51:07 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.7651
2022-03-01 09:51:41 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.6375
2022-03-01 09:52:13 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.7742
2022-03-01 09:52:14 - train: epoch 070, train_loss: 1.6674
2022-03-01 09:53:29 - eval: epoch: 070, acc1: 65.318%, acc5: 86.340%, test_loss: 1.4278, per_image_load_time: 2.272ms, per_image_inference_time: 0.356ms
2022-03-01 09:53:29 - until epoch: 070, best_acc1: 65.318%
2022-03-01 09:53:29 - epoch 071 lr: 0.0010000000000000002
2022-03-01 09:54:08 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.5598
2022-03-01 09:54:41 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.6010
2022-03-01 09:55:13 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.6693
2022-03-01 09:55:46 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.8234
2022-03-01 09:56:20 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.8111
2022-03-01 09:56:52 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.7881
2022-03-01 09:57:25 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.7792
2022-03-01 09:57:58 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.5439
2022-03-01 09:58:31 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.8242
2022-03-01 09:59:03 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.7723
2022-03-01 09:59:36 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.9760
2022-03-01 10:00:09 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.6860
2022-03-01 10:00:42 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.6944
2022-03-01 10:01:16 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.5491
2022-03-01 10:01:49 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.5079
2022-03-01 10:02:23 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.4839
2022-03-01 10:02:55 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.4589
2022-03-01 10:03:27 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.6654
2022-03-01 10:04:01 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.7173
2022-03-01 10:04:34 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.6484
2022-03-01 10:05:07 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.5871
2022-03-01 10:05:40 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.3657
2022-03-01 10:06:14 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.5131
2022-03-01 10:06:47 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.5854
2022-03-01 10:07:20 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.8101
2022-03-01 10:07:53 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.6406
2022-03-01 10:08:27 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.7339
2022-03-01 10:08:59 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.7499
2022-03-01 10:09:33 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.5464
2022-03-01 10:10:06 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.6863
2022-03-01 10:10:39 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.5037
2022-03-01 10:11:11 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.6344
2022-03-01 10:11:45 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.7339
2022-03-01 10:12:18 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.4460
2022-03-01 10:12:51 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.7323
2022-03-01 10:13:23 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.8081
2022-03-01 10:13:57 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.8402
2022-03-01 10:14:30 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.6737
2022-03-01 10:15:04 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.7320
2022-03-01 10:15:36 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.6513
2022-03-01 10:16:10 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.6934
2022-03-01 10:16:43 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.7022
2022-03-01 10:17:16 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.8638
2022-03-01 10:17:49 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.8731
2022-03-01 10:18:22 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.5535
2022-03-01 10:18:56 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.5280
2022-03-01 10:19:29 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.6758
2022-03-01 10:20:03 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.6656
2022-03-01 10:20:36 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.5154
2022-03-01 10:21:09 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.7324
2022-03-01 10:21:10 - train: epoch 071, train_loss: 1.6652
2022-03-01 10:22:25 - eval: epoch: 071, acc1: 65.330%, acc5: 86.384%, test_loss: 1.4260, per_image_load_time: 1.896ms, per_image_inference_time: 0.356ms
2022-03-01 10:22:25 - until epoch: 071, best_acc1: 65.330%
2022-03-01 10:22:25 - epoch 072 lr: 0.0010000000000000002
2022-03-01 10:23:04 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.6483
2022-03-01 10:23:37 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.5979
2022-03-01 10:24:09 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.5427
2022-03-01 10:24:43 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.7893
2022-03-01 10:25:15 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.7439
2022-03-01 10:25:48 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.5838
2022-03-01 10:26:22 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.5689
2022-03-01 10:26:54 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.6513
2022-03-01 10:27:28 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.4355
2022-03-01 10:28:01 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.4811
2022-03-01 10:28:34 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.8353
2022-03-01 10:29:07 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.5329
2022-03-01 10:29:41 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.8380
2022-03-01 10:30:13 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.6710
2022-03-01 10:30:46 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.7205
2022-03-01 10:31:19 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.5307
2022-03-01 10:31:53 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.7436
2022-03-01 10:32:26 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.5304
2022-03-01 10:33:00 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.5382
2022-03-01 10:33:33 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.5162
2022-03-01 10:34:06 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.6178
2022-03-01 10:34:39 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 2.0032
2022-03-01 10:35:12 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.7814
2022-03-01 10:35:45 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.8225
2022-03-01 10:36:18 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.7669
2022-03-01 10:36:52 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.6694
2022-03-01 10:37:25 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.7114
2022-03-01 10:37:58 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.7892
2022-03-01 10:38:31 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.5203
2022-03-01 10:39:05 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.5061
2022-03-01 10:39:38 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.6434
2022-03-01 10:40:11 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.5385
2022-03-01 10:40:45 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.8110
2022-03-01 10:41:17 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.7940
2022-03-01 10:41:51 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.7731
2022-03-01 10:42:23 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.5967
2022-03-01 10:42:57 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.8285
2022-03-01 10:43:30 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.5805
2022-03-01 10:44:03 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.4434
2022-03-01 10:44:37 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.6668
2022-03-01 10:45:10 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.6764
2022-03-01 10:45:43 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.4587
2022-03-01 10:46:16 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.5716
2022-03-01 10:46:50 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.6871
2022-03-01 10:47:23 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.8670
2022-03-01 10:47:57 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.5560
2022-03-01 10:48:30 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.6043
2022-03-01 10:49:04 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.7459
2022-03-01 10:49:37 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.8137
2022-03-01 10:50:10 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.5087
2022-03-01 10:50:11 - train: epoch 072, train_loss: 1.6640
2022-03-01 10:51:26 - eval: epoch: 072, acc1: 65.318%, acc5: 86.356%, test_loss: 1.4219, per_image_load_time: 1.075ms, per_image_inference_time: 0.361ms
2022-03-01 10:51:26 - until epoch: 072, best_acc1: 65.330%
2022-03-01 10:51:26 - epoch 073 lr: 0.0010000000000000002
2022-03-01 10:52:04 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.7340
2022-03-01 10:52:37 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.7275
2022-03-01 10:53:09 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.6998
2022-03-01 10:53:42 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.4231
2022-03-01 10:54:15 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.6082
2022-03-01 10:54:49 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.7117
2022-03-01 10:55:22 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.6190
2022-03-01 10:55:55 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.6640
2022-03-01 10:56:29 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.7017
2022-03-01 10:57:01 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.6484
2022-03-01 10:57:35 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.5406
2022-03-01 10:58:07 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.7825
2022-03-01 10:58:42 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.7768
2022-03-01 10:59:14 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.6767
2022-03-01 10:59:48 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.6318
2022-03-01 11:00:22 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.5982
2022-03-01 11:00:54 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.9766
2022-03-01 11:01:27 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.5681
2022-03-01 11:02:01 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.5605
2022-03-01 11:02:34 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.4603
2022-03-01 11:03:07 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.7733
2022-03-01 11:03:40 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.5558
2022-03-01 11:04:14 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.5651
2022-03-01 11:04:47 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.6095
2022-03-01 11:05:22 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.7093
2022-03-01 11:05:54 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.7621
2022-03-01 11:06:28 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.7714
2022-03-01 11:07:01 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.7651
2022-03-01 11:07:34 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.8176
2022-03-01 11:08:08 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.4149
2022-03-01 11:08:41 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.6569
2022-03-01 11:09:13 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.5424
2022-03-01 11:09:47 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.5163
2022-03-01 11:10:20 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.5662
2022-03-01 11:10:54 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.6227
2022-03-01 11:11:26 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.5540
2022-03-01 11:12:00 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.5986
2022-03-01 11:12:33 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.7145
2022-03-01 11:13:07 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.4982
2022-03-01 11:13:40 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.7337
2022-03-01 11:14:14 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.6815
2022-03-01 11:14:47 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.6845
2022-03-01 11:15:21 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.6096
2022-03-01 11:15:53 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.6741
2022-03-01 11:16:27 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.4636
2022-03-01 11:17:00 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.7045
2022-03-01 11:17:34 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.5948
2022-03-01 11:18:07 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.5411
2022-03-01 11:18:41 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.4819
2022-03-01 11:19:13 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.6585
2022-03-01 11:19:14 - train: epoch 073, train_loss: 1.6590
2022-03-01 11:20:29 - eval: epoch: 073, acc1: 65.454%, acc5: 86.452%, test_loss: 1.4229, per_image_load_time: 2.522ms, per_image_inference_time: 0.365ms
2022-03-01 11:20:29 - until epoch: 073, best_acc1: 65.454%
2022-03-01 11:20:29 - epoch 074 lr: 0.0010000000000000002
2022-03-01 11:21:06 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.8154
2022-03-01 11:21:40 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.6505
2022-03-01 11:22:12 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.5770
2022-03-01 11:22:45 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.8834
2022-03-01 11:23:18 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.6983
2022-03-01 11:23:50 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.5220
2022-03-01 11:24:24 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.5365
2022-03-01 11:24:56 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.6901
2022-03-01 11:25:30 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.5261
2022-03-01 11:26:03 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.7882
2022-03-01 11:26:36 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.6272
2022-03-01 11:27:09 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.5599
2022-03-01 11:27:42 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.6296
2022-03-01 11:28:16 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.4520
2022-03-01 11:28:49 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.6791
2022-03-01 11:29:22 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.5937
2022-03-01 11:29:56 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.8109
2022-03-01 11:30:28 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.7205
2022-03-01 11:31:02 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.5494
2022-03-01 11:31:36 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.5120
2022-03-01 11:32:10 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.5622
2022-03-01 11:32:43 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.8367
2022-03-01 11:33:17 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.8233
2022-03-01 11:33:50 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.4825
2022-03-01 11:34:24 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.5651
2022-03-01 11:34:56 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.7582
2022-03-01 11:35:30 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.5792
2022-03-01 11:36:04 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.8064
2022-03-01 11:36:37 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.6604
2022-03-01 11:37:10 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.6955
2022-03-01 11:37:45 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.7859
2022-03-01 11:38:18 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.5406
2022-03-01 11:38:51 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.7714
2022-03-01 11:39:25 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.6706
2022-03-01 11:39:58 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.7084
2022-03-01 11:40:32 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.5866
2022-03-01 11:41:06 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.8279
2022-03-01 11:41:40 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.4638
2022-03-01 11:42:12 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.4819
2022-03-01 11:42:46 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.6501
2022-03-01 11:43:19 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.7201
2022-03-01 11:43:53 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.8118
2022-03-01 11:44:25 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.6150
2022-03-01 11:45:00 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.6134
2022-03-01 11:45:33 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.4612
2022-03-01 11:46:07 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.7724
2022-03-01 11:46:40 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.5480
2022-03-01 11:47:14 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.5907
2022-03-01 11:47:48 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 2.0032
2022-03-01 11:48:21 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.8973
2022-03-01 11:48:22 - train: epoch 074, train_loss: 1.6558
2022-03-01 11:49:38 - eval: epoch: 074, acc1: 65.412%, acc5: 86.450%, test_loss: 1.4192, per_image_load_time: 2.616ms, per_image_inference_time: 0.336ms
2022-03-01 11:49:38 - until epoch: 074, best_acc1: 65.454%
2022-03-01 11:49:38 - epoch 075 lr: 0.0010000000000000002
2022-03-01 11:50:16 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.8966
2022-03-01 11:50:50 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.6229
2022-03-01 11:51:23 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.6581
2022-03-01 11:51:57 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.6933
2022-03-01 11:52:30 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.7760
2022-03-01 11:53:03 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.5488
2022-03-01 11:53:36 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.8450
2022-03-01 11:54:11 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.6648
2022-03-01 11:54:43 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.7455
2022-03-01 11:55:17 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.4442
2022-03-01 11:55:51 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.6886
2022-03-01 11:56:24 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.6432
2022-03-01 11:56:59 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.6158
2022-03-01 11:57:31 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.4830
2022-03-01 11:58:05 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.6476
2022-03-01 11:58:39 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.5804
2022-03-01 11:59:12 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.7153
2022-03-01 11:59:46 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.6064
2022-03-01 12:00:20 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.6267
2022-03-01 12:00:53 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.7310
2022-03-01 12:01:27 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.5194
2022-03-01 12:02:02 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.7105
2022-03-01 12:02:35 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.5733
2022-03-01 12:03:09 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.7639
2022-03-01 12:03:42 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.6044
2022-03-01 12:04:16 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.8269
2022-03-01 12:04:49 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.5093
2022-03-01 12:05:24 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.6191
2022-03-01 12:05:57 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.6962
2022-03-01 12:06:31 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.7741
2022-03-01 12:07:04 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.6476
2022-03-01 12:07:38 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.6462
2022-03-01 12:08:11 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.5152
2022-03-01 12:08:45 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.4093
2022-03-01 12:09:19 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.6127
2022-03-01 12:09:53 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.7158
2022-03-01 12:10:26 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.8308
2022-03-01 12:11:01 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.6079
2022-03-01 12:11:34 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.6241
2022-03-01 12:12:08 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.5908
2022-03-01 12:12:42 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.5281
2022-03-01 12:13:16 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.6884
2022-03-01 12:13:50 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.8155
2022-03-01 12:14:24 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.4838
2022-03-01 12:14:58 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.5975
2022-03-01 12:15:32 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.6085
2022-03-01 12:16:05 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.8774
2022-03-01 12:16:39 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.5894
2022-03-01 12:17:13 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.4296
2022-03-01 12:17:46 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.6152
2022-03-01 12:17:47 - train: epoch 075, train_loss: 1.6539
2022-03-01 12:19:03 - eval: epoch: 075, acc1: 65.450%, acc5: 86.512%, test_loss: 1.4172, per_image_load_time: 2.583ms, per_image_inference_time: 0.364ms
2022-03-01 12:19:03 - until epoch: 075, best_acc1: 65.454%
2022-03-01 12:19:03 - epoch 076 lr: 0.0010000000000000002
2022-03-01 12:19:42 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.5894
2022-03-01 12:20:15 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.6104
2022-03-01 12:20:48 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.6859
2022-03-01 12:21:22 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.5416
2022-03-01 12:21:56 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.7036
2022-03-01 12:22:29 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.8617
2022-03-01 12:23:03 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.5448
2022-03-01 12:23:37 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.7118
2022-03-01 12:24:10 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.5672
2022-03-01 12:24:44 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.6283
2022-03-01 12:25:18 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.6633
2022-03-01 12:25:51 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.5770
2022-03-01 12:26:24 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.5901
2022-03-01 12:26:58 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.6020
2022-03-01 12:27:32 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.7022
2022-03-01 12:28:06 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.7614
2022-03-01 12:28:40 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.5781
2022-03-01 12:29:13 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.6179
2022-03-01 12:29:47 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.5275
2022-03-01 12:30:20 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.6974
2022-03-01 12:30:55 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.7997
2022-03-01 12:31:28 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.7243
2022-03-01 12:32:01 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.5707
2022-03-01 12:32:36 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.7994
2022-03-01 12:33:08 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.5092
2022-03-01 12:33:43 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.8278
2022-03-01 12:34:16 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.6144
2022-03-01 12:34:49 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.6776
2022-03-01 12:35:23 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.7080
2022-03-01 12:35:56 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.4282
2022-03-01 12:36:30 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.6631
2022-03-01 12:37:04 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.6274
2022-03-01 12:37:37 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.5281
2022-03-01 12:38:11 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.6541
2022-03-01 12:38:44 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.6989
2022-03-01 12:39:18 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.5233
2022-03-01 12:39:51 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.4869
2022-03-01 12:40:26 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.5218
2022-03-01 12:40:59 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.6296
2022-03-01 12:41:33 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.6284
2022-03-01 12:42:06 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.8650
2022-03-01 12:42:40 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.5731
2022-03-01 12:43:14 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.8688
2022-03-01 12:43:48 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.5870
2022-03-01 12:44:22 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.7438
2022-03-01 12:44:56 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.4687
2022-03-01 12:45:29 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.6816
2022-03-01 12:46:03 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.3933
2022-03-01 12:46:37 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.7943
2022-03-01 12:47:10 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.7091
2022-03-01 12:47:11 - train: epoch 076, train_loss: 1.6518
2022-03-01 12:48:28 - eval: epoch: 076, acc1: 65.686%, acc5: 86.484%, test_loss: 1.4153, per_image_load_time: 2.620ms, per_image_inference_time: 0.350ms
2022-03-01 12:48:28 - until epoch: 076, best_acc1: 65.686%
2022-03-01 12:48:28 - epoch 077 lr: 0.0010000000000000002
2022-03-01 12:49:07 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.7904
2022-03-01 12:49:40 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.7015
2022-03-01 12:50:13 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.4322
2022-03-01 12:50:46 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.6939
2022-03-01 12:51:20 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.5470
2022-03-01 12:51:53 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.4435
2022-03-01 12:52:26 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.7239
2022-03-01 12:53:00 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.6133
2022-03-01 12:53:34 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.6152
2022-03-01 12:54:07 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.5474
2022-03-01 12:54:41 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.7782
2022-03-01 12:55:14 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 2.0140
2022-03-01 12:55:48 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.4802
2022-03-01 12:56:21 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.6367
2022-03-01 12:56:55 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.5798
2022-03-01 12:57:28 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.4409
2022-03-01 12:58:02 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.8599
2022-03-01 12:58:36 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.6068
2022-03-01 12:59:10 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.4461
2022-03-01 12:59:43 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.7057
2022-03-01 13:00:18 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.8055
2022-03-01 13:00:50 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.6154
2022-03-01 13:01:24 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.8661
2022-03-01 13:01:57 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.7343
2022-03-01 13:02:32 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.6222
2022-03-01 13:03:05 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.3367
2022-03-01 13:03:39 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.6943
2022-03-01 13:04:12 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.7060
2022-03-01 13:04:46 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.8776
2022-03-01 13:05:19 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.7756
2022-03-01 13:05:54 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.6536
2022-03-01 13:06:27 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.8613
2022-03-01 13:07:01 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.7399
2022-03-01 13:07:34 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.7935
2022-03-01 13:08:09 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.5141
2022-03-01 13:08:42 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.6470
2022-03-01 13:09:16 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.4752
2022-03-01 13:09:49 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.5869
2022-03-01 13:10:23 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.5000
2022-03-01 13:10:57 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.6734
2022-03-01 13:11:31 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.7081
2022-03-01 13:12:05 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.6725
2022-03-01 13:12:39 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.5866
2022-03-01 13:13:11 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.6580
2022-03-01 13:13:45 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.8804
2022-03-01 13:14:18 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.4476
2022-03-01 13:14:51 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.6217
2022-03-01 13:15:27 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.6326
2022-03-01 13:16:02 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.7460
2022-03-01 13:16:34 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.6606
2022-03-01 13:16:35 - train: epoch 077, train_loss: 1.6532
2022-03-01 13:17:49 - eval: epoch: 077, acc1: 65.614%, acc5: 86.492%, test_loss: 1.4147, per_image_load_time: 2.463ms, per_image_inference_time: 0.352ms
2022-03-01 13:17:49 - until epoch: 077, best_acc1: 65.686%
2022-03-01 13:17:49 - epoch 078 lr: 0.0010000000000000002
2022-03-01 13:18:27 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.7220
2022-03-01 13:19:00 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.6401
2022-03-01 13:19:33 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.8601
2022-03-01 13:20:06 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.6230
2022-03-01 13:20:39 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.6078
2022-03-01 13:21:12 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.6195
2022-03-01 13:21:45 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.6139
2022-03-01 13:22:19 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.7843
2022-03-01 13:22:52 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.6217
2022-03-01 13:23:25 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.5043
2022-03-01 13:23:59 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.5132
2022-03-01 13:24:32 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.5945
2022-03-01 13:25:05 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.6892
2022-03-01 13:25:38 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.6589
2022-03-01 13:26:11 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.5610
2022-03-01 13:26:44 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.6798
2022-03-01 13:27:17 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.7266
2022-03-01 13:27:52 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.6318
2022-03-01 13:28:25 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.7506
2022-03-01 13:28:59 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.6251
2022-03-01 13:29:32 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.7816
2022-03-01 13:30:06 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.7803
2022-03-01 13:30:39 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.6850
2022-03-01 13:31:12 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.4815
2022-03-01 13:31:46 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.5489
2022-03-01 13:32:20 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.5613
2022-03-01 13:32:53 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.5021
2022-03-01 13:33:26 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.6952
2022-03-01 13:34:00 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.6035
2022-03-01 13:34:34 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.6679
2022-03-01 13:35:07 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.7265
2022-03-01 13:35:41 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.6029
2022-03-01 13:36:14 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.8059
2022-03-01 13:36:48 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.5688
2022-03-01 13:37:21 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.9059
2022-03-01 13:37:55 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.7698
2022-03-01 13:38:29 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.5978
2022-03-01 13:39:02 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.4794
2022-03-01 13:39:36 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.6687
2022-03-01 13:40:09 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.7157
2022-03-01 13:40:43 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.7254
2022-03-01 13:41:16 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.6196
2022-03-01 13:41:50 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.7162
2022-03-01 13:42:23 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.7224
2022-03-01 13:42:57 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.6611
2022-03-01 13:43:31 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.5006
2022-03-01 13:44:04 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.5024
2022-03-01 13:44:38 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 2.0076
2022-03-01 13:45:11 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.7445
2022-03-01 13:45:44 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.4686
2022-03-01 13:45:45 - train: epoch 078, train_loss: 1.6473
2022-03-01 13:47:00 - eval: epoch: 078, acc1: 65.712%, acc5: 86.468%, test_loss: 1.4124, per_image_load_time: 2.058ms, per_image_inference_time: 0.396ms
2022-03-01 13:47:00 - until epoch: 078, best_acc1: 65.712%
2022-03-01 13:47:00 - epoch 079 lr: 0.0010000000000000002
2022-03-01 13:47:39 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.4905
2022-03-01 13:48:12 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.6918
2022-03-01 13:48:45 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.6962
2022-03-01 13:49:17 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.8523
2022-03-01 13:49:51 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.5765
2022-03-01 13:50:24 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.6098
2022-03-01 13:50:59 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.6877
2022-03-01 13:51:31 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.6866
2022-03-01 13:52:05 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.8624
2022-03-01 13:52:38 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.6590
2022-03-01 13:53:11 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.7690
2022-03-01 13:53:45 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.7020
2022-03-01 13:54:18 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.4773
2022-03-01 13:54:51 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.6042
2022-03-01 13:55:25 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.6530
2022-03-01 13:55:59 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.5798
2022-03-01 13:56:32 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.5157
2022-03-01 13:57:07 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.6373
2022-03-01 13:57:40 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.5154
2022-03-01 13:58:13 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.9475
2022-03-01 13:58:46 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.5141
2022-03-01 13:59:20 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.6697
2022-03-01 13:59:53 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.6203
2022-03-01 14:00:26 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.6748
2022-03-01 14:00:59 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.4696
2022-03-01 14:01:32 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.5445
2022-03-01 14:02:05 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.4678
2022-03-01 14:02:38 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.6017
2022-03-01 14:03:11 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.6468
2022-03-01 14:03:45 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.4318
2022-03-01 14:04:18 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.6760
2022-03-01 14:04:51 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.8509
2022-03-01 14:05:25 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.6906
2022-03-01 14:05:58 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.5002
2022-03-01 14:06:32 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.7052
2022-03-01 14:07:05 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.4384
2022-03-01 14:07:39 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.6012
2022-03-01 14:08:12 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.6813
2022-03-01 14:08:46 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.4551
2022-03-01 14:09:19 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.4254
2022-03-01 14:09:53 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.7180
2022-03-01 14:10:26 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.5045
2022-03-01 14:10:59 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.4974
2022-03-01 14:11:33 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.7336
2022-03-01 14:12:06 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.9519
2022-03-01 14:12:40 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.8289
2022-03-01 14:13:13 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.5432
2022-03-01 14:13:46 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.6512
2022-03-01 14:14:20 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.7450
2022-03-01 14:14:53 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.4863
2022-03-01 14:14:54 - train: epoch 079, train_loss: 1.6472
2022-03-01 14:16:07 - eval: epoch: 079, acc1: 65.648%, acc5: 86.510%, test_loss: 1.4135, per_image_load_time: 1.697ms, per_image_inference_time: 0.383ms
2022-03-01 14:16:08 - until epoch: 079, best_acc1: 65.712%
2022-03-01 14:16:08 - epoch 080 lr: 0.0010000000000000002
2022-03-01 14:16:46 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.6198
2022-03-01 14:17:20 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.5722
2022-03-01 14:17:52 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.6426
2022-03-01 14:18:25 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.4480
2022-03-01 14:18:58 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.4798
2022-03-01 14:19:32 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.6106
2022-03-01 14:20:04 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.4719
2022-03-01 14:20:37 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.5403
2022-03-01 14:21:11 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.6804
2022-03-01 14:21:44 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.6708
2022-03-01 14:22:19 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.5950
2022-03-01 14:22:51 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.6945
2022-03-01 14:23:25 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.5199
2022-03-01 14:23:58 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.4979
2022-03-01 14:24:31 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.5074
2022-03-01 14:25:05 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.5826
2022-03-01 14:25:39 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.7711
2022-03-01 14:26:12 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.7690
2022-03-01 14:26:46 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.5006
2022-03-01 14:27:19 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.7654
2022-03-01 14:27:53 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.8100
2022-03-01 14:28:27 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.7537
2022-03-01 14:29:00 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.6720
2022-03-01 14:29:33 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.8231
2022-03-01 14:30:06 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.4201
2022-03-01 14:30:40 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.6516
2022-03-01 14:31:14 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.6169
2022-03-01 14:31:47 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.8771
2022-03-01 14:32:20 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.7253
2022-03-01 14:32:54 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.6706
2022-03-01 14:33:27 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.7338
2022-03-01 14:34:01 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.3238
2022-03-01 14:34:34 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.7129
2022-03-01 14:35:07 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.5920
2022-03-01 14:35:40 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.5705
2022-03-01 14:36:13 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.6072
2022-03-01 14:36:47 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.7037
2022-03-01 14:37:20 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.7746
2022-03-01 14:37:53 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.5742
2022-03-01 14:38:27 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.8042
2022-03-01 14:38:59 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.7308
2022-03-01 14:39:33 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.7713
2022-03-01 14:40:07 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.8672
2022-03-01 14:40:39 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.5627
2022-03-01 14:41:13 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.5682
2022-03-01 14:41:46 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.5423
2022-03-01 14:42:20 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.6200
2022-03-01 14:42:54 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.5940
2022-03-01 14:43:26 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.9333
2022-03-01 14:44:00 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.3930
2022-03-01 14:44:01 - train: epoch 080, train_loss: 1.6440
2022-03-01 14:45:15 - eval: epoch: 080, acc1: 65.778%, acc5: 86.506%, test_loss: 1.4121, per_image_load_time: 2.110ms, per_image_inference_time: 0.339ms
2022-03-01 14:45:15 - until epoch: 080, best_acc1: 65.778%
2022-03-01 14:45:15 - epoch 081 lr: 0.0010000000000000002
2022-03-01 14:45:53 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.5926
2022-03-01 14:46:26 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.6315
2022-03-01 14:46:59 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.6428
2022-03-01 14:47:32 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.6953
2022-03-01 14:48:05 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 2.0850
2022-03-01 14:48:37 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.4730
2022-03-01 14:49:11 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.6326
2022-03-01 14:49:43 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.7918
2022-03-01 14:50:18 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.7696
2022-03-01 14:50:51 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.8653
2022-03-01 14:51:24 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.5040
2022-03-01 14:51:57 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.6692
2022-03-01 14:52:32 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.5422
2022-03-01 14:53:04 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.4073
2022-03-01 14:53:38 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.7512
2022-03-01 14:54:12 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.5644
2022-03-01 14:54:46 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.6649
2022-03-01 14:55:20 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.5866
2022-03-01 14:55:53 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.5625
2022-03-01 14:56:28 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.8333
2022-03-01 14:57:01 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.8667
2022-03-01 14:57:35 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.5278
2022-03-01 14:58:07 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.3355
2022-03-01 14:58:42 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.7816
2022-03-01 14:59:15 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.8956
2022-03-01 14:59:50 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.6830
2022-03-01 15:00:23 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.7647
2022-03-01 15:00:57 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.5074
2022-03-01 15:01:32 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.5353
2022-03-01 15:02:05 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.5798
2022-03-01 15:02:38 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.5507
2022-03-01 15:03:12 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.5773
2022-03-01 15:03:46 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.4466
2022-03-01 15:04:20 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.6220
2022-03-01 15:04:53 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.5494
2022-03-01 15:05:27 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.4596
2022-03-01 15:06:02 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.7863
2022-03-01 15:06:35 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.6676
2022-03-01 15:07:09 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.7901
2022-03-01 15:07:43 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.4911
2022-03-01 15:08:16 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.6761
2022-03-01 15:08:51 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.7341
2022-03-01 15:09:24 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.8733
2022-03-01 15:09:59 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.7915
2022-03-01 15:10:32 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.8706
2022-03-01 15:11:06 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.4942
2022-03-01 15:11:39 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.7278
2022-03-01 15:12:14 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.6738
2022-03-01 15:12:48 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.7116
2022-03-01 15:13:21 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.5472
2022-03-01 15:13:22 - train: epoch 081, train_loss: 1.6448
2022-03-01 15:14:38 - eval: epoch: 081, acc1: 65.642%, acc5: 86.456%, test_loss: 1.4137, per_image_load_time: 2.206ms, per_image_inference_time: 0.359ms
2022-03-01 15:14:38 - until epoch: 081, best_acc1: 65.778%
2022-03-01 15:14:38 - epoch 082 lr: 0.0010000000000000002
2022-03-01 15:15:17 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.9249
2022-03-01 15:15:50 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.6392
2022-03-01 15:16:24 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.6407
2022-03-01 15:16:56 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.6530
2022-03-01 15:17:29 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.7155
2022-03-01 15:18:03 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.6023
2022-03-01 15:18:37 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.7566
2022-03-01 15:19:10 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.6390
2022-03-01 15:19:43 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.6460
2022-03-01 15:20:16 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.6940
2022-03-01 15:20:50 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.8613
2022-03-01 15:21:23 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.7419
2022-03-01 15:21:58 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.7983
2022-03-01 15:22:31 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.4445
2022-03-01 15:23:05 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.7247
2022-03-01 15:23:38 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.5617
2022-03-01 15:24:12 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.5422
2022-03-01 15:24:46 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.4020
2022-03-01 15:25:20 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.6681
2022-03-01 15:25:53 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.5184
2022-03-01 15:26:27 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.6943
2022-03-01 15:27:01 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.8243
2022-03-01 15:27:35 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.7104
2022-03-01 15:28:08 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.8524
2022-03-01 15:28:42 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.5852
2022-03-01 15:29:16 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.4260
2022-03-01 15:29:49 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.5252
2022-03-01 15:30:24 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.4197
2022-03-01 15:30:57 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.5770
2022-03-01 15:31:32 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.5596
2022-03-01 15:32:04 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.6428
2022-03-01 15:32:39 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.7456
2022-03-01 15:33:13 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.5970
2022-03-01 15:33:47 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.5266
2022-03-01 15:34:20 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.5253
2022-03-01 15:34:54 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.6717
2022-03-01 15:35:27 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.4165
2022-03-01 15:36:01 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.7818
2022-03-01 15:36:34 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.6567
2022-03-01 15:37:09 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.5428
2022-03-01 15:37:42 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.8708
2022-03-01 15:38:16 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.8985
2022-03-01 15:38:48 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.5007
2022-03-01 15:39:23 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.5043
2022-03-01 15:39:57 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.4196
2022-03-01 15:40:31 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.5776
2022-03-01 15:41:04 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.4801
2022-03-01 15:41:38 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.5260
2022-03-01 15:42:12 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.7661
2022-03-01 15:42:45 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.4782
2022-03-01 15:42:46 - train: epoch 082, train_loss: 1.6428
2022-03-01 15:44:01 - eval: epoch: 082, acc1: 65.758%, acc5: 86.516%, test_loss: 1.4103, per_image_load_time: 0.966ms, per_image_inference_time: 0.385ms
2022-03-01 15:44:01 - until epoch: 082, best_acc1: 65.778%
2022-03-01 15:44:01 - epoch 083 lr: 0.0010000000000000002
2022-03-01 15:44:40 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.6243
2022-03-01 15:45:13 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.4544
2022-03-01 15:45:46 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.6641
2022-03-01 15:46:18 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.6003
2022-03-01 15:46:53 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.8001
2022-03-01 15:47:25 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.5311
2022-03-01 15:47:59 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.6044
2022-03-01 15:48:32 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.5204
2022-03-01 15:49:06 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.7354
2022-03-01 15:49:40 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.7191
2022-03-01 15:50:13 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.5794
2022-03-01 15:50:47 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.8343
2022-03-01 15:51:21 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.8009
2022-03-01 15:51:55 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.7737
2022-03-01 15:52:28 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.5609
2022-03-01 15:53:01 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.6517
2022-03-01 15:53:35 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.7890
2022-03-01 15:54:09 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.7667
2022-03-01 15:54:42 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.5993
2022-03-01 15:55:17 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.4537
2022-03-01 15:55:51 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.7497
2022-03-01 15:56:24 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.4474
2022-03-01 15:56:59 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.5605
2022-03-01 15:57:32 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.4470
2022-03-01 15:58:07 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.4524
2022-03-01 15:58:40 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.5617
2022-03-01 15:59:13 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.8701
2022-03-01 15:59:48 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.6482
2022-03-01 16:00:21 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.6782
2022-03-01 16:00:56 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.7507
2022-03-01 16:01:30 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.6195
2022-03-01 16:02:03 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.6389
2022-03-01 16:02:38 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.6875
2022-03-01 16:03:12 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.4486
2022-03-01 16:03:46 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.8291
2022-03-01 16:04:19 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.6584
2022-03-01 16:04:53 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.6019
2022-03-01 16:05:27 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.4816
2022-03-01 16:06:01 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.8205
2022-03-01 16:06:34 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.7799
2022-03-01 16:07:09 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.5183
2022-03-01 16:07:42 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.8889
2022-03-01 16:08:16 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.6339
2022-03-01 16:08:50 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.6400
2022-03-01 16:09:24 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.6435
2022-03-01 16:09:59 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.6933
2022-03-01 16:10:32 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.8522
2022-03-01 16:11:07 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.7181
2022-03-01 16:11:42 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.7188
2022-03-01 16:12:14 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.5816
2022-03-01 16:12:15 - train: epoch 083, train_loss: 1.6409
2022-03-01 16:13:30 - eval: epoch: 083, acc1: 65.752%, acc5: 86.592%, test_loss: 1.4094, per_image_load_time: 2.470ms, per_image_inference_time: 0.373ms
2022-03-01 16:13:30 - until epoch: 083, best_acc1: 65.778%
2022-03-01 16:13:30 - epoch 084 lr: 0.0010000000000000002
2022-03-01 16:14:09 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.6158
2022-03-01 16:14:43 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.5662
2022-03-01 16:15:16 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.4811
2022-03-01 16:15:50 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.6769
2022-03-01 16:16:23 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.5728
2022-03-01 16:16:57 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.7681
2022-03-01 16:17:30 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.6377
2022-03-01 16:18:04 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.8776
2022-03-01 16:18:38 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.6251
2022-03-01 16:19:11 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.8002
2022-03-01 16:19:45 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.5688
2022-03-01 16:20:18 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.8572
2022-03-01 16:20:52 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.7310
2022-03-01 16:21:26 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.5809
2022-03-01 16:21:59 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.6751
2022-03-01 16:22:33 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.5653
2022-03-01 16:23:06 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.8595
2022-03-01 16:23:40 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.6305
2022-03-01 16:24:14 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.7228
2022-03-01 16:24:48 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.3562
2022-03-01 16:25:21 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.6025
2022-03-01 16:25:55 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.4827
2022-03-01 16:26:28 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.6771
2022-03-01 16:27:01 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.4658
2022-03-01 16:27:35 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.9798
2022-03-01 16:28:07 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.6143
2022-03-01 16:28:41 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.8685
2022-03-01 16:29:15 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.6219
2022-03-01 16:29:49 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.8678
2022-03-01 16:30:23 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.7158
2022-03-01 16:30:56 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.7846
2022-03-01 16:31:31 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.6165
2022-03-01 16:32:04 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.7872
2022-03-01 16:32:37 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.6793
2022-03-01 16:33:11 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.7356
2022-03-01 16:33:45 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.5052
2022-03-01 16:34:17 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.8392
2022-03-01 16:34:51 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.6453
2022-03-01 16:35:25 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.7426
2022-03-01 16:35:57 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.6372
2022-03-01 16:36:31 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.4456
2022-03-01 16:37:05 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.6979
2022-03-01 16:37:38 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.6797
2022-03-01 16:38:13 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.6867
2022-03-01 16:38:46 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.4918
2022-03-01 16:39:21 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.5750
2022-03-01 16:39:54 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.7008
2022-03-01 16:40:28 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.5769
2022-03-01 16:41:02 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.4069
2022-03-01 16:41:35 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.4968
2022-03-01 16:41:37 - train: epoch 084, train_loss: 1.6409
2022-03-01 16:42:52 - eval: epoch: 084, acc1: 65.788%, acc5: 86.582%, test_loss: 1.4082, per_image_load_time: 0.798ms, per_image_inference_time: 0.363ms
2022-03-01 16:42:52 - until epoch: 084, best_acc1: 65.788%
2022-03-01 16:42:52 - epoch 085 lr: 0.0010000000000000002
2022-03-01 16:43:30 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.6076
2022-03-01 16:44:04 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.4941
2022-03-01 16:44:38 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.8495
2022-03-01 16:45:10 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.6243
2022-03-01 16:45:44 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.4921
2022-03-01 16:46:17 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.5202
2022-03-01 16:46:51 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.7232
2022-03-01 16:47:23 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.5441
2022-03-01 16:47:56 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.5148
2022-03-01 16:48:29 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.6493
2022-03-01 16:49:02 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.6240
2022-03-01 16:49:34 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.5304
2022-03-01 16:50:08 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.6936
2022-03-01 16:50:42 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.5425
2022-03-01 16:51:16 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.6637
2022-03-01 16:51:49 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.6862
2022-03-01 16:52:23 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 2.0368
2022-03-01 16:52:56 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.2167
2022-03-01 16:53:30 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.4701
2022-03-01 16:54:03 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.5975
2022-03-01 16:54:38 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.4078
2022-03-01 16:55:11 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.6046
2022-03-01 16:55:45 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.5566
2022-03-01 16:56:18 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.6465
2022-03-01 16:56:52 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.7586
2022-03-01 16:57:25 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.7708
2022-03-01 16:58:00 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.6691
2022-03-01 16:58:33 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.8103
2022-03-01 16:59:07 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.6845
2022-03-01 16:59:40 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.9857
2022-03-01 17:00:14 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.7469
2022-03-01 17:00:48 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.8023
2022-03-01 17:01:21 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.7484
2022-03-01 17:01:55 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.6041
2022-03-01 17:02:29 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.7614
2022-03-01 17:03:02 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.9014
2022-03-01 17:03:37 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.3632
2022-03-01 17:04:09 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.6530
2022-03-01 17:04:44 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.7034
2022-03-01 17:05:17 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.6904
2022-03-01 17:05:51 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.5142
2022-03-01 17:06:24 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.7375
2022-03-01 17:06:57 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.5671
2022-03-01 17:07:32 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.5275
2022-03-01 17:08:05 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.6380
2022-03-01 17:08:39 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.5895
2022-03-01 17:09:12 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.7904
2022-03-01 17:09:47 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.4604
2022-03-01 17:10:20 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.6271
2022-03-01 17:10:53 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.5511
2022-03-01 17:10:55 - train: epoch 085, train_loss: 1.6379
2022-03-01 17:12:09 - eval: epoch: 085, acc1: 65.742%, acc5: 86.596%, test_loss: 1.4077, per_image_load_time: 1.207ms, per_image_inference_time: 0.396ms
2022-03-01 17:12:09 - until epoch: 085, best_acc1: 65.788%
2022-03-01 17:12:09 - epoch 086 lr: 0.0010000000000000002
2022-03-01 17:12:48 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.6508
2022-03-01 17:13:21 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.7588
2022-03-01 17:13:55 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.8420
2022-03-01 17:14:27 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.7216
2022-03-01 17:15:01 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.5590
2022-03-01 17:15:34 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.5192
2022-03-01 17:16:08 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.6355
2022-03-01 17:16:41 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.7783
2022-03-01 17:17:14 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.5698
2022-03-01 17:17:48 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.8101
2022-03-01 17:18:21 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.5832
2022-03-01 17:18:55 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.4845
2022-03-01 17:19:28 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.7089
2022-03-01 17:20:01 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.6445
2022-03-01 17:20:35 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.5259
2022-03-01 17:21:09 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.5829
2022-03-01 17:21:43 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.6350
2022-03-01 17:22:16 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.6420
2022-03-01 17:22:50 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.5920
2022-03-01 17:23:24 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.8793
2022-03-01 17:23:57 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.5597
2022-03-01 17:24:31 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.7237
2022-03-01 17:25:05 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.6554
2022-03-01 17:25:38 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.4496
2022-03-01 17:26:12 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.6436
2022-03-01 17:26:45 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.7823
2022-03-01 17:27:19 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.5381
2022-03-01 17:27:53 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.8367
2022-03-01 17:28:26 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.4509
2022-03-01 17:29:00 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.4870
2022-03-01 17:29:34 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.5048
2022-03-01 17:30:07 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.8859
2022-03-01 17:30:41 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.5668
2022-03-01 17:31:15 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.6894
2022-03-01 17:31:49 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.7185
2022-03-01 17:32:22 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.6975
2022-03-01 17:32:57 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.4926
2022-03-01 17:33:30 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.7058
2022-03-01 17:34:04 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.7323
2022-03-01 17:34:37 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.5868
2022-03-01 17:35:11 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.6703
2022-03-01 17:35:44 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.6922
2022-03-01 17:36:19 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.5812
2022-03-01 17:36:51 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.5697
2022-03-01 17:37:26 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.3856
2022-03-01 17:37:59 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.6901
2022-03-01 17:38:33 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.6407
2022-03-01 17:39:07 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.6217
2022-03-01 17:39:41 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.7303
2022-03-01 17:40:14 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.6416
2022-03-01 17:40:15 - train: epoch 086, train_loss: 1.6369
2022-03-01 17:41:31 - eval: epoch: 086, acc1: 65.646%, acc5: 86.512%, test_loss: 1.4104, per_image_load_time: 2.612ms, per_image_inference_time: 0.349ms
2022-03-01 17:41:31 - until epoch: 086, best_acc1: 65.788%
2022-03-01 17:41:31 - epoch 087 lr: 0.0010000000000000002
2022-03-01 17:42:11 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.4071
2022-03-01 17:42:43 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.6561
2022-03-01 17:43:16 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.6114
2022-03-01 17:43:48 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.7097
2022-03-01 17:44:21 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.6902
2022-03-01 17:44:54 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.7939
2022-03-01 17:45:28 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.6304
2022-03-01 17:46:01 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.5529
2022-03-01 17:46:34 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.6209
2022-03-01 17:47:08 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.6390
2022-03-01 17:47:41 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.6042
2022-03-01 17:48:15 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.7602
2022-03-01 17:48:48 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.9576
2022-03-01 17:49:22 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.4790
2022-03-01 17:49:55 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.4873
2022-03-01 17:50:28 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.4229
2022-03-01 17:51:02 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.6656
2022-03-01 17:51:35 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.8137
2022-03-01 17:52:08 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.6480
2022-03-01 17:52:42 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.7894
2022-03-01 17:53:16 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.5862
2022-03-01 17:53:49 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.6593
2022-03-01 17:54:23 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.8239
2022-03-01 17:54:57 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.5822
2022-03-01 17:55:30 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.5585
2022-03-01 17:56:04 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.5772
2022-03-01 17:56:38 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.5060
2022-03-01 17:57:11 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.6177
2022-03-01 17:57:45 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.5026
2022-03-01 17:58:19 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.7501
2022-03-01 17:58:52 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.5419
2022-03-01 17:59:26 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.6368
2022-03-01 17:59:59 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.6727
2022-03-01 18:00:33 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.6061
2022-03-01 18:01:06 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.5253
2022-03-01 18:01:39 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.3678
2022-03-01 18:02:12 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.5534
2022-03-01 18:02:45 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.6137
2022-03-01 18:03:18 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.5738
2022-03-01 18:03:51 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.5268
2022-03-01 18:04:24 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.5550
2022-03-01 18:04:58 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.5944
2022-03-01 18:05:32 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.5280
2022-03-01 18:06:05 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.6845
2022-03-01 18:06:39 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.5684
2022-03-01 18:07:13 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.6724
2022-03-01 18:07:46 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.6521
2022-03-01 18:08:21 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.6258
2022-03-01 18:08:53 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.4931
2022-03-01 18:09:27 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.4697
2022-03-01 18:09:28 - train: epoch 087, train_loss: 1.6337
2022-03-01 18:10:44 - eval: epoch: 087, acc1: 65.816%, acc5: 86.560%, test_loss: 1.4067, per_image_load_time: 2.530ms, per_image_inference_time: 0.355ms
2022-03-01 18:10:44 - until epoch: 087, best_acc1: 65.816%
2022-03-01 18:10:44 - epoch 088 lr: 0.0010000000000000002
2022-03-01 18:11:23 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.4304
2022-03-01 18:11:58 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.6516
2022-03-01 18:12:30 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.7752
2022-03-01 18:13:04 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.5916
2022-03-01 18:13:36 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.7882
2022-03-01 18:14:10 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.6710
2022-03-01 18:14:44 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.6700
2022-03-01 18:15:17 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.5798
2022-03-01 18:15:50 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.8606
2022-03-01 18:16:23 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.5149
2022-03-01 18:16:57 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.8435
2022-03-01 18:17:31 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.7845
2022-03-01 18:18:04 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.7544
2022-03-01 18:18:39 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.5256
2022-03-01 18:19:12 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.5382
2022-03-01 18:19:46 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.5151
2022-03-01 18:20:19 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.6102
2022-03-01 18:20:53 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.6146
2022-03-01 18:21:26 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.8055
2022-03-01 18:22:00 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.6433
2022-03-01 18:22:34 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.7898
2022-03-01 18:23:07 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.6692
2022-03-01 18:23:41 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.5318
2022-03-01 18:24:14 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.7653
2022-03-01 18:24:48 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.7791
2022-03-01 18:25:21 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.6359
2022-03-01 18:25:55 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.6498
2022-03-01 18:26:28 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.6826
2022-03-01 18:27:02 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.5581
2022-03-01 18:27:36 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.8177
2022-03-01 18:28:10 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.4428
2022-03-01 18:28:43 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.5091
2022-03-01 18:29:17 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.4405
2022-03-01 18:29:50 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.4237
2022-03-01 18:30:25 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.6218
2022-03-01 18:30:57 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.5560
2022-03-01 18:31:32 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.6069
2022-03-01 18:32:05 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.6835
2022-03-01 18:32:39 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.5354
2022-03-01 18:33:13 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.5773
2022-03-01 18:33:47 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.6573
2022-03-01 18:34:20 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.5853
2022-03-01 18:34:55 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.5634
2022-03-01 18:35:27 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.5319
2022-03-01 18:36:01 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.7950
2022-03-01 18:36:35 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.8660
2022-03-01 18:37:09 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.7092
2022-03-01 18:37:42 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.6338
2022-03-01 18:38:16 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.6275
2022-03-01 18:38:49 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.5655
2022-03-01 18:38:51 - train: epoch 088, train_loss: 1.6342
2022-03-01 18:40:05 - eval: epoch: 088, acc1: 65.872%, acc5: 86.606%, test_loss: 1.4027, per_image_load_time: 2.404ms, per_image_inference_time: 0.365ms
2022-03-01 18:40:05 - until epoch: 088, best_acc1: 65.872%
2022-03-01 18:40:05 - epoch 089 lr: 0.0010000000000000002
2022-03-01 18:40:44 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.7550
2022-03-01 18:41:16 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.5385
2022-03-01 18:41:49 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.7256
2022-03-01 18:42:23 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.5424
2022-03-01 18:42:56 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.5867
2022-03-01 18:43:29 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.4136
2022-03-01 18:44:02 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.5497
2022-03-01 18:44:36 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.8684
2022-03-01 18:45:09 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.7531
2022-03-01 18:45:43 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.8152
2022-03-01 18:46:16 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.6982
2022-03-01 18:46:50 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.7009
2022-03-01 18:47:23 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.6131
2022-03-01 18:47:56 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.8196
2022-03-01 18:48:30 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.6344
2022-03-01 18:49:03 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.6091
2022-03-01 18:49:38 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.5851
2022-03-01 18:50:10 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.6128
2022-03-01 18:50:44 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.5338
2022-03-01 18:51:17 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.5229
2022-03-01 18:51:50 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.7411
2022-03-01 18:52:24 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.6433
2022-03-01 18:52:57 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.5709
2022-03-01 18:53:31 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.8685
2022-03-01 18:54:05 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.7519
2022-03-01 18:54:38 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.5957
2022-03-01 18:55:11 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.4415
2022-03-01 18:55:45 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.7285
2022-03-01 18:56:18 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.7510
2022-03-01 18:56:52 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.5604
2022-03-01 18:57:25 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.5616
2022-03-01 18:57:59 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.7413
2022-03-01 18:58:31 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.8966
2022-03-01 18:59:05 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.5070
2022-03-01 18:59:39 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.2949
2022-03-01 19:00:12 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.5595
2022-03-01 19:00:45 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.6161
2022-03-01 19:01:18 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.6695
2022-03-01 19:01:53 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.6531
2022-03-01 19:02:26 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.7978
2022-03-01 19:03:00 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.7131
2022-03-01 19:03:33 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.5650
2022-03-01 19:04:07 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.7478
2022-03-01 19:04:40 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.5781
2022-03-01 19:05:14 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.3924
2022-03-01 19:05:48 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.6788
2022-03-01 19:06:22 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.5992
2022-03-01 19:06:56 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.3778
2022-03-01 19:07:30 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.6301
2022-03-01 19:08:03 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.2917
2022-03-01 19:08:04 - train: epoch 089, train_loss: 1.6315
2022-03-01 19:09:19 - eval: epoch: 089, acc1: 65.652%, acc5: 86.684%, test_loss: 1.4066, per_image_load_time: 1.464ms, per_image_inference_time: 0.372ms
2022-03-01 19:09:19 - until epoch: 089, best_acc1: 65.872%
2022-03-01 19:09:19 - epoch 090 lr: 0.0010000000000000002
2022-03-01 19:09:57 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.5569
2022-03-01 19:10:30 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.7841
2022-03-01 19:11:04 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.4827
2022-03-01 19:11:37 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.7597
2022-03-01 19:12:11 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.6115
2022-03-01 19:12:43 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.7757
2022-03-01 19:13:17 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.7319
2022-03-01 19:13:50 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.6102
2022-03-01 19:14:24 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.5493
2022-03-01 19:14:58 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.4237
2022-03-01 19:15:33 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.6591
2022-03-01 19:16:07 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.4931
2022-03-01 19:16:41 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.6509
2022-03-01 19:17:13 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.7864
2022-03-01 19:17:47 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.8942
2022-03-01 19:18:20 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.5949
2022-03-01 19:18:53 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.3377
2022-03-01 19:19:26 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.5115
2022-03-01 19:20:00 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.7236
2022-03-01 19:20:34 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.7742
2022-03-01 19:21:07 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.8428
2022-03-01 19:21:41 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.6395
2022-03-01 19:22:14 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.5637
2022-03-01 19:22:47 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.6564
2022-03-01 19:23:22 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.5019
2022-03-01 19:23:55 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.4969
2022-03-01 19:24:28 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.3734
2022-03-01 19:25:01 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.8630
2022-03-01 19:25:35 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.6746
2022-03-01 19:26:07 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.7606
2022-03-01 19:26:40 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.4724
2022-03-01 19:27:13 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.7344
2022-03-01 19:27:47 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.6596
2022-03-01 19:28:19 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.6391
2022-03-01 19:28:53 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.5272
2022-03-01 19:29:26 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.3432
2022-03-01 19:30:00 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.7595
2022-03-01 19:30:32 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.4110
2022-03-01 19:31:05 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.4384
2022-03-01 19:31:39 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.5388
2022-03-01 19:32:12 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.8862
2022-03-01 19:32:45 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.6984
2022-03-01 19:33:20 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.5176
2022-03-01 19:33:52 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.7763
2022-03-01 19:34:26 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.6220
2022-03-01 19:35:00 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.6350
2022-03-01 19:35:35 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.6040
2022-03-01 19:36:08 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.7371
2022-03-01 19:36:42 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.4681
2022-03-01 19:37:14 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.5134
2022-03-01 19:37:15 - train: epoch 090, train_loss: 1.6327
2022-03-01 19:38:32 - eval: epoch: 090, acc1: 65.892%, acc5: 86.618%, test_loss: 1.4014, per_image_load_time: 1.177ms, per_image_inference_time: 0.366ms
2022-03-01 19:38:32 - until epoch: 090, best_acc1: 65.892%
2022-03-01 19:38:32 - epoch 091 lr: 0.00010000000000000003
2022-03-01 19:39:11 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.5574
2022-03-01 19:39:44 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.6261
2022-03-01 19:40:18 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.5764
2022-03-01 19:40:51 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.7043
2022-03-01 19:41:24 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.6854
2022-03-01 19:41:57 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.7424
2022-03-01 19:42:31 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.5938
2022-03-01 19:43:04 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.3822
2022-03-01 19:43:38 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.7883
2022-03-01 19:44:11 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.4817
2022-03-01 19:44:45 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.4436
2022-03-01 19:45:18 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.6746
2022-03-01 19:45:50 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.5044
2022-03-01 19:46:21 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.8500
2022-03-01 19:46:54 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.7165
2022-03-01 19:47:26 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.5377
2022-03-01 19:47:58 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.7001
2022-03-01 19:48:30 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.8140
2022-03-01 19:49:02 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.6589
2022-03-01 19:49:34 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.6756
2022-03-01 19:50:05 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.5876
2022-03-01 19:50:37 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.6373
2022-03-01 19:51:10 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.6197
2022-03-01 19:51:41 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.5181
2022-03-01 19:52:14 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.6740
2022-03-01 19:52:45 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.5787
2022-03-01 19:53:17 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.6030
2022-03-01 19:53:49 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.5332
2022-03-01 19:54:22 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.6678
2022-03-01 19:54:53 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.6730
2022-03-01 19:55:25 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.5769
2022-03-01 19:55:57 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.7916
2022-03-01 19:56:28 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.4598
2022-03-01 19:57:01 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.6777
2022-03-01 19:57:33 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.5888
2022-03-01 19:58:04 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.5851
2022-03-01 19:58:37 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.8277
2022-03-01 19:59:08 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.5735
2022-03-01 19:59:40 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.5001
2022-03-01 20:00:12 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.5116
2022-03-01 20:00:44 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.6445
2022-03-01 20:01:16 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.7303
2022-03-01 20:01:48 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.5856
2022-03-01 20:02:21 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.4203
2022-03-01 20:02:52 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.5627
2022-03-01 20:03:25 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.6853
2022-03-01 20:03:57 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.6817
2022-03-01 20:04:30 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.6862
2022-03-01 20:05:01 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.6676
2022-03-01 20:05:33 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.9611
2022-03-01 20:05:34 - train: epoch 091, train_loss: 1.6049
2022-03-01 20:06:48 - eval: epoch: 091, acc1: 66.306%, acc5: 86.876%, test_loss: 1.3848, per_image_load_time: 0.585ms, per_image_inference_time: 0.301ms
2022-03-01 20:06:48 - until epoch: 091, best_acc1: 66.306%
2022-03-01 20:06:48 - epoch 092 lr: 0.00010000000000000003
2022-03-01 20:07:25 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.5260
2022-03-01 20:07:58 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.6042
2022-03-01 20:08:30 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.5455
2022-03-01 20:09:02 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.6442
2022-03-01 20:09:34 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.6737
2022-03-01 20:10:07 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.4799
2022-03-01 20:10:38 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.5674
2022-03-01 20:11:10 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.7374
2022-03-01 20:11:43 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.8317
2022-03-01 20:12:15 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.6012
2022-03-01 20:12:49 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.4507
2022-03-01 20:13:22 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.5596
2022-03-01 20:13:55 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.3392
2022-03-01 20:14:27 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.4571
2022-03-01 20:14:59 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.5586
2022-03-01 20:15:31 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.7706
2022-03-01 20:16:03 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.5298
2022-03-01 20:16:34 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.6161
2022-03-01 20:17:07 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.4940
2022-03-01 20:17:39 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.6067
2022-03-01 20:18:10 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.5463
2022-03-01 20:18:43 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.6381
2022-03-01 20:19:14 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.7637
2022-03-01 20:19:46 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.4837
2022-03-01 20:20:18 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.6351
2022-03-01 20:20:50 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.3768
2022-03-01 20:21:23 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.7410
2022-03-01 20:21:54 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.5705
2022-03-01 20:22:26 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.6135
2022-03-01 20:22:57 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.6054
2022-03-01 20:23:30 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.7054
2022-03-01 20:24:02 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.4556
2022-03-01 20:24:34 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.4415
2022-03-01 20:25:06 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.8108
2022-03-01 20:25:38 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.6145
2022-03-01 20:26:09 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.5456
2022-03-01 20:26:42 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.5163
2022-03-01 20:27:13 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.6027
2022-03-01 20:27:46 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.5644
2022-03-01 20:28:18 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.6448
2022-03-01 20:28:51 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.5783
2022-03-01 20:29:23 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.6065
2022-03-01 20:29:55 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.6124
2022-03-01 20:30:26 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.5188
2022-03-01 20:30:59 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.5132
2022-03-01 20:31:31 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.5339
2022-03-01 20:32:03 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.4629
2022-03-01 20:32:35 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.5848
2022-03-01 20:33:07 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.3853
2022-03-01 20:33:39 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.5454
2022-03-01 20:33:40 - train: epoch 092, train_loss: 1.5986
2022-03-01 20:34:53 - eval: epoch: 092, acc1: 66.336%, acc5: 86.910%, test_loss: 1.3844, per_image_load_time: 0.668ms, per_image_inference_time: 0.336ms
2022-03-01 20:34:53 - until epoch: 092, best_acc1: 66.336%
2022-03-01 20:34:53 - epoch 093 lr: 0.00010000000000000003
2022-03-01 20:35:31 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.5913
2022-03-01 20:36:03 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.4483
2022-03-01 20:36:36 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.5682
2022-03-01 20:37:07 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.6559
2022-03-01 20:37:39 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.7797
2022-03-01 20:38:10 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.5720
2022-03-01 20:38:43 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.6997
2022-03-01 20:39:15 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.7181
2022-03-01 20:39:47 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.3755
2022-03-01 20:40:18 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.6570
2022-03-01 20:40:49 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.7204
2022-03-01 20:41:22 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.6753
2022-03-01 20:41:54 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.5622
2022-03-01 20:42:26 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.6719
2022-03-01 20:42:58 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.7129
2022-03-01 20:43:30 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.6478
2022-03-01 20:44:01 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.5890
2022-03-01 20:44:33 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.6805
2022-03-01 20:45:04 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.4457
2022-03-01 20:45:36 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.4567
2022-03-01 20:46:08 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.4426
2022-03-01 20:46:40 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.7879
2022-03-01 20:47:12 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.6031
2022-03-01 20:47:44 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.4267
2022-03-01 20:48:16 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.6001
2022-03-01 20:48:47 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.7800
2022-03-01 20:49:20 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.4102
2022-03-01 20:49:50 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.6419
2022-03-01 20:50:24 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.3549
2022-03-01 20:50:55 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.3871
2022-03-01 20:51:27 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.6622
2022-03-01 20:51:59 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.4785
2022-03-01 20:52:31 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.6461
2022-03-01 20:53:02 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.6902
2022-03-01 20:53:34 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.6669
2022-03-01 20:54:06 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.5660
2022-03-01 20:54:38 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.3999
2022-03-01 20:55:10 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.5717
2022-03-01 20:55:41 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.6389
2022-03-01 20:56:14 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.7353
2022-03-01 20:56:45 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.7947
2022-03-01 20:57:17 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.6168
2022-03-01 20:57:49 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.7608
2022-03-01 20:58:21 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.4061
2022-03-01 20:58:53 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.6540
2022-03-01 20:59:25 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.3846
2022-03-01 20:59:57 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.7211
2022-03-01 21:00:29 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.5823
2022-03-01 21:01:01 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.4769
2022-03-01 21:01:32 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.5637
2022-03-01 21:01:34 - train: epoch 093, train_loss: 1.5967
2022-03-01 21:02:47 - eval: epoch: 093, acc1: 66.266%, acc5: 86.930%, test_loss: 1.3832, per_image_load_time: 0.596ms, per_image_inference_time: 0.346ms
2022-03-01 21:02:47 - until epoch: 093, best_acc1: 66.336%
2022-03-01 21:02:47 - epoch 094 lr: 0.00010000000000000003
2022-03-01 21:03:24 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.6360
2022-03-01 21:03:57 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.6728
2022-03-01 21:04:28 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.6750
2022-03-01 21:05:01 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.7419
2022-03-01 21:05:32 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.4220
2022-03-01 21:06:04 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.4256
2022-03-01 21:06:36 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.6448
2022-03-01 21:07:09 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.4908
2022-03-01 21:07:40 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.6376
2022-03-01 21:08:12 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.7170
2022-03-01 21:08:44 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.6740
2022-03-01 21:09:15 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.5852
2022-03-01 21:09:48 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.6251
2022-03-01 21:10:19 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.6467
2022-03-01 21:10:51 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.5852
2022-03-01 21:11:22 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.8865
2022-03-01 21:11:55 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.4649
2022-03-01 21:12:27 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.4665
2022-03-01 21:12:59 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.5699
2022-03-01 21:13:30 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.3737
2022-03-01 21:14:01 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.4327
2022-03-01 21:14:34 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.6524
2022-03-01 21:15:06 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.5615
2022-03-01 21:15:38 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.4635
2022-03-01 21:16:09 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.5479
2022-03-01 21:16:41 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.5694
2022-03-01 21:17:14 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.5449
2022-03-01 21:17:45 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.8098
2022-03-01 21:18:18 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.3987
2022-03-01 21:18:49 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.6090
2022-03-01 21:19:21 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.6501
2022-03-01 21:19:52 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.7540
2022-03-01 21:20:24 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.6025
2022-03-01 21:20:56 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.5903
2022-03-01 21:21:28 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.6982
2022-03-01 21:21:59 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.5207
2022-03-01 21:22:32 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.6577
2022-03-01 21:23:03 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.4361
2022-03-01 21:23:35 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.7014
2022-03-01 21:24:06 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.6270
2022-03-01 21:24:39 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.5566
2022-03-01 21:25:10 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.6090
2022-03-01 21:25:42 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.9341
2022-03-01 21:26:14 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.6171
2022-03-01 21:26:46 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.6132
2022-03-01 21:27:18 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.5741
2022-03-01 21:27:50 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.6830
2022-03-01 21:28:22 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.4662
2022-03-01 21:28:55 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.5910
2022-03-01 21:29:26 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.7037
2022-03-01 21:29:27 - train: epoch 094, train_loss: 1.5964
2022-03-01 21:30:40 - eval: epoch: 094, acc1: 66.390%, acc5: 86.916%, test_loss: 1.3813, per_image_load_time: 0.853ms, per_image_inference_time: 0.344ms
2022-03-01 21:30:41 - until epoch: 094, best_acc1: 66.390%
2022-03-01 21:30:41 - epoch 095 lr: 0.00010000000000000003
2022-03-01 21:31:18 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.8466
2022-03-01 21:31:51 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.7787
2022-03-01 21:32:24 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.5767
2022-03-01 21:32:58 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.4663
2022-03-01 21:33:29 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.6612
2022-03-01 21:34:02 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.6151
2022-03-01 21:34:33 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.6309
2022-03-01 21:35:06 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.6479
2022-03-01 21:35:38 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.7801
2022-03-01 21:36:11 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.6827
2022-03-01 21:36:43 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.5403
2022-03-01 21:37:15 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.4878
2022-03-01 21:37:46 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.4290
2022-03-01 21:38:19 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.5742
2022-03-01 21:38:50 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.5468
2022-03-01 21:39:22 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.4463
2022-03-01 21:39:53 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.4539
2022-03-01 21:40:26 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.5372
2022-03-01 21:40:58 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.6069
2022-03-01 21:41:30 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.6042
2022-03-01 21:42:01 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.7187
2022-03-01 21:42:33 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.2894
2022-03-01 21:43:05 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.6345
2022-03-01 21:43:38 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.5216
2022-03-01 21:44:09 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.4769
2022-03-01 21:44:41 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.4983
2022-03-01 21:45:13 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.6363
2022-03-01 21:45:45 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.5298
2022-03-01 21:46:17 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.6096
2022-03-01 21:46:50 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.7634
2022-03-01 21:47:21 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.6364
2022-03-01 21:47:53 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.6282
2022-03-01 21:48:24 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.5608
2022-03-01 21:48:56 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.3034
2022-03-01 21:49:28 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.3738
2022-03-01 21:50:00 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.3555
2022-03-01 21:50:32 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.4964
2022-03-01 21:51:05 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.4751
2022-03-01 21:51:36 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.6120
2022-03-01 21:52:08 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.5138
2022-03-01 21:52:39 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.5677
2022-03-01 21:53:11 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.5126
2022-03-01 21:53:43 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.7943
2022-03-01 21:54:15 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.9564
2022-03-01 21:54:47 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.5830
2022-03-01 21:55:20 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.5428
2022-03-01 21:55:51 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.5635
2022-03-01 21:56:24 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.4612
2022-03-01 21:56:55 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.4791
2022-03-01 21:57:27 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.5028
2022-03-01 21:57:28 - train: epoch 095, train_loss: 1.5913
2022-03-01 21:58:41 - eval: epoch: 095, acc1: 66.420%, acc5: 86.942%, test_loss: 1.3816, per_image_load_time: 0.628ms, per_image_inference_time: 0.341ms
2022-03-01 21:58:41 - until epoch: 095, best_acc1: 66.420%
2022-03-01 21:58:41 - epoch 096 lr: 0.00010000000000000003
2022-03-01 21:59:18 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.7731
2022-03-01 21:59:50 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.5259
2022-03-01 22:00:23 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.4851
2022-03-01 22:00:54 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.3405
2022-03-01 22:01:27 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.6074
2022-03-01 22:01:58 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.7181
2022-03-01 22:02:30 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.7747
2022-03-01 22:03:02 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.5128
2022-03-01 22:03:34 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.6354
2022-03-01 22:04:05 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.5310
2022-03-01 22:04:37 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.5331
2022-03-01 22:05:09 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.4464
2022-03-01 22:05:41 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.6159
2022-03-01 22:06:13 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.6119
2022-03-01 22:06:44 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.5288
2022-03-01 22:07:17 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.3894
2022-03-01 22:07:48 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.5395
2022-03-01 22:08:20 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.7706
2022-03-01 22:08:52 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.7848
2022-03-01 22:09:24 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.4732
2022-03-01 22:09:56 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.5897
2022-03-01 22:10:28 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.4156
2022-03-01 22:11:00 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.6014
2022-03-01 22:11:32 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.5310
2022-03-01 22:12:04 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.5282
2022-03-01 22:12:36 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.3686
2022-03-01 22:13:09 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.7296
2022-03-01 22:13:42 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.6002
2022-03-01 22:14:17 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.5834
2022-03-01 22:14:50 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.6371
2022-03-01 22:15:24 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.7112
2022-03-01 22:15:57 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.6254
2022-03-01 22:16:31 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.6815
2022-03-01 22:17:04 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.6062
2022-03-01 22:17:38 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.4213
2022-03-01 22:18:11 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.4048
2022-03-01 22:18:46 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.5569
2022-03-01 22:19:19 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.5447
2022-03-01 22:19:53 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.4983
2022-03-01 22:20:26 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.6648
