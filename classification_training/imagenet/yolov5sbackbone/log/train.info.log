2022-03-01 22:21:01 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.5506
2022-03-01 22:21:34 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.4928
2022-03-01 22:22:08 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.3485
2022-03-01 22:22:41 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.4903
2022-03-01 22:23:15 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.6845
2022-03-01 22:23:48 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.5241
2022-03-01 22:24:23 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.7329
2022-03-01 22:24:56 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.6635
2022-03-01 22:25:31 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.6639
2022-03-01 22:26:02 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.8029
2022-03-01 22:26:04 - train: epoch 096, train_loss: 1.5895
2022-03-01 22:27:18 - eval: epoch: 096, acc1: 66.396%, acc5: 86.954%, test_loss: 1.3794, per_image_load_time: 0.682ms, per_image_inference_time: 0.337ms
2022-03-01 22:27:18 - until epoch: 096, best_acc1: 66.420%
2022-03-01 22:27:18 - epoch 097 lr: 0.00010000000000000003
2022-03-01 22:27:57 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.6644
2022-03-01 22:28:30 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.6006
2022-03-01 22:29:04 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.6264
2022-03-01 22:29:37 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.6736
2022-03-01 22:30:11 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.7120
2022-03-01 22:30:44 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.5373
2022-03-01 22:31:19 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.5067
2022-03-01 22:31:51 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.4779
2022-03-01 22:32:25 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.5757
2022-03-01 22:32:58 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.7433
2022-03-01 22:33:31 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.4034
2022-03-01 22:34:05 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.6323
2022-03-01 22:34:38 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.6909
2022-03-01 22:35:12 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.6143
2022-03-01 22:35:45 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.6122
2022-03-01 22:36:18 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.5883
2022-03-01 22:36:52 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.4137
2022-03-01 22:37:25 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.7096
2022-03-01 22:37:59 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.6280
2022-03-01 22:38:32 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.4973
2022-03-01 22:39:06 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.5334
2022-03-01 22:39:39 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.5674
2022-03-01 22:40:13 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.5598
2022-03-01 22:40:46 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.6054
2022-03-01 22:41:19 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.6020
2022-03-01 22:41:53 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.6133
2022-03-01 22:42:27 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.4516
2022-03-01 22:43:00 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.4936
2022-03-01 22:43:34 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.5486
2022-03-01 22:44:08 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.5567
2022-03-01 22:44:42 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.4737
2022-03-01 22:45:15 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.7564
2022-03-01 22:45:49 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.6762
2022-03-01 22:46:23 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.6596
2022-03-01 22:46:56 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.7381
2022-03-01 22:47:30 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.6540
2022-03-01 22:48:01 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.3238
2022-03-01 22:48:35 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.4670
2022-03-01 22:49:08 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.2821
2022-03-01 22:49:42 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.6192
2022-03-01 22:50:15 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.5255
2022-03-01 22:50:49 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.3545
2022-03-01 22:51:23 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.4971
2022-03-01 22:51:57 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.5484
2022-03-01 22:52:30 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.6953
2022-03-01 22:53:05 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.5285
2022-03-01 22:53:39 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.7213
2022-03-01 22:54:13 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.5970
2022-03-01 22:54:48 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.8305
2022-03-01 22:55:20 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.4288
2022-03-01 22:55:22 - train: epoch 097, train_loss: 1.5920
2022-03-01 22:56:37 - eval: epoch: 097, acc1: 66.436%, acc5: 87.018%, test_loss: 1.3790, per_image_load_time: 0.688ms, per_image_inference_time: 0.360ms
2022-03-01 22:56:37 - until epoch: 097, best_acc1: 66.436%
2022-03-01 22:56:37 - epoch 098 lr: 0.00010000000000000003
2022-03-01 22:57:16 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.6632
2022-03-01 22:57:49 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.6424
2022-03-01 22:58:22 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.6803
2022-03-01 22:58:55 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.5737
2022-03-01 22:59:26 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.6362
2022-03-01 22:59:59 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.6496
2022-03-01 23:00:33 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.5966
2022-03-01 23:01:06 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.6442
2022-03-01 23:01:40 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.6900
2022-03-01 23:02:12 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.4518
2022-03-01 23:02:46 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.7518
2022-03-01 23:03:19 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.5175
2022-03-01 23:03:53 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.6276
2022-03-01 23:04:26 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.5753
2022-03-01 23:05:00 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.6890
2022-03-01 23:05:33 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.6646
2022-03-01 23:06:07 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.6708
2022-03-01 23:06:40 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.6528
2022-03-01 23:07:14 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.6395
2022-03-01 23:07:48 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.7236
2022-03-01 23:08:21 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.6041
2022-03-01 23:08:55 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.5428
2022-03-01 23:09:28 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.4395
2022-03-01 23:10:02 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.7077
2022-03-01 23:10:35 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.7006
2022-03-01 23:11:09 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.4579
2022-03-01 23:11:42 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.6252
2022-03-01 23:12:15 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.5475
2022-03-01 23:12:49 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.4590
2022-03-01 23:13:24 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.4446
2022-03-01 23:13:57 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.5949
2022-03-01 23:14:31 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.3619
2022-03-01 23:15:04 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.5705
2022-03-01 23:15:38 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 1.7940
2022-03-01 23:16:11 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.6560
2022-03-01 23:16:45 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.8241
2022-03-01 23:17:19 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.6044
2022-03-01 23:17:53 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.5091
2022-03-01 23:18:26 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 1.4608
2022-03-01 23:19:00 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.6873
2022-03-01 23:19:33 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.7699
2022-03-01 23:20:07 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.6399
2022-03-01 23:20:42 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 1.5847
2022-03-01 23:21:15 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.7808
2022-03-01 23:21:49 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.4427
2022-03-01 23:22:23 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.5429
2022-03-01 23:22:58 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.5312
2022-03-01 23:23:32 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 1.4684
2022-03-01 23:24:05 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.5636
2022-03-01 23:24:39 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.5120
2022-03-01 23:24:40 - train: epoch 098, train_loss: 1.5922
2022-03-01 23:25:56 - eval: epoch: 098, acc1: 66.332%, acc5: 86.922%, test_loss: 1.3787, per_image_load_time: 0.752ms, per_image_inference_time: 0.353ms
2022-03-01 23:25:56 - until epoch: 098, best_acc1: 66.436%
2022-03-01 23:25:56 - epoch 099 lr: 0.00010000000000000003
2022-03-01 23:26:35 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.4741
2022-03-01 23:27:09 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.4937
2022-03-01 23:27:43 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.5944
2022-03-01 23:28:16 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.6344
2022-03-01 23:28:50 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 1.5505
2022-03-01 23:29:22 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.6480
2022-03-01 23:29:55 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.4630
2022-03-01 23:30:29 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.4709
2022-03-01 23:31:02 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.6279
2022-03-01 23:31:36 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 1.4372
2022-03-01 23:32:09 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.6887
2022-03-01 23:32:43 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.4509
2022-03-01 23:33:17 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.7167
2022-03-01 23:33:50 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.6337
2022-03-01 23:34:23 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.4812
2022-03-01 23:34:56 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.5786
2022-03-01 23:35:30 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.4401
2022-03-01 23:36:03 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.7631
2022-03-01 23:36:37 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.7041
2022-03-01 23:37:10 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.5245
2022-03-01 23:37:43 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.4814
2022-03-01 23:38:18 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.4136
2022-03-01 23:38:51 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.5756
2022-03-01 23:39:24 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.8459
2022-03-01 23:39:58 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 1.4700
2022-03-01 23:40:32 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.5673
2022-03-01 23:41:05 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.4253
2022-03-01 23:41:39 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.6394
2022-03-01 23:42:12 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.5723
2022-03-01 23:42:46 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.8486
2022-03-01 23:43:19 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.4307
2022-03-01 23:43:53 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.6813
2022-03-01 23:44:27 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.6551
2022-03-01 23:45:00 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.5941
2022-03-01 23:45:34 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.7173
2022-03-01 23:46:07 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.7888
2022-03-01 23:46:41 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.4972
2022-03-01 23:47:14 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.7718
2022-03-01 23:47:49 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.5970
2022-03-01 23:48:22 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.7068
2022-03-01 23:48:56 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.3681
2022-03-01 23:49:29 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.8106
2022-03-01 23:50:03 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.6701
2022-03-01 23:50:36 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.6301
2022-03-01 23:51:10 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.6684
2022-03-01 23:51:43 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.6889
2022-03-01 23:52:17 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.5903
2022-03-01 23:52:52 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.5558
2022-03-01 23:53:25 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.6611
2022-03-01 23:53:58 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.6117
2022-03-01 23:54:00 - train: epoch 099, train_loss: 1.5880
2022-03-01 23:55:16 - eval: epoch: 099, acc1: 66.448%, acc5: 86.904%, test_loss: 1.3798, per_image_load_time: 0.662ms, per_image_inference_time: 0.355ms
2022-03-01 23:55:16 - until epoch: 099, best_acc1: 66.448%
2022-03-01 23:55:16 - epoch 100 lr: 0.00010000000000000003
2022-03-01 23:55:54 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.6944
2022-03-01 23:56:28 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.6615
2022-03-01 23:57:01 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.6383
2022-03-01 23:57:34 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.4370
2022-03-01 23:58:08 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.6622
2022-03-01 23:58:41 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.6303
2022-03-01 23:59:14 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 1.4491
2022-03-01 23:59:48 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.6394
2022-03-02 00:00:21 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.3539
2022-03-02 00:00:55 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.5367
2022-03-02 00:01:29 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 1.5770
2022-03-02 00:02:03 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.5190
2022-03-02 00:02:35 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 1.5563
2022-03-02 00:03:10 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.7940
2022-03-02 00:03:42 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.5557
2022-03-02 00:04:16 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.5204
2022-03-02 00:04:50 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.3764
2022-03-02 00:05:23 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.7104
2022-03-02 00:05:57 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.6814
2022-03-02 00:06:30 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.6359
2022-03-02 00:07:04 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.5864
2022-03-02 00:07:37 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.7334
2022-03-02 00:08:11 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.6561
2022-03-02 00:08:44 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.7416
2022-03-02 00:09:18 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.7411
2022-03-02 00:09:51 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.4595
2022-03-02 00:10:25 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.6449
2022-03-02 00:10:59 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.4952
2022-03-02 00:11:33 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.7825
2022-03-02 00:12:06 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 1.5080
2022-03-02 00:12:40 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 1.3928
2022-03-02 00:13:15 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.8532
2022-03-02 00:13:48 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.5931
2022-03-02 00:14:21 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.6142
2022-03-02 00:14:55 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 1.4096
2022-03-02 00:15:29 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 1.4943
2022-03-02 00:16:03 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.4768
2022-03-02 00:16:36 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.4027
2022-03-02 00:17:11 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.5728
2022-03-02 00:17:45 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 1.7099
2022-03-02 00:18:18 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.6959
2022-03-02 00:18:53 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.5878
2022-03-02 00:19:26 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 1.3411
2022-03-02 00:20:00 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.6564
2022-03-02 00:20:33 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 1.4009
2022-03-02 00:21:07 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 1.5741
2022-03-02 00:21:40 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.7460
2022-03-02 00:22:14 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 1.3576
2022-03-02 00:22:47 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.6048
2022-03-02 00:23:20 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.5922
2022-03-02 00:23:22 - train: epoch 100, train_loss: 1.5893
2022-03-02 00:24:38 - eval: epoch: 100, acc1: 66.338%, acc5: 86.990%, test_loss: 1.3801, per_image_load_time: 0.668ms, per_image_inference_time: 0.365ms
2022-03-02 00:24:38 - until epoch: 100, best_acc1: 66.448%
2022-03-02 00:24:38 - train done. model: yolov5sbackbone, train time: 48.662 hours, best_acc1: 66.448%
