2022-07-24 12:02:22 - network: RegNetX_400MF
2022-07-24 12:02:22 - num_classes: 1000
2022-07-24 12:02:22 - input_image_size: 224
2022-07-24 12:02:22 - scale: 1.1428571428571428
2022-07-24 12:02:22 - trained_model_path: 
2022-07-24 12:02:22 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-24 12:02:22 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-24 12:02:22 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7faa892ac280>
2022-07-24 12:02:22 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7faa892ac430>
2022-07-24 12:02:22 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7faa892ac460>
2022-07-24 12:02:22 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7faa892ac4c0>
2022-07-24 12:02:22 - seed: 0
2022-07-24 12:02:22 - batch_size: 256
2022-07-24 12:02:22 - num_workers: 16
2022-07-24 12:02:22 - optimizer: ('SGD', {'lr': 0.2, 'momentum': 0.9, 'global_weight_decay': False, 'nesterov': True, 'weight_decay': 5e-05, 'no_weight_decay_layer_name_list': []})
2022-07-24 12:02:22 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-24 12:02:22 - epochs: 100
2022-07-24 12:02:22 - print_interval: 100
2022-07-24 12:02:22 - accumulation_steps: 1
2022-07-24 12:02:22 - sync_bn: False
2022-07-24 12:02:22 - apex: True
2022-07-24 12:02:22 - use_ema_model: False
2022-07-24 12:02:22 - ema_model_decay: 0.9999
2022-07-24 12:02:22 - gpus_type: NVIDIA RTX A5000
2022-07-24 12:02:22 - gpus_num: 2
2022-07-24 12:02:22 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7faa6c21dd70>
2022-07-24 12:02:24 - --------------------parameters--------------------
2022-07-24 12:02:24 - name: conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.0.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.weight, grad: True
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.bias, grad: True
2022-07-24 12:02:24 - name: fc.weight, grad: True
2022-07-24 12:02:24 - name: fc.bias, grad: True
2022-07-24 12:02:24 - --------------------buffers--------------------
2022-07-24 12:02:24 - name: conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.running_mean, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.running_var, grad: False
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.num_batches_tracked, grad: False
2022-07-24 12:02:24 - -----------no weight decay layers--------------
2022-07-24 12:02:24 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-24 12:02:24 - -------------weight decay layers---------------
2022-07-24 12:02:24 - name: conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer1.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer2.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer3.6.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.6.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.7.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.8.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.9.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.10.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: layer4.11.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - name: fc.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-07-24 12:02:24 - epoch 001 lr: 0.200000
2022-07-24 12:04:24 - train: epoch 0001, iter [00100, 05004], lr: 0.040799, loss: 6.9350
2022-07-24 12:06:10 - train: epoch 0001, iter [00200, 05004], lr: 0.041599, loss: 6.8709
2022-07-24 12:07:56 - train: epoch 0001, iter [00300, 05004], lr: 0.042398, loss: 6.7130
2022-07-24 12:09:43 - train: epoch 0001, iter [00400, 05004], lr: 0.043197, loss: 6.6762
2022-07-24 12:11:32 - train: epoch 0001, iter [00500, 05004], lr: 0.043997, loss: 6.5965
2022-07-24 12:13:21 - train: epoch 0001, iter [00600, 05004], lr: 0.044796, loss: 6.3812
2022-07-24 12:15:08 - train: epoch 0001, iter [00700, 05004], lr: 0.045596, loss: 6.5090
2022-07-24 12:16:57 - train: epoch 0001, iter [00800, 05004], lr: 0.046395, loss: 6.3747
2022-07-24 12:19:01 - train: epoch 0001, iter [00900, 05004], lr: 0.047194, loss: 6.2364
2022-07-24 12:20:46 - train: epoch 0001, iter [01000, 05004], lr: 0.047994, loss: 6.1720
2022-07-24 12:22:35 - train: epoch 0001, iter [01100, 05004], lr: 0.048793, loss: 6.1189
2022-07-24 12:24:22 - train: epoch 0001, iter [01200, 05004], lr: 0.049592, loss: 5.9380
2022-07-24 12:26:12 - train: epoch 0001, iter [01300, 05004], lr: 0.050392, loss: 5.8994
2022-07-24 12:28:00 - train: epoch 0001, iter [01400, 05004], lr: 0.051191, loss: 5.9431
2022-07-24 12:29:54 - train: epoch 0001, iter [01500, 05004], lr: 0.051990, loss: 5.8556
2022-07-24 12:31:40 - train: epoch 0001, iter [01600, 05004], lr: 0.052790, loss: 5.8800
2022-07-24 12:33:29 - train: epoch 0001, iter [01700, 05004], lr: 0.053589, loss: 5.6928
2022-07-24 12:35:19 - train: epoch 0001, iter [01800, 05004], lr: 0.054388, loss: 5.6823
2022-07-24 12:37:01 - train: epoch 0001, iter [01900, 05004], lr: 0.055188, loss: 5.5370
2022-07-24 12:38:48 - train: epoch 0001, iter [02000, 05004], lr: 0.055987, loss: 5.5207
2022-07-24 12:40:33 - train: epoch 0001, iter [02100, 05004], lr: 0.056787, loss: 5.5300
2022-07-24 12:42:21 - train: epoch 0001, iter [02200, 05004], lr: 0.057586, loss: 5.3230
2022-07-24 12:44:04 - train: epoch 0001, iter [02300, 05004], lr: 0.058385, loss: 5.1886
2022-07-24 12:45:51 - train: epoch 0001, iter [02400, 05004], lr: 0.059185, loss: 5.4308
2022-07-24 12:47:40 - train: epoch 0001, iter [02500, 05004], lr: 0.059984, loss: 5.3035
2022-07-24 12:49:28 - train: epoch 0001, iter [02600, 05004], lr: 0.060783, loss: 5.2092
2022-07-24 12:51:17 - train: epoch 0001, iter [02700, 05004], lr: 0.061583, loss: 5.3067
2022-07-24 12:53:05 - train: epoch 0001, iter [02800, 05004], lr: 0.062382, loss: 5.2035
2022-07-24 12:54:53 - train: epoch 0001, iter [02900, 05004], lr: 0.063181, loss: 5.0268
2022-07-24 12:56:39 - train: epoch 0001, iter [03000, 05004], lr: 0.063981, loss: 5.1667
2022-07-24 12:58:27 - train: epoch 0001, iter [03100, 05004], lr: 0.064780, loss: 5.0878
2022-07-24 13:00:17 - train: epoch 0001, iter [03200, 05004], lr: 0.065580, loss: 5.0858
2022-07-24 13:02:08 - train: epoch 0001, iter [03300, 05004], lr: 0.066379, loss: 4.8406
2022-07-24 13:03:52 - train: epoch 0001, iter [03400, 05004], lr: 0.067178, loss: 4.9333
2022-07-24 13:05:40 - train: epoch 0001, iter [03500, 05004], lr: 0.067978, loss: 4.8962
2022-07-24 13:07:28 - train: epoch 0001, iter [03600, 05004], lr: 0.068777, loss: 4.8218
2022-07-24 13:09:14 - train: epoch 0001, iter [03700, 05004], lr: 0.069576, loss: 4.7963
2022-07-24 13:11:02 - train: epoch 0001, iter [03800, 05004], lr: 0.070376, loss: 4.7769
2022-07-24 13:12:55 - train: epoch 0001, iter [03900, 05004], lr: 0.071175, loss: 4.7506
2022-07-24 13:14:53 - train: epoch 0001, iter [04000, 05004], lr: 0.071974, loss: 4.7338
2022-07-24 13:16:52 - train: epoch 0001, iter [04100, 05004], lr: 0.072774, loss: 4.7869
2022-07-24 13:18:52 - train: epoch 0001, iter [04200, 05004], lr: 0.073573, loss: 4.6099
2022-07-24 13:20:50 - train: epoch 0001, iter [04300, 05004], lr: 0.074373, loss: 4.5949
2022-07-24 13:22:45 - train: epoch 0001, iter [04400, 05004], lr: 0.075172, loss: 4.5285
2022-07-24 13:24:39 - train: epoch 0001, iter [04500, 05004], lr: 0.075971, loss: 4.4288
2022-07-24 13:26:38 - train: epoch 0001, iter [04600, 05004], lr: 0.076771, loss: 4.6183
2022-07-24 13:28:37 - train: epoch 0001, iter [04700, 05004], lr: 0.077570, loss: 4.4455
2022-07-24 13:30:36 - train: epoch 0001, iter [04800, 05004], lr: 0.078369, loss: 4.6330
2022-07-24 13:32:32 - train: epoch 0001, iter [04900, 05004], lr: 0.079169, loss: 4.6321
2022-07-24 13:34:24 - train: epoch 0001, iter [05000, 05004], lr: 0.079968, loss: 4.4034
2022-07-24 13:34:28 - train: epoch 001, train_loss: 5.4107
2022-07-24 13:38:33 - eval: epoch: 001, acc1: 17.808%, acc5: 38.962%, test_loss: 4.1570, per_image_load_time: 2.057ms, per_image_inference_time: 0.667ms
2022-07-24 13:38:34 - until epoch: 001, best_acc1: 17.808%
2022-07-24 13:38:34 - epoch 002 lr: 0.080008
2022-07-24 13:40:53 - train: epoch 0002, iter [00100, 05004], lr: 0.080799, loss: 4.2565
2022-07-24 13:42:56 - train: epoch 0002, iter [00200, 05004], lr: 0.081599, loss: 4.2315
2022-07-24 13:44:50 - train: epoch 0002, iter [00300, 05004], lr: 0.082398, loss: 4.4774
2022-07-24 13:46:51 - train: epoch 0002, iter [00400, 05004], lr: 0.083197, loss: 4.3364
2022-07-24 13:48:54 - train: epoch 0002, iter [00500, 05004], lr: 0.083997, loss: 4.1531
2022-07-24 13:50:51 - train: epoch 0002, iter [00600, 05004], lr: 0.084796, loss: 4.1829
2022-07-24 13:52:51 - train: epoch 0002, iter [00700, 05004], lr: 0.085596, loss: 4.3702
2022-07-24 13:54:42 - train: epoch 0002, iter [00800, 05004], lr: 0.086395, loss: 4.1036
2022-07-24 13:57:03 - train: epoch 0002, iter [00900, 05004], lr: 0.087194, loss: 4.0409
2022-07-24 13:59:03 - train: epoch 0002, iter [01000, 05004], lr: 0.087994, loss: 4.2607
2022-07-24 14:01:00 - train: epoch 0002, iter [01100, 05004], lr: 0.088793, loss: 4.2973
2022-07-24 14:03:01 - train: epoch 0002, iter [01200, 05004], lr: 0.089592, loss: 4.1264
2022-07-24 14:04:58 - train: epoch 0002, iter [01300, 05004], lr: 0.090392, loss: 4.2229
2022-07-24 14:06:48 - train: epoch 0002, iter [01400, 05004], lr: 0.091191, loss: 4.3622
2022-07-24 14:08:47 - train: epoch 0002, iter [01500, 05004], lr: 0.091990, loss: 4.1297
2022-07-24 14:10:48 - train: epoch 0002, iter [01600, 05004], lr: 0.092790, loss: 4.0286
2022-07-24 14:12:42 - train: epoch 0002, iter [01700, 05004], lr: 0.093589, loss: 4.0422
2022-07-24 14:14:40 - train: epoch 0002, iter [01800, 05004], lr: 0.094388, loss: 4.1165
2022-07-24 14:16:38 - train: epoch 0002, iter [01900, 05004], lr: 0.095188, loss: 3.8046
2022-07-24 14:18:35 - train: epoch 0002, iter [02000, 05004], lr: 0.095987, loss: 3.6068
2022-07-24 14:20:33 - train: epoch 0002, iter [02100, 05004], lr: 0.096787, loss: 4.0932
2022-07-24 14:22:27 - train: epoch 0002, iter [02200, 05004], lr: 0.097586, loss: 3.8813
2022-07-24 14:24:24 - train: epoch 0002, iter [02300, 05004], lr: 0.098385, loss: 3.9576
2022-07-24 14:26:18 - train: epoch 0002, iter [02400, 05004], lr: 0.099185, loss: 3.8457
2022-07-24 14:28:10 - train: epoch 0002, iter [02500, 05004], lr: 0.099984, loss: 3.7223
2022-07-24 14:30:08 - train: epoch 0002, iter [02600, 05004], lr: 0.100783, loss: 3.8313
2022-07-24 14:32:01 - train: epoch 0002, iter [02700, 05004], lr: 0.101583, loss: 3.9321
2022-07-24 14:33:54 - train: epoch 0002, iter [02800, 05004], lr: 0.102382, loss: 3.8903
2022-07-24 14:35:57 - train: epoch 0002, iter [02900, 05004], lr: 0.103181, loss: 3.8524
2022-07-24 14:37:47 - train: epoch 0002, iter [03000, 05004], lr: 0.103981, loss: 3.7812
2022-07-24 14:39:44 - train: epoch 0002, iter [03100, 05004], lr: 0.104780, loss: 3.7616
2022-07-24 14:41:36 - train: epoch 0002, iter [03200, 05004], lr: 0.105580, loss: 3.8137
2022-07-24 14:43:32 - train: epoch 0002, iter [03300, 05004], lr: 0.106379, loss: 3.9437
2022-07-24 14:45:34 - train: epoch 0002, iter [03400, 05004], lr: 0.107178, loss: 3.7794
2022-07-24 14:47:33 - train: epoch 0002, iter [03500, 05004], lr: 0.107978, loss: 3.5718
2022-07-24 14:49:29 - train: epoch 0002, iter [03600, 05004], lr: 0.108777, loss: 3.6921
2022-07-24 14:51:36 - train: epoch 0002, iter [03700, 05004], lr: 0.109576, loss: 3.9126
2022-07-24 14:53:35 - train: epoch 0002, iter [03800, 05004], lr: 0.110376, loss: 3.5926
2022-07-24 14:55:34 - train: epoch 0002, iter [03900, 05004], lr: 0.111175, loss: 3.8136
2022-07-24 14:57:31 - train: epoch 0002, iter [04000, 05004], lr: 0.111974, loss: 3.5221
2022-07-24 14:59:30 - train: epoch 0002, iter [04100, 05004], lr: 0.112774, loss: 3.7805
2022-07-24 15:01:33 - train: epoch 0002, iter [04200, 05004], lr: 0.113573, loss: 3.6635
2022-07-24 15:03:34 - train: epoch 0002, iter [04300, 05004], lr: 0.114373, loss: 3.7012
2022-07-24 15:05:30 - train: epoch 0002, iter [04400, 05004], lr: 0.115172, loss: 3.5362
2022-07-24 15:07:33 - train: epoch 0002, iter [04500, 05004], lr: 0.115971, loss: 3.4491
2022-07-24 15:09:30 - train: epoch 0002, iter [04600, 05004], lr: 0.116771, loss: 3.4187
2022-07-24 15:11:27 - train: epoch 0002, iter [04700, 05004], lr: 0.117570, loss: 3.5016
2022-07-24 15:13:19 - train: epoch 0002, iter [04800, 05004], lr: 0.118369, loss: 3.5628
2022-07-24 15:15:08 - train: epoch 0002, iter [04900, 05004], lr: 0.119169, loss: 3.5289
2022-07-24 15:16:58 - train: epoch 0002, iter [05000, 05004], lr: 0.119968, loss: 3.5631
2022-07-24 15:17:02 - train: epoch 002, train_loss: 3.8996
2022-07-24 15:20:51 - eval: epoch: 002, acc1: 28.388%, acc5: 53.484%, test_loss: 3.4180, per_image_load_time: 1.428ms, per_image_inference_time: 0.595ms
2022-07-24 15:20:52 - until epoch: 002, best_acc1: 28.388%
2022-07-24 15:20:52 - epoch 003 lr: 0.120008
2022-07-24 15:23:05 - train: epoch 0003, iter [00100, 05004], lr: 0.120799, loss: 3.7043
2022-07-24 15:25:00 - train: epoch 0003, iter [00200, 05004], lr: 0.121599, loss: 3.5711
2022-07-24 15:26:58 - train: epoch 0003, iter [00300, 05004], lr: 0.122398, loss: 3.6831
2022-07-24 15:28:58 - train: epoch 0003, iter [00400, 05004], lr: 0.123197, loss: 3.5331
2022-07-24 15:30:55 - train: epoch 0003, iter [00500, 05004], lr: 0.123997, loss: 3.5437
2022-07-24 15:32:56 - train: epoch 0003, iter [00600, 05004], lr: 0.124796, loss: 3.2954
2022-07-24 15:34:57 - train: epoch 0003, iter [00700, 05004], lr: 0.125596, loss: 3.6186
2022-07-24 15:36:59 - train: epoch 0003, iter [00800, 05004], lr: 0.126395, loss: 3.6974
2022-07-24 15:39:15 - train: epoch 0003, iter [00900, 05004], lr: 0.127194, loss: 3.3780
2022-07-24 15:41:21 - train: epoch 0003, iter [01000, 05004], lr: 0.127994, loss: 3.3576
2022-07-24 15:43:21 - train: epoch 0003, iter [01100, 05004], lr: 0.128793, loss: 3.2149
2022-07-24 15:45:21 - train: epoch 0003, iter [01200, 05004], lr: 0.129592, loss: 3.3448
2022-07-24 15:47:27 - train: epoch 0003, iter [01300, 05004], lr: 0.130392, loss: 3.3864
2022-07-24 15:49:28 - train: epoch 0003, iter [01400, 05004], lr: 0.131191, loss: 3.3654
2022-07-24 15:51:28 - train: epoch 0003, iter [01500, 05004], lr: 0.131990, loss: 3.6517
2022-07-24 15:53:32 - train: epoch 0003, iter [01600, 05004], lr: 0.132790, loss: 3.3552
2022-07-24 15:55:33 - train: epoch 0003, iter [01700, 05004], lr: 0.133589, loss: 3.3151
2022-07-24 15:57:36 - train: epoch 0003, iter [01800, 05004], lr: 0.134388, loss: 3.3780
2022-07-24 15:59:29 - train: epoch 0003, iter [01900, 05004], lr: 0.135188, loss: 3.4596
2022-07-24 16:01:34 - train: epoch 0003, iter [02000, 05004], lr: 0.135987, loss: 3.4937
2022-07-24 16:03:33 - train: epoch 0003, iter [02100, 05004], lr: 0.136787, loss: 3.6486
2022-07-24 16:05:35 - train: epoch 0003, iter [02200, 05004], lr: 0.137586, loss: 3.6756
2022-07-24 16:07:35 - train: epoch 0003, iter [02300, 05004], lr: 0.138385, loss: 3.2966
2022-07-24 16:09:33 - train: epoch 0003, iter [02400, 05004], lr: 0.139185, loss: 3.2983
2022-07-24 16:11:30 - train: epoch 0003, iter [02500, 05004], lr: 0.139984, loss: 3.3844
2022-07-24 16:13:30 - train: epoch 0003, iter [02600, 05004], lr: 0.140783, loss: 3.5296
2022-07-24 16:15:32 - train: epoch 0003, iter [02700, 05004], lr: 0.141583, loss: 3.4694
2022-07-24 16:17:28 - train: epoch 0003, iter [02800, 05004], lr: 0.142382, loss: 3.3633
2022-07-24 16:19:29 - train: epoch 0003, iter [02900, 05004], lr: 0.143181, loss: 3.4006
2022-07-24 16:21:30 - train: epoch 0003, iter [03000, 05004], lr: 0.143981, loss: 3.3502
2022-07-24 16:23:37 - train: epoch 0003, iter [03100, 05004], lr: 0.144780, loss: 3.6179
2022-07-24 16:25:32 - train: epoch 0003, iter [03200, 05004], lr: 0.145580, loss: 3.3153
2022-07-24 16:27:33 - train: epoch 0003, iter [03300, 05004], lr: 0.146379, loss: 3.3091
2022-07-24 16:29:35 - train: epoch 0003, iter [03400, 05004], lr: 0.147178, loss: 3.4195
2022-07-24 16:31:34 - train: epoch 0003, iter [03500, 05004], lr: 0.147978, loss: 3.1153
2022-07-24 16:33:34 - train: epoch 0003, iter [03600, 05004], lr: 0.148777, loss: 3.2701
2022-07-24 16:35:36 - train: epoch 0003, iter [03700, 05004], lr: 0.149576, loss: 3.4180
2022-07-24 16:37:31 - train: epoch 0003, iter [03800, 05004], lr: 0.150376, loss: 3.4198
2022-07-24 16:39:29 - train: epoch 0003, iter [03900, 05004], lr: 0.151175, loss: 3.3626
2022-07-24 16:41:26 - train: epoch 0003, iter [04000, 05004], lr: 0.151974, loss: 3.1565
2022-07-24 16:43:30 - train: epoch 0003, iter [04100, 05004], lr: 0.152774, loss: 3.2427
2022-07-24 16:45:32 - train: epoch 0003, iter [04200, 05004], lr: 0.153573, loss: 3.1140
2022-07-24 16:47:28 - train: epoch 0003, iter [04300, 05004], lr: 0.154373, loss: 3.2604
2022-07-24 16:49:31 - train: epoch 0003, iter [04400, 05004], lr: 0.155172, loss: 3.2684
2022-07-24 16:51:34 - train: epoch 0003, iter [04500, 05004], lr: 0.155971, loss: 3.3046
2022-07-24 16:53:32 - train: epoch 0003, iter [04600, 05004], lr: 0.156771, loss: 3.2125
2022-07-24 16:55:29 - train: epoch 0003, iter [04700, 05004], lr: 0.157570, loss: 3.0503
2022-07-24 16:57:29 - train: epoch 0003, iter [04800, 05004], lr: 0.158369, loss: 3.2687
2022-07-24 16:59:29 - train: epoch 0003, iter [04900, 05004], lr: 0.159169, loss: 3.2361
2022-07-24 17:01:30 - train: epoch 0003, iter [05000, 05004], lr: 0.159968, loss: 3.3009
2022-07-24 17:01:34 - train: epoch 003, train_loss: 3.3597
2022-07-24 17:05:42 - eval: epoch: 003, acc1: 35.244%, acc5: 61.388%, test_loss: 3.0079, per_image_load_time: 5.603ms, per_image_inference_time: 0.720ms
2022-07-24 17:05:42 - until epoch: 003, best_acc1: 35.244%
2022-07-24 17:05:42 - epoch 004 lr: 0.160008
2022-07-24 17:07:59 - train: epoch 0004, iter [00100, 05004], lr: 0.160799, loss: 3.1920
2022-07-24 17:10:00 - train: epoch 0004, iter [00200, 05004], lr: 0.161599, loss: 2.9670
2022-07-24 17:12:04 - train: epoch 0004, iter [00300, 05004], lr: 0.162398, loss: 2.9700
2022-07-24 17:14:07 - train: epoch 0004, iter [00400, 05004], lr: 0.163197, loss: 3.0956
2022-07-24 17:16:04 - train: epoch 0004, iter [00500, 05004], lr: 0.163997, loss: 2.9811
2022-07-24 17:18:02 - train: epoch 0004, iter [00600, 05004], lr: 0.164796, loss: 3.3845
2022-07-24 17:20:10 - train: epoch 0004, iter [00700, 05004], lr: 0.165596, loss: 3.2944
2022-07-24 17:22:09 - train: epoch 0004, iter [00800, 05004], lr: 0.166395, loss: 3.1862
2022-07-24 17:24:28 - train: epoch 0004, iter [00900, 05004], lr: 0.167194, loss: 3.0472
2022-07-24 17:26:22 - train: epoch 0004, iter [01000, 05004], lr: 0.167994, loss: 3.1283
2022-07-24 17:28:36 - train: epoch 0004, iter [01100, 05004], lr: 0.168793, loss: 3.3780
2022-07-24 17:30:45 - train: epoch 0004, iter [01200, 05004], lr: 0.169592, loss: 2.9688
2022-07-24 17:32:50 - train: epoch 0004, iter [01300, 05004], lr: 0.170392, loss: 2.9210
2022-07-24 17:34:58 - train: epoch 0004, iter [01400, 05004], lr: 0.171191, loss: 3.2865
2022-07-24 17:37:00 - train: epoch 0004, iter [01500, 05004], lr: 0.171990, loss: 3.3481
2022-07-24 17:39:04 - train: epoch 0004, iter [01600, 05004], lr: 0.172790, loss: 3.2326
2022-07-24 17:41:15 - train: epoch 0004, iter [01700, 05004], lr: 0.173589, loss: 3.0916
2022-07-24 17:43:26 - train: epoch 0004, iter [01800, 05004], lr: 0.174388, loss: 3.1673
2022-07-24 17:45:25 - train: epoch 0004, iter [01900, 05004], lr: 0.175188, loss: 3.1292
2022-07-24 17:47:35 - train: epoch 0004, iter [02000, 05004], lr: 0.175987, loss: 2.9928
2022-07-24 17:49:37 - train: epoch 0004, iter [02100, 05004], lr: 0.176787, loss: 3.6071
2022-07-24 17:51:49 - train: epoch 0004, iter [02200, 05004], lr: 0.177586, loss: 3.1839
2022-07-24 17:53:50 - train: epoch 0004, iter [02300, 05004], lr: 0.178385, loss: 2.8016
2022-07-24 17:55:53 - train: epoch 0004, iter [02400, 05004], lr: 0.179185, loss: 2.9490
2022-07-24 17:58:01 - train: epoch 0004, iter [02500, 05004], lr: 0.179984, loss: 2.9610
2022-07-24 18:00:08 - train: epoch 0004, iter [02600, 05004], lr: 0.180783, loss: 3.2263
2022-07-24 18:02:08 - train: epoch 0004, iter [02700, 05004], lr: 0.181583, loss: 2.8866
2022-07-24 18:04:12 - train: epoch 0004, iter [02800, 05004], lr: 0.182382, loss: 3.1953
2022-07-24 18:06:19 - train: epoch 0004, iter [02900, 05004], lr: 0.183181, loss: 3.0662
2022-07-24 18:08:21 - train: epoch 0004, iter [03000, 05004], lr: 0.183981, loss: 3.0309
2022-07-24 18:10:27 - train: epoch 0004, iter [03100, 05004], lr: 0.184780, loss: 3.0923
2022-07-24 18:12:29 - train: epoch 0004, iter [03200, 05004], lr: 0.185580, loss: 3.1345
2022-07-24 18:14:38 - train: epoch 0004, iter [03300, 05004], lr: 0.186379, loss: 3.0939
2022-07-24 18:16:41 - train: epoch 0004, iter [03400, 05004], lr: 0.187178, loss: 2.7922
2022-07-24 18:18:40 - train: epoch 0004, iter [03500, 05004], lr: 0.187978, loss: 3.0701
2022-07-24 18:20:43 - train: epoch 0004, iter [03600, 05004], lr: 0.188777, loss: 2.9817
2022-07-24 18:22:47 - train: epoch 0004, iter [03700, 05004], lr: 0.189576, loss: 2.9554
2022-07-24 18:24:49 - train: epoch 0004, iter [03800, 05004], lr: 0.190376, loss: 2.8139
2022-07-24 18:26:54 - train: epoch 0004, iter [03900, 05004], lr: 0.191175, loss: 2.7189
2022-07-24 18:28:54 - train: epoch 0004, iter [04000, 05004], lr: 0.191974, loss: 2.8224
2022-07-24 18:30:59 - train: epoch 0004, iter [04100, 05004], lr: 0.192774, loss: 3.0391
2022-07-24 18:33:10 - train: epoch 0004, iter [04200, 05004], lr: 0.193573, loss: 2.9829
2022-07-24 18:35:13 - train: epoch 0004, iter [04300, 05004], lr: 0.194373, loss: 3.0931
2022-07-24 18:37:15 - train: epoch 0004, iter [04400, 05004], lr: 0.195172, loss: 3.1005
2022-07-24 18:39:23 - train: epoch 0004, iter [04500, 05004], lr: 0.195971, loss: 2.6687
2022-07-24 18:41:24 - train: epoch 0004, iter [04600, 05004], lr: 0.196771, loss: 2.9474
2022-07-24 18:43:27 - train: epoch 0004, iter [04700, 05004], lr: 0.197570, loss: 3.1507
2022-07-24 18:45:29 - train: epoch 0004, iter [04800, 05004], lr: 0.198369, loss: 2.9679
2022-07-24 18:47:32 - train: epoch 0004, iter [04900, 05004], lr: 0.199169, loss: 3.0047
2022-07-24 18:49:32 - train: epoch 0004, iter [05000, 05004], lr: 0.199968, loss: 2.9110
2022-07-24 18:49:36 - train: epoch 004, train_loss: 3.0978
2022-07-24 18:53:44 - eval: epoch: 004, acc1: 38.436%, acc5: 64.936%, test_loss: 2.7911, per_image_load_time: 2.290ms, per_image_inference_time: 0.597ms
2022-07-24 18:53:45 - until epoch: 004, best_acc1: 38.436%
2022-07-24 18:53:45 - epoch 005 lr: 0.200008
2022-07-24 18:56:07 - train: epoch 0005, iter [00100, 05004], lr: 0.200799, loss: 2.9245
2022-07-24 18:58:17 - train: epoch 0005, iter [00200, 05004], lr: 0.201599, loss: 3.0861
2022-07-24 19:00:24 - train: epoch 0005, iter [00300, 05004], lr: 0.202398, loss: 3.1593
2022-07-24 19:02:28 - train: epoch 0005, iter [00400, 05004], lr: 0.203197, loss: 3.0016
2022-07-24 19:04:36 - train: epoch 0005, iter [00500, 05004], lr: 0.203997, loss: 2.8256
2022-07-24 19:06:40 - train: epoch 0005, iter [00600, 05004], lr: 0.204796, loss: 2.8831
2022-07-24 19:08:34 - train: epoch 0005, iter [00700, 05004], lr: 0.205596, loss: 2.8231
2022-07-24 19:10:57 - train: epoch 0005, iter [00800, 05004], lr: 0.206395, loss: 3.0820
2022-07-24 19:13:00 - train: epoch 0005, iter [00900, 05004], lr: 0.207194, loss: 2.9445
2022-07-24 19:15:20 - train: epoch 0005, iter [01000, 05004], lr: 0.207994, loss: 2.9434
2022-07-24 19:17:30 - train: epoch 0005, iter [01100, 05004], lr: 0.208793, loss: 3.2055
2022-07-24 19:19:39 - train: epoch 0005, iter [01200, 05004], lr: 0.209592, loss: 2.8506
2022-07-24 19:21:51 - train: epoch 0005, iter [01300, 05004], lr: 0.210392, loss: 3.0009
2022-07-24 19:24:00 - train: epoch 0005, iter [01400, 05004], lr: 0.211191, loss: 3.0598
2022-07-24 19:26:06 - train: epoch 0005, iter [01500, 05004], lr: 0.211990, loss: 2.9452
2022-07-24 19:28:10 - train: epoch 0005, iter [01600, 05004], lr: 0.212790, loss: 2.7573
2022-07-24 19:30:17 - train: epoch 0005, iter [01700, 05004], lr: 0.213589, loss: 3.1039
2022-07-24 19:32:27 - train: epoch 0005, iter [01800, 05004], lr: 0.214388, loss: 2.8445
2022-07-24 19:34:30 - train: epoch 0005, iter [01900, 05004], lr: 0.215188, loss: 2.9415
2022-07-24 19:36:37 - train: epoch 0005, iter [02000, 05004], lr: 0.215987, loss: 3.0481
2022-07-24 19:38:47 - train: epoch 0005, iter [02100, 05004], lr: 0.216787, loss: 2.7801
2022-07-24 19:40:54 - train: epoch 0005, iter [02200, 05004], lr: 0.217586, loss: 3.0430
2022-07-24 19:43:02 - train: epoch 0005, iter [02300, 05004], lr: 0.218385, loss: 2.7989
2022-07-24 19:45:10 - train: epoch 0005, iter [02400, 05004], lr: 0.219185, loss: 2.7988
2022-07-24 19:47:17 - train: epoch 0005, iter [02500, 05004], lr: 0.219984, loss: 3.0533
2022-07-24 19:49:23 - train: epoch 0005, iter [02600, 05004], lr: 0.220783, loss: 3.2160
2022-07-24 19:51:38 - train: epoch 0005, iter [02700, 05004], lr: 0.221583, loss: 2.9626
2022-07-24 19:53:43 - train: epoch 0005, iter [02800, 05004], lr: 0.222382, loss: 2.8091
2022-07-24 19:55:50 - train: epoch 0005, iter [02900, 05004], lr: 0.223181, loss: 2.7405
2022-07-24 19:57:58 - train: epoch 0005, iter [03000, 05004], lr: 0.223981, loss: 2.8448
2022-07-24 20:00:04 - train: epoch 0005, iter [03100, 05004], lr: 0.224780, loss: 2.8159
2022-07-24 20:02:06 - train: epoch 0005, iter [03200, 05004], lr: 0.225580, loss: 3.0028
2022-07-24 20:04:16 - train: epoch 0005, iter [03300, 05004], lr: 0.226379, loss: 2.9729
2022-07-24 20:06:22 - train: epoch 0005, iter [03400, 05004], lr: 0.227178, loss: 2.8121
2022-07-24 20:08:28 - train: epoch 0005, iter [03500, 05004], lr: 0.227978, loss: 2.8850
2022-07-24 20:10:30 - train: epoch 0005, iter [03600, 05004], lr: 0.228777, loss: 2.9175
2022-07-24 20:12:41 - train: epoch 0005, iter [03700, 05004], lr: 0.229576, loss: 2.9107
2022-07-24 20:14:47 - train: epoch 0005, iter [03800, 05004], lr: 0.230376, loss: 2.9000
2022-07-24 20:16:53 - train: epoch 0005, iter [03900, 05004], lr: 0.231175, loss: 3.1105
2022-07-24 20:19:02 - train: epoch 0005, iter [04000, 05004], lr: 0.231974, loss: 2.8905
2022-07-24 20:21:04 - train: epoch 0005, iter [04100, 05004], lr: 0.232774, loss: 2.9392
2022-07-24 20:23:10 - train: epoch 0005, iter [04200, 05004], lr: 0.233573, loss: 3.1338
2022-07-24 20:25:08 - train: epoch 0005, iter [04300, 05004], lr: 0.234373, loss: 3.0188
2022-07-24 20:27:17 - train: epoch 0005, iter [04400, 05004], lr: 0.235172, loss: 2.9906
2022-07-24 20:29:21 - train: epoch 0005, iter [04500, 05004], lr: 0.235971, loss: 3.0444
2022-07-24 20:31:16 - train: epoch 0005, iter [04600, 05004], lr: 0.236771, loss: 2.8983
2022-07-24 20:33:22 - train: epoch 0005, iter [04700, 05004], lr: 0.237570, loss: 2.7899
2022-07-24 20:35:15 - train: epoch 0005, iter [04800, 05004], lr: 0.238369, loss: 2.7703
2022-07-24 20:37:19 - train: epoch 0005, iter [04900, 05004], lr: 0.239169, loss: 3.1708
2022-07-24 20:39:17 - train: epoch 0005, iter [05000, 05004], lr: 0.239968, loss: 2.9634
2022-07-24 20:39:21 - train: epoch 005, train_loss: 2.9507
2022-07-24 20:43:19 - eval: epoch: 005, acc1: 40.178%, acc5: 67.208%, test_loss: 2.6805, per_image_load_time: 1.569ms, per_image_inference_time: 0.620ms
2022-07-24 20:43:19 - until epoch: 005, best_acc1: 40.178%
2022-07-24 20:43:19 - epoch 006 lr: 0.200000
2022-07-24 20:45:39 - train: epoch 0006, iter [00100, 05004], lr: 0.200000, loss: 2.7770
2022-07-24 20:47:37 - train: epoch 0006, iter [00200, 05004], lr: 0.200000, loss: 2.8559
2022-07-24 20:49:37 - train: epoch 0006, iter [00300, 05004], lr: 0.200000, loss: 2.7527
2022-07-24 20:51:37 - train: epoch 0006, iter [00400, 05004], lr: 0.200000, loss: 2.7987
2022-07-24 20:53:41 - train: epoch 0006, iter [00500, 05004], lr: 0.199999, loss: 2.5278
2022-07-24 20:55:48 - train: epoch 0006, iter [00600, 05004], lr: 0.199999, loss: 2.7739
2022-07-24 20:57:44 - train: epoch 0006, iter [00700, 05004], lr: 0.199999, loss: 2.9303
2022-07-24 21:00:13 - train: epoch 0006, iter [00800, 05004], lr: 0.199999, loss: 2.8527
2022-07-24 21:02:13 - train: epoch 0006, iter [00900, 05004], lr: 0.199998, loss: 2.6227
2022-07-24 21:04:29 - train: epoch 0006, iter [01000, 05004], lr: 0.199998, loss: 2.6635
2022-07-24 21:06:31 - train: epoch 0006, iter [01100, 05004], lr: 0.199997, loss: 2.8575
2022-07-24 21:08:35 - train: epoch 0006, iter [01200, 05004], lr: 0.199997, loss: 2.8380
2022-07-24 21:10:38 - train: epoch 0006, iter [01300, 05004], lr: 0.199996, loss: 2.9499
2022-07-24 21:12:43 - train: epoch 0006, iter [01400, 05004], lr: 0.199996, loss: 2.8012
2022-07-24 21:14:48 - train: epoch 0006, iter [01500, 05004], lr: 0.199995, loss: 2.9879
2022-07-24 21:16:57 - train: epoch 0006, iter [01600, 05004], lr: 0.199994, loss: 2.6203
2022-07-24 21:19:04 - train: epoch 0006, iter [01700, 05004], lr: 0.199994, loss: 2.9945
2022-07-24 21:21:05 - train: epoch 0006, iter [01800, 05004], lr: 0.199993, loss: 2.9250
2022-07-24 21:23:12 - train: epoch 0006, iter [01900, 05004], lr: 0.199992, loss: 2.6955
2022-07-24 21:25:15 - train: epoch 0006, iter [02000, 05004], lr: 0.199991, loss: 2.9309
2022-07-24 21:27:18 - train: epoch 0006, iter [02100, 05004], lr: 0.199990, loss: 2.8046
2022-07-24 21:29:18 - train: epoch 0006, iter [02200, 05004], lr: 0.199989, loss: 2.5952
2022-07-24 21:31:23 - train: epoch 0006, iter [02300, 05004], lr: 0.199988, loss: 2.5167
2022-07-24 21:33:23 - train: epoch 0006, iter [02400, 05004], lr: 0.199987, loss: 2.5856
2022-07-24 21:35:23 - train: epoch 0006, iter [02500, 05004], lr: 0.199986, loss: 2.6473
2022-07-24 21:37:26 - train: epoch 0006, iter [02600, 05004], lr: 0.199985, loss: 2.7260
2022-07-24 21:39:28 - train: epoch 0006, iter [02700, 05004], lr: 0.199984, loss: 2.8072
2022-07-24 21:41:27 - train: epoch 0006, iter [02800, 05004], lr: 0.199983, loss: 2.2035
2022-07-24 21:43:31 - train: epoch 0006, iter [02900, 05004], lr: 0.199982, loss: 2.9446
2022-07-24 21:45:31 - train: epoch 0006, iter [03000, 05004], lr: 0.199980, loss: 2.6937
2022-07-24 21:47:30 - train: epoch 0006, iter [03100, 05004], lr: 0.199979, loss: 2.5314
2022-07-24 21:49:32 - train: epoch 0006, iter [03200, 05004], lr: 0.199978, loss: 2.7389
2022-07-24 21:51:31 - train: epoch 0006, iter [03300, 05004], lr: 0.199976, loss: 2.6792
2022-07-24 21:53:31 - train: epoch 0006, iter [03400, 05004], lr: 0.199975, loss: 2.8924
2022-07-24 21:55:28 - train: epoch 0006, iter [03500, 05004], lr: 0.199973, loss: 2.9577
2022-07-24 21:57:28 - train: epoch 0006, iter [03600, 05004], lr: 0.199972, loss: 2.8722
2022-07-24 21:59:25 - train: epoch 0006, iter [03700, 05004], lr: 0.199970, loss: 2.7360
2022-07-24 22:01:30 - train: epoch 0006, iter [03800, 05004], lr: 0.199968, loss: 2.5263
2022-07-24 22:03:27 - train: epoch 0006, iter [03900, 05004], lr: 0.199967, loss: 2.6399
2022-07-24 22:05:27 - train: epoch 0006, iter [04000, 05004], lr: 0.199965, loss: 2.8133
2022-07-24 22:07:31 - train: epoch 0006, iter [04100, 05004], lr: 0.199963, loss: 3.0335
2022-07-24 22:09:32 - train: epoch 0006, iter [04200, 05004], lr: 0.199961, loss: 2.6666
2022-07-24 22:11:32 - train: epoch 0006, iter [04300, 05004], lr: 0.199960, loss: 2.7550
2022-07-24 22:13:38 - train: epoch 0006, iter [04400, 05004], lr: 0.199958, loss: 2.5288
2022-07-24 22:15:36 - train: epoch 0006, iter [04500, 05004], lr: 0.199956, loss: 2.7350
2022-07-24 22:17:37 - train: epoch 0006, iter [04600, 05004], lr: 0.199954, loss: 2.8086
2022-07-24 22:19:37 - train: epoch 0006, iter [04700, 05004], lr: 0.199952, loss: 2.7338
2022-07-24 22:21:32 - train: epoch 0006, iter [04800, 05004], lr: 0.199950, loss: 2.8046
2022-07-24 22:23:32 - train: epoch 0006, iter [04900, 05004], lr: 0.199948, loss: 2.7067
2022-07-24 22:25:28 - train: epoch 0006, iter [05000, 05004], lr: 0.199945, loss: 2.6565
2022-07-24 22:25:32 - train: epoch 006, train_loss: 2.7561
2022-07-24 22:29:46 - eval: epoch: 006, acc1: 43.756%, acc5: 70.142%, test_loss: 2.4979, per_image_load_time: 3.649ms, per_image_inference_time: 0.796ms
2022-07-24 22:29:47 - until epoch: 006, best_acc1: 43.756%
2022-07-24 22:29:47 - epoch 007 lr: 0.199945
2022-07-24 22:32:05 - train: epoch 0007, iter [00100, 05004], lr: 0.199943, loss: 2.6497
2022-07-24 22:34:10 - train: epoch 0007, iter [00200, 05004], lr: 0.199941, loss: 2.7688
2022-07-24 22:36:19 - train: epoch 0007, iter [00300, 05004], lr: 0.199939, loss: 2.8640
2022-07-24 22:38:23 - train: epoch 0007, iter [00400, 05004], lr: 0.199936, loss: 2.7060
2022-07-24 22:40:24 - train: epoch 0007, iter [00500, 05004], lr: 0.199934, loss: 2.4856
2022-07-24 22:42:25 - train: epoch 0007, iter [00600, 05004], lr: 0.199931, loss: 2.7885
2022-07-24 22:44:25 - train: epoch 0007, iter [00700, 05004], lr: 0.199929, loss: 2.5824
2022-07-24 22:46:47 - train: epoch 0007, iter [00800, 05004], lr: 0.199926, loss: 2.6464
2022-07-24 22:48:49 - train: epoch 0007, iter [00900, 05004], lr: 0.199924, loss: 2.6830
2022-07-24 22:50:55 - train: epoch 0007, iter [01000, 05004], lr: 0.199921, loss: 2.7000
2022-07-24 22:53:03 - train: epoch 0007, iter [01100, 05004], lr: 0.199919, loss: 2.6920
2022-07-24 22:55:09 - train: epoch 0007, iter [01200, 05004], lr: 0.199916, loss: 2.5778
2022-07-24 22:57:12 - train: epoch 0007, iter [01300, 05004], lr: 0.199913, loss: 2.2830
2022-07-24 22:59:15 - train: epoch 0007, iter [01400, 05004], lr: 0.199910, loss: 2.7051
2022-07-24 23:01:17 - train: epoch 0007, iter [01500, 05004], lr: 0.199908, loss: 2.7412
2022-07-24 23:03:21 - train: epoch 0007, iter [01600, 05004], lr: 0.199905, loss: 2.6900
2022-07-24 23:05:26 - train: epoch 0007, iter [01700, 05004], lr: 0.199902, loss: 2.5547
2022-07-24 23:07:27 - train: epoch 0007, iter [01800, 05004], lr: 0.199899, loss: 2.7051
2022-07-24 23:09:30 - train: epoch 0007, iter [01900, 05004], lr: 0.199896, loss: 2.8245
2022-07-24 23:11:34 - train: epoch 0007, iter [02000, 05004], lr: 0.199893, loss: 2.3541
2022-07-24 23:13:34 - train: epoch 0007, iter [02100, 05004], lr: 0.199890, loss: 2.8049
2022-07-24 23:15:40 - train: epoch 0007, iter [02200, 05004], lr: 0.199887, loss: 2.5116
2022-07-24 23:17:39 - train: epoch 0007, iter [02300, 05004], lr: 0.199884, loss: 2.8745
2022-07-24 23:19:38 - train: epoch 0007, iter [02400, 05004], lr: 0.199880, loss: 2.9266
2022-07-24 23:21:40 - train: epoch 0007, iter [02500, 05004], lr: 0.199877, loss: 2.6482
2022-07-24 23:23:48 - train: epoch 0007, iter [02600, 05004], lr: 0.199874, loss: 2.7156
2022-07-24 23:25:49 - train: epoch 0007, iter [02700, 05004], lr: 0.199870, loss: 2.5537
2022-07-24 23:27:52 - train: epoch 0007, iter [02800, 05004], lr: 0.199867, loss: 2.6632
2022-07-24 23:29:57 - train: epoch 0007, iter [02900, 05004], lr: 0.199864, loss: 2.5525
2022-07-24 23:31:57 - train: epoch 0007, iter [03000, 05004], lr: 0.199860, loss: 2.6712
2022-07-24 23:33:59 - train: epoch 0007, iter [03100, 05004], lr: 0.199857, loss: 2.5037
2022-07-24 23:36:00 - train: epoch 0007, iter [03200, 05004], lr: 0.199853, loss: 2.5135
2022-07-24 23:38:02 - train: epoch 0007, iter [03300, 05004], lr: 0.199849, loss: 2.9210
2022-07-24 23:40:04 - train: epoch 0007, iter [03400, 05004], lr: 0.199846, loss: 2.5879
2022-07-24 23:42:05 - train: epoch 0007, iter [03500, 05004], lr: 0.199842, loss: 2.8594
2022-07-24 23:44:11 - train: epoch 0007, iter [03600, 05004], lr: 0.199838, loss: 2.5090
2022-07-24 23:46:12 - train: epoch 0007, iter [03700, 05004], lr: 0.199835, loss: 2.7042
2022-07-24 23:48:14 - train: epoch 0007, iter [03800, 05004], lr: 0.199831, loss: 2.7618
2022-07-24 23:50:18 - train: epoch 0007, iter [03900, 05004], lr: 0.199827, loss: 2.5602
2022-07-24 23:52:15 - train: epoch 0007, iter [04000, 05004], lr: 0.199823, loss: 2.8775
2022-07-24 23:54:14 - train: epoch 0007, iter [04100, 05004], lr: 0.199819, loss: 2.5603
2022-07-24 23:56:16 - train: epoch 0007, iter [04200, 05004], lr: 0.199815, loss: 2.6875
2022-07-24 23:58:16 - train: epoch 0007, iter [04300, 05004], lr: 0.199811, loss: 2.9118
2022-07-25 00:00:13 - train: epoch 0007, iter [04400, 05004], lr: 0.199807, loss: 2.5155
2022-07-25 00:02:14 - train: epoch 0007, iter [04500, 05004], lr: 0.199803, loss: 2.7558
2022-07-25 00:04:10 - train: epoch 0007, iter [04600, 05004], lr: 0.199799, loss: 2.8135
2022-07-25 00:06:12 - train: epoch 0007, iter [04700, 05004], lr: 0.199794, loss: 2.6027
2022-07-25 00:08:15 - train: epoch 0007, iter [04800, 05004], lr: 0.199790, loss: 2.7526
2022-07-25 00:10:14 - train: epoch 0007, iter [04900, 05004], lr: 0.199786, loss: 2.5960
2022-07-25 00:12:10 - train: epoch 0007, iter [05000, 05004], lr: 0.199782, loss: 2.7152
2022-07-25 00:12:14 - train: epoch 007, train_loss: 2.6719
2022-07-25 00:16:13 - eval: epoch: 007, acc1: 46.682%, acc5: 72.820%, test_loss: 2.3549, per_image_load_time: 3.215ms, per_image_inference_time: 0.770ms
2022-07-25 00:16:14 - until epoch: 007, best_acc1: 46.682%
2022-07-25 00:16:14 - epoch 008 lr: 0.199781
2022-07-25 00:18:35 - train: epoch 0008, iter [00100, 05004], lr: 0.199777, loss: 2.5999
2022-07-25 00:20:35 - train: epoch 0008, iter [00200, 05004], lr: 0.199773, loss: 2.7962
2022-07-25 00:22:32 - train: epoch 0008, iter [00300, 05004], lr: 0.199768, loss: 2.5239
2022-07-25 00:24:29 - train: epoch 0008, iter [00400, 05004], lr: 0.199764, loss: 2.6847
2022-07-25 00:26:28 - train: epoch 0008, iter [00500, 05004], lr: 0.199759, loss: 2.3897
2022-07-25 00:28:43 - train: epoch 0008, iter [00600, 05004], lr: 0.199754, loss: 2.6489
2022-07-25 00:30:48 - train: epoch 0008, iter [00700, 05004], lr: 0.199750, loss: 2.6621
2022-07-25 00:32:57 - train: epoch 0008, iter [00800, 05004], lr: 0.199745, loss: 2.3707
2022-07-25 00:35:00 - train: epoch 0008, iter [00900, 05004], lr: 0.199740, loss: 2.5260
2022-07-25 00:37:02 - train: epoch 0008, iter [01000, 05004], lr: 0.199736, loss: 2.8214
2022-07-25 00:39:01 - train: epoch 0008, iter [01100, 05004], lr: 0.199731, loss: 2.3735
2022-07-25 00:41:03 - train: epoch 0008, iter [01200, 05004], lr: 0.199726, loss: 2.5076
2022-07-25 00:43:06 - train: epoch 0008, iter [01300, 05004], lr: 0.199721, loss: 2.6812
2022-07-25 00:45:09 - train: epoch 0008, iter [01400, 05004], lr: 0.199716, loss: 2.5418
2022-07-25 00:47:11 - train: epoch 0008, iter [01500, 05004], lr: 0.199711, loss: 2.7677
2022-07-25 00:49:11 - train: epoch 0008, iter [01600, 05004], lr: 0.199706, loss: 2.6734
2022-07-25 00:51:12 - train: epoch 0008, iter [01700, 05004], lr: 0.199701, loss: 2.5148
2022-07-25 00:53:14 - train: epoch 0008, iter [01800, 05004], lr: 0.199696, loss: 2.6798
2022-07-25 00:55:12 - train: epoch 0008, iter [01900, 05004], lr: 0.199691, loss: 2.5671
2022-07-25 00:57:14 - train: epoch 0008, iter [02000, 05004], lr: 0.199685, loss: 2.7774
2022-07-25 00:59:17 - train: epoch 0008, iter [02100, 05004], lr: 0.199680, loss: 2.6926
2022-07-25 01:01:11 - train: epoch 0008, iter [02200, 05004], lr: 0.199675, loss: 2.6841
2022-07-25 01:03:14 - train: epoch 0008, iter [02300, 05004], lr: 0.199669, loss: 2.8135
2022-07-25 01:05:16 - train: epoch 0008, iter [02400, 05004], lr: 0.199664, loss: 2.5307
2022-07-25 01:07:13 - train: epoch 0008, iter [02500, 05004], lr: 0.199659, loss: 2.7120
2022-07-25 01:09:17 - train: epoch 0008, iter [02600, 05004], lr: 0.199653, loss: 2.5824
2022-07-25 01:11:20 - train: epoch 0008, iter [02700, 05004], lr: 0.199648, loss: 2.7641
2022-07-25 01:13:22 - train: epoch 0008, iter [02800, 05004], lr: 0.199642, loss: 2.6851
2022-07-25 01:15:28 - train: epoch 0008, iter [02900, 05004], lr: 0.199636, loss: 2.5142
2022-07-25 01:17:28 - train: epoch 0008, iter [03000, 05004], lr: 0.199631, loss: 2.6989
2022-07-25 01:19:32 - train: epoch 0008, iter [03100, 05004], lr: 0.199625, loss: 2.6662
2022-07-25 01:21:33 - train: epoch 0008, iter [03200, 05004], lr: 0.199619, loss: 2.8007
2022-07-25 01:23:32 - train: epoch 0008, iter [03300, 05004], lr: 0.199614, loss: 2.6126
2022-07-25 01:25:29 - train: epoch 0008, iter [03400, 05004], lr: 0.199608, loss: 2.4763
2022-07-25 01:27:29 - train: epoch 0008, iter [03500, 05004], lr: 0.199602, loss: 2.7194
2022-07-25 01:29:31 - train: epoch 0008, iter [03600, 05004], lr: 0.199596, loss: 2.8062
2022-07-25 01:31:33 - train: epoch 0008, iter [03700, 05004], lr: 0.199590, loss: 2.4354
2022-07-25 01:33:35 - train: epoch 0008, iter [03800, 05004], lr: 0.199584, loss: 2.5142
2022-07-25 01:35:33 - train: epoch 0008, iter [03900, 05004], lr: 0.199578, loss: 2.6213
2022-07-25 01:37:34 - train: epoch 0008, iter [04000, 05004], lr: 0.199572, loss: 3.0572
2022-07-25 01:39:36 - train: epoch 0008, iter [04100, 05004], lr: 0.199566, loss: 2.5980
2022-07-25 01:41:37 - train: epoch 0008, iter [04200, 05004], lr: 0.199560, loss: 2.5589
2022-07-25 01:43:43 - train: epoch 0008, iter [04300, 05004], lr: 0.199553, loss: 2.4462
2022-07-25 01:45:47 - train: epoch 0008, iter [04400, 05004], lr: 0.199547, loss: 2.6385
2022-07-25 01:47:46 - train: epoch 0008, iter [04500, 05004], lr: 0.199541, loss: 2.5089
2022-07-25 01:49:46 - train: epoch 0008, iter [04600, 05004], lr: 0.199534, loss: 2.4172
2022-07-25 01:51:46 - train: epoch 0008, iter [04700, 05004], lr: 0.199528, loss: 2.5245
2022-07-25 01:53:44 - train: epoch 0008, iter [04800, 05004], lr: 0.199522, loss: 2.6406
2022-07-25 01:55:45 - train: epoch 0008, iter [04900, 05004], lr: 0.199515, loss: 2.6183
2022-07-25 01:57:40 - train: epoch 0008, iter [05000, 05004], lr: 0.199509, loss: 2.5960
2022-07-25 01:57:44 - train: epoch 008, train_loss: 2.6125
2022-07-25 02:01:48 - eval: epoch: 008, acc1: 46.230%, acc5: 72.890%, test_loss: 2.3587, per_image_load_time: 7.725ms, per_image_inference_time: 0.811ms
2022-07-25 02:01:48 - until epoch: 008, best_acc1: 46.682%
2022-07-25 02:01:48 - epoch 009 lr: 0.199508
2022-07-25 02:04:05 - train: epoch 0009, iter [00100, 05004], lr: 0.199502, loss: 2.1814
2022-07-25 02:06:06 - train: epoch 0009, iter [00200, 05004], lr: 0.199495, loss: 2.4760
2022-07-25 02:08:10 - train: epoch 0009, iter [00300, 05004], lr: 0.199488, loss: 2.5303
2022-07-25 02:10:12 - train: epoch 0009, iter [00400, 05004], lr: 0.199482, loss: 2.7543
2022-07-25 02:12:22 - train: epoch 0009, iter [00500, 05004], lr: 0.199475, loss: 2.6736
2022-07-25 02:14:31 - train: epoch 0009, iter [00600, 05004], lr: 0.199468, loss: 2.5170
2022-07-25 02:16:47 - train: epoch 0009, iter [00700, 05004], lr: 0.199461, loss: 2.5286
2022-07-25 02:18:53 - train: epoch 0009, iter [00800, 05004], lr: 0.199455, loss: 2.3797
2022-07-25 02:20:58 - train: epoch 0009, iter [00900, 05004], lr: 0.199448, loss: 2.3384
2022-07-25 02:23:05 - train: epoch 0009, iter [01000, 05004], lr: 0.199441, loss: 2.4317
2022-07-25 02:25:06 - train: epoch 0009, iter [01100, 05004], lr: 0.199434, loss: 2.7091
2022-07-25 02:27:10 - train: epoch 0009, iter [01200, 05004], lr: 0.199427, loss: 2.6302
2022-07-25 02:29:14 - train: epoch 0009, iter [01300, 05004], lr: 0.199420, loss: 2.7027
2022-07-25 02:31:16 - train: epoch 0009, iter [01400, 05004], lr: 0.199412, loss: 2.2623
2022-07-25 02:33:24 - train: epoch 0009, iter [01500, 05004], lr: 0.199405, loss: 2.6706
2022-07-25 02:35:26 - train: epoch 0009, iter [01600, 05004], lr: 0.199398, loss: 2.6124
2022-07-25 02:37:28 - train: epoch 0009, iter [01700, 05004], lr: 0.199391, loss: 2.5507
2022-07-25 02:39:28 - train: epoch 0009, iter [01800, 05004], lr: 0.199383, loss: 2.6700
2022-07-25 02:41:34 - train: epoch 0009, iter [01900, 05004], lr: 0.199376, loss: 2.1098
2022-07-25 02:43:36 - train: epoch 0009, iter [02000, 05004], lr: 0.199369, loss: 2.4614
2022-07-25 02:45:36 - train: epoch 0009, iter [02100, 05004], lr: 0.199361, loss: 2.6268
2022-07-25 02:47:39 - train: epoch 0009, iter [02200, 05004], lr: 0.199354, loss: 2.5307
2022-07-25 02:49:42 - train: epoch 0009, iter [02300, 05004], lr: 0.199346, loss: 2.4554
2022-07-25 02:51:50 - train: epoch 0009, iter [02400, 05004], lr: 0.199339, loss: 2.7018
2022-07-25 02:53:51 - train: epoch 0009, iter [02500, 05004], lr: 0.199331, loss: 2.3732
2022-07-25 02:55:52 - train: epoch 0009, iter [02600, 05004], lr: 0.199323, loss: 2.4596
2022-07-25 02:57:54 - train: epoch 0009, iter [02700, 05004], lr: 0.199316, loss: 2.6532
2022-07-25 02:59:53 - train: epoch 0009, iter [02800, 05004], lr: 0.199308, loss: 2.6610
2022-07-25 03:01:58 - train: epoch 0009, iter [02900, 05004], lr: 0.199300, loss: 2.4849
2022-07-25 03:04:00 - train: epoch 0009, iter [03000, 05004], lr: 0.199292, loss: 2.5022
2022-07-25 03:05:57 - train: epoch 0009, iter [03100, 05004], lr: 0.199285, loss: 2.6198
2022-07-25 03:08:07 - train: epoch 0009, iter [03200, 05004], lr: 0.199277, loss: 2.6174
2022-07-25 03:10:07 - train: epoch 0009, iter [03300, 05004], lr: 0.199269, loss: 2.6855
2022-07-25 03:12:07 - train: epoch 0009, iter [03400, 05004], lr: 0.199261, loss: 2.6957
2022-07-25 03:14:08 - train: epoch 0009, iter [03500, 05004], lr: 0.199253, loss: 2.5251
2022-07-25 03:16:10 - train: epoch 0009, iter [03600, 05004], lr: 0.199245, loss: 2.3970
2022-07-25 03:18:14 - train: epoch 0009, iter [03700, 05004], lr: 0.199236, loss: 2.4778
2022-07-25 03:20:11 - train: epoch 0009, iter [03800, 05004], lr: 0.199228, loss: 2.6689
2022-07-25 03:22:14 - train: epoch 0009, iter [03900, 05004], lr: 0.199220, loss: 2.4876
2022-07-25 03:24:11 - train: epoch 0009, iter [04000, 05004], lr: 0.199212, loss: 2.6608
2022-07-25 03:26:17 - train: epoch 0009, iter [04100, 05004], lr: 0.199203, loss: 2.4723
2022-07-25 03:28:20 - train: epoch 0009, iter [04200, 05004], lr: 0.199195, loss: 2.3796
2022-07-25 03:30:20 - train: epoch 0009, iter [04300, 05004], lr: 0.199187, loss: 2.4079
2022-07-25 03:32:18 - train: epoch 0009, iter [04400, 05004], lr: 0.199178, loss: 2.5462
2022-07-25 03:34:19 - train: epoch 0009, iter [04500, 05004], lr: 0.199170, loss: 2.6532
2022-07-25 03:36:17 - train: epoch 0009, iter [04600, 05004], lr: 0.199161, loss: 2.7285
2022-07-25 03:38:16 - train: epoch 0009, iter [04700, 05004], lr: 0.199153, loss: 2.7563
2022-07-25 03:40:14 - train: epoch 0009, iter [04800, 05004], lr: 0.199144, loss: 2.6627
2022-07-25 03:42:16 - train: epoch 0009, iter [04900, 05004], lr: 0.199135, loss: 2.8451
2022-07-25 03:44:12 - train: epoch 0009, iter [05000, 05004], lr: 0.199127, loss: 2.4411
2022-07-25 03:44:16 - train: epoch 009, train_loss: 2.5616
2022-07-25 03:48:19 - eval: epoch: 009, acc1: 47.736%, acc5: 73.766%, test_loss: 2.2933, per_image_load_time: 2.120ms, per_image_inference_time: 0.690ms
2022-07-25 03:48:20 - until epoch: 009, best_acc1: 47.736%
2022-07-25 03:48:20 - epoch 010 lr: 0.199126
2022-07-25 03:50:40 - train: epoch 0010, iter [00100, 05004], lr: 0.199118, loss: 2.5539
2022-07-25 03:52:38 - train: epoch 0010, iter [00200, 05004], lr: 0.199109, loss: 2.5149
2022-07-25 03:54:22 - train: epoch 0010, iter [00300, 05004], lr: 0.199100, loss: 2.4307
2022-07-25 03:56:48 - train: epoch 0010, iter [00400, 05004], lr: 0.199091, loss: 2.5998
2022-07-25 03:58:47 - train: epoch 0010, iter [00500, 05004], lr: 0.199082, loss: 2.4768
2022-07-25 04:00:59 - train: epoch 0010, iter [00600, 05004], lr: 0.199073, loss: 2.6724
2022-07-25 04:03:04 - train: epoch 0010, iter [00700, 05004], lr: 0.199064, loss: 2.4356
2022-07-25 04:05:03 - train: epoch 0010, iter [00800, 05004], lr: 0.199055, loss: 2.4746
2022-07-25 04:07:02 - train: epoch 0010, iter [00900, 05004], lr: 0.199046, loss: 2.3571
2022-07-25 04:09:04 - train: epoch 0010, iter [01000, 05004], lr: 0.199037, loss: 2.3931
2022-07-25 04:11:03 - train: epoch 0010, iter [01100, 05004], lr: 0.199028, loss: 2.4078
2022-07-25 04:13:01 - train: epoch 0010, iter [01200, 05004], lr: 0.199019, loss: 2.2339
2022-07-25 04:15:09 - train: epoch 0010, iter [01300, 05004], lr: 0.199009, loss: 2.3885
2022-07-25 04:17:05 - train: epoch 0010, iter [01400, 05004], lr: 0.199000, loss: 2.6332
2022-07-25 04:19:05 - train: epoch 0010, iter [01500, 05004], lr: 0.198991, loss: 2.2492
2022-07-25 04:21:09 - train: epoch 0010, iter [01600, 05004], lr: 0.198981, loss: 2.4585
2022-07-25 04:23:04 - train: epoch 0010, iter [01700, 05004], lr: 0.198972, loss: 2.5963
2022-07-25 04:25:04 - train: epoch 0010, iter [01800, 05004], lr: 0.198963, loss: 2.2770
2022-07-25 04:27:04 - train: epoch 0010, iter [01900, 05004], lr: 0.198953, loss: 2.6472
2022-07-25 04:29:06 - train: epoch 0010, iter [02000, 05004], lr: 0.198943, loss: 2.5366
2022-07-25 04:31:05 - train: epoch 0010, iter [02100, 05004], lr: 0.198934, loss: 2.4915
2022-07-25 04:33:03 - train: epoch 0010, iter [02200, 05004], lr: 0.198924, loss: 2.5789
2022-07-25 04:35:02 - train: epoch 0010, iter [02300, 05004], lr: 0.198914, loss: 2.6371
2022-07-25 04:37:03 - train: epoch 0010, iter [02400, 05004], lr: 0.198905, loss: 2.6260
2022-07-25 04:39:02 - train: epoch 0010, iter [02500, 05004], lr: 0.198895, loss: 2.5505
2022-07-25 04:40:55 - train: epoch 0010, iter [02600, 05004], lr: 0.198885, loss: 2.7448
2022-07-25 04:42:57 - train: epoch 0010, iter [02700, 05004], lr: 0.198875, loss: 2.2954
2022-07-25 04:44:55 - train: epoch 0010, iter [02800, 05004], lr: 0.198865, loss: 2.6021
2022-07-25 04:46:57 - train: epoch 0010, iter [02900, 05004], lr: 0.198855, loss: 2.6269
2022-07-25 04:48:56 - train: epoch 0010, iter [03000, 05004], lr: 0.198845, loss: 2.4574
2022-07-25 04:50:53 - train: epoch 0010, iter [03100, 05004], lr: 0.198835, loss: 2.7368
2022-07-25 04:52:52 - train: epoch 0010, iter [03200, 05004], lr: 0.198825, loss: 2.4785
2022-07-25 04:54:48 - train: epoch 0010, iter [03300, 05004], lr: 0.198815, loss: 2.7462
2022-07-25 04:56:51 - train: epoch 0010, iter [03400, 05004], lr: 0.198805, loss: 2.6402
2022-07-25 04:58:45 - train: epoch 0010, iter [03500, 05004], lr: 0.198795, loss: 2.6633
2022-07-25 05:00:42 - train: epoch 0010, iter [03600, 05004], lr: 0.198785, loss: 2.7812
2022-07-25 05:02:41 - train: epoch 0010, iter [03700, 05004], lr: 0.198774, loss: 2.5576
2022-07-25 05:04:40 - train: epoch 0010, iter [03800, 05004], lr: 0.198764, loss: 2.6216
2022-07-25 05:06:40 - train: epoch 0010, iter [03900, 05004], lr: 0.198754, loss: 2.3120
2022-07-25 05:08:38 - train: epoch 0010, iter [04000, 05004], lr: 0.198743, loss: 2.4831
2022-07-25 05:10:39 - train: epoch 0010, iter [04100, 05004], lr: 0.198733, loss: 2.1948
2022-07-25 05:12:38 - train: epoch 0010, iter [04200, 05004], lr: 0.198722, loss: 2.5554
2022-07-25 05:14:37 - train: epoch 0010, iter [04300, 05004], lr: 0.198712, loss: 2.5441
2022-07-25 05:16:39 - train: epoch 0010, iter [04400, 05004], lr: 0.198701, loss: 2.4344
2022-07-25 05:18:42 - train: epoch 0010, iter [04500, 05004], lr: 0.198690, loss: 2.3850
2022-07-25 05:20:38 - train: epoch 0010, iter [04600, 05004], lr: 0.198680, loss: 2.2700
2022-07-25 05:22:38 - train: epoch 0010, iter [04700, 05004], lr: 0.198669, loss: 2.4334
2022-07-25 05:24:32 - train: epoch 0010, iter [04800, 05004], lr: 0.198658, loss: 2.4188
2022-07-25 05:26:25 - train: epoch 0010, iter [04900, 05004], lr: 0.198647, loss: 2.4256
2022-07-25 05:28:20 - train: epoch 0010, iter [05000, 05004], lr: 0.198637, loss: 2.4274
2022-07-25 05:28:23 - train: epoch 010, train_loss: 2.5251
2022-07-25 05:32:19 - eval: epoch: 010, acc1: 47.858%, acc5: 73.826%, test_loss: 2.2775, per_image_load_time: 1.251ms, per_image_inference_time: 0.536ms
2022-07-25 05:32:19 - until epoch: 010, best_acc1: 47.858%
2022-07-25 05:32:19 - epoch 011 lr: 0.198636
2022-07-25 05:34:28 - train: epoch 0011, iter [00100, 05004], lr: 0.198625, loss: 2.3812
2022-07-25 05:36:22 - train: epoch 0011, iter [00200, 05004], lr: 0.198614, loss: 2.6816
2022-07-25 05:38:13 - train: epoch 0011, iter [00300, 05004], lr: 0.198603, loss: 2.5312
2022-07-25 05:40:25 - train: epoch 0011, iter [00400, 05004], lr: 0.198592, loss: 2.7377
2022-07-25 05:42:22 - train: epoch 0011, iter [00500, 05004], lr: 0.198581, loss: 2.2505
2022-07-25 05:44:22 - train: epoch 0011, iter [00600, 05004], lr: 0.198570, loss: 2.5457
2022-07-25 05:46:21 - train: epoch 0011, iter [00700, 05004], lr: 0.198559, loss: 2.5661
2022-07-25 05:48:23 - train: epoch 0011, iter [00800, 05004], lr: 0.198548, loss: 2.5784
2022-07-25 05:50:23 - train: epoch 0011, iter [00900, 05004], lr: 0.198536, loss: 2.5138
2022-07-25 05:52:24 - train: epoch 0011, iter [01000, 05004], lr: 0.198525, loss: 2.3097
2022-07-25 05:54:18 - train: epoch 0011, iter [01100, 05004], lr: 0.198514, loss: 2.6154
2022-07-25 05:56:16 - train: epoch 0011, iter [01200, 05004], lr: 0.198503, loss: 2.6272
2022-07-25 05:58:11 - train: epoch 0011, iter [01300, 05004], lr: 0.198491, loss: 2.7003
2022-07-25 06:00:07 - train: epoch 0011, iter [01400, 05004], lr: 0.198480, loss: 2.5753
2022-07-25 06:02:06 - train: epoch 0011, iter [01500, 05004], lr: 0.198468, loss: 2.4408
2022-07-25 06:03:59 - train: epoch 0011, iter [01600, 05004], lr: 0.198457, loss: 2.5115
2022-07-25 06:05:58 - train: epoch 0011, iter [01700, 05004], lr: 0.198445, loss: 2.6068
2022-07-25 06:08:01 - train: epoch 0011, iter [01800, 05004], lr: 0.198433, loss: 2.3894
2022-07-25 06:10:01 - train: epoch 0011, iter [01900, 05004], lr: 0.198422, loss: 2.3341
2022-07-25 06:12:04 - train: epoch 0011, iter [02000, 05004], lr: 0.198410, loss: 2.5713
2022-07-25 06:14:04 - train: epoch 0011, iter [02100, 05004], lr: 0.198398, loss: 2.2160
2022-07-25 06:16:08 - train: epoch 0011, iter [02200, 05004], lr: 0.198386, loss: 2.5567
2022-07-25 06:18:09 - train: epoch 0011, iter [02300, 05004], lr: 0.198375, loss: 2.5246
2022-07-25 06:20:07 - train: epoch 0011, iter [02400, 05004], lr: 0.198363, loss: 2.3905
2022-07-25 06:22:12 - train: epoch 0011, iter [02500, 05004], lr: 0.198351, loss: 2.4486
2022-07-25 06:24:08 - train: epoch 0011, iter [02600, 05004], lr: 0.198339, loss: 2.5196
2022-07-25 06:26:09 - train: epoch 0011, iter [02700, 05004], lr: 0.198327, loss: 2.6167
2022-07-25 06:28:08 - train: epoch 0011, iter [02800, 05004], lr: 0.198315, loss: 2.2635
2022-07-25 06:30:09 - train: epoch 0011, iter [02900, 05004], lr: 0.198303, loss: 2.3112
2022-07-25 06:32:10 - train: epoch 0011, iter [03000, 05004], lr: 0.198290, loss: 2.7949
2022-07-25 06:34:06 - train: epoch 0011, iter [03100, 05004], lr: 0.198278, loss: 2.4747
2022-07-25 06:36:09 - train: epoch 0011, iter [03200, 05004], lr: 0.198266, loss: 2.3276
2022-07-25 06:38:10 - train: epoch 0011, iter [03300, 05004], lr: 0.198254, loss: 2.7038
2022-07-25 06:40:13 - train: epoch 0011, iter [03400, 05004], lr: 0.198241, loss: 2.5662
2022-07-25 06:42:16 - train: epoch 0011, iter [03500, 05004], lr: 0.198229, loss: 2.4294
2022-07-25 06:44:20 - train: epoch 0011, iter [03600, 05004], lr: 0.198217, loss: 2.4690
2022-07-25 06:46:25 - train: epoch 0011, iter [03700, 05004], lr: 0.198204, loss: 2.7231
2022-07-25 06:48:27 - train: epoch 0011, iter [03800, 05004], lr: 0.198192, loss: 2.4428
2022-07-25 06:50:30 - train: epoch 0011, iter [03900, 05004], lr: 0.198179, loss: 2.7012
2022-07-25 06:52:32 - train: epoch 0011, iter [04000, 05004], lr: 0.198167, loss: 2.5690
2022-07-25 06:54:35 - train: epoch 0011, iter [04100, 05004], lr: 0.198154, loss: 2.5936
2022-07-25 06:56:36 - train: epoch 0011, iter [04200, 05004], lr: 0.198141, loss: 2.4044
2022-07-25 06:58:34 - train: epoch 0011, iter [04300, 05004], lr: 0.198129, loss: 2.5067
2022-07-25 07:00:32 - train: epoch 0011, iter [04400, 05004], lr: 0.198116, loss: 2.4214
2022-07-25 07:02:32 - train: epoch 0011, iter [04500, 05004], lr: 0.198103, loss: 2.2442
2022-07-25 07:04:32 - train: epoch 0011, iter [04600, 05004], lr: 0.198090, loss: 2.5453
2022-07-25 07:06:36 - train: epoch 0011, iter [04700, 05004], lr: 0.198077, loss: 2.1281
2022-07-25 07:08:33 - train: epoch 0011, iter [04800, 05004], lr: 0.198064, loss: 2.2117
2022-07-25 07:10:36 - train: epoch 0011, iter [04900, 05004], lr: 0.198052, loss: 2.4131
2022-07-25 07:12:33 - train: epoch 0011, iter [05000, 05004], lr: 0.198039, loss: 2.3238
2022-07-25 07:12:37 - train: epoch 011, train_loss: 2.4925
2022-07-25 07:16:41 - eval: epoch: 011, acc1: 48.916%, acc5: 74.828%, test_loss: 2.2221, per_image_load_time: 1.575ms, per_image_inference_time: 0.639ms
2022-07-25 07:16:41 - until epoch: 011, best_acc1: 48.916%
2022-07-25 07:16:41 - epoch 012 lr: 0.198038
2022-07-25 07:19:00 - train: epoch 0012, iter [00100, 05004], lr: 0.198025, loss: 2.2484
2022-07-25 07:20:52 - train: epoch 0012, iter [00200, 05004], lr: 0.198012, loss: 2.3587
2022-07-25 07:23:06 - train: epoch 0012, iter [00300, 05004], lr: 0.197999, loss: 2.3956
2022-07-25 07:25:04 - train: epoch 0012, iter [00400, 05004], lr: 0.197986, loss: 2.6766
2022-07-25 07:27:17 - train: epoch 0012, iter [00500, 05004], lr: 0.197972, loss: 2.5795
2022-07-25 07:29:15 - train: epoch 0012, iter [00600, 05004], lr: 0.197959, loss: 2.3050
2022-07-25 07:31:21 - train: epoch 0012, iter [00700, 05004], lr: 0.197946, loss: 2.1726
2022-07-25 07:33:21 - train: epoch 0012, iter [00800, 05004], lr: 0.197932, loss: 2.5564
2022-07-25 07:35:24 - train: epoch 0012, iter [00900, 05004], lr: 0.197919, loss: 2.4532
2022-07-25 07:37:33 - train: epoch 0012, iter [01000, 05004], lr: 0.197906, loss: 2.2350
2022-07-25 07:39:33 - train: epoch 0012, iter [01100, 05004], lr: 0.197892, loss: 2.6606
2022-07-25 07:41:31 - train: epoch 0012, iter [01200, 05004], lr: 0.197879, loss: 2.2805
2022-07-25 07:43:35 - train: epoch 0012, iter [01300, 05004], lr: 0.197865, loss: 2.3427
2022-07-25 07:45:35 - train: epoch 0012, iter [01400, 05004], lr: 0.197851, loss: 2.7258
2022-07-25 07:47:44 - train: epoch 0012, iter [01500, 05004], lr: 0.197838, loss: 2.5437
2022-07-25 07:49:42 - train: epoch 0012, iter [01600, 05004], lr: 0.197824, loss: 2.5707
2022-07-25 07:51:46 - train: epoch 0012, iter [01700, 05004], lr: 0.197810, loss: 2.4354
2022-07-25 07:53:47 - train: epoch 0012, iter [01800, 05004], lr: 0.197797, loss: 2.4533
2022-07-25 07:55:49 - train: epoch 0012, iter [01900, 05004], lr: 0.197783, loss: 2.4344
2022-07-25 07:57:50 - train: epoch 0012, iter [02000, 05004], lr: 0.197769, loss: 2.7306
2022-07-25 07:59:48 - train: epoch 0012, iter [02100, 05004], lr: 0.197755, loss: 2.5409
2022-07-25 08:01:52 - train: epoch 0012, iter [02200, 05004], lr: 0.197741, loss: 2.6072
2022-07-25 08:03:57 - train: epoch 0012, iter [02300, 05004], lr: 0.197727, loss: 2.4746
2022-07-25 08:06:01 - train: epoch 0012, iter [02400, 05004], lr: 0.197713, loss: 2.5687
2022-07-25 08:08:11 - train: epoch 0012, iter [02500, 05004], lr: 0.197699, loss: 2.1545
2022-07-25 08:10:12 - train: epoch 0012, iter [02600, 05004], lr: 0.197685, loss: 2.2569
2022-07-25 08:12:14 - train: epoch 0012, iter [02700, 05004], lr: 0.197671, loss: 2.3133
2022-07-25 08:14:18 - train: epoch 0012, iter [02800, 05004], lr: 0.197656, loss: 2.4792
2022-07-25 08:16:16 - train: epoch 0012, iter [02900, 05004], lr: 0.197642, loss: 2.5003
2022-07-25 08:18:14 - train: epoch 0012, iter [03000, 05004], lr: 0.197628, loss: 2.1769
2022-07-25 08:20:12 - train: epoch 0012, iter [03100, 05004], lr: 0.197614, loss: 2.5293
2022-07-25 08:22:07 - train: epoch 0012, iter [03200, 05004], lr: 0.197599, loss: 2.3227
2022-07-25 08:24:07 - train: epoch 0012, iter [03300, 05004], lr: 0.197585, loss: 2.3105
2022-07-25 08:26:09 - train: epoch 0012, iter [03400, 05004], lr: 0.197570, loss: 2.5076
2022-07-25 08:28:06 - train: epoch 0012, iter [03500, 05004], lr: 0.197556, loss: 2.5213
2022-07-25 08:30:04 - train: epoch 0012, iter [03600, 05004], lr: 0.197541, loss: 2.4568
2022-07-25 08:32:06 - train: epoch 0012, iter [03700, 05004], lr: 0.197527, loss: 2.3672
2022-07-25 08:34:07 - train: epoch 0012, iter [03800, 05004], lr: 0.197512, loss: 2.4564
2022-07-25 08:36:05 - train: epoch 0012, iter [03900, 05004], lr: 0.197497, loss: 2.3952
2022-07-25 08:38:06 - train: epoch 0012, iter [04000, 05004], lr: 0.197483, loss: 2.5796
2022-07-25 08:40:13 - train: epoch 0012, iter [04100, 05004], lr: 0.197468, loss: 2.4979
2022-07-25 08:42:09 - train: epoch 0012, iter [04200, 05004], lr: 0.197453, loss: 2.4406
2022-07-25 08:44:08 - train: epoch 0012, iter [04300, 05004], lr: 0.197438, loss: 2.5066
2022-07-25 08:46:14 - train: epoch 0012, iter [04400, 05004], lr: 0.197423, loss: 2.3671
2022-07-25 08:48:14 - train: epoch 0012, iter [04500, 05004], lr: 0.197409, loss: 2.3713
2022-07-25 08:50:13 - train: epoch 0012, iter [04600, 05004], lr: 0.197394, loss: 2.5769
2022-07-25 08:52:12 - train: epoch 0012, iter [04700, 05004], lr: 0.197379, loss: 2.4296
2022-07-25 08:54:09 - train: epoch 0012, iter [04800, 05004], lr: 0.197364, loss: 2.4375
2022-07-25 08:56:14 - train: epoch 0012, iter [04900, 05004], lr: 0.197348, loss: 2.2874
2022-07-25 08:58:09 - train: epoch 0012, iter [05000, 05004], lr: 0.197333, loss: 2.1171
2022-07-25 08:58:13 - train: epoch 012, train_loss: 2.4664
2022-07-25 09:02:20 - eval: epoch: 012, acc1: 49.464%, acc5: 75.268%, test_loss: 2.1903, per_image_load_time: 8.766ms, per_image_inference_time: 0.673ms
2022-07-25 09:02:21 - until epoch: 012, best_acc1: 49.464%
2022-07-25 09:02:21 - epoch 013 lr: 0.197333
2022-07-25 09:04:45 - train: epoch 0013, iter [00100, 05004], lr: 0.197317, loss: 2.3725
2022-07-25 09:07:08 - train: epoch 0013, iter [00200, 05004], lr: 0.197302, loss: 2.4790
2022-07-25 09:09:06 - train: epoch 0013, iter [00300, 05004], lr: 0.197287, loss: 2.3121
2022-07-25 09:11:17 - train: epoch 0013, iter [00400, 05004], lr: 0.197272, loss: 2.0811
2022-07-25 09:13:12 - train: epoch 0013, iter [00500, 05004], lr: 0.197256, loss: 2.3815
2022-07-25 09:15:14 - train: epoch 0013, iter [00600, 05004], lr: 0.197241, loss: 2.3343
2022-07-25 09:17:15 - train: epoch 0013, iter [00700, 05004], lr: 0.197225, loss: 2.3462
2022-07-25 09:19:24 - train: epoch 0013, iter [00800, 05004], lr: 0.197210, loss: 2.5656
2022-07-25 09:21:33 - train: epoch 0013, iter [00900, 05004], lr: 0.197194, loss: 2.4467
2022-07-25 09:23:36 - train: epoch 0013, iter [01000, 05004], lr: 0.197179, loss: 2.2805
2022-07-25 09:25:36 - train: epoch 0013, iter [01100, 05004], lr: 0.197163, loss: 2.5711
2022-07-25 09:27:36 - train: epoch 0013, iter [01200, 05004], lr: 0.197148, loss: 2.5803
2022-07-25 09:29:32 - train: epoch 0013, iter [01300, 05004], lr: 0.197132, loss: 2.3815
2022-07-25 09:31:34 - train: epoch 0013, iter [01400, 05004], lr: 0.197116, loss: 2.5962
2022-07-25 09:33:35 - train: epoch 0013, iter [01500, 05004], lr: 0.197100, loss: 2.4417
2022-07-25 09:35:33 - train: epoch 0013, iter [01600, 05004], lr: 0.197085, loss: 2.2867
2022-07-25 09:37:34 - train: epoch 0013, iter [01700, 05004], lr: 0.197069, loss: 2.3872
2022-07-25 09:39:28 - train: epoch 0013, iter [01800, 05004], lr: 0.197053, loss: 2.4704
2022-07-25 09:41:28 - train: epoch 0013, iter [01900, 05004], lr: 0.197037, loss: 2.3810
2022-07-25 09:43:28 - train: epoch 0013, iter [02000, 05004], lr: 0.197021, loss: 2.7152
2022-07-25 09:45:29 - train: epoch 0013, iter [02100, 05004], lr: 0.197005, loss: 2.7346
2022-07-25 09:47:37 - train: epoch 0013, iter [02200, 05004], lr: 0.196989, loss: 2.3546
2022-07-25 09:49:39 - train: epoch 0013, iter [02300, 05004], lr: 0.196973, loss: 2.4525
2022-07-25 09:51:39 - train: epoch 0013, iter [02400, 05004], lr: 0.196957, loss: 2.4814
2022-07-25 09:53:38 - train: epoch 0013, iter [02500, 05004], lr: 0.196940, loss: 2.6563
2022-07-25 09:55:37 - train: epoch 0013, iter [02600, 05004], lr: 0.196924, loss: 2.4385
2022-07-25 09:57:33 - train: epoch 0013, iter [02700, 05004], lr: 0.196908, loss: 2.4822
2022-07-25 09:59:39 - train: epoch 0013, iter [02800, 05004], lr: 0.196891, loss: 2.4169
2022-07-25 10:01:35 - train: epoch 0013, iter [02900, 05004], lr: 0.196875, loss: 2.5466
2022-07-25 10:03:33 - train: epoch 0013, iter [03000, 05004], lr: 0.196859, loss: 2.3992
2022-07-25 10:05:32 - train: epoch 0013, iter [03100, 05004], lr: 0.196842, loss: 2.3472
2022-07-25 10:07:32 - train: epoch 0013, iter [03200, 05004], lr: 0.196826, loss: 2.5301
2022-07-25 10:09:29 - train: epoch 0013, iter [03300, 05004], lr: 0.196809, loss: 2.4192
2022-07-25 10:11:28 - train: epoch 0013, iter [03400, 05004], lr: 0.196793, loss: 2.2791
2022-07-25 10:13:20 - train: epoch 0013, iter [03500, 05004], lr: 0.196776, loss: 2.4018
2022-07-25 10:15:23 - train: epoch 0013, iter [03600, 05004], lr: 0.196759, loss: 2.5919
2022-07-25 10:17:27 - train: epoch 0013, iter [03700, 05004], lr: 0.196743, loss: 2.2531
2022-07-25 10:19:30 - train: epoch 0013, iter [03800, 05004], lr: 0.196726, loss: 2.5997
2022-07-25 10:21:33 - train: epoch 0013, iter [03900, 05004], lr: 0.196709, loss: 2.5049
2022-07-25 10:23:34 - train: epoch 0013, iter [04000, 05004], lr: 0.196692, loss: 2.3552
2022-07-25 10:25:34 - train: epoch 0013, iter [04100, 05004], lr: 0.196675, loss: 2.4959
2022-07-25 10:27:40 - train: epoch 0013, iter [04200, 05004], lr: 0.196658, loss: 2.4149
2022-07-25 10:29:41 - train: epoch 0013, iter [04300, 05004], lr: 0.196641, loss: 2.5925
2022-07-25 10:31:39 - train: epoch 0013, iter [04400, 05004], lr: 0.196624, loss: 2.2339
2022-07-25 10:33:40 - train: epoch 0013, iter [04500, 05004], lr: 0.196607, loss: 2.3932
2022-07-25 10:35:36 - train: epoch 0013, iter [04600, 05004], lr: 0.196590, loss: 2.3153
2022-07-25 10:37:32 - train: epoch 0013, iter [04700, 05004], lr: 0.196573, loss: 2.3389
2022-07-25 10:39:26 - train: epoch 0013, iter [04800, 05004], lr: 0.196556, loss: 2.5075
2022-07-25 10:41:23 - train: epoch 0013, iter [04900, 05004], lr: 0.196539, loss: 2.5106
2022-07-25 10:43:21 - train: epoch 0013, iter [05000, 05004], lr: 0.196522, loss: 2.5576
2022-07-25 10:43:26 - train: epoch 013, train_loss: 2.4448
2022-07-25 10:47:31 - eval: epoch: 013, acc1: 50.126%, acc5: 75.878%, test_loss: 2.1620, per_image_load_time: 8.423ms, per_image_inference_time: 0.713ms
2022-07-25 10:47:31 - until epoch: 013, best_acc1: 50.126%
2022-07-25 10:47:31 - epoch 014 lr: 0.196521
2022-07-25 10:50:09 - train: epoch 0014, iter [00100, 05004], lr: 0.196504, loss: 2.3454
2022-07-25 10:52:18 - train: epoch 0014, iter [00200, 05004], lr: 0.196486, loss: 2.3086
2022-07-25 10:54:25 - train: epoch 0014, iter [00300, 05004], lr: 0.196469, loss: 2.2747
2022-07-25 10:56:27 - train: epoch 0014, iter [00400, 05004], lr: 0.196451, loss: 2.3062
2022-07-25 10:58:34 - train: epoch 0014, iter [00500, 05004], lr: 0.196434, loss: 2.1220
2022-07-25 11:00:38 - train: epoch 0014, iter [00600, 05004], lr: 0.196416, loss: 2.4277
2022-07-25 11:02:46 - train: epoch 0014, iter [00700, 05004], lr: 0.196399, loss: 2.0972
2022-07-25 11:04:46 - train: epoch 0014, iter [00800, 05004], lr: 0.196381, loss: 2.2971
2022-07-25 11:06:45 - train: epoch 0014, iter [00900, 05004], lr: 0.196364, loss: 2.3525
2022-07-25 11:08:45 - train: epoch 0014, iter [01000, 05004], lr: 0.196346, loss: 2.4251
2022-07-25 11:10:43 - train: epoch 0014, iter [01100, 05004], lr: 0.196328, loss: 2.4171
2022-07-25 11:12:47 - train: epoch 0014, iter [01200, 05004], lr: 0.196310, loss: 2.4095
2022-07-25 11:14:43 - train: epoch 0014, iter [01300, 05004], lr: 0.196293, loss: 2.7582
2022-07-25 11:16:40 - train: epoch 0014, iter [01400, 05004], lr: 0.196275, loss: 2.4935
2022-07-25 11:18:38 - train: epoch 0014, iter [01500, 05004], lr: 0.196257, loss: 2.4966
2022-07-25 11:20:40 - train: epoch 0014, iter [01600, 05004], lr: 0.196239, loss: 2.5022
2022-07-25 11:22:41 - train: epoch 0014, iter [01700, 05004], lr: 0.196221, loss: 2.5720
2022-07-25 11:24:44 - train: epoch 0014, iter [01800, 05004], lr: 0.196203, loss: 2.6766
2022-07-25 11:26:46 - train: epoch 0014, iter [01900, 05004], lr: 0.196185, loss: 2.3537
2022-07-25 11:28:47 - train: epoch 0014, iter [02000, 05004], lr: 0.196167, loss: 2.4014
2022-07-25 11:30:44 - train: epoch 0014, iter [02100, 05004], lr: 0.196149, loss: 2.5620
2022-07-25 11:32:40 - train: epoch 0014, iter [02200, 05004], lr: 0.196131, loss: 2.4959
2022-07-25 11:34:40 - train: epoch 0014, iter [02300, 05004], lr: 0.196112, loss: 2.2790
2022-07-25 11:36:35 - train: epoch 0014, iter [02400, 05004], lr: 0.196094, loss: 2.4885
2022-07-25 11:38:31 - train: epoch 0014, iter [02500, 05004], lr: 0.196076, loss: 2.2865
2022-07-25 11:40:25 - train: epoch 0014, iter [02600, 05004], lr: 0.196057, loss: 2.5910
2022-07-25 11:42:19 - train: epoch 0014, iter [02700, 05004], lr: 0.196039, loss: 2.4436
2022-07-25 11:44:13 - train: epoch 0014, iter [02800, 05004], lr: 0.196021, loss: 2.7412
2022-07-25 11:46:12 - train: epoch 0014, iter [02900, 05004], lr: 0.196002, loss: 2.6084
2022-07-25 11:48:04 - train: epoch 0014, iter [03000, 05004], lr: 0.195984, loss: 2.5360
2022-07-25 11:49:59 - train: epoch 0014, iter [03100, 05004], lr: 0.195965, loss: 2.4976
2022-07-25 11:51:52 - train: epoch 0014, iter [03200, 05004], lr: 0.195946, loss: 2.6887
2022-07-25 11:53:45 - train: epoch 0014, iter [03300, 05004], lr: 0.195928, loss: 2.2350
2022-07-25 11:55:43 - train: epoch 0014, iter [03400, 05004], lr: 0.195909, loss: 2.4296
2022-07-25 11:57:37 - train: epoch 0014, iter [03500, 05004], lr: 0.195890, loss: 2.4195
2022-07-25 11:59:36 - train: epoch 0014, iter [03600, 05004], lr: 0.195872, loss: 2.3881
2022-07-25 12:01:29 - train: epoch 0014, iter [03700, 05004], lr: 0.195853, loss: 2.3517
