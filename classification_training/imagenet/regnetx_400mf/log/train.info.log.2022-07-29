2022-07-29 12:07:17 - train: epoch 0069, iter [04900, 05004], lr: 0.048162, loss: 1.5493
2022-07-29 12:09:12 - train: epoch 0069, iter [05000, 05004], lr: 0.048105, loss: 1.6152
2022-07-29 12:09:16 - train: epoch 069, train_loss: 1.7060
2022-07-29 12:13:35 - eval: epoch: 069, acc1: 63.808%, acc5: 86.072%, test_loss: 1.4789, per_image_load_time: 8.764ms, per_image_inference_time: 0.729ms
2022-07-29 12:13:36 - until epoch: 069, best_acc1: 63.808%
2022-07-29 12:13:36 - epoch 070 lr: 0.048102
2022-07-29 12:15:58 - train: epoch 0070, iter [00100, 05004], lr: 0.048047, loss: 1.4624
2022-07-29 12:17:58 - train: epoch 0070, iter [00200, 05004], lr: 0.047990, loss: 1.6129
2022-07-29 12:19:58 - train: epoch 0070, iter [00300, 05004], lr: 0.047934, loss: 1.9275
2022-07-29 12:21:50 - train: epoch 0070, iter [00400, 05004], lr: 0.047877, loss: 1.6032
2022-07-29 12:23:50 - train: epoch 0070, iter [00500, 05004], lr: 0.047821, loss: 1.7419
2022-07-29 12:25:42 - train: epoch 0070, iter [00600, 05004], lr: 0.047765, loss: 1.5930
2022-07-29 12:27:36 - train: epoch 0070, iter [00700, 05004], lr: 0.047708, loss: 1.5449
2022-07-29 12:29:33 - train: epoch 0070, iter [00800, 05004], lr: 0.047652, loss: 1.7271
2022-07-29 12:31:37 - train: epoch 0070, iter [00900, 05004], lr: 0.047596, loss: 1.7048
2022-07-29 12:33:38 - train: epoch 0070, iter [01000, 05004], lr: 0.047539, loss: 1.8709
2022-07-29 12:35:38 - train: epoch 0070, iter [01100, 05004], lr: 0.047483, loss: 1.9725
2022-07-29 12:37:39 - train: epoch 0070, iter [01200, 05004], lr: 0.047427, loss: 1.8725
2022-07-29 12:39:37 - train: epoch 0070, iter [01300, 05004], lr: 0.047371, loss: 1.6680
2022-07-29 12:41:38 - train: epoch 0070, iter [01400, 05004], lr: 0.047314, loss: 1.5977
2022-07-29 12:43:36 - train: epoch 0070, iter [01500, 05004], lr: 0.047258, loss: 1.8397
2022-07-29 12:45:33 - train: epoch 0070, iter [01600, 05004], lr: 0.047202, loss: 1.7154
2022-07-29 12:47:31 - train: epoch 0070, iter [01700, 05004], lr: 0.047146, loss: 1.7972
2022-07-29 12:49:26 - train: epoch 0070, iter [01800, 05004], lr: 0.047090, loss: 1.4795
2022-07-29 12:51:19 - train: epoch 0070, iter [01900, 05004], lr: 0.047034, loss: 1.8375
2022-07-29 12:53:17 - train: epoch 0070, iter [02000, 05004], lr: 0.046978, loss: 1.6360
2022-07-29 12:55:13 - train: epoch 0070, iter [02100, 05004], lr: 0.046922, loss: 1.8518
2022-07-29 12:57:11 - train: epoch 0070, iter [02200, 05004], lr: 0.046866, loss: 1.7727
2022-07-29 12:59:04 - train: epoch 0070, iter [02300, 05004], lr: 0.046810, loss: 1.8491
2022-07-29 13:01:02 - train: epoch 0070, iter [02400, 05004], lr: 0.046754, loss: 1.7879
2022-07-29 13:02:52 - train: epoch 0070, iter [02500, 05004], lr: 0.046698, loss: 1.5651
2022-07-29 13:04:46 - train: epoch 0070, iter [02600, 05004], lr: 0.046642, loss: 1.5455
2022-07-29 13:06:40 - train: epoch 0070, iter [02700, 05004], lr: 0.046586, loss: 1.6056
2022-07-29 13:08:36 - train: epoch 0070, iter [02800, 05004], lr: 0.046530, loss: 1.7820
2022-07-29 13:10:32 - train: epoch 0070, iter [02900, 05004], lr: 0.046474, loss: 1.6122
2022-07-29 13:12:34 - train: epoch 0070, iter [03000, 05004], lr: 0.046419, loss: 1.6827
2022-07-29 13:14:23 - train: epoch 0070, iter [03100, 05004], lr: 0.046363, loss: 1.7100
2022-07-29 13:16:17 - train: epoch 0070, iter [03200, 05004], lr: 0.046307, loss: 1.6494
2022-07-29 13:18:18 - train: epoch 0070, iter [03300, 05004], lr: 0.046251, loss: 1.5286
2022-07-29 13:20:15 - train: epoch 0070, iter [03400, 05004], lr: 0.046196, loss: 1.6681
2022-07-29 13:22:12 - train: epoch 0070, iter [03500, 05004], lr: 0.046140, loss: 1.7255
2022-07-29 13:24:06 - train: epoch 0070, iter [03600, 05004], lr: 0.046084, loss: 1.8122
2022-07-29 13:26:03 - train: epoch 0070, iter [03700, 05004], lr: 0.046029, loss: 1.9217
2022-07-29 13:27:59 - train: epoch 0070, iter [03800, 05004], lr: 0.045973, loss: 1.7029
2022-07-29 13:29:58 - train: epoch 0070, iter [03900, 05004], lr: 0.045917, loss: 1.5968
2022-07-29 13:31:54 - train: epoch 0070, iter [04000, 05004], lr: 0.045862, loss: 1.7059
2022-07-29 13:33:49 - train: epoch 0070, iter [04100, 05004], lr: 0.045806, loss: 1.6833
2022-07-29 13:35:46 - train: epoch 0070, iter [04200, 05004], lr: 0.045751, loss: 1.7803
2022-07-29 13:37:43 - train: epoch 0070, iter [04300, 05004], lr: 0.045695, loss: 1.6033
2022-07-29 13:39:35 - train: epoch 0070, iter [04400, 05004], lr: 0.045640, loss: 1.7660
2022-07-29 13:41:37 - train: epoch 0070, iter [04500, 05004], lr: 0.045584, loss: 1.8293
2022-07-29 13:43:30 - train: epoch 0070, iter [04600, 05004], lr: 0.045529, loss: 1.9196
2022-07-29 13:45:20 - train: epoch 0070, iter [04700, 05004], lr: 0.045473, loss: 1.5704
2022-07-29 13:47:18 - train: epoch 0070, iter [04800, 05004], lr: 0.045418, loss: 1.6035
2022-07-29 13:49:16 - train: epoch 0070, iter [04900, 05004], lr: 0.045363, loss: 1.6975
2022-07-29 13:51:07 - train: epoch 0070, iter [05000, 05004], lr: 0.045307, loss: 1.6601
2022-07-29 13:51:11 - train: epoch 070, train_loss: 1.6873
2022-07-29 13:55:26 - eval: epoch: 070, acc1: 64.584%, acc5: 86.430%, test_loss: 1.4554, per_image_load_time: 6.817ms, per_image_inference_time: 0.714ms
2022-07-29 13:55:26 - until epoch: 070, best_acc1: 64.584%
2022-07-29 13:55:26 - epoch 071 lr: 0.045305
2022-07-29 13:57:46 - train: epoch 0071, iter [00100, 05004], lr: 0.045250, loss: 1.4306
2022-07-29 13:59:39 - train: epoch 0071, iter [00200, 05004], lr: 0.045195, loss: 1.5703
2022-07-29 14:01:43 - train: epoch 0071, iter [00300, 05004], lr: 0.045139, loss: 1.7476
2022-07-29 14:03:43 - train: epoch 0071, iter [00400, 05004], lr: 0.045084, loss: 1.7655
2022-07-29 14:05:41 - train: epoch 0071, iter [00500, 05004], lr: 0.045029, loss: 1.8208
2022-07-29 14:07:36 - train: epoch 0071, iter [00600, 05004], lr: 0.044974, loss: 1.8031
2022-07-29 14:09:24 - train: epoch 0071, iter [00700, 05004], lr: 0.044918, loss: 1.7747
2022-07-29 14:11:50 - train: epoch 0071, iter [00800, 05004], lr: 0.044863, loss: 1.7464
2022-07-29 14:13:49 - train: epoch 0071, iter [00900, 05004], lr: 0.044808, loss: 1.8869
2022-07-29 14:15:55 - train: epoch 0071, iter [01000, 05004], lr: 0.044753, loss: 1.5322
2022-07-29 14:18:02 - train: epoch 0071, iter [01100, 05004], lr: 0.044698, loss: 1.6977
2022-07-29 14:20:11 - train: epoch 0071, iter [01200, 05004], lr: 0.044643, loss: 1.7202
2022-07-29 14:22:12 - train: epoch 0071, iter [01300, 05004], lr: 0.044588, loss: 1.8725
2022-07-29 14:24:12 - train: epoch 0071, iter [01400, 05004], lr: 0.044533, loss: 1.6156
2022-07-29 14:26:14 - train: epoch 0071, iter [01500, 05004], lr: 0.044478, loss: 1.6059
2022-07-29 14:28:14 - train: epoch 0071, iter [01600, 05004], lr: 0.044423, loss: 1.4426
2022-07-29 14:30:16 - train: epoch 0071, iter [01700, 05004], lr: 0.044368, loss: 1.7015
2022-07-29 14:32:14 - train: epoch 0071, iter [01800, 05004], lr: 0.044313, loss: 1.9212
2022-07-29 14:34:14 - train: epoch 0071, iter [01900, 05004], lr: 0.044258, loss: 1.5904
2022-07-29 14:36:14 - train: epoch 0071, iter [02000, 05004], lr: 0.044203, loss: 1.6837
2022-07-29 14:38:05 - train: epoch 0071, iter [02100, 05004], lr: 0.044149, loss: 1.6180
2022-07-29 14:40:02 - train: epoch 0071, iter [02200, 05004], lr: 0.044094, loss: 1.5244
2022-07-29 14:41:57 - train: epoch 0071, iter [02300, 05004], lr: 0.044039, loss: 1.6549
2022-07-29 14:43:55 - train: epoch 0071, iter [02400, 05004], lr: 0.043984, loss: 1.5316
2022-07-29 14:45:55 - train: epoch 0071, iter [02500, 05004], lr: 0.043930, loss: 1.8064
2022-07-29 14:47:56 - train: epoch 0071, iter [02600, 05004], lr: 0.043875, loss: 1.4725
2022-07-29 14:49:52 - train: epoch 0071, iter [02700, 05004], lr: 0.043820, loss: 1.6683
2022-07-29 14:51:57 - train: epoch 0071, iter [02800, 05004], lr: 0.043766, loss: 1.8161
2022-07-29 14:53:56 - train: epoch 0071, iter [02900, 05004], lr: 0.043711, loss: 1.4429
2022-07-29 14:55:56 - train: epoch 0071, iter [03000, 05004], lr: 0.043656, loss: 1.6844
2022-07-29 14:57:52 - train: epoch 0071, iter [03100, 05004], lr: 0.043602, loss: 1.4948
2022-07-29 14:59:48 - train: epoch 0071, iter [03200, 05004], lr: 0.043547, loss: 1.4703
2022-07-29 15:01:46 - train: epoch 0071, iter [03300, 05004], lr: 0.043493, loss: 1.8212
2022-07-29 15:03:45 - train: epoch 0071, iter [03400, 05004], lr: 0.043438, loss: 1.5220
2022-07-29 15:05:38 - train: epoch 0071, iter [03500, 05004], lr: 0.043384, loss: 1.6145
2022-07-29 15:07:37 - train: epoch 0071, iter [03600, 05004], lr: 0.043329, loss: 1.6567
2022-07-29 15:09:39 - train: epoch 0071, iter [03700, 05004], lr: 0.043275, loss: 1.7718
2022-07-29 15:11:41 - train: epoch 0071, iter [03800, 05004], lr: 0.043220, loss: 1.6423
2022-07-29 15:13:43 - train: epoch 0071, iter [03900, 05004], lr: 0.043166, loss: 1.7540
2022-07-29 15:15:34 - train: epoch 0071, iter [04000, 05004], lr: 0.043112, loss: 1.6260
2022-07-29 15:17:31 - train: epoch 0071, iter [04100, 05004], lr: 0.043057, loss: 1.7757
2022-07-29 15:19:26 - train: epoch 0071, iter [04200, 05004], lr: 0.043003, loss: 1.9205
2022-07-29 15:21:21 - train: epoch 0071, iter [04300, 05004], lr: 0.042949, loss: 1.4788
2022-07-29 15:23:21 - train: epoch 0071, iter [04400, 05004], lr: 0.042894, loss: 1.8231
2022-07-29 15:25:22 - train: epoch 0071, iter [04500, 05004], lr: 0.042840, loss: 1.7272
2022-07-29 15:27:14 - train: epoch 0071, iter [04600, 05004], lr: 0.042786, loss: 1.6668
2022-07-29 15:29:17 - train: epoch 0071, iter [04700, 05004], lr: 0.042732, loss: 1.7424
2022-07-29 15:31:15 - train: epoch 0071, iter [04800, 05004], lr: 0.042678, loss: 1.5451
2022-07-29 15:33:08 - train: epoch 0071, iter [04900, 05004], lr: 0.042623, loss: 1.4912
2022-07-29 15:35:04 - train: epoch 0071, iter [05000, 05004], lr: 0.042569, loss: 1.5966
2022-07-29 15:35:07 - train: epoch 071, train_loss: 1.6662
2022-07-29 15:39:20 - eval: epoch: 071, acc1: 65.196%, acc5: 86.668%, test_loss: 1.4335, per_image_load_time: 4.093ms, per_image_inference_time: 0.683ms
2022-07-29 15:39:21 - until epoch: 071, best_acc1: 65.196%
2022-07-29 15:39:21 - epoch 072 lr: 0.042567
2022-07-29 15:41:36 - train: epoch 0072, iter [00100, 05004], lr: 0.042513, loss: 1.7742
2022-07-29 15:43:29 - train: epoch 0072, iter [00200, 05004], lr: 0.042459, loss: 1.3718
2022-07-29 15:45:27 - train: epoch 0072, iter [00300, 05004], lr: 0.042405, loss: 1.7466
2022-07-29 15:47:22 - train: epoch 0072, iter [00400, 05004], lr: 0.042351, loss: 1.6315
2022-07-29 15:49:24 - train: epoch 0072, iter [00500, 05004], lr: 0.042297, loss: 1.5408
2022-07-29 15:51:20 - train: epoch 0072, iter [00600, 05004], lr: 0.042243, loss: 1.5340
2022-07-29 15:53:32 - train: epoch 0072, iter [00700, 05004], lr: 0.042189, loss: 1.5687
2022-07-29 15:55:24 - train: epoch 0072, iter [00800, 05004], lr: 0.042135, loss: 1.7112
2022-07-29 15:57:27 - train: epoch 0072, iter [00900, 05004], lr: 0.042081, loss: 1.6936
2022-07-29 15:59:25 - train: epoch 0072, iter [01000, 05004], lr: 0.042027, loss: 1.3858
2022-07-29 16:01:21 - train: epoch 0072, iter [01100, 05004], lr: 0.041974, loss: 1.8860
2022-07-29 16:03:17 - train: epoch 0072, iter [01200, 05004], lr: 0.041920, loss: 1.5332
2022-07-29 16:05:19 - train: epoch 0072, iter [01300, 05004], lr: 0.041866, loss: 1.7128
2022-07-29 16:07:20 - train: epoch 0072, iter [01400, 05004], lr: 0.041812, loss: 1.6105
2022-07-29 16:09:20 - train: epoch 0072, iter [01500, 05004], lr: 0.041758, loss: 1.6658
2022-07-29 16:11:22 - train: epoch 0072, iter [01600, 05004], lr: 0.041705, loss: 1.4721
2022-07-29 16:13:15 - train: epoch 0072, iter [01700, 05004], lr: 0.041651, loss: 1.5331
2022-07-29 16:15:10 - train: epoch 0072, iter [01800, 05004], lr: 0.041597, loss: 1.4727
2022-07-29 16:17:12 - train: epoch 0072, iter [01900, 05004], lr: 0.041544, loss: 1.7266
2022-07-29 16:19:08 - train: epoch 0072, iter [02000, 05004], lr: 0.041490, loss: 1.5992
2022-07-29 16:21:11 - train: epoch 0072, iter [02100, 05004], lr: 0.041437, loss: 1.6697
2022-07-29 16:23:15 - train: epoch 0072, iter [02200, 05004], lr: 0.041383, loss: 1.8238
2022-07-29 16:25:20 - train: epoch 0072, iter [02300, 05004], lr: 0.041330, loss: 1.7359
2022-07-29 16:27:17 - train: epoch 0072, iter [02400, 05004], lr: 0.041276, loss: 1.5325
2022-07-29 16:29:20 - train: epoch 0072, iter [02500, 05004], lr: 0.041223, loss: 1.6764
2022-07-29 16:31:22 - train: epoch 0072, iter [02600, 05004], lr: 0.041169, loss: 1.6947
2022-07-29 16:33:21 - train: epoch 0072, iter [02700, 05004], lr: 0.041116, loss: 1.7840
2022-07-29 16:35:21 - train: epoch 0072, iter [02800, 05004], lr: 0.041062, loss: 1.7078
2022-07-29 16:37:20 - train: epoch 0072, iter [02900, 05004], lr: 0.041009, loss: 1.5613
2022-07-29 16:39:23 - train: epoch 0072, iter [03000, 05004], lr: 0.040956, loss: 1.7477
2022-07-29 16:41:25 - train: epoch 0072, iter [03100, 05004], lr: 0.040902, loss: 1.8215
2022-07-29 16:43:27 - train: epoch 0072, iter [03200, 05004], lr: 0.040849, loss: 1.4783
2022-07-29 16:45:27 - train: epoch 0072, iter [03300, 05004], lr: 0.040796, loss: 1.6699
2022-07-29 16:47:31 - train: epoch 0072, iter [03400, 05004], lr: 0.040742, loss: 1.7589
2022-07-29 16:49:28 - train: epoch 0072, iter [03500, 05004], lr: 0.040689, loss: 1.5330
2022-07-29 16:51:27 - train: epoch 0072, iter [03600, 05004], lr: 0.040636, loss: 1.5643
2022-07-29 16:53:26 - train: epoch 0072, iter [03700, 05004], lr: 0.040583, loss: 1.9536
2022-07-29 16:55:23 - train: epoch 0072, iter [03800, 05004], lr: 0.040530, loss: 1.7594
2022-07-29 16:57:18 - train: epoch 0072, iter [03900, 05004], lr: 0.040477, loss: 1.6021
2022-07-29 16:59:20 - train: epoch 0072, iter [04000, 05004], lr: 0.040423, loss: 1.5630
2022-07-29 17:01:20 - train: epoch 0072, iter [04100, 05004], lr: 0.040370, loss: 1.8523
2022-07-29 17:03:16 - train: epoch 0072, iter [04200, 05004], lr: 0.040317, loss: 1.6466
2022-07-29 17:05:21 - train: epoch 0072, iter [04300, 05004], lr: 0.040264, loss: 1.5416
2022-07-29 17:07:17 - train: epoch 0072, iter [04400, 05004], lr: 0.040211, loss: 1.5967
2022-07-29 17:09:19 - train: epoch 0072, iter [04500, 05004], lr: 0.040158, loss: 1.7058
2022-07-29 17:11:17 - train: epoch 0072, iter [04600, 05004], lr: 0.040105, loss: 1.5840
2022-07-29 17:13:17 - train: epoch 0072, iter [04700, 05004], lr: 0.040053, loss: 1.6822
2022-07-29 17:15:14 - train: epoch 0072, iter [04800, 05004], lr: 0.040000, loss: 1.9701
2022-07-29 17:17:14 - train: epoch 0072, iter [04900, 05004], lr: 0.039947, loss: 1.7394
2022-07-29 17:19:10 - train: epoch 0072, iter [05000, 05004], lr: 0.039894, loss: 1.6144
2022-07-29 17:19:15 - train: epoch 072, train_loss: 1.6469
2022-07-29 17:23:21 - eval: epoch: 072, acc1: 65.062%, acc5: 86.528%, test_loss: 1.4382, per_image_load_time: 5.684ms, per_image_inference_time: 0.683ms
2022-07-29 17:23:22 - until epoch: 072, best_acc1: 65.196%
2022-07-29 17:23:22 - epoch 073 lr: 0.039891
2022-07-29 17:25:42 - train: epoch 0073, iter [00100, 05004], lr: 0.039839, loss: 2.0402
2022-07-29 17:27:41 - train: epoch 0073, iter [00200, 05004], lr: 0.039786, loss: 1.6767
2022-07-29 17:29:45 - train: epoch 0073, iter [00300, 05004], lr: 0.039734, loss: 1.8064
2022-07-29 17:31:50 - train: epoch 0073, iter [00400, 05004], lr: 0.039681, loss: 1.4382
2022-07-29 17:33:54 - train: epoch 0073, iter [00500, 05004], lr: 0.039628, loss: 1.4950
2022-07-29 17:36:05 - train: epoch 0073, iter [00600, 05004], lr: 0.039575, loss: 1.5525
2022-07-29 17:38:00 - train: epoch 0073, iter [00700, 05004], lr: 0.039523, loss: 1.6959
2022-07-29 17:40:10 - train: epoch 0073, iter [00800, 05004], lr: 0.039470, loss: 1.5473
2022-07-29 17:42:07 - train: epoch 0073, iter [00900, 05004], lr: 0.039418, loss: 1.4909
2022-07-29 17:44:08 - train: epoch 0073, iter [01000, 05004], lr: 0.039365, loss: 1.4845
2022-07-29 17:46:14 - train: epoch 0073, iter [01100, 05004], lr: 0.039313, loss: 1.7576
2022-07-29 17:48:15 - train: epoch 0073, iter [01200, 05004], lr: 0.039260, loss: 1.6648
2022-07-29 17:50:17 - train: epoch 0073, iter [01300, 05004], lr: 0.039208, loss: 1.6281
2022-07-29 17:52:15 - train: epoch 0073, iter [01400, 05004], lr: 0.039155, loss: 1.4673
2022-07-29 17:54:16 - train: epoch 0073, iter [01500, 05004], lr: 0.039103, loss: 1.6587
2022-07-29 17:56:14 - train: epoch 0073, iter [01600, 05004], lr: 0.039050, loss: 1.5525
2022-07-29 17:58:18 - train: epoch 0073, iter [01700, 05004], lr: 0.038998, loss: 1.5849
2022-07-29 18:00:14 - train: epoch 0073, iter [01800, 05004], lr: 0.038945, loss: 1.6360
2022-07-29 18:02:12 - train: epoch 0073, iter [01900, 05004], lr: 0.038893, loss: 1.7132
2022-07-29 18:04:02 - train: epoch 0073, iter [02000, 05004], lr: 0.038841, loss: 1.5823
2022-07-29 18:05:53 - train: epoch 0073, iter [02100, 05004], lr: 0.038789, loss: 1.6652
2022-07-29 18:07:50 - train: epoch 0073, iter [02200, 05004], lr: 0.038736, loss: 1.6684
2022-07-29 18:09:43 - train: epoch 0073, iter [02300, 05004], lr: 0.038684, loss: 1.6103
2022-07-29 18:11:37 - train: epoch 0073, iter [02400, 05004], lr: 0.038632, loss: 1.7194
2022-07-29 18:13:28 - train: epoch 0073, iter [02500, 05004], lr: 0.038580, loss: 1.6176
2022-07-29 18:15:23 - train: epoch 0073, iter [02600, 05004], lr: 0.038528, loss: 1.6848
2022-07-29 18:17:13 - train: epoch 0073, iter [02700, 05004], lr: 0.038476, loss: 1.4345
2022-07-29 18:19:06 - train: epoch 0073, iter [02800, 05004], lr: 0.038423, loss: 1.6826
2022-07-29 18:20:59 - train: epoch 0073, iter [02900, 05004], lr: 0.038371, loss: 1.7435
2022-07-29 18:22:51 - train: epoch 0073, iter [03000, 05004], lr: 0.038319, loss: 1.5045
2022-07-29 18:24:43 - train: epoch 0073, iter [03100, 05004], lr: 0.038267, loss: 1.5773
2022-07-29 18:26:36 - train: epoch 0073, iter [03200, 05004], lr: 0.038215, loss: 1.3608
2022-07-29 18:28:29 - train: epoch 0073, iter [03300, 05004], lr: 0.038163, loss: 1.6277
2022-07-29 18:30:18 - train: epoch 0073, iter [03400, 05004], lr: 0.038111, loss: 1.6941
2022-07-29 18:32:17 - train: epoch 0073, iter [03500, 05004], lr: 0.038060, loss: 1.5896
2022-07-29 18:34:06 - train: epoch 0073, iter [03600, 05004], lr: 0.038008, loss: 1.4470
2022-07-29 18:36:00 - train: epoch 0073, iter [03700, 05004], lr: 0.037956, loss: 1.7040
2022-07-29 18:37:52 - train: epoch 0073, iter [03800, 05004], lr: 0.037904, loss: 1.6819
2022-07-29 18:39:40 - train: epoch 0073, iter [03900, 05004], lr: 0.037852, loss: 1.5865
2022-07-29 18:41:31 - train: epoch 0073, iter [04000, 05004], lr: 0.037801, loss: 1.7805
2022-07-29 18:43:27 - train: epoch 0073, iter [04100, 05004], lr: 0.037749, loss: 1.5877
2022-07-29 18:45:21 - train: epoch 0073, iter [04200, 05004], lr: 0.037697, loss: 1.9665
2022-07-29 18:47:07 - train: epoch 0073, iter [04300, 05004], lr: 0.037645, loss: 1.6531
2022-07-29 18:49:01 - train: epoch 0073, iter [04400, 05004], lr: 0.037594, loss: 1.7381
2022-07-29 18:50:54 - train: epoch 0073, iter [04500, 05004], lr: 0.037542, loss: 1.5129
2022-07-29 18:52:50 - train: epoch 0073, iter [04600, 05004], lr: 0.037491, loss: 1.9723
2022-07-29 18:54:47 - train: epoch 0073, iter [04700, 05004], lr: 0.037439, loss: 1.5948
2022-07-29 18:56:50 - train: epoch 0073, iter [04800, 05004], lr: 0.037387, loss: 1.6822
2022-07-29 18:58:47 - train: epoch 0073, iter [04900, 05004], lr: 0.037336, loss: 1.4977
2022-07-29 19:00:43 - train: epoch 0073, iter [05000, 05004], lr: 0.037284, loss: 1.6854
2022-07-29 19:00:46 - train: epoch 073, train_loss: 1.6286
2022-07-29 19:04:45 - eval: epoch: 073, acc1: 65.662%, acc5: 87.054%, test_loss: 1.4168, per_image_load_time: 7.236ms, per_image_inference_time: 0.602ms
2022-07-29 19:04:46 - until epoch: 073, best_acc1: 65.662%
2022-07-29 19:04:46 - epoch 074 lr: 0.037282
2022-07-29 19:07:08 - train: epoch 0074, iter [00100, 05004], lr: 0.037231, loss: 1.4476
2022-07-29 19:09:08 - train: epoch 0074, iter [00200, 05004], lr: 0.037179, loss: 1.5546
2022-07-29 19:11:14 - train: epoch 0074, iter [00300, 05004], lr: 0.037128, loss: 1.7225
2022-07-29 19:13:11 - train: epoch 0074, iter [00400, 05004], lr: 0.037077, loss: 1.7777
2022-07-29 19:15:37 - train: epoch 0074, iter [00500, 05004], lr: 0.037025, loss: 1.5542
2022-07-29 19:17:34 - train: epoch 0074, iter [00600, 05004], lr: 0.036974, loss: 1.4950
2022-07-29 19:19:33 - train: epoch 0074, iter [00700, 05004], lr: 0.036923, loss: 1.4860
2022-07-29 19:21:34 - train: epoch 0074, iter [00800, 05004], lr: 0.036871, loss: 1.7554
2022-07-29 19:23:28 - train: epoch 0074, iter [00900, 05004], lr: 0.036820, loss: 1.5114
2022-07-29 19:25:25 - train: epoch 0074, iter [01000, 05004], lr: 0.036769, loss: 1.7114
2022-07-29 19:27:25 - train: epoch 0074, iter [01100, 05004], lr: 0.036718, loss: 1.7242
2022-07-29 19:29:26 - train: epoch 0074, iter [01200, 05004], lr: 0.036667, loss: 1.6058
2022-07-29 19:31:24 - train: epoch 0074, iter [01300, 05004], lr: 0.036616, loss: 1.6458
2022-07-29 19:33:20 - train: epoch 0074, iter [01400, 05004], lr: 0.036564, loss: 1.5708
2022-07-29 19:35:18 - train: epoch 0074, iter [01500, 05004], lr: 0.036513, loss: 1.8434
2022-07-29 19:37:19 - train: epoch 0074, iter [01600, 05004], lr: 0.036462, loss: 1.4651
2022-07-29 19:39:20 - train: epoch 0074, iter [01700, 05004], lr: 0.036411, loss: 1.7650
2022-07-29 19:41:16 - train: epoch 0074, iter [01800, 05004], lr: 0.036360, loss: 1.7437
2022-07-29 19:43:18 - train: epoch 0074, iter [01900, 05004], lr: 0.036309, loss: 1.7561
2022-07-29 19:45:13 - train: epoch 0074, iter [02000, 05004], lr: 0.036258, loss: 1.3910
2022-07-29 19:47:08 - train: epoch 0074, iter [02100, 05004], lr: 0.036208, loss: 1.5552
2022-07-29 19:49:04 - train: epoch 0074, iter [02200, 05004], lr: 0.036157, loss: 1.8092
2022-07-29 19:50:58 - train: epoch 0074, iter [02300, 05004], lr: 0.036106, loss: 1.5731
2022-07-29 19:52:55 - train: epoch 0074, iter [02400, 05004], lr: 0.036055, loss: 1.6432
2022-07-29 19:54:47 - train: epoch 0074, iter [02500, 05004], lr: 0.036004, loss: 1.6345
2022-07-29 19:56:48 - train: epoch 0074, iter [02600, 05004], lr: 0.035953, loss: 1.4880
2022-07-29 19:58:45 - train: epoch 0074, iter [02700, 05004], lr: 0.035903, loss: 1.5168
2022-07-29 20:00:42 - train: epoch 0074, iter [02800, 05004], lr: 0.035852, loss: 1.7857
2022-07-29 20:02:41 - train: epoch 0074, iter [02900, 05004], lr: 0.035801, loss: 1.7917
2022-07-29 20:04:35 - train: epoch 0074, iter [03000, 05004], lr: 0.035751, loss: 1.5840
2022-07-29 20:06:30 - train: epoch 0074, iter [03100, 05004], lr: 0.035700, loss: 1.8417
2022-07-29 20:08:28 - train: epoch 0074, iter [03200, 05004], lr: 0.035649, loss: 1.3533
2022-07-29 20:10:24 - train: epoch 0074, iter [03300, 05004], lr: 0.035599, loss: 1.7919
2022-07-29 20:12:23 - train: epoch 0074, iter [03400, 05004], lr: 0.035548, loss: 1.6965
2022-07-29 20:14:16 - train: epoch 0074, iter [03500, 05004], lr: 0.035498, loss: 1.5149
2022-07-29 20:16:07 - train: epoch 0074, iter [03600, 05004], lr: 0.035447, loss: 1.6066
2022-07-29 20:18:05 - train: epoch 0074, iter [03700, 05004], lr: 0.035397, loss: 1.7086
2022-07-29 20:20:05 - train: epoch 0074, iter [03800, 05004], lr: 0.035346, loss: 1.4879
2022-07-29 20:22:04 - train: epoch 0074, iter [03900, 05004], lr: 0.035296, loss: 1.4209
2022-07-29 20:24:05 - train: epoch 0074, iter [04000, 05004], lr: 0.035246, loss: 1.7467
2022-07-29 20:26:02 - train: epoch 0074, iter [04100, 05004], lr: 0.035195, loss: 1.5423
2022-07-29 20:28:02 - train: epoch 0074, iter [04200, 05004], lr: 0.035145, loss: 1.6283
2022-07-29 20:30:00 - train: epoch 0074, iter [04300, 05004], lr: 0.035095, loss: 1.6695
2022-07-29 20:32:00 - train: epoch 0074, iter [04400, 05004], lr: 0.035044, loss: 1.6177
2022-07-29 20:33:53 - train: epoch 0074, iter [04500, 05004], lr: 0.034994, loss: 1.5640
2022-07-29 20:35:52 - train: epoch 0074, iter [04600, 05004], lr: 0.034944, loss: 1.7874
2022-07-29 20:37:51 - train: epoch 0074, iter [04700, 05004], lr: 0.034894, loss: 1.6489
2022-07-29 20:39:47 - train: epoch 0074, iter [04800, 05004], lr: 0.034844, loss: 1.5053
2022-07-29 20:41:45 - train: epoch 0074, iter [04900, 05004], lr: 0.034794, loss: 1.9040
2022-07-29 20:43:43 - train: epoch 0074, iter [05000, 05004], lr: 0.034743, loss: 1.6358
2022-07-29 20:43:46 - train: epoch 074, train_loss: 1.6052
2022-07-29 20:48:07 - eval: epoch: 074, acc1: 65.626%, acc5: 87.094%, test_loss: 1.4035, per_image_load_time: 9.073ms, per_image_inference_time: 0.643ms
2022-07-29 20:48:07 - until epoch: 074, best_acc1: 65.662%
2022-07-29 20:48:07 - epoch 075 lr: 0.034741
2022-07-29 20:50:28 - train: epoch 0075, iter [00100, 05004], lr: 0.034691, loss: 1.6436
2022-07-29 20:52:34 - train: epoch 0075, iter [00200, 05004], lr: 0.034641, loss: 1.4390
2022-07-29 20:54:36 - train: epoch 0075, iter [00300, 05004], lr: 0.034591, loss: 1.6579
2022-07-29 20:57:00 - train: epoch 0075, iter [00400, 05004], lr: 0.034541, loss: 1.6967
2022-07-29 20:58:57 - train: epoch 0075, iter [00500, 05004], lr: 0.034491, loss: 1.8560
2022-07-29 21:00:54 - train: epoch 0075, iter [00600, 05004], lr: 0.034441, loss: 1.4609
2022-07-29 21:02:57 - train: epoch 0075, iter [00700, 05004], lr: 0.034392, loss: 1.6529
2022-07-29 21:04:58 - train: epoch 0075, iter [00800, 05004], lr: 0.034342, loss: 1.4835
2022-07-29 21:07:01 - train: epoch 0075, iter [00900, 05004], lr: 0.034292, loss: 1.4634
2022-07-29 21:09:01 - train: epoch 0075, iter [01000, 05004], lr: 0.034242, loss: 1.2322
2022-07-29 21:11:04 - train: epoch 0075, iter [01100, 05004], lr: 0.034192, loss: 1.7155
2022-07-29 21:13:09 - train: epoch 0075, iter [01200, 05004], lr: 0.034143, loss: 1.3139
2022-07-29 21:15:10 - train: epoch 0075, iter [01300, 05004], lr: 0.034093, loss: 1.4771
2022-07-29 21:17:08 - train: epoch 0075, iter [01400, 05004], lr: 0.034043, loss: 1.4668
2022-07-29 21:19:07 - train: epoch 0075, iter [01500, 05004], lr: 0.033994, loss: 1.5238
2022-07-29 21:21:01 - train: epoch 0075, iter [01600, 05004], lr: 0.033944, loss: 1.5046
2022-07-29 21:22:59 - train: epoch 0075, iter [01700, 05004], lr: 0.033894, loss: 1.5203
2022-07-29 21:24:56 - train: epoch 0075, iter [01800, 05004], lr: 0.033845, loss: 1.7294
2022-07-29 21:26:54 - train: epoch 0075, iter [01900, 05004], lr: 0.033795, loss: 1.4374
2022-07-29 21:28:54 - train: epoch 0075, iter [02000, 05004], lr: 0.033746, loss: 1.6606
2022-07-29 21:30:56 - train: epoch 0075, iter [02100, 05004], lr: 0.033696, loss: 1.4213
2022-07-29 21:32:58 - train: epoch 0075, iter [02200, 05004], lr: 0.033647, loss: 1.6020
2022-07-29 21:34:57 - train: epoch 0075, iter [02300, 05004], lr: 0.033597, loss: 1.4815
2022-07-29 21:36:55 - train: epoch 0075, iter [02400, 05004], lr: 0.033548, loss: 1.6706
2022-07-29 21:38:52 - train: epoch 0075, iter [02500, 05004], lr: 0.033499, loss: 1.7383
2022-07-29 21:40:47 - train: epoch 0075, iter [02600, 05004], lr: 0.033449, loss: 1.5684
2022-07-29 21:42:44 - train: epoch 0075, iter [02700, 05004], lr: 0.033400, loss: 1.4796
2022-07-29 21:44:38 - train: epoch 0075, iter [02800, 05004], lr: 0.033351, loss: 1.5092
2022-07-29 21:46:34 - train: epoch 0075, iter [02900, 05004], lr: 0.033301, loss: 1.7802
2022-07-29 21:48:36 - train: epoch 0075, iter [03000, 05004], lr: 0.033252, loss: 1.6653
2022-07-29 21:50:33 - train: epoch 0075, iter [03100, 05004], lr: 0.033203, loss: 1.4934
2022-07-29 21:52:30 - train: epoch 0075, iter [03200, 05004], lr: 0.033154, loss: 1.5532
2022-07-29 21:54:27 - train: epoch 0075, iter [03300, 05004], lr: 0.033105, loss: 1.6950
2022-07-29 21:56:24 - train: epoch 0075, iter [03400, 05004], lr: 0.033056, loss: 1.5209
2022-07-29 21:58:18 - train: epoch 0075, iter [03500, 05004], lr: 0.033006, loss: 1.6160
2022-07-29 22:00:15 - train: epoch 0075, iter [03600, 05004], lr: 0.032957, loss: 1.6823
2022-07-29 22:02:11 - train: epoch 0075, iter [03700, 05004], lr: 0.032908, loss: 1.7447
2022-07-29 22:04:08 - train: epoch 0075, iter [03800, 05004], lr: 0.032859, loss: 1.6485
2022-07-29 22:06:02 - train: epoch 0075, iter [03900, 05004], lr: 0.032810, loss: 1.5284
2022-07-29 22:08:04 - train: epoch 0075, iter [04000, 05004], lr: 0.032761, loss: 1.4830
2022-07-29 22:10:02 - train: epoch 0075, iter [04100, 05004], lr: 0.032713, loss: 1.4402
2022-07-29 22:11:58 - train: epoch 0075, iter [04200, 05004], lr: 0.032664, loss: 1.5335
2022-07-29 22:13:55 - train: epoch 0075, iter [04300, 05004], lr: 0.032615, loss: 1.6743
2022-07-29 22:15:59 - train: epoch 0075, iter [04400, 05004], lr: 0.032566, loss: 1.5258
2022-07-29 22:17:58 - train: epoch 0075, iter [04500, 05004], lr: 0.032517, loss: 1.4884
2022-07-29 22:19:57 - train: epoch 0075, iter [04600, 05004], lr: 0.032469, loss: 1.4295
2022-07-29 22:21:55 - train: epoch 0075, iter [04700, 05004], lr: 0.032420, loss: 1.8610
2022-07-29 22:23:52 - train: epoch 0075, iter [04800, 05004], lr: 0.032371, loss: 1.5316
2022-07-29 22:25:50 - train: epoch 0075, iter [04900, 05004], lr: 0.032322, loss: 1.5446
2022-07-29 22:27:41 - train: epoch 0075, iter [05000, 05004], lr: 0.032274, loss: 1.5250
2022-07-29 22:27:45 - train: epoch 075, train_loss: 1.5858
2022-07-29 22:31:59 - eval: epoch: 075, acc1: 65.906%, acc5: 87.092%, test_loss: 1.4016, per_image_load_time: 8.813ms, per_image_inference_time: 0.705ms
2022-07-29 22:31:59 - until epoch: 075, best_acc1: 65.906%
2022-07-29 22:31:59 - epoch 076 lr: 0.032271
2022-07-29 22:34:15 - train: epoch 0076, iter [00100, 05004], lr: 0.032223, loss: 1.4027
2022-07-29 22:36:35 - train: epoch 0076, iter [00200, 05004], lr: 0.032175, loss: 1.5021
2022-07-29 22:38:36 - train: epoch 0076, iter [00300, 05004], lr: 0.032126, loss: 1.4579
2022-07-29 22:40:42 - train: epoch 0076, iter [00400, 05004], lr: 0.032078, loss: 1.6006
2022-07-29 22:42:47 - train: epoch 0076, iter [00500, 05004], lr: 0.032029, loss: 1.6205
2022-07-29 22:44:50 - train: epoch 0076, iter [00600, 05004], lr: 0.031981, loss: 1.5729
2022-07-29 22:46:49 - train: epoch 0076, iter [00700, 05004], lr: 0.031932, loss: 1.3286
2022-07-29 22:48:55 - train: epoch 0076, iter [00800, 05004], lr: 0.031884, loss: 1.4601
2022-07-29 22:50:52 - train: epoch 0076, iter [00900, 05004], lr: 0.031835, loss: 1.5832
2022-07-29 22:52:48 - train: epoch 0076, iter [01000, 05004], lr: 0.031787, loss: 1.4148
2022-07-29 22:54:47 - train: epoch 0076, iter [01100, 05004], lr: 0.031739, loss: 1.3864
2022-07-29 22:56:39 - train: epoch 0076, iter [01200, 05004], lr: 0.031691, loss: 1.5367
2022-07-29 22:58:39 - train: epoch 0076, iter [01300, 05004], lr: 0.031642, loss: 1.7297
2022-07-29 23:00:34 - train: epoch 0076, iter [01400, 05004], lr: 0.031594, loss: 1.4699
2022-07-29 23:02:33 - train: epoch 0076, iter [01500, 05004], lr: 0.031546, loss: 1.5720
2022-07-29 23:04:23 - train: epoch 0076, iter [01600, 05004], lr: 0.031498, loss: 1.4487
2022-07-29 23:06:21 - train: epoch 0076, iter [01700, 05004], lr: 0.031450, loss: 1.3603
2022-07-29 23:08:20 - train: epoch 0076, iter [01800, 05004], lr: 0.031401, loss: 1.4058
2022-07-29 23:10:19 - train: epoch 0076, iter [01900, 05004], lr: 0.031353, loss: 1.6653
2022-07-29 23:12:17 - train: epoch 0076, iter [02000, 05004], lr: 0.031305, loss: 1.6567
2022-07-29 23:14:17 - train: epoch 0076, iter [02100, 05004], lr: 0.031257, loss: 1.7221
2022-07-29 23:16:13 - train: epoch 0076, iter [02200, 05004], lr: 0.031209, loss: 1.4608
2022-07-29 23:18:08 - train: epoch 0076, iter [02300, 05004], lr: 0.031161, loss: 1.5216
2022-07-29 23:20:07 - train: epoch 0076, iter [02400, 05004], lr: 0.031114, loss: 1.8721
2022-07-29 23:21:58 - train: epoch 0076, iter [02500, 05004], lr: 0.031066, loss: 1.5605
2022-07-29 23:24:04 - train: epoch 0076, iter [02600, 05004], lr: 0.031018, loss: 1.9171
2022-07-29 23:26:00 - train: epoch 0076, iter [02700, 05004], lr: 0.030970, loss: 1.4201
2022-07-29 23:27:57 - train: epoch 0076, iter [02800, 05004], lr: 0.030922, loss: 1.4655
2022-07-29 23:29:52 - train: epoch 0076, iter [02900, 05004], lr: 0.030874, loss: 1.7205
2022-07-29 23:31:49 - train: epoch 0076, iter [03000, 05004], lr: 0.030827, loss: 1.3831
2022-07-29 23:33:41 - train: epoch 0076, iter [03100, 05004], lr: 0.030779, loss: 1.7294
2022-07-29 23:35:41 - train: epoch 0076, iter [03200, 05004], lr: 0.030731, loss: 1.6047
2022-07-29 23:37:34 - train: epoch 0076, iter [03300, 05004], lr: 0.030684, loss: 1.3859
2022-07-29 23:39:29 - train: epoch 0076, iter [03400, 05004], lr: 0.030636, loss: 1.5770
2022-07-29 23:41:24 - train: epoch 0076, iter [03500, 05004], lr: 0.030588, loss: 1.3701
2022-07-29 23:43:21 - train: epoch 0076, iter [03600, 05004], lr: 0.030541, loss: 1.6228
2022-07-29 23:45:21 - train: epoch 0076, iter [03700, 05004], lr: 0.030493, loss: 1.3932
2022-07-29 23:47:11 - train: epoch 0076, iter [03800, 05004], lr: 0.030446, loss: 1.4757
2022-07-29 23:49:09 - train: epoch 0076, iter [03900, 05004], lr: 0.030398, loss: 1.6041
2022-07-29 23:51:10 - train: epoch 0076, iter [04000, 05004], lr: 0.030351, loss: 1.5396
2022-07-29 23:53:10 - train: epoch 0076, iter [04100, 05004], lr: 0.030303, loss: 1.6876
2022-07-29 23:55:09 - train: epoch 0076, iter [04200, 05004], lr: 0.030256, loss: 1.6760
2022-07-29 23:57:09 - train: epoch 0076, iter [04300, 05004], lr: 0.030209, loss: 1.5901
2022-07-29 23:59:08 - train: epoch 0076, iter [04400, 05004], lr: 0.030161, loss: 1.6353
2022-07-30 00:01:06 - train: epoch 0076, iter [04500, 05004], lr: 0.030114, loss: 1.5696
2022-07-30 00:03:09 - train: epoch 0076, iter [04600, 05004], lr: 0.030067, loss: 1.6499
2022-07-30 00:05:12 - train: epoch 0076, iter [04700, 05004], lr: 0.030020, loss: 1.7969
2022-07-30 00:07:09 - train: epoch 0076, iter [04800, 05004], lr: 0.029972, loss: 1.5104
2022-07-30 00:09:05 - train: epoch 0076, iter [04900, 05004], lr: 0.029925, loss: 1.7526
2022-07-30 00:11:03 - train: epoch 0076, iter [05000, 05004], lr: 0.029878, loss: 1.6004
2022-07-30 00:11:07 - train: epoch 076, train_loss: 1.5656
2022-07-30 00:15:09 - eval: epoch: 076, acc1: 66.368%, acc5: 87.488%, test_loss: 1.3809, per_image_load_time: 8.456ms, per_image_inference_time: 0.668ms
2022-07-30 00:15:09 - until epoch: 076, best_acc1: 66.368%
2022-07-30 00:15:09 - epoch 077 lr: 0.029876
2022-07-30 00:17:47 - train: epoch 0077, iter [00100, 05004], lr: 0.029829, loss: 1.7502
2022-07-30 00:19:44 - train: epoch 0077, iter [00200, 05004], lr: 0.029782, loss: 1.5783
2022-07-30 00:21:51 - train: epoch 0077, iter [00300, 05004], lr: 0.029735, loss: 1.4332
2022-07-30 00:23:53 - train: epoch 0077, iter [00400, 05004], lr: 0.029688, loss: 1.4951
2022-07-30 00:25:57 - train: epoch 0077, iter [00500, 05004], lr: 0.029641, loss: 1.3657
2022-07-30 00:27:58 - train: epoch 0077, iter [00600, 05004], lr: 0.029594, loss: 1.3247
2022-07-30 00:29:55 - train: epoch 0077, iter [00700, 05004], lr: 0.029547, loss: 1.5447
2022-07-30 00:31:58 - train: epoch 0077, iter [00800, 05004], lr: 0.029500, loss: 1.6178
2022-07-30 00:33:58 - train: epoch 0077, iter [00900, 05004], lr: 0.029454, loss: 1.5831
2022-07-30 00:35:55 - train: epoch 0077, iter [01000, 05004], lr: 0.029407, loss: 1.4511
2022-07-30 00:37:53 - train: epoch 0077, iter [01100, 05004], lr: 0.029360, loss: 1.7614
2022-07-30 00:39:53 - train: epoch 0077, iter [01200, 05004], lr: 0.029313, loss: 1.6428
2022-07-30 00:41:54 - train: epoch 0077, iter [01300, 05004], lr: 0.029266, loss: 1.2851
2022-07-30 00:43:48 - train: epoch 0077, iter [01400, 05004], lr: 0.029220, loss: 1.5327
2022-07-30 00:45:47 - train: epoch 0077, iter [01500, 05004], lr: 0.029173, loss: 1.4683
2022-07-30 00:47:45 - train: epoch 0077, iter [01600, 05004], lr: 0.029126, loss: 1.5666
2022-07-30 00:49:44 - train: epoch 0077, iter [01700, 05004], lr: 0.029080, loss: 1.7800
2022-07-30 00:51:42 - train: epoch 0077, iter [01800, 05004], lr: 0.029033, loss: 1.3877
2022-07-30 00:53:39 - train: epoch 0077, iter [01900, 05004], lr: 0.028987, loss: 1.2859
2022-07-30 00:55:37 - train: epoch 0077, iter [02000, 05004], lr: 0.028940, loss: 1.6159
2022-07-30 00:57:36 - train: epoch 0077, iter [02100, 05004], lr: 0.028894, loss: 1.6982
2022-07-30 00:59:38 - train: epoch 0077, iter [02200, 05004], lr: 0.028847, loss: 1.7664
2022-07-30 01:01:34 - train: epoch 0077, iter [02300, 05004], lr: 0.028801, loss: 1.5575
2022-07-30 01:03:30 - train: epoch 0077, iter [02400, 05004], lr: 0.028754, loss: 1.5912
2022-07-30 01:05:27 - train: epoch 0077, iter [02500, 05004], lr: 0.028708, loss: 1.6503
2022-07-30 01:07:25 - train: epoch 0077, iter [02600, 05004], lr: 0.028662, loss: 1.5248
2022-07-30 01:09:27 - train: epoch 0077, iter [02700, 05004], lr: 0.028615, loss: 1.7387
2022-07-30 01:11:22 - train: epoch 0077, iter [02800, 05004], lr: 0.028569, loss: 1.6549
2022-07-30 01:13:22 - train: epoch 0077, iter [02900, 05004], lr: 0.028523, loss: 1.6180
2022-07-30 01:15:22 - train: epoch 0077, iter [03000, 05004], lr: 0.028477, loss: 1.6222
2022-07-30 01:17:19 - train: epoch 0077, iter [03100, 05004], lr: 0.028431, loss: 1.5535
2022-07-30 01:19:13 - train: epoch 0077, iter [03200, 05004], lr: 0.028384, loss: 1.6390
2022-07-30 01:21:14 - train: epoch 0077, iter [03300, 05004], lr: 0.028338, loss: 1.3790
2022-07-30 01:23:09 - train: epoch 0077, iter [03400, 05004], lr: 0.028292, loss: 1.5480
2022-07-30 01:25:03 - train: epoch 0077, iter [03500, 05004], lr: 0.028246, loss: 1.4972
2022-07-30 01:27:00 - train: epoch 0077, iter [03600, 05004], lr: 0.028200, loss: 1.4883
2022-07-30 01:29:03 - train: epoch 0077, iter [03700, 05004], lr: 0.028154, loss: 1.4666
2022-07-30 01:31:05 - train: epoch 0077, iter [03800, 05004], lr: 0.028108, loss: 1.5813
2022-07-30 01:33:01 - train: epoch 0077, iter [03900, 05004], lr: 0.028062, loss: 1.4468
2022-07-30 01:34:58 - train: epoch 0077, iter [04000, 05004], lr: 0.028016, loss: 1.6130
2022-07-30 01:36:53 - train: epoch 0077, iter [04100, 05004], lr: 0.027971, loss: 1.6366
2022-07-30 01:38:54 - train: epoch 0077, iter [04200, 05004], lr: 0.027925, loss: 1.5785
2022-07-30 01:40:47 - train: epoch 0077, iter [04300, 05004], lr: 0.027879, loss: 1.3839
2022-07-30 01:42:41 - train: epoch 0077, iter [04400, 05004], lr: 0.027833, loss: 1.3655
2022-07-30 01:44:42 - train: epoch 0077, iter [04500, 05004], lr: 0.027787, loss: 1.5399
2022-07-30 01:46:39 - train: epoch 0077, iter [04600, 05004], lr: 0.027742, loss: 1.3001
2022-07-30 01:48:39 - train: epoch 0077, iter [04700, 05004], lr: 0.027696, loss: 1.4206
2022-07-30 01:50:38 - train: epoch 0077, iter [04800, 05004], lr: 0.027650, loss: 1.3757
2022-07-30 01:52:36 - train: epoch 0077, iter [04900, 05004], lr: 0.027605, loss: 1.7103
2022-07-30 01:54:30 - train: epoch 0077, iter [05000, 05004], lr: 0.027559, loss: 1.6544
2022-07-30 01:54:34 - train: epoch 077, train_loss: 1.5400
2022-07-30 01:58:52 - eval: epoch: 077, acc1: 66.742%, acc5: 87.690%, test_loss: 1.3622, per_image_load_time: 7.879ms, per_image_inference_time: 0.755ms
2022-07-30 01:58:52 - until epoch: 077, best_acc1: 66.742%
2022-07-30 01:58:52 - epoch 078 lr: 0.027557
2022-07-30 02:01:13 - train: epoch 0078, iter [00100, 05004], lr: 0.027512, loss: 1.4421
2022-07-30 02:03:15 - train: epoch 0078, iter [00200, 05004], lr: 0.027466, loss: 1.6205
2022-07-30 02:05:19 - train: epoch 0078, iter [00300, 05004], lr: 0.027421, loss: 1.5686
2022-07-30 02:07:20 - train: epoch 0078, iter [00400, 05004], lr: 0.027376, loss: 1.3279
2022-07-30 02:09:26 - train: epoch 0078, iter [00500, 05004], lr: 0.027330, loss: 1.3507
2022-07-30 02:11:23 - train: epoch 0078, iter [00600, 05004], lr: 0.027285, loss: 1.4081
2022-07-30 02:13:24 - train: epoch 0078, iter [00700, 05004], lr: 0.027239, loss: 1.5183
2022-07-30 02:15:29 - train: epoch 0078, iter [00800, 05004], lr: 0.027194, loss: 1.6268
2022-07-30 02:17:29 - train: epoch 0078, iter [00900, 05004], lr: 0.027149, loss: 1.5043
2022-07-30 02:19:33 - train: epoch 0078, iter [01000, 05004], lr: 0.027103, loss: 1.5552
2022-07-30 02:21:36 - train: epoch 0078, iter [01100, 05004], lr: 0.027058, loss: 1.2924
2022-07-30 02:23:36 - train: epoch 0078, iter [01200, 05004], lr: 0.027013, loss: 1.3664
2022-07-30 02:25:33 - train: epoch 0078, iter [01300, 05004], lr: 0.026968, loss: 1.3491
2022-07-30 02:27:26 - train: epoch 0078, iter [01400, 05004], lr: 0.026923, loss: 1.2406
2022-07-30 02:29:32 - train: epoch 0078, iter [01500, 05004], lr: 0.026878, loss: 1.3307
2022-07-30 02:31:33 - train: epoch 0078, iter [01600, 05004], lr: 0.026833, loss: 1.5349
2022-07-30 02:33:30 - train: epoch 0078, iter [01700, 05004], lr: 0.026788, loss: 1.5082
2022-07-30 02:35:31 - train: epoch 0078, iter [01800, 05004], lr: 0.026743, loss: 1.5967
2022-07-30 02:37:33 - train: epoch 0078, iter [01900, 05004], lr: 0.026698, loss: 1.5974
2022-07-30 02:39:29 - train: epoch 0078, iter [02000, 05004], lr: 0.026653, loss: 1.5267
2022-07-30 02:41:34 - train: epoch 0078, iter [02100, 05004], lr: 0.026608, loss: 1.8482
2022-07-30 02:43:35 - train: epoch 0078, iter [02200, 05004], lr: 0.026563, loss: 1.4105
2022-07-30 02:45:36 - train: epoch 0078, iter [02300, 05004], lr: 0.026518, loss: 1.3724
2022-07-30 02:47:38 - train: epoch 0078, iter [02400, 05004], lr: 0.026473, loss: 1.5941
2022-07-30 02:49:36 - train: epoch 0078, iter [02500, 05004], lr: 0.026429, loss: 1.4397
2022-07-30 02:51:39 - train: epoch 0078, iter [02600, 05004], lr: 0.026384, loss: 1.5392
2022-07-30 02:53:36 - train: epoch 0078, iter [02700, 05004], lr: 0.026339, loss: 1.5124
2022-07-30 02:55:36 - train: epoch 0078, iter [02800, 05004], lr: 0.026294, loss: 1.5902
2022-07-30 02:57:33 - train: epoch 0078, iter [02900, 05004], lr: 0.026250, loss: 1.5343
2022-07-30 02:59:30 - train: epoch 0078, iter [03000, 05004], lr: 0.026205, loss: 1.5624
2022-07-30 03:01:30 - train: epoch 0078, iter [03100, 05004], lr: 0.026161, loss: 1.7025
2022-07-30 03:03:30 - train: epoch 0078, iter [03200, 05004], lr: 0.026116, loss: 1.4824
2022-07-30 03:05:28 - train: epoch 0078, iter [03300, 05004], lr: 0.026071, loss: 1.8187
2022-07-30 03:07:24 - train: epoch 0078, iter [03400, 05004], lr: 0.026027, loss: 1.4396
2022-07-30 03:09:26 - train: epoch 0078, iter [03500, 05004], lr: 0.025983, loss: 1.5002
2022-07-30 03:11:22 - train: epoch 0078, iter [03600, 05004], lr: 0.025938, loss: 1.5988
2022-07-30 03:13:23 - train: epoch 0078, iter [03700, 05004], lr: 0.025894, loss: 1.5228
2022-07-30 03:15:23 - train: epoch 0078, iter [03800, 05004], lr: 0.025849, loss: 1.3807
2022-07-30 03:17:21 - train: epoch 0078, iter [03900, 05004], lr: 0.025805, loss: 1.5874
2022-07-30 03:19:23 - train: epoch 0078, iter [04000, 05004], lr: 0.025761, loss: 1.4943
2022-07-30 03:21:22 - train: epoch 0078, iter [04100, 05004], lr: 0.025716, loss: 1.6480
2022-07-30 03:23:25 - train: epoch 0078, iter [04200, 05004], lr: 0.025672, loss: 1.6829
2022-07-30 03:25:23 - train: epoch 0078, iter [04300, 05004], lr: 0.025628, loss: 1.6239
2022-07-30 03:27:22 - train: epoch 0078, iter [04400, 05004], lr: 0.025584, loss: 1.5729
2022-07-30 03:29:22 - train: epoch 0078, iter [04500, 05004], lr: 0.025540, loss: 1.3619
2022-07-30 03:31:18 - train: epoch 0078, iter [04600, 05004], lr: 0.025496, loss: 1.4803
2022-07-30 03:33:14 - train: epoch 0078, iter [04700, 05004], lr: 0.025452, loss: 1.5075
2022-07-30 03:35:13 - train: epoch 0078, iter [04800, 05004], lr: 0.025408, loss: 1.6615
2022-07-30 03:37:09 - train: epoch 0078, iter [04900, 05004], lr: 0.025364, loss: 1.5420
2022-07-30 03:39:02 - train: epoch 0078, iter [05000, 05004], lr: 0.025320, loss: 1.3805
2022-07-30 03:39:06 - train: epoch 078, train_loss: 1.5219
2022-07-30 03:43:31 - eval: epoch: 078, acc1: 66.814%, acc5: 87.832%, test_loss: 1.3453, per_image_load_time: 5.025ms, per_image_inference_time: 0.629ms
2022-07-30 03:43:32 - until epoch: 078, best_acc1: 66.814%
2022-07-30 03:43:32 - epoch 079 lr: 0.025317
2022-07-30 03:45:55 - train: epoch 0079, iter [00100, 05004], lr: 0.025274, loss: 1.5411
2022-07-30 03:47:54 - train: epoch 0079, iter [00200, 05004], lr: 0.025230, loss: 1.4203
2022-07-30 03:49:57 - train: epoch 0079, iter [00300, 05004], lr: 0.025186, loss: 1.3652
2022-07-30 03:52:01 - train: epoch 0079, iter [00400, 05004], lr: 0.025142, loss: 1.5802
2022-07-30 03:54:03 - train: epoch 0079, iter [00500, 05004], lr: 0.025099, loss: 1.4058
2022-07-30 03:56:02 - train: epoch 0079, iter [00600, 05004], lr: 0.025055, loss: 1.4461
2022-07-30 03:57:59 - train: epoch 0079, iter [00700, 05004], lr: 0.025011, loss: 1.5286
2022-07-30 03:59:54 - train: epoch 0079, iter [00800, 05004], lr: 0.024967, loss: 1.5406
2022-07-30 04:01:57 - train: epoch 0079, iter [00900, 05004], lr: 0.024924, loss: 1.4561
2022-07-30 04:03:57 - train: epoch 0079, iter [01000, 05004], lr: 0.024880, loss: 1.5336
2022-07-30 04:05:51 - train: epoch 0079, iter [01100, 05004], lr: 0.024836, loss: 1.5682
2022-07-30 04:07:53 - train: epoch 0079, iter [01200, 05004], lr: 0.024793, loss: 1.6947
2022-07-30 04:09:52 - train: epoch 0079, iter [01300, 05004], lr: 0.024749, loss: 1.3728
2022-07-30 04:11:51 - train: epoch 0079, iter [01400, 05004], lr: 0.024706, loss: 1.5324
2022-07-30 04:13:48 - train: epoch 0079, iter [01500, 05004], lr: 0.024662, loss: 1.6768
2022-07-30 04:15:44 - train: epoch 0079, iter [01600, 05004], lr: 0.024619, loss: 1.3669
2022-07-30 04:17:44 - train: epoch 0079, iter [01700, 05004], lr: 0.024575, loss: 1.4551
2022-07-30 04:19:45 - train: epoch 0079, iter [01800, 05004], lr: 0.024532, loss: 1.3203
2022-07-30 04:21:43 - train: epoch 0079, iter [01900, 05004], lr: 0.024489, loss: 1.5195
2022-07-30 04:23:37 - train: epoch 0079, iter [02000, 05004], lr: 0.024445, loss: 1.6315
2022-07-30 04:25:32 - train: epoch 0079, iter [02100, 05004], lr: 0.024402, loss: 1.4275
2022-07-30 04:27:36 - train: epoch 0079, iter [02200, 05004], lr: 0.024359, loss: 1.6980
2022-07-30 04:29:29 - train: epoch 0079, iter [02300, 05004], lr: 0.024316, loss: 1.5344
2022-07-30 04:31:26 - train: epoch 0079, iter [02400, 05004], lr: 0.024273, loss: 1.4249
2022-07-30 04:33:25 - train: epoch 0079, iter [02500, 05004], lr: 0.024229, loss: 1.5010
2022-07-30 04:35:22 - train: epoch 0079, iter [02600, 05004], lr: 0.024186, loss: 1.3804
2022-07-30 04:37:20 - train: epoch 0079, iter [02700, 05004], lr: 0.024143, loss: 1.3283
2022-07-30 04:39:19 - train: epoch 0079, iter [02800, 05004], lr: 0.024100, loss: 1.4478
2022-07-30 04:41:18 - train: epoch 0079, iter [02900, 05004], lr: 0.024057, loss: 1.4219
2022-07-30 04:43:14 - train: epoch 0079, iter [03000, 05004], lr: 0.024014, loss: 1.5891
2022-07-30 04:45:09 - train: epoch 0079, iter [03100, 05004], lr: 0.023971, loss: 1.5186
2022-07-30 04:47:14 - train: epoch 0079, iter [03200, 05004], lr: 0.023928, loss: 1.5078
2022-07-30 04:49:11 - train: epoch 0079, iter [03300, 05004], lr: 0.023885, loss: 1.4544
2022-07-30 04:51:14 - train: epoch 0079, iter [03400, 05004], lr: 0.023843, loss: 1.4084
2022-07-30 04:53:16 - train: epoch 0079, iter [03500, 05004], lr: 0.023800, loss: 1.6751
2022-07-30 04:55:18 - train: epoch 0079, iter [03600, 05004], lr: 0.023757, loss: 1.3860
2022-07-30 04:57:19 - train: epoch 0079, iter [03700, 05004], lr: 0.023714, loss: 1.3942
2022-07-30 04:59:14 - train: epoch 0079, iter [03800, 05004], lr: 0.023672, loss: 1.3874
2022-07-30 05:01:18 - train: epoch 0079, iter [03900, 05004], lr: 0.023629, loss: 1.4427
2022-07-30 05:03:20 - train: epoch 0079, iter [04000, 05004], lr: 0.023586, loss: 1.2745
2022-07-30 05:05:23 - train: epoch 0079, iter [04100, 05004], lr: 0.023544, loss: 1.7603
2022-07-30 05:07:23 - train: epoch 0079, iter [04200, 05004], lr: 0.023501, loss: 1.3246
2022-07-30 05:09:27 - train: epoch 0079, iter [04300, 05004], lr: 0.023458, loss: 1.2204
2022-07-30 05:11:25 - train: epoch 0079, iter [04400, 05004], lr: 0.023416, loss: 1.7426
2022-07-30 05:13:29 - train: epoch 0079, iter [04500, 05004], lr: 0.023373, loss: 1.6292
2022-07-30 05:15:28 - train: epoch 0079, iter [04600, 05004], lr: 0.023331, loss: 1.5934
2022-07-30 05:17:31 - train: epoch 0079, iter [04700, 05004], lr: 0.023289, loss: 1.4511
2022-07-30 05:19:30 - train: epoch 0079, iter [04800, 05004], lr: 0.023246, loss: 1.7221
2022-07-30 05:21:24 - train: epoch 0079, iter [04900, 05004], lr: 0.023204, loss: 1.4850
2022-07-30 05:23:23 - train: epoch 0079, iter [05000, 05004], lr: 0.023162, loss: 1.3104
2022-07-30 05:23:27 - train: epoch 079, train_loss: 1.4987
2022-07-30 05:27:59 - eval: epoch: 079, acc1: 67.182%, acc5: 88.002%, test_loss: 1.3379, per_image_load_time: 8.298ms, per_image_inference_time: 0.770ms
2022-07-30 05:27:59 - until epoch: 079, best_acc1: 67.182%
2022-07-30 05:27:59 - epoch 080 lr: 0.023159
2022-07-30 05:30:24 - train: epoch 0080, iter [00100, 05004], lr: 0.023118, loss: 1.3887
2022-07-30 05:32:25 - train: epoch 0080, iter [00200, 05004], lr: 0.023075, loss: 1.5673
2022-07-30 05:34:30 - train: epoch 0080, iter [00300, 05004], lr: 0.023033, loss: 1.5860
2022-07-30 05:36:36 - train: epoch 0080, iter [00400, 05004], lr: 0.022991, loss: 1.3074
2022-07-30 05:38:45 - train: epoch 0080, iter [00500, 05004], lr: 0.022949, loss: 1.2517
2022-07-30 05:40:46 - train: epoch 0080, iter [00600, 05004], lr: 0.022907, loss: 1.3345
2022-07-30 05:42:47 - train: epoch 0080, iter [00700, 05004], lr: 0.022865, loss: 1.3372
2022-07-30 05:44:50 - train: epoch 0080, iter [00800, 05004], lr: 0.022823, loss: 1.4183
2022-07-30 05:46:53 - train: epoch 0080, iter [00900, 05004], lr: 0.022781, loss: 1.4701
2022-07-30 05:48:54 - train: epoch 0080, iter [01000, 05004], lr: 0.022739, loss: 1.3319
2022-07-30 05:51:02 - train: epoch 0080, iter [01100, 05004], lr: 0.022697, loss: 1.4157
2022-07-30 05:52:59 - train: epoch 0080, iter [01200, 05004], lr: 0.022655, loss: 1.2847
2022-07-30 05:55:07 - train: epoch 0080, iter [01300, 05004], lr: 0.022613, loss: 1.3684
2022-07-30 05:57:08 - train: epoch 0080, iter [01400, 05004], lr: 0.022571, loss: 1.4577
2022-07-30 05:59:11 - train: epoch 0080, iter [01500, 05004], lr: 0.022529, loss: 1.4780
2022-07-30 06:01:14 - train: epoch 0080, iter [01600, 05004], lr: 0.022488, loss: 1.5914
2022-07-30 06:03:15 - train: epoch 0080, iter [01700, 05004], lr: 0.022446, loss: 1.6365
2022-07-30 06:05:18 - train: epoch 0080, iter [01800, 05004], lr: 0.022404, loss: 1.5495
2022-07-30 06:07:23 - train: epoch 0080, iter [01900, 05004], lr: 0.022362, loss: 1.5484
2022-07-30 06:09:25 - train: epoch 0080, iter [02000, 05004], lr: 0.022321, loss: 1.4554
2022-07-30 06:11:24 - train: epoch 0080, iter [02100, 05004], lr: 0.022279, loss: 1.8000
2022-07-30 06:13:25 - train: epoch 0080, iter [02200, 05004], lr: 0.022238, loss: 1.5293
2022-07-30 06:15:29 - train: epoch 0080, iter [02300, 05004], lr: 0.022196, loss: 1.6560
2022-07-30 06:17:28 - train: epoch 0080, iter [02400, 05004], lr: 0.022155, loss: 1.3737
2022-07-30 06:19:30 - train: epoch 0080, iter [02500, 05004], lr: 0.022113, loss: 1.4443
2022-07-30 06:21:34 - train: epoch 0080, iter [02600, 05004], lr: 0.022072, loss: 1.4936
2022-07-30 06:23:36 - train: epoch 0080, iter [02700, 05004], lr: 0.022030, loss: 1.5294
2022-07-30 06:25:40 - train: epoch 0080, iter [02800, 05004], lr: 0.021989, loss: 1.5071
2022-07-30 06:27:40 - train: epoch 0080, iter [02900, 05004], lr: 0.021948, loss: 1.3996
2022-07-30 06:29:42 - train: epoch 0080, iter [03000, 05004], lr: 0.021906, loss: 1.5257
2022-07-30 06:31:38 - train: epoch 0080, iter [03100, 05004], lr: 0.021865, loss: 1.6159
2022-07-30 06:33:40 - train: epoch 0080, iter [03200, 05004], lr: 0.021824, loss: 1.1008
2022-07-30 06:35:41 - train: epoch 0080, iter [03300, 05004], lr: 0.021783, loss: 1.5209
2022-07-30 06:37:41 - train: epoch 0080, iter [03400, 05004], lr: 0.021741, loss: 1.3824
2022-07-30 06:39:50 - train: epoch 0080, iter [03500, 05004], lr: 0.021700, loss: 1.5334
2022-07-30 06:41:51 - train: epoch 0080, iter [03600, 05004], lr: 0.021659, loss: 1.6077
2022-07-30 06:43:52 - train: epoch 0080, iter [03700, 05004], lr: 0.021618, loss: 1.3648
2022-07-30 06:45:52 - train: epoch 0080, iter [03800, 05004], lr: 0.021577, loss: 1.4506
2022-07-30 06:47:55 - train: epoch 0080, iter [03900, 05004], lr: 0.021536, loss: 1.4245
2022-07-30 06:49:57 - train: epoch 0080, iter [04000, 05004], lr: 0.021495, loss: 1.5865
2022-07-30 06:51:57 - train: epoch 0080, iter [04100, 05004], lr: 0.021454, loss: 1.6548
2022-07-30 06:54:01 - train: epoch 0080, iter [04200, 05004], lr: 0.021413, loss: 1.4643
2022-07-30 06:55:58 - train: epoch 0080, iter [04300, 05004], lr: 0.021373, loss: 1.6685
2022-07-30 06:58:00 - train: epoch 0080, iter [04400, 05004], lr: 0.021332, loss: 1.4997
2022-07-30 06:59:59 - train: epoch 0080, iter [04500, 05004], lr: 0.021291, loss: 1.4708
2022-07-30 07:01:57 - train: epoch 0080, iter [04600, 05004], lr: 0.021250, loss: 1.4365
2022-07-30 07:04:00 - train: epoch 0080, iter [04700, 05004], lr: 0.021210, loss: 1.6453
2022-07-30 07:05:59 - train: epoch 0080, iter [04800, 05004], lr: 0.021169, loss: 1.6286
2022-07-30 07:08:02 - train: epoch 0080, iter [04900, 05004], lr: 0.021128, loss: 1.7315
2022-07-30 07:10:03 - train: epoch 0080, iter [05000, 05004], lr: 0.021088, loss: 1.5114
2022-07-30 07:10:06 - train: epoch 080, train_loss: 1.4744
2022-07-30 07:14:25 - eval: epoch: 080, acc1: 67.886%, acc5: 88.218%, test_loss: 1.3231, per_image_load_time: 6.447ms, per_image_inference_time: 0.873ms
2022-07-30 07:14:25 - until epoch: 080, best_acc1: 67.886%
2022-07-30 07:14:25 - epoch 081 lr: 0.021086
2022-07-30 07:16:50 - train: epoch 0081, iter [00100, 05004], lr: 0.021045, loss: 1.2985
2022-07-30 07:18:58 - train: epoch 0081, iter [00200, 05004], lr: 0.021005, loss: 1.2623
2022-07-30 07:21:01 - train: epoch 0081, iter [00300, 05004], lr: 0.020964, loss: 1.4776
2022-07-30 07:23:03 - train: epoch 0081, iter [00400, 05004], lr: 0.020924, loss: 1.3644
2022-07-30 07:25:11 - train: epoch 0081, iter [00500, 05004], lr: 0.020883, loss: 1.5644
2022-07-30 07:27:16 - train: epoch 0081, iter [00600, 05004], lr: 0.020843, loss: 1.4761
2022-07-30 07:29:17 - train: epoch 0081, iter [00700, 05004], lr: 0.020803, loss: 1.3180
2022-07-30 07:31:26 - train: epoch 0081, iter [00800, 05004], lr: 0.020762, loss: 1.6621
2022-07-30 07:33:25 - train: epoch 0081, iter [00900, 05004], lr: 0.020722, loss: 1.4804
2022-07-30 07:35:27 - train: epoch 0081, iter [01000, 05004], lr: 0.020682, loss: 1.7422
2022-07-30 07:37:31 - train: epoch 0081, iter [01100, 05004], lr: 0.020642, loss: 1.4920
2022-07-30 07:39:31 - train: epoch 0081, iter [01200, 05004], lr: 0.020601, loss: 1.4763
2022-07-30 07:41:33 - train: epoch 0081, iter [01300, 05004], lr: 0.020561, loss: 1.4485
2022-07-30 07:43:38 - train: epoch 0081, iter [01400, 05004], lr: 0.020521, loss: 1.2172
2022-07-30 07:45:45 - train: epoch 0081, iter [01500, 05004], lr: 0.020481, loss: 1.5696
2022-07-30 07:47:48 - train: epoch 0081, iter [01600, 05004], lr: 0.020441, loss: 1.3416
2022-07-30 07:49:56 - train: epoch 0081, iter [01700, 05004], lr: 0.020401, loss: 1.3087
2022-07-30 07:52:01 - train: epoch 0081, iter [01800, 05004], lr: 0.020361, loss: 1.4367
2022-07-30 07:54:03 - train: epoch 0081, iter [01900, 05004], lr: 0.020321, loss: 1.3679
2022-07-30 07:56:05 - train: epoch 0081, iter [02000, 05004], lr: 0.020281, loss: 1.3626
2022-07-30 07:58:11 - train: epoch 0081, iter [02100, 05004], lr: 0.020241, loss: 1.3607
2022-07-30 08:00:12 - train: epoch 0081, iter [02200, 05004], lr: 0.020201, loss: 1.4352
2022-07-30 08:02:19 - train: epoch 0081, iter [02300, 05004], lr: 0.020162, loss: 1.1736
2022-07-30 08:04:19 - train: epoch 0081, iter [02400, 05004], lr: 0.020122, loss: 1.3656
2022-07-30 08:06:20 - train: epoch 0081, iter [02500, 05004], lr: 0.020082, loss: 1.5430
2022-07-30 08:08:24 - train: epoch 0081, iter [02600, 05004], lr: 0.020042, loss: 1.7199
2022-07-30 08:10:27 - train: epoch 0081, iter [02700, 05004], lr: 0.020003, loss: 1.5435
2022-07-30 08:12:34 - train: epoch 0081, iter [02800, 05004], lr: 0.019963, loss: 1.3009
2022-07-30 08:14:32 - train: epoch 0081, iter [02900, 05004], lr: 0.019923, loss: 1.4576
2022-07-30 08:16:33 - train: epoch 0081, iter [03000, 05004], lr: 0.019884, loss: 1.5093
2022-07-30 08:18:34 - train: epoch 0081, iter [03100, 05004], lr: 0.019844, loss: 1.3906
2022-07-30 08:20:37 - train: epoch 0081, iter [03200, 05004], lr: 0.019805, loss: 1.3271
2022-07-30 08:22:36 - train: epoch 0081, iter [03300, 05004], lr: 0.019765, loss: 1.4410
2022-07-30 08:24:37 - train: epoch 0081, iter [03400, 05004], lr: 0.019726, loss: 1.4712
2022-07-30 08:26:37 - train: epoch 0081, iter [03500, 05004], lr: 0.019687, loss: 1.5136
2022-07-30 08:28:36 - train: epoch 0081, iter [03600, 05004], lr: 0.019647, loss: 1.1255
2022-07-30 08:30:36 - train: epoch 0081, iter [03700, 05004], lr: 0.019608, loss: 1.4875
2022-07-30 08:32:40 - train: epoch 0081, iter [03800, 05004], lr: 0.019569, loss: 1.4843
2022-07-30 08:34:38 - train: epoch 0081, iter [03900, 05004], lr: 0.019529, loss: 1.5607
2022-07-30 08:36:37 - train: epoch 0081, iter [04000, 05004], lr: 0.019490, loss: 1.5539
2022-07-30 08:38:41 - train: epoch 0081, iter [04100, 05004], lr: 0.019451, loss: 1.2098
2022-07-30 08:40:41 - train: epoch 0081, iter [04200, 05004], lr: 0.019412, loss: 1.4749
2022-07-30 08:42:42 - train: epoch 0081, iter [04300, 05004], lr: 0.019373, loss: 1.5247
2022-07-30 08:44:49 - train: epoch 0081, iter [04400, 05004], lr: 0.019334, loss: 1.5215
2022-07-30 08:46:44 - train: epoch 0081, iter [04500, 05004], lr: 0.019295, loss: 1.4951
2022-07-30 08:48:53 - train: epoch 0081, iter [04600, 05004], lr: 0.019256, loss: 1.5024
2022-07-30 08:50:51 - train: epoch 0081, iter [04700, 05004], lr: 0.019217, loss: 1.5623
2022-07-30 08:52:52 - train: epoch 0081, iter [04800, 05004], lr: 0.019178, loss: 1.4302
2022-07-30 08:54:54 - train: epoch 0081, iter [04900, 05004], lr: 0.019139, loss: 1.4040
2022-07-30 08:56:47 - train: epoch 0081, iter [05000, 05004], lr: 0.019100, loss: 1.2915
2022-07-30 08:56:49 - train: epoch 081, train_loss: 1.4501
2022-07-30 09:01:06 - eval: epoch: 081, acc1: 67.882%, acc5: 88.326%, test_loss: 1.3129, per_image_load_time: 4.253ms, per_image_inference_time: 0.809ms
2022-07-30 09:01:06 - until epoch: 081, best_acc1: 67.886%
2022-07-30 09:01:06 - epoch 082 lr: 0.019098
2022-07-30 09:03:33 - train: epoch 0082, iter [00100, 05004], lr: 0.019059, loss: 1.2606
2022-07-30 09:05:38 - train: epoch 0082, iter [00200, 05004], lr: 0.019021, loss: 1.1771
2022-07-30 09:07:39 - train: epoch 0082, iter [00300, 05004], lr: 0.018982, loss: 1.3000
2022-07-30 09:09:39 - train: epoch 0082, iter [00400, 05004], lr: 0.018943, loss: 1.4863
2022-07-30 09:11:38 - train: epoch 0082, iter [00500, 05004], lr: 0.018905, loss: 1.3482
2022-07-30 09:13:32 - train: epoch 0082, iter [00600, 05004], lr: 0.018866, loss: 1.4007
2022-07-30 09:15:31 - train: epoch 0082, iter [00700, 05004], lr: 0.018827, loss: 1.6753
2022-07-30 09:17:30 - train: epoch 0082, iter [00800, 05004], lr: 0.018789, loss: 1.3076
2022-07-30 09:19:31 - train: epoch 0082, iter [00900, 05004], lr: 0.018750, loss: 1.4874
2022-07-30 09:21:24 - train: epoch 0082, iter [01000, 05004], lr: 0.018712, loss: 1.3247
2022-07-30 09:23:24 - train: epoch 0082, iter [01100, 05004], lr: 0.018673, loss: 1.6354
2022-07-30 09:25:19 - train: epoch 0082, iter [01200, 05004], lr: 0.018635, loss: 1.5195
2022-07-30 09:27:19 - train: epoch 0082, iter [01300, 05004], lr: 0.018596, loss: 1.6063
2022-07-30 09:29:17 - train: epoch 0082, iter [01400, 05004], lr: 0.018558, loss: 1.3901
2022-07-30 09:31:19 - train: epoch 0082, iter [01500, 05004], lr: 0.018520, loss: 1.5014
2022-07-30 09:33:13 - train: epoch 0082, iter [01600, 05004], lr: 0.018481, loss: 1.5346
2022-07-30 09:35:10 - train: epoch 0082, iter [01700, 05004], lr: 0.018443, loss: 1.4326
2022-07-30 09:37:08 - train: epoch 0082, iter [01800, 05004], lr: 0.018405, loss: 1.4713
2022-07-30 09:39:08 - train: epoch 0082, iter [01900, 05004], lr: 0.018367, loss: 1.4472
2022-07-30 09:41:02 - train: epoch 0082, iter [02000, 05004], lr: 0.018329, loss: 1.2651
2022-07-30 09:42:58 - train: epoch 0082, iter [02100, 05004], lr: 0.018290, loss: 1.4742
2022-07-30 09:44:55 - train: epoch 0082, iter [02200, 05004], lr: 0.018252, loss: 1.4432
2022-07-30 09:46:54 - train: epoch 0082, iter [02300, 05004], lr: 0.018214, loss: 1.3965
2022-07-30 09:48:53 - train: epoch 0082, iter [02400, 05004], lr: 0.018176, loss: 1.5958
2022-07-30 09:50:59 - train: epoch 0082, iter [02500, 05004], lr: 0.018138, loss: 1.4903
2022-07-30 09:52:55 - train: epoch 0082, iter [02600, 05004], lr: 0.018100, loss: 1.3240
2022-07-30 09:54:58 - train: epoch 0082, iter [02700, 05004], lr: 0.018062, loss: 1.2910
2022-07-30 09:56:53 - train: epoch 0082, iter [02800, 05004], lr: 0.018025, loss: 1.1352
2022-07-30 09:58:56 - train: epoch 0082, iter [02900, 05004], lr: 0.017987, loss: 1.2279
2022-07-30 10:00:56 - train: epoch 0082, iter [03000, 05004], lr: 0.017949, loss: 1.4837
2022-07-30 10:02:48 - train: epoch 0082, iter [03100, 05004], lr: 0.017911, loss: 1.4513
2022-07-30 10:04:49 - train: epoch 0082, iter [03200, 05004], lr: 0.017873, loss: 1.4906
2022-07-30 10:06:43 - train: epoch 0082, iter [03300, 05004], lr: 0.017836, loss: 1.4409
2022-07-30 10:08:45 - train: epoch 0082, iter [03400, 05004], lr: 0.017798, loss: 1.4296
2022-07-30 10:10:44 - train: epoch 0082, iter [03500, 05004], lr: 0.017761, loss: 1.3175
2022-07-30 10:12:44 - train: epoch 0082, iter [03600, 05004], lr: 0.017723, loss: 1.3672
2022-07-30 10:14:41 - train: epoch 0082, iter [03700, 05004], lr: 0.017685, loss: 1.2695
2022-07-30 10:16:39 - train: epoch 0082, iter [03800, 05004], lr: 0.017648, loss: 1.4642
2022-07-30 10:18:40 - train: epoch 0082, iter [03900, 05004], lr: 0.017610, loss: 1.4976
2022-07-30 10:20:36 - train: epoch 0082, iter [04000, 05004], lr: 0.017573, loss: 1.3977
2022-07-30 10:22:37 - train: epoch 0082, iter [04100, 05004], lr: 0.017536, loss: 1.6076
2022-07-30 10:24:34 - train: epoch 0082, iter [04200, 05004], lr: 0.017498, loss: 1.5181
2022-07-30 10:26:37 - train: epoch 0082, iter [04300, 05004], lr: 0.017461, loss: 1.1541
2022-07-30 10:28:38 - train: epoch 0082, iter [04400, 05004], lr: 0.017424, loss: 1.2669
2022-07-30 10:30:37 - train: epoch 0082, iter [04500, 05004], lr: 0.017386, loss: 1.3112
2022-07-30 10:32:32 - train: epoch 0082, iter [04600, 05004], lr: 0.017349, loss: 1.5160
2022-07-30 10:34:31 - train: epoch 0082, iter [04700, 05004], lr: 0.017312, loss: 1.1485
2022-07-30 10:36:27 - train: epoch 0082, iter [04800, 05004], lr: 0.017275, loss: 1.4850
2022-07-30 10:38:29 - train: epoch 0082, iter [04900, 05004], lr: 0.017238, loss: 1.6526
2022-07-30 10:40:21 - train: epoch 0082, iter [05000, 05004], lr: 0.017201, loss: 1.3177
2022-07-30 10:40:23 - train: epoch 082, train_loss: 1.4246
2022-07-30 10:44:35 - eval: epoch: 082, acc1: 68.948%, acc5: 88.866%, test_loss: 1.2713, per_image_load_time: 8.654ms, per_image_inference_time: 0.639ms
2022-07-30 10:44:35 - until epoch: 082, best_acc1: 68.948%
2022-07-30 10:44:35 - epoch 083 lr: 0.017199
2022-07-30 10:46:54 - train: epoch 0083, iter [00100, 05004], lr: 0.017162, loss: 1.3042
2022-07-30 10:48:55 - train: epoch 0083, iter [00200, 05004], lr: 0.017125, loss: 1.3009
2022-07-30 10:50:56 - train: epoch 0083, iter [00300, 05004], lr: 0.017088, loss: 1.3155
2022-07-30 10:52:56 - train: epoch 0083, iter [00400, 05004], lr: 0.017051, loss: 1.2859
2022-07-30 10:54:58 - train: epoch 0083, iter [00500, 05004], lr: 0.017014, loss: 1.3610
2022-07-30 10:57:02 - train: epoch 0083, iter [00600, 05004], lr: 0.016977, loss: 1.2007
2022-07-30 10:58:57 - train: epoch 0083, iter [00700, 05004], lr: 0.016941, loss: 1.4094
2022-07-30 11:00:59 - train: epoch 0083, iter [00800, 05004], lr: 0.016904, loss: 1.5087
2022-07-30 11:02:59 - train: epoch 0083, iter [00900, 05004], lr: 0.016867, loss: 1.4536
2022-07-30 11:04:56 - train: epoch 0083, iter [01000, 05004], lr: 0.016830, loss: 1.5819
2022-07-30 11:06:54 - train: epoch 0083, iter [01100, 05004], lr: 0.016794, loss: 1.4288
2022-07-30 11:08:56 - train: epoch 0083, iter [01200, 05004], lr: 0.016757, loss: 1.3845
2022-07-30 11:10:59 - train: epoch 0083, iter [01300, 05004], lr: 0.016720, loss: 1.4831
2022-07-30 11:12:58 - train: epoch 0083, iter [01400, 05004], lr: 0.016684, loss: 1.3514
2022-07-30 11:15:01 - train: epoch 0083, iter [01500, 05004], lr: 0.016647, loss: 1.3755
2022-07-30 11:17:02 - train: epoch 0083, iter [01600, 05004], lr: 0.016611, loss: 1.4160
2022-07-30 11:19:03 - train: epoch 0083, iter [01700, 05004], lr: 0.016574, loss: 1.5523
2022-07-30 11:21:01 - train: epoch 0083, iter [01800, 05004], lr: 0.016538, loss: 1.6812
2022-07-30 11:23:03 - train: epoch 0083, iter [01900, 05004], lr: 0.016502, loss: 1.3763
2022-07-30 11:25:04 - train: epoch 0083, iter [02000, 05004], lr: 0.016465, loss: 1.1616
2022-07-30 11:27:02 - train: epoch 0083, iter [02100, 05004], lr: 0.016429, loss: 1.4253
2022-07-30 11:28:58 - train: epoch 0083, iter [02200, 05004], lr: 0.016393, loss: 1.4460
2022-07-30 11:31:00 - train: epoch 0083, iter [02300, 05004], lr: 0.016356, loss: 1.4815
2022-07-30 11:32:58 - train: epoch 0083, iter [02400, 05004], lr: 0.016320, loss: 1.2859
2022-07-30 11:34:58 - train: epoch 0083, iter [02500, 05004], lr: 0.016284, loss: 1.3521
2022-07-30 11:36:55 - train: epoch 0083, iter [02600, 05004], lr: 0.016248, loss: 1.2561
2022-07-30 11:38:54 - train: epoch 0083, iter [02700, 05004], lr: 0.016212, loss: 1.3306
2022-07-30 11:40:57 - train: epoch 0083, iter [02800, 05004], lr: 0.016176, loss: 1.2026
2022-07-30 11:42:57 - train: epoch 0083, iter [02900, 05004], lr: 0.016140, loss: 1.3202
2022-07-30 11:44:51 - train: epoch 0083, iter [03000, 05004], lr: 0.016104, loss: 1.4324
2022-07-30 11:46:45 - train: epoch 0083, iter [03100, 05004], lr: 0.016068, loss: 1.3536
2022-07-30 11:48:48 - train: epoch 0083, iter [03200, 05004], lr: 0.016032, loss: 1.2561
2022-07-30 11:50:48 - train: epoch 0083, iter [03300, 05004], lr: 0.015996, loss: 1.4472
2022-07-30 11:52:47 - train: epoch 0083, iter [03400, 05004], lr: 0.015960, loss: 1.3953
2022-07-30 11:54:44 - train: epoch 0083, iter [03500, 05004], lr: 0.015924, loss: 1.3956
2022-07-30 11:56:43 - train: epoch 0083, iter [03600, 05004], lr: 0.015889, loss: 1.3966
2022-07-30 11:58:38 - train: epoch 0083, iter [03700, 05004], lr: 0.015853, loss: 1.2351
2022-07-30 12:00:36 - train: epoch 0083, iter [03800, 05004], lr: 0.015817, loss: 1.4014
2022-07-30 12:02:27 - train: epoch 0083, iter [03900, 05004], lr: 0.015782, loss: 1.3296
2022-07-30 12:04:27 - train: epoch 0083, iter [04000, 05004], lr: 0.015746, loss: 1.6233
2022-07-30 12:06:25 - train: epoch 0083, iter [04100, 05004], lr: 0.015710, loss: 1.5283
