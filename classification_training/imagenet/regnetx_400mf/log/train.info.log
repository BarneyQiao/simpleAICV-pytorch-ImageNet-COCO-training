2022-07-30 12:08:20 - train: epoch 0083, iter [04200, 05004], lr: 0.015675, loss: 1.7369
2022-07-30 12:10:17 - train: epoch 0083, iter [04300, 05004], lr: 0.015639, loss: 1.3825
2022-07-30 12:12:18 - train: epoch 0083, iter [04400, 05004], lr: 0.015604, loss: 1.2800
2022-07-30 12:14:16 - train: epoch 0083, iter [04500, 05004], lr: 0.015568, loss: 1.4543
2022-07-30 12:16:10 - train: epoch 0083, iter [04600, 05004], lr: 0.015533, loss: 1.5288
2022-07-30 12:18:05 - train: epoch 0083, iter [04700, 05004], lr: 0.015498, loss: 1.6015
2022-07-30 12:19:58 - train: epoch 0083, iter [04800, 05004], lr: 0.015462, loss: 1.4328
2022-07-30 12:21:55 - train: epoch 0083, iter [04900, 05004], lr: 0.015427, loss: 1.4653
2022-07-30 12:23:34 - train: epoch 0083, iter [05000, 05004], lr: 0.015392, loss: 1.5109
2022-07-30 12:23:39 - train: epoch 083, train_loss: 1.3986
2022-07-30 12:27:39 - eval: epoch: 083, acc1: 69.034%, acc5: 88.966%, test_loss: 1.2625, per_image_load_time: 8.297ms, per_image_inference_time: 0.657ms
2022-07-30 12:27:39 - until epoch: 083, best_acc1: 69.034%
2022-07-30 12:27:39 - epoch 084 lr: 0.015390
2022-07-30 12:30:03 - train: epoch 0084, iter [00100, 05004], lr: 0.015355, loss: 1.3189
2022-07-30 12:32:12 - train: epoch 0084, iter [00200, 05004], lr: 0.015320, loss: 1.4907
2022-07-30 12:34:11 - train: epoch 0084, iter [00300, 05004], lr: 0.015285, loss: 1.2860
2022-07-30 12:36:06 - train: epoch 0084, iter [00400, 05004], lr: 0.015250, loss: 1.2689
2022-07-30 12:38:04 - train: epoch 0084, iter [00500, 05004], lr: 0.015215, loss: 1.4664
2022-07-30 12:40:02 - train: epoch 0084, iter [00600, 05004], lr: 0.015180, loss: 1.5626
2022-07-30 12:42:02 - train: epoch 0084, iter [00700, 05004], lr: 0.015145, loss: 1.3891
2022-07-30 12:44:05 - train: epoch 0084, iter [00800, 05004], lr: 0.015110, loss: 1.5567
2022-07-30 12:46:02 - train: epoch 0084, iter [00900, 05004], lr: 0.015075, loss: 1.4584
2022-07-30 12:47:58 - train: epoch 0084, iter [01000, 05004], lr: 0.015040, loss: 1.3786
2022-07-30 12:49:58 - train: epoch 0084, iter [01100, 05004], lr: 0.015005, loss: 1.3193
2022-07-30 12:52:00 - train: epoch 0084, iter [01200, 05004], lr: 0.014970, loss: 1.7315
2022-07-30 12:54:00 - train: epoch 0084, iter [01300, 05004], lr: 0.014936, loss: 1.3530
2022-07-30 12:56:04 - train: epoch 0084, iter [01400, 05004], lr: 0.014901, loss: 1.4570
2022-07-30 12:58:04 - train: epoch 0084, iter [01500, 05004], lr: 0.014866, loss: 1.2554
2022-07-30 13:00:04 - train: epoch 0084, iter [01600, 05004], lr: 0.014832, loss: 1.2761
2022-07-30 13:02:05 - train: epoch 0084, iter [01700, 05004], lr: 0.014797, loss: 1.4864
2022-07-30 13:04:03 - train: epoch 0084, iter [01800, 05004], lr: 0.014762, loss: 1.4754
2022-07-30 13:06:00 - train: epoch 0084, iter [01900, 05004], lr: 0.014728, loss: 1.4204
2022-07-30 13:08:01 - train: epoch 0084, iter [02000, 05004], lr: 0.014693, loss: 1.3283
2022-07-30 13:10:00 - train: epoch 0084, iter [02100, 05004], lr: 0.014659, loss: 1.2469
2022-07-30 13:12:01 - train: epoch 0084, iter [02200, 05004], lr: 0.014624, loss: 1.2554
2022-07-30 13:13:56 - train: epoch 0084, iter [02300, 05004], lr: 0.014590, loss: 1.3084
2022-07-30 13:15:55 - train: epoch 0084, iter [02400, 05004], lr: 0.014556, loss: 1.2292
2022-07-30 13:17:50 - train: epoch 0084, iter [02500, 05004], lr: 0.014521, loss: 1.6891
2022-07-30 13:19:50 - train: epoch 0084, iter [02600, 05004], lr: 0.014487, loss: 1.3053
2022-07-30 13:21:49 - train: epoch 0084, iter [02700, 05004], lr: 0.014453, loss: 1.4566
2022-07-30 13:23:51 - train: epoch 0084, iter [02800, 05004], lr: 0.014419, loss: 1.2946
2022-07-30 13:25:46 - train: epoch 0084, iter [02900, 05004], lr: 0.014385, loss: 1.4236
2022-07-30 13:27:46 - train: epoch 0084, iter [03000, 05004], lr: 0.014350, loss: 1.3067
2022-07-30 13:29:48 - train: epoch 0084, iter [03100, 05004], lr: 0.014316, loss: 1.3867
2022-07-30 13:31:47 - train: epoch 0084, iter [03200, 05004], lr: 0.014282, loss: 1.3132
2022-07-30 13:33:43 - train: epoch 0084, iter [03300, 05004], lr: 0.014248, loss: 1.3518
2022-07-30 13:35:39 - train: epoch 0084, iter [03400, 05004], lr: 0.014214, loss: 1.4058
2022-07-30 13:37:36 - train: epoch 0084, iter [03500, 05004], lr: 0.014180, loss: 1.3345
2022-07-30 13:39:36 - train: epoch 0084, iter [03600, 05004], lr: 0.014146, loss: 1.2989
2022-07-30 13:41:34 - train: epoch 0084, iter [03700, 05004], lr: 0.014113, loss: 1.8255
2022-07-30 13:43:36 - train: epoch 0084, iter [03800, 05004], lr: 0.014079, loss: 1.3981
2022-07-30 13:45:38 - train: epoch 0084, iter [03900, 05004], lr: 0.014045, loss: 1.4339
2022-07-30 13:47:35 - train: epoch 0084, iter [04000, 05004], lr: 0.014011, loss: 1.4098
2022-07-30 13:49:36 - train: epoch 0084, iter [04100, 05004], lr: 0.013977, loss: 1.2783
2022-07-30 13:51:36 - train: epoch 0084, iter [04200, 05004], lr: 0.013944, loss: 1.2846
2022-07-30 13:53:32 - train: epoch 0084, iter [04300, 05004], lr: 0.013910, loss: 1.3938
2022-07-30 13:55:29 - train: epoch 0084, iter [04400, 05004], lr: 0.013877, loss: 1.6429
2022-07-30 13:57:27 - train: epoch 0084, iter [04500, 05004], lr: 0.013843, loss: 1.2941
2022-07-30 13:59:27 - train: epoch 0084, iter [04600, 05004], lr: 0.013809, loss: 1.3272
2022-07-30 14:01:19 - train: epoch 0084, iter [04700, 05004], lr: 0.013776, loss: 1.4027
2022-07-30 14:03:23 - train: epoch 0084, iter [04800, 05004], lr: 0.013742, loss: 1.2200
2022-07-30 14:05:20 - train: epoch 0084, iter [04900, 05004], lr: 0.013709, loss: 1.2111
2022-07-30 14:07:14 - train: epoch 0084, iter [05000, 05004], lr: 0.013676, loss: 1.4366
2022-07-30 14:07:18 - train: epoch 084, train_loss: 1.3740
2022-07-30 14:11:39 - eval: epoch: 084, acc1: 69.212%, acc5: 89.210%, test_loss: 1.2457, per_image_load_time: 8.928ms, per_image_inference_time: 0.747ms
2022-07-30 14:11:40 - until epoch: 084, best_acc1: 69.212%
2022-07-30 14:11:40 - epoch 085 lr: 0.013674
2022-07-30 14:13:53 - train: epoch 0085, iter [00100, 05004], lr: 0.013641, loss: 1.1795
2022-07-30 14:15:53 - train: epoch 0085, iter [00200, 05004], lr: 0.013608, loss: 1.1139
2022-07-30 14:17:46 - train: epoch 0085, iter [00300, 05004], lr: 0.013574, loss: 1.4028
2022-07-30 14:19:40 - train: epoch 0085, iter [00400, 05004], lr: 0.013541, loss: 1.4757
2022-07-30 14:21:36 - train: epoch 0085, iter [00500, 05004], lr: 0.013508, loss: 1.3158
2022-07-30 14:23:28 - train: epoch 0085, iter [00600, 05004], lr: 0.013475, loss: 1.2182
2022-07-30 14:25:22 - train: epoch 0085, iter [00700, 05004], lr: 0.013442, loss: 1.4061
2022-07-30 14:27:15 - train: epoch 0085, iter [00800, 05004], lr: 0.013409, loss: 1.2417
2022-07-30 14:29:01 - train: epoch 0085, iter [00900, 05004], lr: 0.013376, loss: 1.3102
2022-07-30 14:30:55 - train: epoch 0085, iter [01000, 05004], lr: 0.013343, loss: 1.4350
2022-07-30 14:32:48 - train: epoch 0085, iter [01100, 05004], lr: 0.013310, loss: 1.2444
2022-07-30 14:34:44 - train: epoch 0085, iter [01200, 05004], lr: 0.013277, loss: 1.4751
2022-07-30 14:36:34 - train: epoch 0085, iter [01300, 05004], lr: 0.013244, loss: 1.4834
2022-07-30 14:38:25 - train: epoch 0085, iter [01400, 05004], lr: 0.013211, loss: 1.3809
2022-07-30 14:40:15 - train: epoch 0085, iter [01500, 05004], lr: 0.013178, loss: 1.4235
2022-07-30 14:42:05 - train: epoch 0085, iter [01600, 05004], lr: 0.013145, loss: 1.2228
2022-07-30 14:43:54 - train: epoch 0085, iter [01700, 05004], lr: 0.013113, loss: 1.6009
2022-07-30 14:45:42 - train: epoch 0085, iter [01800, 05004], lr: 0.013080, loss: 1.1744
2022-07-30 14:47:30 - train: epoch 0085, iter [01900, 05004], lr: 0.013047, loss: 1.2505
2022-07-30 14:49:24 - train: epoch 0085, iter [02000, 05004], lr: 0.013015, loss: 1.2905
2022-07-30 14:51:08 - train: epoch 0085, iter [02100, 05004], lr: 0.012982, loss: 1.1668
2022-07-30 14:52:57 - train: epoch 0085, iter [02200, 05004], lr: 0.012950, loss: 1.3572
2022-07-30 14:54:47 - train: epoch 0085, iter [02300, 05004], lr: 0.012917, loss: 1.1997
2022-07-30 14:56:39 - train: epoch 0085, iter [02400, 05004], lr: 0.012885, loss: 1.3687
2022-07-30 14:58:31 - train: epoch 0085, iter [02500, 05004], lr: 0.012852, loss: 1.5118
2022-07-30 15:00:20 - train: epoch 0085, iter [02600, 05004], lr: 0.012820, loss: 1.2524
2022-07-30 15:02:13 - train: epoch 0085, iter [02700, 05004], lr: 0.012787, loss: 1.0428
2022-07-30 15:04:01 - train: epoch 0085, iter [02800, 05004], lr: 0.012755, loss: 1.4768
2022-07-30 15:06:00 - train: epoch 0085, iter [02900, 05004], lr: 0.012723, loss: 1.3234
2022-07-30 15:08:03 - train: epoch 0085, iter [03000, 05004], lr: 0.012691, loss: 1.6012
2022-07-30 15:09:55 - train: epoch 0085, iter [03100, 05004], lr: 0.012658, loss: 1.3265
2022-07-30 15:11:52 - train: epoch 0085, iter [03200, 05004], lr: 0.012626, loss: 1.5085
2022-07-30 15:13:40 - train: epoch 0085, iter [03300, 05004], lr: 0.012594, loss: 1.3133
2022-07-30 15:15:33 - train: epoch 0085, iter [03400, 05004], lr: 0.012562, loss: 1.4275
2022-07-30 15:17:32 - train: epoch 0085, iter [03500, 05004], lr: 0.012530, loss: 1.3433
2022-07-30 15:19:28 - train: epoch 0085, iter [03600, 05004], lr: 0.012498, loss: 1.5956
2022-07-30 15:21:24 - train: epoch 0085, iter [03700, 05004], lr: 0.012466, loss: 1.1583
2022-07-30 15:23:13 - train: epoch 0085, iter [03800, 05004], lr: 0.012434, loss: 1.4857
2022-07-30 15:25:03 - train: epoch 0085, iter [03900, 05004], lr: 0.012402, loss: 1.3133
2022-07-30 15:27:00 - train: epoch 0085, iter [04000, 05004], lr: 0.012370, loss: 1.5312
2022-07-30 15:28:54 - train: epoch 0085, iter [04100, 05004], lr: 0.012339, loss: 1.2612
2022-07-30 15:30:52 - train: epoch 0085, iter [04200, 05004], lr: 0.012307, loss: 1.3828
2022-07-30 15:32:47 - train: epoch 0085, iter [04300, 05004], lr: 0.012275, loss: 1.1777
2022-07-30 15:34:45 - train: epoch 0085, iter [04400, 05004], lr: 0.012243, loss: 1.3849
2022-07-30 15:36:40 - train: epoch 0085, iter [04500, 05004], lr: 0.012212, loss: 1.4726
2022-07-30 15:38:48 - train: epoch 0085, iter [04600, 05004], lr: 0.012180, loss: 1.3533
2022-07-30 15:40:49 - train: epoch 0085, iter [04700, 05004], lr: 0.012148, loss: 1.6536
2022-07-30 15:42:51 - train: epoch 0085, iter [04800, 05004], lr: 0.012117, loss: 1.1418
2022-07-30 15:44:55 - train: epoch 0085, iter [04900, 05004], lr: 0.012085, loss: 1.3937
2022-07-30 15:46:51 - train: epoch 0085, iter [05000, 05004], lr: 0.012054, loss: 1.4452
2022-07-30 15:46:56 - train: epoch 085, train_loss: 1.3497
2022-07-30 15:50:40 - eval: epoch: 085, acc1: 69.630%, acc5: 89.296%, test_loss: 1.2377, per_image_load_time: 7.489ms, per_image_inference_time: 0.654ms
2022-07-30 15:50:41 - until epoch: 085, best_acc1: 69.630%
2022-07-30 15:50:41 - epoch 086 lr: 0.012052
2022-07-30 15:52:59 - train: epoch 0086, iter [00100, 05004], lr: 0.012021, loss: 1.2102
2022-07-30 15:55:00 - train: epoch 0086, iter [00200, 05004], lr: 0.011990, loss: 1.2058
2022-07-30 15:57:03 - train: epoch 0086, iter [00300, 05004], lr: 0.011958, loss: 1.5640
2022-07-30 15:59:01 - train: epoch 0086, iter [00400, 05004], lr: 0.011927, loss: 1.5084
2022-07-30 16:00:59 - train: epoch 0086, iter [00500, 05004], lr: 0.011896, loss: 1.3883
2022-07-30 16:02:56 - train: epoch 0086, iter [00600, 05004], lr: 0.011865, loss: 1.2867
2022-07-30 16:04:56 - train: epoch 0086, iter [00700, 05004], lr: 0.011833, loss: 1.0795
2022-07-30 16:07:02 - train: epoch 0086, iter [00800, 05004], lr: 0.011802, loss: 1.2401
2022-07-30 16:08:58 - train: epoch 0086, iter [00900, 05004], lr: 0.011771, loss: 1.2921
2022-07-30 16:11:03 - train: epoch 0086, iter [01000, 05004], lr: 0.011740, loss: 1.4902
2022-07-30 16:13:01 - train: epoch 0086, iter [01100, 05004], lr: 0.011709, loss: 1.3756
2022-07-30 16:15:00 - train: epoch 0086, iter [01200, 05004], lr: 0.011678, loss: 1.2832
2022-07-30 16:17:05 - train: epoch 0086, iter [01300, 05004], lr: 0.011647, loss: 1.3346
2022-07-30 16:19:06 - train: epoch 0086, iter [01400, 05004], lr: 0.011616, loss: 1.3152
2022-07-30 16:21:13 - train: epoch 0086, iter [01500, 05004], lr: 0.011585, loss: 1.1105
2022-07-30 16:23:09 - train: epoch 0086, iter [01600, 05004], lr: 0.011554, loss: 1.3531
2022-07-30 16:25:10 - train: epoch 0086, iter [01700, 05004], lr: 0.011523, loss: 1.3793
2022-07-30 16:27:12 - train: epoch 0086, iter [01800, 05004], lr: 0.011493, loss: 1.4999
2022-07-30 16:29:11 - train: epoch 0086, iter [01900, 05004], lr: 0.011462, loss: 1.3919
2022-07-30 16:31:17 - train: epoch 0086, iter [02000, 05004], lr: 0.011431, loss: 1.3130
2022-07-30 16:33:17 - train: epoch 0086, iter [02100, 05004], lr: 0.011401, loss: 1.0783
2022-07-30 16:35:14 - train: epoch 0086, iter [02200, 05004], lr: 0.011370, loss: 1.2460
2022-07-30 16:37:17 - train: epoch 0086, iter [02300, 05004], lr: 0.011339, loss: 1.2743
2022-07-30 16:39:15 - train: epoch 0086, iter [02400, 05004], lr: 0.011309, loss: 1.3811
2022-07-30 16:41:14 - train: epoch 0086, iter [02500, 05004], lr: 0.011278, loss: 1.4146
2022-07-30 16:43:10 - train: epoch 0086, iter [02600, 05004], lr: 0.011248, loss: 1.3547
2022-07-30 16:45:07 - train: epoch 0086, iter [02700, 05004], lr: 0.011217, loss: 1.2433
2022-07-30 16:47:11 - train: epoch 0086, iter [02800, 05004], lr: 0.011187, loss: 1.3150
2022-07-30 16:49:13 - train: epoch 0086, iter [02900, 05004], lr: 0.011157, loss: 1.1636
2022-07-30 16:51:08 - train: epoch 0086, iter [03000, 05004], lr: 0.011126, loss: 1.1384
2022-07-30 16:53:07 - train: epoch 0086, iter [03100, 05004], lr: 0.011096, loss: 1.1587
2022-07-30 16:55:02 - train: epoch 0086, iter [03200, 05004], lr: 0.011066, loss: 1.4924
2022-07-30 16:57:08 - train: epoch 0086, iter [03300, 05004], lr: 0.011036, loss: 1.4627
2022-07-30 16:59:06 - train: epoch 0086, iter [03400, 05004], lr: 0.011005, loss: 1.4129
2022-07-30 17:01:04 - train: epoch 0086, iter [03500, 05004], lr: 0.010975, loss: 1.3185
2022-07-30 17:03:07 - train: epoch 0086, iter [03600, 05004], lr: 0.010945, loss: 1.1863
2022-07-30 17:05:05 - train: epoch 0086, iter [03700, 05004], lr: 0.010915, loss: 1.1890
2022-07-30 17:07:07 - train: epoch 0086, iter [03800, 05004], lr: 0.010885, loss: 1.3987
2022-07-30 17:09:04 - train: epoch 0086, iter [03900, 05004], lr: 0.010855, loss: 1.1041
2022-07-30 17:10:59 - train: epoch 0086, iter [04000, 05004], lr: 0.010825, loss: 1.2136
2022-07-30 17:12:59 - train: epoch 0086, iter [04100, 05004], lr: 0.010795, loss: 1.3800
2022-07-30 17:15:02 - train: epoch 0086, iter [04200, 05004], lr: 0.010766, loss: 1.4474
2022-07-30 17:17:07 - train: epoch 0086, iter [04300, 05004], lr: 0.010736, loss: 1.3646
2022-07-30 17:19:06 - train: epoch 0086, iter [04400, 05004], lr: 0.010706, loss: 1.3553
2022-07-30 17:21:05 - train: epoch 0086, iter [04500, 05004], lr: 0.010676, loss: 1.2838
2022-07-30 17:23:03 - train: epoch 0086, iter [04600, 05004], lr: 0.010647, loss: 1.4694
2022-07-30 17:25:13 - train: epoch 0086, iter [04700, 05004], lr: 0.010617, loss: 1.2580
2022-07-30 17:27:15 - train: epoch 0086, iter [04800, 05004], lr: 0.010587, loss: 1.2486
2022-07-30 17:29:17 - train: epoch 0086, iter [04900, 05004], lr: 0.010558, loss: 1.4169
2022-07-30 17:31:20 - train: epoch 0086, iter [05000, 05004], lr: 0.010528, loss: 1.5016
2022-07-30 17:31:26 - train: epoch 086, train_loss: 1.3238
2022-07-30 17:35:07 - eval: epoch: 086, acc1: 69.926%, acc5: 89.488%, test_loss: 1.2267, per_image_load_time: 4.159ms, per_image_inference_time: 0.660ms
2022-07-30 17:35:08 - until epoch: 086, best_acc1: 69.926%
2022-07-30 17:35:08 - epoch 087 lr: 0.010527
2022-07-30 17:37:29 - train: epoch 0087, iter [00100, 05004], lr: 0.010498, loss: 1.3262
2022-07-30 17:39:35 - train: epoch 0087, iter [00200, 05004], lr: 0.010468, loss: 1.3075
2022-07-30 17:41:32 - train: epoch 0087, iter [00300, 05004], lr: 0.010439, loss: 1.2894
2022-07-30 17:43:30 - train: epoch 0087, iter [00400, 05004], lr: 0.010409, loss: 1.4540
2022-07-30 17:45:31 - train: epoch 0087, iter [00500, 05004], lr: 0.010380, loss: 1.1112
2022-07-30 17:47:31 - train: epoch 0087, iter [00600, 05004], lr: 0.010351, loss: 1.2826
2022-07-30 17:49:39 - train: epoch 0087, iter [00700, 05004], lr: 0.010321, loss: 1.0924
2022-07-30 17:51:37 - train: epoch 0087, iter [00800, 05004], lr: 0.010292, loss: 1.1645
2022-07-30 17:53:37 - train: epoch 0087, iter [00900, 05004], lr: 0.010263, loss: 1.3669
2022-07-30 17:55:42 - train: epoch 0087, iter [01000, 05004], lr: 0.010234, loss: 1.1652
2022-07-30 17:57:41 - train: epoch 0087, iter [01100, 05004], lr: 0.010205, loss: 1.3297
2022-07-30 17:59:39 - train: epoch 0087, iter [01200, 05004], lr: 0.010176, loss: 1.3016
2022-07-30 18:01:37 - train: epoch 0087, iter [01300, 05004], lr: 0.010147, loss: 1.3896
2022-07-30 18:03:37 - train: epoch 0087, iter [01400, 05004], lr: 0.010118, loss: 1.1066
2022-07-30 18:05:39 - train: epoch 0087, iter [01500, 05004], lr: 0.010089, loss: 1.3661
2022-07-30 18:07:36 - train: epoch 0087, iter [01600, 05004], lr: 0.010060, loss: 1.2155
2022-07-30 18:09:39 - train: epoch 0087, iter [01700, 05004], lr: 0.010031, loss: 1.4623
2022-07-30 18:11:42 - train: epoch 0087, iter [01800, 05004], lr: 0.010002, loss: 1.4499
2022-07-30 18:13:45 - train: epoch 0087, iter [01900, 05004], lr: 0.009973, loss: 1.3312
2022-07-30 18:15:43 - train: epoch 0087, iter [02000, 05004], lr: 0.009945, loss: 1.3609
2022-07-30 18:17:42 - train: epoch 0087, iter [02100, 05004], lr: 0.009916, loss: 1.2932
2022-07-30 18:19:42 - train: epoch 0087, iter [02200, 05004], lr: 0.009887, loss: 1.2389
2022-07-30 18:21:43 - train: epoch 0087, iter [02300, 05004], lr: 0.009859, loss: 1.4731
2022-07-30 18:23:39 - train: epoch 0087, iter [02400, 05004], lr: 0.009830, loss: 1.2148
2022-07-30 18:25:43 - train: epoch 0087, iter [02500, 05004], lr: 0.009801, loss: 1.2674
2022-07-30 18:27:40 - train: epoch 0087, iter [02600, 05004], lr: 0.009773, loss: 1.4293
2022-07-30 18:29:40 - train: epoch 0087, iter [02700, 05004], lr: 0.009744, loss: 1.1725
2022-07-30 18:31:43 - train: epoch 0087, iter [02800, 05004], lr: 0.009716, loss: 1.2258
2022-07-30 18:33:43 - train: epoch 0087, iter [02900, 05004], lr: 0.009688, loss: 1.0197
2022-07-30 18:35:42 - train: epoch 0087, iter [03000, 05004], lr: 0.009659, loss: 1.1984
2022-07-30 18:37:42 - train: epoch 0087, iter [03100, 05004], lr: 0.009631, loss: 1.2336
2022-07-30 18:39:40 - train: epoch 0087, iter [03200, 05004], lr: 0.009603, loss: 1.2442
2022-07-30 18:41:42 - train: epoch 0087, iter [03300, 05004], lr: 0.009574, loss: 1.2984
2022-07-30 18:43:41 - train: epoch 0087, iter [03400, 05004], lr: 0.009546, loss: 1.2619
2022-07-30 18:45:41 - train: epoch 0087, iter [03500, 05004], lr: 0.009518, loss: 1.1814
2022-07-30 18:47:39 - train: epoch 0087, iter [03600, 05004], lr: 0.009490, loss: 1.2985
2022-07-30 18:49:38 - train: epoch 0087, iter [03700, 05004], lr: 0.009462, loss: 1.3036
2022-07-30 18:51:36 - train: epoch 0087, iter [03800, 05004], lr: 0.009434, loss: 1.2177
2022-07-30 18:53:32 - train: epoch 0087, iter [03900, 05004], lr: 0.009406, loss: 1.2718
2022-07-30 18:55:26 - train: epoch 0087, iter [04000, 05004], lr: 0.009378, loss: 1.2895
2022-07-30 18:57:26 - train: epoch 0087, iter [04100, 05004], lr: 0.009350, loss: 1.3896
2022-07-30 18:59:27 - train: epoch 0087, iter [04200, 05004], lr: 0.009322, loss: 1.3318
2022-07-30 19:01:27 - train: epoch 0087, iter [04300, 05004], lr: 0.009294, loss: 1.2614
2022-07-30 19:03:22 - train: epoch 0087, iter [04400, 05004], lr: 0.009266, loss: 1.3148
2022-07-30 19:05:28 - train: epoch 0087, iter [04500, 05004], lr: 0.009239, loss: 1.3613
2022-07-30 19:07:23 - train: epoch 0087, iter [04600, 05004], lr: 0.009211, loss: 1.4167
2022-07-30 19:09:27 - train: epoch 0087, iter [04700, 05004], lr: 0.009183, loss: 0.9840
2022-07-30 19:11:27 - train: epoch 0087, iter [04800, 05004], lr: 0.009156, loss: 1.2811
2022-07-30 19:13:22 - train: epoch 0087, iter [04900, 05004], lr: 0.009128, loss: 1.1012
2022-07-30 19:15:09 - train: epoch 0087, iter [05000, 05004], lr: 0.009100, loss: 1.1880
2022-07-30 19:15:15 - train: epoch 087, train_loss: 1.2959
2022-07-30 19:19:07 - eval: epoch: 087, acc1: 70.164%, acc5: 89.580%, test_loss: 1.2162, per_image_load_time: 6.799ms, per_image_inference_time: 0.526ms
2022-07-30 19:19:08 - until epoch: 087, best_acc1: 70.164%
2022-07-30 19:19:08 - epoch 088 lr: 0.009099
2022-07-30 19:21:38 - train: epoch 0088, iter [00100, 05004], lr: 0.009072, loss: 1.2792
2022-07-30 19:23:45 - train: epoch 0088, iter [00200, 05004], lr: 0.009044, loss: 1.1939
2022-07-30 19:25:47 - train: epoch 0088, iter [00300, 05004], lr: 0.009017, loss: 1.3396
2022-07-30 19:27:50 - train: epoch 0088, iter [00400, 05004], lr: 0.008989, loss: 1.4166
2022-07-30 19:29:48 - train: epoch 0088, iter [00500, 05004], lr: 0.008962, loss: 1.2696
2022-07-30 19:31:45 - train: epoch 0088, iter [00600, 05004], lr: 0.008935, loss: 1.3527
2022-07-30 19:33:44 - train: epoch 0088, iter [00700, 05004], lr: 0.008908, loss: 1.2172
2022-07-30 19:35:42 - train: epoch 0088, iter [00800, 05004], lr: 0.008880, loss: 1.4830
2022-07-30 19:37:35 - train: epoch 0088, iter [00900, 05004], lr: 0.008853, loss: 1.2195
2022-07-30 19:39:30 - train: epoch 0088, iter [01000, 05004], lr: 0.008826, loss: 1.2563
2022-07-30 19:41:28 - train: epoch 0088, iter [01100, 05004], lr: 0.008799, loss: 1.1927
2022-07-30 19:43:17 - train: epoch 0088, iter [01200, 05004], lr: 0.008772, loss: 1.2059
2022-07-30 19:45:15 - train: epoch 0088, iter [01300, 05004], lr: 0.008745, loss: 1.2871
2022-07-30 19:47:10 - train: epoch 0088, iter [01400, 05004], lr: 0.008718, loss: 1.1373
2022-07-30 19:49:02 - train: epoch 0088, iter [01500, 05004], lr: 0.008691, loss: 1.1691
2022-07-30 19:50:56 - train: epoch 0088, iter [01600, 05004], lr: 0.008664, loss: 1.0955
2022-07-30 19:52:47 - train: epoch 0088, iter [01700, 05004], lr: 0.008637, loss: 1.2518
2022-07-30 19:54:41 - train: epoch 0088, iter [01800, 05004], lr: 0.008610, loss: 1.1488
2022-07-30 19:56:34 - train: epoch 0088, iter [01900, 05004], lr: 0.008583, loss: 1.3295
2022-07-30 19:58:29 - train: epoch 0088, iter [02000, 05004], lr: 0.008556, loss: 1.2750
2022-07-30 20:00:20 - train: epoch 0088, iter [02100, 05004], lr: 0.008530, loss: 1.1712
2022-07-30 20:02:13 - train: epoch 0088, iter [02200, 05004], lr: 0.008503, loss: 1.2494
2022-07-30 20:04:08 - train: epoch 0088, iter [02300, 05004], lr: 0.008476, loss: 1.2291
2022-07-30 20:06:01 - train: epoch 0088, iter [02400, 05004], lr: 0.008450, loss: 1.4170
2022-07-30 20:07:54 - train: epoch 0088, iter [02500, 05004], lr: 0.008423, loss: 1.2544
2022-07-30 20:09:51 - train: epoch 0088, iter [02600, 05004], lr: 0.008397, loss: 1.2749
2022-07-30 20:11:41 - train: epoch 0088, iter [02700, 05004], lr: 0.008370, loss: 1.3363
2022-07-30 20:13:32 - train: epoch 0088, iter [02800, 05004], lr: 0.008344, loss: 1.3126
2022-07-30 20:15:25 - train: epoch 0088, iter [02900, 05004], lr: 0.008317, loss: 1.1492
2022-07-30 20:17:22 - train: epoch 0088, iter [03000, 05004], lr: 0.008291, loss: 1.5357
2022-07-30 20:19:14 - train: epoch 0088, iter [03100, 05004], lr: 0.008265, loss: 1.0767
2022-07-30 20:21:06 - train: epoch 0088, iter [03200, 05004], lr: 0.008238, loss: 1.2437
2022-07-30 20:23:01 - train: epoch 0088, iter [03300, 05004], lr: 0.008212, loss: 1.1063
2022-07-30 20:24:55 - train: epoch 0088, iter [03400, 05004], lr: 0.008186, loss: 1.1935
2022-07-30 20:26:59 - train: epoch 0088, iter [03500, 05004], lr: 0.008160, loss: 1.3089
2022-07-30 20:29:00 - train: epoch 0088, iter [03600, 05004], lr: 0.008134, loss: 1.3850
2022-07-30 20:31:00 - train: epoch 0088, iter [03700, 05004], lr: 0.008108, loss: 1.5133
2022-07-30 20:33:02 - train: epoch 0088, iter [03800, 05004], lr: 0.008081, loss: 1.2937
2022-07-30 20:35:00 - train: epoch 0088, iter [03900, 05004], lr: 0.008055, loss: 1.1066
2022-07-30 20:37:07 - train: epoch 0088, iter [04000, 05004], lr: 0.008029, loss: 1.1505
2022-07-30 20:39:06 - train: epoch 0088, iter [04100, 05004], lr: 0.008004, loss: 1.1703
2022-07-30 20:41:01 - train: epoch 0088, iter [04200, 05004], lr: 0.007978, loss: 1.3439
2022-07-30 20:43:02 - train: epoch 0088, iter [04300, 05004], lr: 0.007952, loss: 1.2512
2022-07-30 20:44:58 - train: epoch 0088, iter [04400, 05004], lr: 0.007926, loss: 1.0170
2022-07-30 20:46:56 - train: epoch 0088, iter [04500, 05004], lr: 0.007900, loss: 1.2296
2022-07-30 20:48:55 - train: epoch 0088, iter [04600, 05004], lr: 0.007875, loss: 1.5172
2022-07-30 20:50:52 - train: epoch 0088, iter [04700, 05004], lr: 0.007849, loss: 1.1980
2022-07-30 20:52:52 - train: epoch 0088, iter [04800, 05004], lr: 0.007823, loss: 1.2011
2022-07-30 20:54:49 - train: epoch 0088, iter [04900, 05004], lr: 0.007798, loss: 1.3452
2022-07-30 20:56:44 - train: epoch 0088, iter [05000, 05004], lr: 0.007772, loss: 1.2068
2022-07-30 20:56:49 - train: epoch 088, train_loss: 1.2706
2022-07-30 21:00:21 - eval: epoch: 088, acc1: 70.520%, acc5: 89.782%, test_loss: 1.1970, per_image_load_time: 5.815ms, per_image_inference_time: 0.591ms
2022-07-30 21:00:22 - until epoch: 088, best_acc1: 70.520%
2022-07-30 21:00:22 - epoch 089 lr: 0.007771
2022-07-30 21:02:36 - train: epoch 0089, iter [00100, 05004], lr: 0.007746, loss: 1.4155
2022-07-30 21:04:39 - train: epoch 0089, iter [00200, 05004], lr: 0.007720, loss: 1.0803
2022-07-30 21:06:38 - train: epoch 0089, iter [00300, 05004], lr: 0.007695, loss: 1.3494
2022-07-30 21:08:35 - train: epoch 0089, iter [00400, 05004], lr: 0.007669, loss: 1.0965
2022-07-30 21:10:31 - train: epoch 0089, iter [00500, 05004], lr: 0.007644, loss: 1.2015
2022-07-30 21:12:34 - train: epoch 0089, iter [00600, 05004], lr: 0.007618, loss: 1.0576
2022-07-30 21:14:30 - train: epoch 0089, iter [00700, 05004], lr: 0.007593, loss: 1.2060
2022-07-30 21:16:28 - train: epoch 0089, iter [00800, 05004], lr: 0.007568, loss: 1.3298
2022-07-30 21:18:26 - train: epoch 0089, iter [00900, 05004], lr: 0.007543, loss: 1.2482
2022-07-30 21:20:24 - train: epoch 0089, iter [01000, 05004], lr: 0.007518, loss: 1.3395
2022-07-30 21:22:23 - train: epoch 0089, iter [01100, 05004], lr: 0.007493, loss: 1.0677
2022-07-30 21:24:25 - train: epoch 0089, iter [01200, 05004], lr: 0.007467, loss: 1.4426
2022-07-30 21:26:21 - train: epoch 0089, iter [01300, 05004], lr: 0.007442, loss: 1.2302
2022-07-30 21:28:17 - train: epoch 0089, iter [01400, 05004], lr: 0.007417, loss: 1.2361
2022-07-30 21:30:14 - train: epoch 0089, iter [01500, 05004], lr: 0.007392, loss: 1.1792
2022-07-30 21:32:15 - train: epoch 0089, iter [01600, 05004], lr: 0.007368, loss: 0.9728
2022-07-30 21:34:17 - train: epoch 0089, iter [01700, 05004], lr: 0.007343, loss: 1.2867
2022-07-30 21:36:04 - train: epoch 0089, iter [01800, 05004], lr: 0.007318, loss: 1.3897
2022-07-30 21:38:01 - train: epoch 0089, iter [01900, 05004], lr: 0.007293, loss: 1.1989
2022-07-30 21:40:02 - train: epoch 0089, iter [02000, 05004], lr: 0.007268, loss: 1.1570
2022-07-30 21:41:56 - train: epoch 0089, iter [02100, 05004], lr: 0.007244, loss: 1.2105
2022-07-30 21:43:50 - train: epoch 0089, iter [02200, 05004], lr: 0.007219, loss: 1.2720
2022-07-30 21:45:51 - train: epoch 0089, iter [02300, 05004], lr: 0.007194, loss: 1.0994
2022-07-30 21:47:48 - train: epoch 0089, iter [02400, 05004], lr: 0.007170, loss: 1.2387
2022-07-30 21:49:46 - train: epoch 0089, iter [02500, 05004], lr: 0.007145, loss: 1.2657
2022-07-30 21:51:45 - train: epoch 0089, iter [02600, 05004], lr: 0.007121, loss: 1.2295
2022-07-30 21:53:39 - train: epoch 0089, iter [02700, 05004], lr: 0.007096, loss: 1.2999
2022-07-30 21:55:36 - train: epoch 0089, iter [02800, 05004], lr: 0.007072, loss: 1.2422
2022-07-30 21:57:31 - train: epoch 0089, iter [02900, 05004], lr: 0.007047, loss: 1.3574
2022-07-30 21:59:28 - train: epoch 0089, iter [03000, 05004], lr: 0.007023, loss: 1.2562
2022-07-30 22:01:29 - train: epoch 0089, iter [03100, 05004], lr: 0.006999, loss: 1.2539
2022-07-30 22:03:21 - train: epoch 0089, iter [03200, 05004], lr: 0.006974, loss: 1.3491
2022-07-30 22:05:19 - train: epoch 0089, iter [03300, 05004], lr: 0.006950, loss: 1.2225
2022-07-30 22:07:16 - train: epoch 0089, iter [03400, 05004], lr: 0.006926, loss: 1.2517
2022-07-30 22:09:14 - train: epoch 0089, iter [03500, 05004], lr: 0.006902, loss: 1.0777
2022-07-30 22:11:09 - train: epoch 0089, iter [03600, 05004], lr: 0.006878, loss: 1.3101
2022-07-30 22:13:09 - train: epoch 0089, iter [03700, 05004], lr: 0.006854, loss: 1.2851
2022-07-30 22:15:05 - train: epoch 0089, iter [03800, 05004], lr: 0.006830, loss: 1.1544
2022-07-30 22:17:08 - train: epoch 0089, iter [03900, 05004], lr: 0.006806, loss: 1.3368
2022-07-30 22:19:04 - train: epoch 0089, iter [04000, 05004], lr: 0.006782, loss: 1.1671
2022-07-30 22:21:05 - train: epoch 0089, iter [04100, 05004], lr: 0.006758, loss: 1.3125
2022-07-30 22:23:00 - train: epoch 0089, iter [04200, 05004], lr: 0.006734, loss: 1.2752
2022-07-30 22:25:02 - train: epoch 0089, iter [04300, 05004], lr: 0.006710, loss: 1.3150
2022-07-30 22:26:59 - train: epoch 0089, iter [04400, 05004], lr: 0.006686, loss: 1.2337
2022-07-30 22:28:53 - train: epoch 0089, iter [04500, 05004], lr: 0.006663, loss: 1.1134
2022-07-30 22:30:49 - train: epoch 0089, iter [04600, 05004], lr: 0.006639, loss: 1.2365
2022-07-30 22:32:50 - train: epoch 0089, iter [04700, 05004], lr: 0.006615, loss: 1.3163
2022-07-30 22:34:49 - train: epoch 0089, iter [04800, 05004], lr: 0.006592, loss: 1.2702
2022-07-30 22:36:44 - train: epoch 0089, iter [04900, 05004], lr: 0.006568, loss: 1.3749
2022-07-30 22:38:30 - train: epoch 0089, iter [05000, 05004], lr: 0.006544, loss: 1.0511
2022-07-30 22:38:34 - train: epoch 089, train_loss: 1.2469
2022-07-30 22:42:28 - eval: epoch: 089, acc1: 70.648%, acc5: 89.986%, test_loss: 1.1827, per_image_load_time: 5.370ms, per_image_inference_time: 0.531ms
2022-07-30 22:42:29 - until epoch: 089, best_acc1: 70.648%
2022-07-30 22:42:29 - epoch 090 lr: 0.006543
2022-07-30 22:44:53 - train: epoch 0090, iter [00100, 05004], lr: 0.006520, loss: 1.3363
2022-07-30 22:46:55 - train: epoch 0090, iter [00200, 05004], lr: 0.006497, loss: 1.1726
2022-07-30 22:48:51 - train: epoch 0090, iter [00300, 05004], lr: 0.006473, loss: 1.0705
2022-07-30 22:50:47 - train: epoch 0090, iter [00400, 05004], lr: 0.006450, loss: 1.3590
2022-07-30 22:52:46 - train: epoch 0090, iter [00500, 05004], lr: 0.006426, loss: 1.2019
2022-07-30 22:54:40 - train: epoch 0090, iter [00600, 05004], lr: 0.006403, loss: 1.3753
2022-07-30 22:56:41 - train: epoch 0090, iter [00700, 05004], lr: 0.006380, loss: 1.1012
2022-07-30 22:58:41 - train: epoch 0090, iter [00800, 05004], lr: 0.006357, loss: 1.1877
2022-07-30 23:00:42 - train: epoch 0090, iter [00900, 05004], lr: 0.006334, loss: 1.2962
2022-07-30 23:02:38 - train: epoch 0090, iter [01000, 05004], lr: 0.006310, loss: 1.1250
2022-07-30 23:04:36 - train: epoch 0090, iter [01100, 05004], lr: 0.006287, loss: 1.2862
2022-07-30 23:06:40 - train: epoch 0090, iter [01200, 05004], lr: 0.006264, loss: 1.0968
2022-07-30 23:08:38 - train: epoch 0090, iter [01300, 05004], lr: 0.006241, loss: 1.2698
2022-07-30 23:10:33 - train: epoch 0090, iter [01400, 05004], lr: 0.006218, loss: 1.2304
2022-07-30 23:12:32 - train: epoch 0090, iter [01500, 05004], lr: 0.006195, loss: 1.5971
2022-07-30 23:14:32 - train: epoch 0090, iter [01600, 05004], lr: 0.006173, loss: 1.1840
2022-07-30 23:16:27 - train: epoch 0090, iter [01700, 05004], lr: 0.006150, loss: 1.0714
2022-07-30 23:18:22 - train: epoch 0090, iter [01800, 05004], lr: 0.006127, loss: 1.3229
2022-07-30 23:20:25 - train: epoch 0090, iter [01900, 05004], lr: 0.006104, loss: 1.2143
2022-07-30 23:22:22 - train: epoch 0090, iter [02000, 05004], lr: 0.006081, loss: 1.3741
2022-07-30 23:24:20 - train: epoch 0090, iter [02100, 05004], lr: 0.006059, loss: 1.4082
2022-07-30 23:26:21 - train: epoch 0090, iter [02200, 05004], lr: 0.006036, loss: 1.1973
2022-07-30 23:28:27 - train: epoch 0090, iter [02300, 05004], lr: 0.006014, loss: 1.4454
2022-07-30 23:30:35 - train: epoch 0090, iter [02400, 05004], lr: 0.005991, loss: 1.1338
2022-07-30 23:32:51 - train: epoch 0090, iter [02500, 05004], lr: 0.005969, loss: 1.0712
2022-07-30 23:34:54 - train: epoch 0090, iter [02600, 05004], lr: 0.005946, loss: 1.2238
2022-07-30 23:37:04 - train: epoch 0090, iter [02700, 05004], lr: 0.005924, loss: 1.2701
2022-07-30 23:39:10 - train: epoch 0090, iter [02800, 05004], lr: 0.005901, loss: 1.1720
2022-07-30 23:41:13 - train: epoch 0090, iter [02900, 05004], lr: 0.005879, loss: 1.2875
2022-07-30 23:43:17 - train: epoch 0090, iter [03000, 05004], lr: 0.005857, loss: 1.2382
2022-07-30 23:45:29 - train: epoch 0090, iter [03100, 05004], lr: 0.005834, loss: 1.0115
2022-07-30 23:47:30 - train: epoch 0090, iter [03200, 05004], lr: 0.005812, loss: 1.1353
2022-07-30 23:49:36 - train: epoch 0090, iter [03300, 05004], lr: 0.005790, loss: 1.3557
2022-07-30 23:51:41 - train: epoch 0090, iter [03400, 05004], lr: 0.005768, loss: 1.2907
2022-07-30 23:53:49 - train: epoch 0090, iter [03500, 05004], lr: 0.005746, loss: 1.2496
2022-07-30 23:55:49 - train: epoch 0090, iter [03600, 05004], lr: 0.005724, loss: 1.1302
2022-07-30 23:57:52 - train: epoch 0090, iter [03700, 05004], lr: 0.005702, loss: 1.3456
2022-07-30 23:59:58 - train: epoch 0090, iter [03800, 05004], lr: 0.005680, loss: 1.1987
2022-07-31 00:01:59 - train: epoch 0090, iter [03900, 05004], lr: 0.005658, loss: 1.1055
2022-07-31 00:04:04 - train: epoch 0090, iter [04000, 05004], lr: 0.005636, loss: 1.2197
2022-07-31 00:06:15 - train: epoch 0090, iter [04100, 05004], lr: 0.005614, loss: 1.2686
2022-07-31 00:08:15 - train: epoch 0090, iter [04200, 05004], lr: 0.005592, loss: 1.0538
2022-07-31 00:10:18 - train: epoch 0090, iter [04300, 05004], lr: 0.005570, loss: 1.1945
2022-07-31 00:12:20 - train: epoch 0090, iter [04400, 05004], lr: 0.005549, loss: 1.3194
2022-07-31 00:14:25 - train: epoch 0090, iter [04500, 05004], lr: 0.005527, loss: 1.0414
2022-07-31 00:16:31 - train: epoch 0090, iter [04600, 05004], lr: 0.005505, loss: 1.1637
2022-07-31 00:18:34 - train: epoch 0090, iter [04700, 05004], lr: 0.005484, loss: 1.4076
2022-07-31 00:20:32 - train: epoch 0090, iter [04800, 05004], lr: 0.005462, loss: 1.4796
2022-07-31 00:22:37 - train: epoch 0090, iter [04900, 05004], lr: 0.005441, loss: 1.1331
2022-07-31 00:24:38 - train: epoch 0090, iter [05000, 05004], lr: 0.005419, loss: 1.0576
2022-07-31 00:24:42 - train: epoch 090, train_loss: 1.2191
2022-07-31 00:29:20 - eval: epoch: 090, acc1: 70.958%, acc5: 90.036%, test_loss: 1.1782, per_image_load_time: 3.029ms, per_image_inference_time: 0.656ms
2022-07-31 00:29:21 - until epoch: 090, best_acc1: 70.958%
2022-07-31 00:29:21 - epoch 091 lr: 0.005418
2022-07-31 00:31:57 - train: epoch 0091, iter [00100, 05004], lr: 0.005397, loss: 0.9775
2022-07-31 00:34:14 - train: epoch 0091, iter [00200, 05004], lr: 0.005375, loss: 1.1279
2022-07-31 00:36:34 - train: epoch 0091, iter [00300, 05004], lr: 0.005354, loss: 1.2302
2022-07-31 00:38:46 - train: epoch 0091, iter [00400, 05004], lr: 0.005333, loss: 1.3218
2022-07-31 00:40:59 - train: epoch 0091, iter [00500, 05004], lr: 0.005312, loss: 1.0148
2022-07-31 00:43:12 - train: epoch 0091, iter [00600, 05004], lr: 0.005290, loss: 1.2187
2022-07-31 00:45:24 - train: epoch 0091, iter [00700, 05004], lr: 0.005269, loss: 1.3299
2022-07-31 00:47:30 - train: epoch 0091, iter [00800, 05004], lr: 0.005248, loss: 0.9304
2022-07-31 00:49:40 - train: epoch 0091, iter [00900, 05004], lr: 0.005227, loss: 1.1291
2022-07-31 00:51:46 - train: epoch 0091, iter [01000, 05004], lr: 0.005206, loss: 1.1913
2022-07-31 00:53:52 - train: epoch 0091, iter [01100, 05004], lr: 0.005185, loss: 0.9824
2022-07-31 00:56:02 - train: epoch 0091, iter [01200, 05004], lr: 0.005164, loss: 1.3746
2022-07-31 00:58:18 - train: epoch 0091, iter [01300, 05004], lr: 0.005143, loss: 1.1624
2022-07-31 01:00:24 - train: epoch 0091, iter [01400, 05004], lr: 0.005122, loss: 1.1728
2022-07-31 01:02:28 - train: epoch 0091, iter [01500, 05004], lr: 0.005101, loss: 1.1907
2022-07-31 01:04:35 - train: epoch 0091, iter [01600, 05004], lr: 0.005080, loss: 1.1846
2022-07-31 01:06:47 - train: epoch 0091, iter [01700, 05004], lr: 0.005059, loss: 1.4017
2022-07-31 01:09:03 - train: epoch 0091, iter [01800, 05004], lr: 0.005039, loss: 1.1014
2022-07-31 01:11:12 - train: epoch 0091, iter [01900, 05004], lr: 0.005018, loss: 1.2005
2022-07-31 01:13:20 - train: epoch 0091, iter [02000, 05004], lr: 0.004997, loss: 0.9393
2022-07-31 01:15:23 - train: epoch 0091, iter [02100, 05004], lr: 0.004977, loss: 1.1001
2022-07-31 01:17:29 - train: epoch 0091, iter [02200, 05004], lr: 0.004956, loss: 1.2801
2022-07-31 01:19:36 - train: epoch 0091, iter [02300, 05004], lr: 0.004936, loss: 1.2412
2022-07-31 01:21:45 - train: epoch 0091, iter [02400, 05004], lr: 0.004915, loss: 1.1497
2022-07-31 01:23:55 - train: epoch 0091, iter [02500, 05004], lr: 0.004895, loss: 1.3548
2022-07-31 01:26:06 - train: epoch 0091, iter [02600, 05004], lr: 0.004874, loss: 1.1778
2022-07-31 01:28:12 - train: epoch 0091, iter [02700, 05004], lr: 0.004854, loss: 1.2215
2022-07-31 01:30:17 - train: epoch 0091, iter [02800, 05004], lr: 0.004834, loss: 1.2301
2022-07-31 01:32:26 - train: epoch 0091, iter [02900, 05004], lr: 0.004813, loss: 1.4695
2022-07-31 01:34:32 - train: epoch 0091, iter [03000, 05004], lr: 0.004793, loss: 1.2100
2022-07-31 01:36:43 - train: epoch 0091, iter [03100, 05004], lr: 0.004773, loss: 1.1161
2022-07-31 01:38:50 - train: epoch 0091, iter [03200, 05004], lr: 0.004753, loss: 1.4599
2022-07-31 01:40:52 - train: epoch 0091, iter [03300, 05004], lr: 0.004733, loss: 1.0942
2022-07-31 01:43:00 - train: epoch 0091, iter [03400, 05004], lr: 0.004713, loss: 1.2322
2022-07-31 01:45:03 - train: epoch 0091, iter [03500, 05004], lr: 0.004693, loss: 1.1568
2022-07-31 01:47:05 - train: epoch 0091, iter [03600, 05004], lr: 0.004673, loss: 1.2712
2022-07-31 01:49:08 - train: epoch 0091, iter [03700, 05004], lr: 0.004653, loss: 1.3006
2022-07-31 01:51:12 - train: epoch 0091, iter [03800, 05004], lr: 0.004633, loss: 1.3088
2022-07-31 01:53:16 - train: epoch 0091, iter [03900, 05004], lr: 0.004613, loss: 1.0421
2022-07-31 01:55:20 - train: epoch 0091, iter [04000, 05004], lr: 0.004593, loss: 1.2694
2022-07-31 01:57:15 - train: epoch 0091, iter [04100, 05004], lr: 0.004573, loss: 1.3134
2022-07-31 01:59:11 - train: epoch 0091, iter [04200, 05004], lr: 0.004554, loss: 1.1896
2022-07-31 02:01:08 - train: epoch 0091, iter [04300, 05004], lr: 0.004534, loss: 1.0416
2022-07-31 02:03:05 - train: epoch 0091, iter [04400, 05004], lr: 0.004514, loss: 1.0225
2022-07-31 02:05:06 - train: epoch 0091, iter [04500, 05004], lr: 0.004495, loss: 1.0739
2022-07-31 02:07:05 - train: epoch 0091, iter [04600, 05004], lr: 0.004475, loss: 1.1722
2022-07-31 02:09:03 - train: epoch 0091, iter [04700, 05004], lr: 0.004456, loss: 1.1919
2022-07-31 02:11:12 - train: epoch 0091, iter [04800, 05004], lr: 0.004436, loss: 1.3217
2022-07-31 02:13:21 - train: epoch 0091, iter [04900, 05004], lr: 0.004417, loss: 1.0878
2022-07-31 02:15:11 - train: epoch 0091, iter [05000, 05004], lr: 0.004397, loss: 1.2990
2022-07-31 02:15:16 - train: epoch 091, train_loss: 1.1996
2022-07-31 02:19:19 - eval: epoch: 091, acc1: 71.152%, acc5: 90.268%, test_loss: 1.1667, per_image_load_time: 7.760ms, per_image_inference_time: 0.770ms
2022-07-31 02:19:20 - until epoch: 091, best_acc1: 71.152%
2022-07-31 02:19:20 - epoch 092 lr: 0.004396
2022-07-31 02:22:14 - train: epoch 0092, iter [00100, 05004], lr: 0.004377, loss: 1.2038
2022-07-31 02:24:30 - train: epoch 0092, iter [00200, 05004], lr: 0.004358, loss: 1.1065
2022-07-31 02:26:42 - train: epoch 0092, iter [00300, 05004], lr: 0.004338, loss: 1.2755
2022-07-31 02:28:59 - train: epoch 0092, iter [00400, 05004], lr: 0.004319, loss: 1.2210
2022-07-31 02:31:10 - train: epoch 0092, iter [00500, 05004], lr: 0.004300, loss: 1.2728
2022-07-31 02:33:23 - train: epoch 0092, iter [00600, 05004], lr: 0.004281, loss: 1.2386
2022-07-31 02:35:28 - train: epoch 0092, iter [00700, 05004], lr: 0.004262, loss: 1.2195
2022-07-31 02:37:35 - train: epoch 0092, iter [00800, 05004], lr: 0.004243, loss: 1.3289
2022-07-31 02:39:38 - train: epoch 0092, iter [00900, 05004], lr: 0.004224, loss: 1.3558
2022-07-31 02:41:47 - train: epoch 0092, iter [01000, 05004], lr: 0.004205, loss: 0.9836
2022-07-31 02:43:52 - train: epoch 0092, iter [01100, 05004], lr: 0.004186, loss: 1.1016
2022-07-31 02:46:00 - train: epoch 0092, iter [01200, 05004], lr: 0.004167, loss: 1.1583
2022-07-31 02:48:06 - train: epoch 0092, iter [01300, 05004], lr: 0.004148, loss: 1.2020
2022-07-31 02:50:13 - train: epoch 0092, iter [01400, 05004], lr: 0.004129, loss: 1.1505
2022-07-31 02:52:26 - train: epoch 0092, iter [01500, 05004], lr: 0.004110, loss: 1.0415
2022-07-31 02:54:27 - train: epoch 0092, iter [01600, 05004], lr: 0.004092, loss: 1.2159
2022-07-31 02:56:36 - train: epoch 0092, iter [01700, 05004], lr: 0.004073, loss: 1.2283
2022-07-31 02:58:42 - train: epoch 0092, iter [01800, 05004], lr: 0.004054, loss: 1.3051
2022-07-31 03:00:50 - train: epoch 0092, iter [01900, 05004], lr: 0.004036, loss: 1.0611
2022-07-31 03:02:58 - train: epoch 0092, iter [02000, 05004], lr: 0.004017, loss: 1.0682
2022-07-31 03:05:00 - train: epoch 0092, iter [02100, 05004], lr: 0.003999, loss: 1.1179
2022-07-31 03:07:07 - train: epoch 0092, iter [02200, 05004], lr: 0.003980, loss: 1.1069
2022-07-31 03:09:14 - train: epoch 0092, iter [02300, 05004], lr: 0.003962, loss: 1.1719
2022-07-31 03:11:13 - train: epoch 0092, iter [02400, 05004], lr: 0.003943, loss: 1.0585
2022-07-31 03:13:20 - train: epoch 0092, iter [02500, 05004], lr: 0.003925, loss: 1.1745
2022-07-31 03:15:26 - train: epoch 0092, iter [02600, 05004], lr: 0.003907, loss: 1.1026
2022-07-31 03:17:31 - train: epoch 0092, iter [02700, 05004], lr: 0.003888, loss: 1.2090
2022-07-31 03:19:38 - train: epoch 0092, iter [02800, 05004], lr: 0.003870, loss: 1.3936
2022-07-31 03:21:47 - train: epoch 0092, iter [02900, 05004], lr: 0.003852, loss: 1.2530
2022-07-31 03:23:44 - train: epoch 0092, iter [03000, 05004], lr: 0.003834, loss: 1.0835
2022-07-31 03:25:55 - train: epoch 0092, iter [03100, 05004], lr: 0.003816, loss: 1.2175
2022-07-31 03:28:00 - train: epoch 0092, iter [03200, 05004], lr: 0.003798, loss: 0.9894
2022-07-31 03:30:10 - train: epoch 0092, iter [03300, 05004], lr: 0.003780, loss: 1.2590
2022-07-31 03:32:21 - train: epoch 0092, iter [03400, 05004], lr: 0.003762, loss: 1.3620
2022-07-31 03:34:25 - train: epoch 0092, iter [03500, 05004], lr: 0.003744, loss: 1.1814
2022-07-31 03:36:26 - train: epoch 0092, iter [03600, 05004], lr: 0.003726, loss: 1.1701
2022-07-31 03:38:33 - train: epoch 0092, iter [03700, 05004], lr: 0.003708, loss: 1.1355
2022-07-31 03:40:43 - train: epoch 0092, iter [03800, 05004], lr: 0.003690, loss: 1.4670
2022-07-31 03:42:48 - train: epoch 0092, iter [03900, 05004], lr: 0.003672, loss: 1.3137
2022-07-31 03:44:50 - train: epoch 0092, iter [04000, 05004], lr: 0.003655, loss: 1.1919
2022-07-31 03:47:00 - train: epoch 0092, iter [04100, 05004], lr: 0.003637, loss: 1.0813
2022-07-31 03:49:05 - train: epoch 0092, iter [04200, 05004], lr: 0.003619, loss: 1.1682
2022-07-31 03:51:12 - train: epoch 0092, iter [04300, 05004], lr: 0.003602, loss: 1.2078
2022-07-31 03:53:16 - train: epoch 0092, iter [04400, 05004], lr: 0.003584, loss: 1.2751
2022-07-31 03:55:25 - train: epoch 0092, iter [04500, 05004], lr: 0.003567, loss: 1.1195
2022-07-31 03:57:26 - train: epoch 0092, iter [04600, 05004], lr: 0.003549, loss: 1.2659
2022-07-31 03:59:24 - train: epoch 0092, iter [04700, 05004], lr: 0.003532, loss: 1.0399
2022-07-31 04:01:34 - train: epoch 0092, iter [04800, 05004], lr: 0.003514, loss: 1.1545
2022-07-31 04:03:36 - train: epoch 0092, iter [04900, 05004], lr: 0.003497, loss: 1.1754
2022-07-31 04:05:45 - train: epoch 0092, iter [05000, 05004], lr: 0.003480, loss: 1.2760
2022-07-31 04:05:51 - train: epoch 092, train_loss: 1.1783
2022-07-31 04:10:00 - eval: epoch: 092, acc1: 71.492%, acc5: 90.384%, test_loss: 1.1570, per_image_load_time: 8.144ms, per_image_inference_time: 0.824ms
2022-07-31 04:10:01 - until epoch: 092, best_acc1: 71.492%
2022-07-31 04:10:01 - epoch 093 lr: 0.003479
2022-07-31 04:12:34 - train: epoch 0093, iter [00100, 05004], lr: 0.003462, loss: 1.1282
2022-07-31 04:14:36 - train: epoch 0093, iter [00200, 05004], lr: 0.003445, loss: 0.9733
2022-07-31 04:16:42 - train: epoch 0093, iter [00300, 05004], lr: 0.003427, loss: 1.1184
2022-07-31 04:18:52 - train: epoch 0093, iter [00400, 05004], lr: 0.003410, loss: 1.2467
2022-07-31 04:21:01 - train: epoch 0093, iter [00500, 05004], lr: 0.003393, loss: 1.2121
2022-07-31 04:23:02 - train: epoch 0093, iter [00600, 05004], lr: 0.003376, loss: 1.1312
2022-07-31 04:25:08 - train: epoch 0093, iter [00700, 05004], lr: 0.003359, loss: 1.2690
2022-07-31 04:27:13 - train: epoch 0093, iter [00800, 05004], lr: 0.003342, loss: 1.0714
2022-07-31 04:29:16 - train: epoch 0093, iter [00900, 05004], lr: 0.003325, loss: 1.2300
2022-07-31 04:31:22 - train: epoch 0093, iter [01000, 05004], lr: 0.003308, loss: 1.0270
2022-07-31 04:33:30 - train: epoch 0093, iter [01100, 05004], lr: 0.003292, loss: 1.0365
2022-07-31 04:35:33 - train: epoch 0093, iter [01200, 05004], lr: 0.003275, loss: 1.1193
2022-07-31 04:37:40 - train: epoch 0093, iter [01300, 05004], lr: 0.003258, loss: 1.1795
2022-07-31 04:39:43 - train: epoch 0093, iter [01400, 05004], lr: 0.003241, loss: 1.1694
2022-07-31 04:41:51 - train: epoch 0093, iter [01500, 05004], lr: 0.003225, loss: 1.4187
2022-07-31 04:43:49 - train: epoch 0093, iter [01600, 05004], lr: 0.003208, loss: 1.2202
2022-07-31 04:45:51 - train: epoch 0093, iter [01700, 05004], lr: 0.003191, loss: 1.1838
2022-07-31 04:47:55 - train: epoch 0093, iter [01800, 05004], lr: 0.003175, loss: 1.3311
2022-07-31 04:49:57 - train: epoch 0093, iter [01900, 05004], lr: 0.003158, loss: 0.9773
2022-07-31 04:52:04 - train: epoch 0093, iter [02000, 05004], lr: 0.003142, loss: 1.0944
2022-07-31 04:54:07 - train: epoch 0093, iter [02100, 05004], lr: 0.003126, loss: 1.0700
2022-07-31 04:56:10 - train: epoch 0093, iter [02200, 05004], lr: 0.003109, loss: 1.1886
2022-07-31 04:58:17 - train: epoch 0093, iter [02300, 05004], lr: 0.003093, loss: 1.1409
2022-07-31 05:00:14 - train: epoch 0093, iter [02400, 05004], lr: 0.003077, loss: 1.0327
2022-07-31 05:02:16 - train: epoch 0093, iter [02500, 05004], lr: 0.003060, loss: 1.0542
2022-07-31 05:04:23 - train: epoch 0093, iter [02600, 05004], lr: 0.003044, loss: 1.2097
2022-07-31 05:06:25 - train: epoch 0093, iter [02700, 05004], lr: 0.003028, loss: 1.0783
2022-07-31 05:08:26 - train: epoch 0093, iter [02800, 05004], lr: 0.003012, loss: 0.9747
2022-07-31 05:10:29 - train: epoch 0093, iter [02900, 05004], lr: 0.002996, loss: 1.1147
2022-07-31 05:12:28 - train: epoch 0093, iter [03000, 05004], lr: 0.002980, loss: 1.0317
2022-07-31 05:14:27 - train: epoch 0093, iter [03100, 05004], lr: 0.002964, loss: 1.2977
2022-07-31 05:16:27 - train: epoch 0093, iter [03200, 05004], lr: 0.002948, loss: 0.9749
2022-07-31 05:18:27 - train: epoch 0093, iter [03300, 05004], lr: 0.002932, loss: 1.0069
2022-07-31 05:20:26 - train: epoch 0093, iter [03400, 05004], lr: 0.002916, loss: 1.2172
2022-07-31 05:22:32 - train: epoch 0093, iter [03500, 05004], lr: 0.002900, loss: 1.0470
2022-07-31 05:24:33 - train: epoch 0093, iter [03600, 05004], lr: 0.002884, loss: 1.3262
2022-07-31 05:26:27 - train: epoch 0093, iter [03700, 05004], lr: 0.002869, loss: 1.0740
2022-07-31 05:28:31 - train: epoch 0093, iter [03800, 05004], lr: 0.002853, loss: 1.1549
2022-07-31 05:30:30 - train: epoch 0093, iter [03900, 05004], lr: 0.002837, loss: 1.1278
2022-07-31 05:32:31 - train: epoch 0093, iter [04000, 05004], lr: 0.002822, loss: 1.3908
2022-07-31 05:34:37 - train: epoch 0093, iter [04100, 05004], lr: 0.002806, loss: 1.2015
2022-07-31 05:36:35 - train: epoch 0093, iter [04200, 05004], lr: 0.002791, loss: 1.0693
2022-07-31 05:38:33 - train: epoch 0093, iter [04300, 05004], lr: 0.002775, loss: 1.2905
2022-07-31 05:40:32 - train: epoch 0093, iter [04400, 05004], lr: 0.002760, loss: 1.0110
2022-07-31 05:42:33 - train: epoch 0093, iter [04500, 05004], lr: 0.002744, loss: 1.1672
2022-07-31 05:44:32 - train: epoch 0093, iter [04600, 05004], lr: 0.002729, loss: 1.0576
2022-07-31 05:46:31 - train: epoch 0093, iter [04700, 05004], lr: 0.002714, loss: 1.4480
2022-07-31 05:48:30 - train: epoch 0093, iter [04800, 05004], lr: 0.002698, loss: 1.0154
2022-07-31 05:50:24 - train: epoch 0093, iter [04900, 05004], lr: 0.002683, loss: 1.1739
2022-07-31 05:52:08 - train: epoch 0093, iter [05000, 05004], lr: 0.002668, loss: 1.0502
2022-07-31 05:52:12 - train: epoch 093, train_loss: 1.1536
2022-07-31 05:56:30 - eval: epoch: 093, acc1: 71.858%, acc5: 90.442%, test_loss: 1.1465, per_image_load_time: 6.959ms, per_image_inference_time: 0.700ms
2022-07-31 05:56:30 - until epoch: 093, best_acc1: 71.858%
2022-07-31 05:56:30 - epoch 094 lr: 0.002667
2022-07-31 05:57:40 - train: epoch 0094, iter [00100, 05004], lr: 0.002652, loss: 1.1225
2022-07-31 05:58:37 - train: epoch 0094, iter [00200, 05004], lr: 0.002637, loss: 1.0655
2022-07-31 05:59:36 - train: epoch 0094, iter [00300, 05004], lr: 0.002622, loss: 1.0178
2022-07-31 06:00:33 - train: epoch 0094, iter [00400, 05004], lr: 0.002607, loss: 1.1394
2022-07-31 06:01:32 - train: epoch 0094, iter [00500, 05004], lr: 0.002592, loss: 1.0664
2022-07-31 06:02:29 - train: epoch 0094, iter [00600, 05004], lr: 0.002577, loss: 0.9844
2022-07-31 06:03:25 - train: epoch 0094, iter [00700, 05004], lr: 0.002562, loss: 1.1732
2022-07-31 06:04:23 - train: epoch 0094, iter [00800, 05004], lr: 0.002547, loss: 1.1588
2022-07-31 06:05:21 - train: epoch 0094, iter [00900, 05004], lr: 0.002533, loss: 1.0846
2022-07-31 06:06:18 - train: epoch 0094, iter [01000, 05004], lr: 0.002518, loss: 0.9089
2022-07-31 06:07:16 - train: epoch 0094, iter [01100, 05004], lr: 0.002503, loss: 1.3719
2022-07-31 06:08:11 - train: epoch 0094, iter [01200, 05004], lr: 0.002488, loss: 1.2148
2022-07-31 06:09:09 - train: epoch 0094, iter [01300, 05004], lr: 0.002474, loss: 1.1294
2022-07-31 06:10:07 - train: epoch 0094, iter [01400, 05004], lr: 0.002459, loss: 0.9671
2022-07-31 06:11:04 - train: epoch 0094, iter [01500, 05004], lr: 0.002445, loss: 1.1451
2022-07-31 06:12:02 - train: epoch 0094, iter [01600, 05004], lr: 0.002430, loss: 1.3553
2022-07-31 06:12:59 - train: epoch 0094, iter [01700, 05004], lr: 0.002416, loss: 1.1072
2022-07-31 06:13:57 - train: epoch 0094, iter [01800, 05004], lr: 0.002401, loss: 1.0391
2022-07-31 06:14:55 - train: epoch 0094, iter [01900, 05004], lr: 0.002387, loss: 1.1137
2022-07-31 06:15:52 - train: epoch 0094, iter [02000, 05004], lr: 0.002373, loss: 0.9447
2022-07-31 06:16:49 - train: epoch 0094, iter [02100, 05004], lr: 0.002358, loss: 1.0063
2022-07-31 06:17:48 - train: epoch 0094, iter [02200, 05004], lr: 0.002344, loss: 1.2303
2022-07-31 06:18:43 - train: epoch 0094, iter [02300, 05004], lr: 0.002330, loss: 0.9958
2022-07-31 06:19:43 - train: epoch 0094, iter [02400, 05004], lr: 0.002316, loss: 1.2031
2022-07-31 06:20:39 - train: epoch 0094, iter [02500, 05004], lr: 0.002302, loss: 1.2117
2022-07-31 06:21:38 - train: epoch 0094, iter [02600, 05004], lr: 0.002288, loss: 1.1775
2022-07-31 06:22:36 - train: epoch 0094, iter [02700, 05004], lr: 0.002273, loss: 1.1145
2022-07-31 06:23:33 - train: epoch 0094, iter [02800, 05004], lr: 0.002260, loss: 1.1077
2022-07-31 06:24:30 - train: epoch 0094, iter [02900, 05004], lr: 0.002246, loss: 1.0234
2022-07-31 06:25:28 - train: epoch 0094, iter [03000, 05004], lr: 0.002232, loss: 1.3185
2022-07-31 06:26:26 - train: epoch 0094, iter [03100, 05004], lr: 0.002218, loss: 1.2816
2022-07-31 06:27:22 - train: epoch 0094, iter [03200, 05004], lr: 0.002204, loss: 1.2550
2022-07-31 06:28:21 - train: epoch 0094, iter [03300, 05004], lr: 0.002190, loss: 1.1717
2022-07-31 06:29:19 - train: epoch 0094, iter [03400, 05004], lr: 0.002176, loss: 1.2615
2022-07-31 06:30:16 - train: epoch 0094, iter [03500, 05004], lr: 0.002163, loss: 1.2796
2022-07-31 06:31:14 - train: epoch 0094, iter [03600, 05004], lr: 0.002149, loss: 1.1235
2022-07-31 06:32:11 - train: epoch 0094, iter [03700, 05004], lr: 0.002136, loss: 1.3268
2022-07-31 06:33:09 - train: epoch 0094, iter [03800, 05004], lr: 0.002122, loss: 1.0210
2022-07-31 06:34:06 - train: epoch 0094, iter [03900, 05004], lr: 0.002108, loss: 1.0675
2022-07-31 06:35:02 - train: epoch 0094, iter [04000, 05004], lr: 0.002095, loss: 1.1356
2022-07-31 06:36:00 - train: epoch 0094, iter [04100, 05004], lr: 0.002082, loss: 1.1962
2022-07-31 06:36:58 - train: epoch 0094, iter [04200, 05004], lr: 0.002068, loss: 1.0725
2022-07-31 06:37:55 - train: epoch 0094, iter [04300, 05004], lr: 0.002055, loss: 1.3080
2022-07-31 06:38:51 - train: epoch 0094, iter [04400, 05004], lr: 0.002041, loss: 1.1506
2022-07-31 06:39:51 - train: epoch 0094, iter [04500, 05004], lr: 0.002028, loss: 1.0970
2022-07-31 06:40:46 - train: epoch 0094, iter [04600, 05004], lr: 0.002015, loss: 1.1479
2022-07-31 06:41:46 - train: epoch 0094, iter [04700, 05004], lr: 0.002002, loss: 1.4257
2022-07-31 06:42:42 - train: epoch 0094, iter [04800, 05004], lr: 0.001989, loss: 1.0844
2022-07-31 06:43:39 - train: epoch 0094, iter [04900, 05004], lr: 0.001976, loss: 1.2719
2022-07-31 06:44:33 - train: epoch 0094, iter [05000, 05004], lr: 0.001963, loss: 1.0274
2022-07-31 06:44:34 - train: epoch 094, train_loss: 1.1403
2022-07-31 06:46:42 - eval: epoch: 094, acc1: 72.020%, acc5: 90.564%, test_loss: 1.1397, per_image_load_time: 3.543ms, per_image_inference_time: 0.474ms
2022-07-31 06:46:42 - until epoch: 094, best_acc1: 72.020%
2022-07-31 06:46:42 - epoch 095 lr: 0.001962
2022-07-31 06:47:50 - train: epoch 0095, iter [00100, 05004], lr: 0.001949, loss: 1.1968
2022-07-31 06:48:50 - train: epoch 0095, iter [00200, 05004], lr: 0.001936, loss: 1.2470
2022-07-31 06:49:47 - train: epoch 0095, iter [00300, 05004], lr: 0.001923, loss: 0.9537
2022-07-31 06:50:47 - train: epoch 0095, iter [00400, 05004], lr: 0.001910, loss: 1.0373
2022-07-31 06:51:44 - train: epoch 0095, iter [00500, 05004], lr: 0.001897, loss: 1.1733
2022-07-31 06:52:40 - train: epoch 0095, iter [00600, 05004], lr: 0.001885, loss: 1.1339
2022-07-31 06:53:36 - train: epoch 0095, iter [00700, 05004], lr: 0.001872, loss: 1.1059
2022-07-31 06:54:36 - train: epoch 0095, iter [00800, 05004], lr: 0.001859, loss: 1.2789
2022-07-31 06:55:32 - train: epoch 0095, iter [00900, 05004], lr: 0.001846, loss: 1.2020
2022-07-31 06:56:30 - train: epoch 0095, iter [01000, 05004], lr: 0.001834, loss: 1.1941
2022-07-31 06:57:28 - train: epoch 0095, iter [01100, 05004], lr: 0.001821, loss: 0.9371
2022-07-31 06:58:26 - train: epoch 0095, iter [01200, 05004], lr: 0.001809, loss: 1.0638
2022-07-31 06:59:22 - train: epoch 0095, iter [01300, 05004], lr: 0.001796, loss: 1.0229
2022-07-31 07:00:20 - train: epoch 0095, iter [01400, 05004], lr: 0.001784, loss: 1.0579
2022-07-31 07:01:20 - train: epoch 0095, iter [01500, 05004], lr: 0.001771, loss: 1.0999
2022-07-31 07:02:18 - train: epoch 0095, iter [01600, 05004], lr: 0.001759, loss: 0.8338
2022-07-31 07:03:18 - train: epoch 0095, iter [01700, 05004], lr: 0.001747, loss: 1.1290
2022-07-31 07:04:16 - train: epoch 0095, iter [01800, 05004], lr: 0.001734, loss: 1.2622
2022-07-31 07:05:14 - train: epoch 0095, iter [01900, 05004], lr: 0.001722, loss: 1.1517
2022-07-31 07:06:12 - train: epoch 0095, iter [02000, 05004], lr: 0.001710, loss: 1.2498
2022-07-31 07:07:09 - train: epoch 0095, iter [02100, 05004], lr: 0.001698, loss: 1.0280
2022-07-31 07:08:08 - train: epoch 0095, iter [02200, 05004], lr: 0.001686, loss: 0.9217
2022-07-31 07:09:07 - train: epoch 0095, iter [02300, 05004], lr: 0.001674, loss: 1.1968
2022-07-31 07:10:02 - train: epoch 0095, iter [02400, 05004], lr: 0.001662, loss: 0.9493
2022-07-31 07:11:00 - train: epoch 0095, iter [02500, 05004], lr: 0.001650, loss: 1.1159
2022-07-31 07:11:58 - train: epoch 0095, iter [02600, 05004], lr: 0.001638, loss: 1.1004
2022-07-31 07:12:56 - train: epoch 0095, iter [02700, 05004], lr: 0.001626, loss: 1.1159
2022-07-31 07:13:54 - train: epoch 0095, iter [02800, 05004], lr: 0.001614, loss: 1.0830
2022-07-31 07:14:52 - train: epoch 0095, iter [02900, 05004], lr: 0.001602, loss: 1.2176
2022-07-31 07:15:48 - train: epoch 0095, iter [03000, 05004], lr: 0.001590, loss: 1.3037
2022-07-31 07:16:46 - train: epoch 0095, iter [03100, 05004], lr: 0.001579, loss: 1.1584
2022-07-31 07:17:43 - train: epoch 0095, iter [03200, 05004], lr: 0.001567, loss: 1.1837
2022-07-31 07:18:41 - train: epoch 0095, iter [03300, 05004], lr: 0.001555, loss: 1.0834
2022-07-31 07:19:38 - train: epoch 0095, iter [03400, 05004], lr: 0.001544, loss: 0.9364
2022-07-31 07:20:37 - train: epoch 0095, iter [03500, 05004], lr: 0.001532, loss: 1.2120
2022-07-31 07:21:34 - train: epoch 0095, iter [03600, 05004], lr: 0.001521, loss: 1.0466
2022-07-31 07:22:31 - train: epoch 0095, iter [03700, 05004], lr: 0.001509, loss: 1.1082
2022-07-31 07:23:28 - train: epoch 0095, iter [03800, 05004], lr: 0.001498, loss: 1.1049
2022-07-31 07:24:25 - train: epoch 0095, iter [03900, 05004], lr: 0.001487, loss: 1.0153
2022-07-31 07:25:22 - train: epoch 0095, iter [04000, 05004], lr: 0.001475, loss: 1.0280
2022-07-31 07:26:19 - train: epoch 0095, iter [04100, 05004], lr: 0.001464, loss: 1.0691
2022-07-31 07:27:17 - train: epoch 0095, iter [04200, 05004], lr: 0.001453, loss: 1.0064
2022-07-31 07:28:13 - train: epoch 0095, iter [04300, 05004], lr: 0.001442, loss: 1.1165
2022-07-31 07:29:11 - train: epoch 0095, iter [04400, 05004], lr: 0.001430, loss: 1.1992
2022-07-31 07:30:06 - train: epoch 0095, iter [04500, 05004], lr: 0.001419, loss: 0.8536
2022-07-31 07:31:01 - train: epoch 0095, iter [04600, 05004], lr: 0.001408, loss: 1.0982
2022-07-31 07:31:58 - train: epoch 0095, iter [04700, 05004], lr: 0.001397, loss: 1.0494
2022-07-31 07:32:54 - train: epoch 0095, iter [04800, 05004], lr: 0.001386, loss: 0.9214
2022-07-31 07:33:51 - train: epoch 0095, iter [04900, 05004], lr: 0.001375, loss: 0.9302
2022-07-31 07:34:44 - train: epoch 0095, iter [05000, 05004], lr: 0.001364, loss: 1.1829
2022-07-31 07:34:45 - train: epoch 095, train_loss: 1.1230
2022-07-31 07:36:55 - eval: epoch: 095, acc1: 72.102%, acc5: 90.660%, test_loss: 1.1345, per_image_load_time: 1.644ms, per_image_inference_time: 0.414ms
2022-07-31 07:36:56 - until epoch: 095, best_acc1: 72.102%
2022-07-31 07:36:56 - epoch 096 lr: 0.001364
2022-07-31 07:38:04 - train: epoch 0096, iter [00100, 05004], lr: 0.001353, loss: 1.1270
2022-07-31 07:39:03 - train: epoch 0096, iter [00200, 05004], lr: 0.001342, loss: 0.9669
2022-07-31 07:40:02 - train: epoch 0096, iter [00300, 05004], lr: 0.001331, loss: 0.9591
2022-07-31 07:41:01 - train: epoch 0096, iter [00400, 05004], lr: 0.001321, loss: 0.8793
2022-07-31 07:42:00 - train: epoch 0096, iter [00500, 05004], lr: 0.001310, loss: 1.2024
2022-07-31 07:42:57 - train: epoch 0096, iter [00600, 05004], lr: 0.001299, loss: 1.1616
2022-07-31 07:43:54 - train: epoch 0096, iter [00700, 05004], lr: 0.001289, loss: 1.1527
2022-07-31 07:44:52 - train: epoch 0096, iter [00800, 05004], lr: 0.001278, loss: 0.9022
2022-07-31 07:45:47 - train: epoch 0096, iter [00900, 05004], lr: 0.001268, loss: 1.1319
2022-07-31 07:46:45 - train: epoch 0096, iter [01000, 05004], lr: 0.001257, loss: 1.0835
2022-07-31 07:47:43 - train: epoch 0096, iter [01100, 05004], lr: 0.001247, loss: 1.0824
2022-07-31 07:48:40 - train: epoch 0096, iter [01200, 05004], lr: 0.001236, loss: 0.8753
2022-07-31 07:49:37 - train: epoch 0096, iter [01300, 05004], lr: 0.001226, loss: 1.1832
2022-07-31 07:50:37 - train: epoch 0096, iter [01400, 05004], lr: 0.001216, loss: 1.0378
2022-07-31 07:51:35 - train: epoch 0096, iter [01500, 05004], lr: 0.001206, loss: 1.0103
2022-07-31 07:52:33 - train: epoch 0096, iter [01600, 05004], lr: 0.001195, loss: 1.0659
2022-07-31 07:53:34 - train: epoch 0096, iter [01700, 05004], lr: 0.001185, loss: 1.0195
2022-07-31 07:54:32 - train: epoch 0096, iter [01800, 05004], lr: 0.001175, loss: 1.2689
2022-07-31 07:55:30 - train: epoch 0096, iter [01900, 05004], lr: 0.001165, loss: 1.2283
2022-07-31 07:56:31 - train: epoch 0096, iter [02000, 05004], lr: 0.001155, loss: 1.0848
2022-07-31 07:57:29 - train: epoch 0096, iter [02100, 05004], lr: 0.001145, loss: 1.2419
2022-07-31 07:58:28 - train: epoch 0096, iter [02200, 05004], lr: 0.001135, loss: 0.9432
2022-07-31 07:59:26 - train: epoch 0096, iter [02300, 05004], lr: 0.001125, loss: 1.3554
2022-07-31 08:00:27 - train: epoch 0096, iter [02400, 05004], lr: 0.001115, loss: 1.0777
2022-07-31 08:01:25 - train: epoch 0096, iter [02500, 05004], lr: 0.001105, loss: 1.1642
2022-07-31 08:02:25 - train: epoch 0096, iter [02600, 05004], lr: 0.001096, loss: 1.0540
2022-07-31 08:03:24 - train: epoch 0096, iter [02700, 05004], lr: 0.001086, loss: 1.1248
2022-07-31 08:04:23 - train: epoch 0096, iter [02800, 05004], lr: 0.001076, loss: 1.1765
2022-07-31 08:05:23 - train: epoch 0096, iter [02900, 05004], lr: 0.001067, loss: 1.1416
2022-07-31 08:06:22 - train: epoch 0096, iter [03000, 05004], lr: 0.001057, loss: 1.2711
2022-07-31 08:07:22 - train: epoch 0096, iter [03100, 05004], lr: 0.001047, loss: 1.0447
2022-07-31 08:08:20 - train: epoch 0096, iter [03200, 05004], lr: 0.001038, loss: 1.0459
2022-07-31 08:09:19 - train: epoch 0096, iter [03300, 05004], lr: 0.001028, loss: 1.1569
2022-07-31 08:10:17 - train: epoch 0096, iter [03400, 05004], lr: 0.001019, loss: 1.0172
2022-07-31 08:11:17 - train: epoch 0096, iter [03500, 05004], lr: 0.001010, loss: 1.0427
2022-07-31 08:12:16 - train: epoch 0096, iter [03600, 05004], lr: 0.001000, loss: 1.0882
2022-07-31 08:13:16 - train: epoch 0096, iter [03700, 05004], lr: 0.000991, loss: 1.1513
2022-07-31 08:14:15 - train: epoch 0096, iter [03800, 05004], lr: 0.000982, loss: 1.0364
2022-07-31 08:15:14 - train: epoch 0096, iter [03900, 05004], lr: 0.000972, loss: 0.9156
2022-07-31 08:16:13 - train: epoch 0096, iter [04000, 05004], lr: 0.000963, loss: 1.1336
2022-07-31 08:17:12 - train: epoch 0096, iter [04100, 05004], lr: 0.000954, loss: 1.1587
2022-07-31 08:18:11 - train: epoch 0096, iter [04200, 05004], lr: 0.000945, loss: 1.1301
2022-07-31 08:19:11 - train: epoch 0096, iter [04300, 05004], lr: 0.000936, loss: 0.8455
2022-07-31 08:20:08 - train: epoch 0096, iter [04400, 05004], lr: 0.000927, loss: 1.0139
2022-07-31 08:21:10 - train: epoch 0096, iter [04500, 05004], lr: 0.000918, loss: 1.0723
2022-07-31 08:22:08 - train: epoch 0096, iter [04600, 05004], lr: 0.000909, loss: 0.9329
2022-07-31 08:23:08 - train: epoch 0096, iter [04700, 05004], lr: 0.000900, loss: 1.2074
2022-07-31 08:24:06 - train: epoch 0096, iter [04800, 05004], lr: 0.000891, loss: 1.2439
2022-07-31 08:25:05 - train: epoch 0096, iter [04900, 05004], lr: 0.000883, loss: 1.2644
2022-07-31 08:26:01 - train: epoch 0096, iter [05000, 05004], lr: 0.000874, loss: 1.1691
2022-07-31 08:26:02 - train: epoch 096, train_loss: 1.1087
2022-07-31 08:28:13 - eval: epoch: 096, acc1: 72.156%, acc5: 90.694%, test_loss: 1.1307, per_image_load_time: 3.765ms, per_image_inference_time: 0.455ms
2022-07-31 08:28:13 - until epoch: 096, best_acc1: 72.156%
2022-07-31 08:28:13 - epoch 097 lr: 0.000874
2022-07-31 08:29:21 - train: epoch 0097, iter [00100, 05004], lr: 0.000865, loss: 1.1773
2022-07-31 08:30:20 - train: epoch 0097, iter [00200, 05004], lr: 0.000856, loss: 1.0814
2022-07-31 08:31:21 - train: epoch 0097, iter [00300, 05004], lr: 0.000848, loss: 1.1712
2022-07-31 08:32:18 - train: epoch 0097, iter [00400, 05004], lr: 0.000839, loss: 1.3435
2022-07-31 08:33:14 - train: epoch 0097, iter [00500, 05004], lr: 0.000831, loss: 1.1987
2022-07-31 08:34:12 - train: epoch 0097, iter [00600, 05004], lr: 0.000822, loss: 1.1967
2022-07-31 08:35:09 - train: epoch 0097, iter [00700, 05004], lr: 0.000814, loss: 1.1454
2022-07-31 08:36:07 - train: epoch 0097, iter [00800, 05004], lr: 0.000805, loss: 0.9219
2022-07-31 08:37:04 - train: epoch 0097, iter [00900, 05004], lr: 0.000797, loss: 1.0738
2022-07-31 08:38:01 - train: epoch 0097, iter [01000, 05004], lr: 0.000789, loss: 1.1760
2022-07-31 08:38:59 - train: epoch 0097, iter [01100, 05004], lr: 0.000780, loss: 0.9725
2022-07-31 08:39:56 - train: epoch 0097, iter [01200, 05004], lr: 0.000772, loss: 1.2105
2022-07-31 08:40:54 - train: epoch 0097, iter [01300, 05004], lr: 0.000764, loss: 1.2083
2022-07-31 08:41:54 - train: epoch 0097, iter [01400, 05004], lr: 0.000756, loss: 1.2782
2022-07-31 08:42:50 - train: epoch 0097, iter [01500, 05004], lr: 0.000748, loss: 0.9149
2022-07-31 08:43:47 - train: epoch 0097, iter [01600, 05004], lr: 0.000740, loss: 1.2148
2022-07-31 08:44:46 - train: epoch 0097, iter [01700, 05004], lr: 0.000732, loss: 1.2087
2022-07-31 08:45:44 - train: epoch 0097, iter [01800, 05004], lr: 0.000724, loss: 1.1164
2022-07-31 08:46:42 - train: epoch 0097, iter [01900, 05004], lr: 0.000716, loss: 1.1160
2022-07-31 08:47:39 - train: epoch 0097, iter [02000, 05004], lr: 0.000708, loss: 1.0423
2022-07-31 08:48:36 - train: epoch 0097, iter [02100, 05004], lr: 0.000700, loss: 1.1449
2022-07-31 08:49:34 - train: epoch 0097, iter [02200, 05004], lr: 0.000692, loss: 1.1118
2022-07-31 08:50:32 - train: epoch 0097, iter [02300, 05004], lr: 0.000685, loss: 1.0761
2022-07-31 08:51:30 - train: epoch 0097, iter [02400, 05004], lr: 0.000677, loss: 1.0535
2022-07-31 08:52:28 - train: epoch 0097, iter [02500, 05004], lr: 0.000669, loss: 1.1157
2022-07-31 08:53:28 - train: epoch 0097, iter [02600, 05004], lr: 0.000662, loss: 1.1219
2022-07-31 08:54:24 - train: epoch 0097, iter [02700, 05004], lr: 0.000654, loss: 1.0713
2022-07-31 08:55:22 - train: epoch 0097, iter [02800, 05004], lr: 0.000647, loss: 1.1057
2022-07-31 08:56:20 - train: epoch 0097, iter [02900, 05004], lr: 0.000639, loss: 0.9484
2022-07-31 08:57:19 - train: epoch 0097, iter [03000, 05004], lr: 0.000632, loss: 1.1736
2022-07-31 08:58:16 - train: epoch 0097, iter [03100, 05004], lr: 0.000624, loss: 1.0858
2022-07-31 08:59:15 - train: epoch 0097, iter [03200, 05004], lr: 0.000617, loss: 1.2753
2022-07-31 09:00:12 - train: epoch 0097, iter [03300, 05004], lr: 0.000610, loss: 1.1809
2022-07-31 09:01:11 - train: epoch 0097, iter [03400, 05004], lr: 0.000602, loss: 1.1230
2022-07-31 09:02:07 - train: epoch 0097, iter [03500, 05004], lr: 0.000595, loss: 1.2501
2022-07-31 09:03:07 - train: epoch 0097, iter [03600, 05004], lr: 0.000588, loss: 0.9303
2022-07-31 09:04:04 - train: epoch 0097, iter [03700, 05004], lr: 0.000581, loss: 0.9053
2022-07-31 09:05:02 - train: epoch 0097, iter [03800, 05004], lr: 0.000574, loss: 0.9198
2022-07-31 09:06:00 - train: epoch 0097, iter [03900, 05004], lr: 0.000567, loss: 1.1845
2022-07-31 09:06:57 - train: epoch 0097, iter [04000, 05004], lr: 0.000560, loss: 1.0079
2022-07-31 09:07:56 - train: epoch 0097, iter [04100, 05004], lr: 0.000553, loss: 1.1568
2022-07-31 09:08:52 - train: epoch 0097, iter [04200, 05004], lr: 0.000546, loss: 0.9995
2022-07-31 09:09:49 - train: epoch 0097, iter [04300, 05004], lr: 0.000539, loss: 0.9820
2022-07-31 09:10:46 - train: epoch 0097, iter [04400, 05004], lr: 0.000532, loss: 1.0341
2022-07-31 09:11:43 - train: epoch 0097, iter [04500, 05004], lr: 0.000525, loss: 1.1691
2022-07-31 09:12:41 - train: epoch 0097, iter [04600, 05004], lr: 0.000519, loss: 1.0501
2022-07-31 09:13:40 - train: epoch 0097, iter [04700, 05004], lr: 0.000512, loss: 0.9249
2022-07-31 09:14:39 - train: epoch 0097, iter [04800, 05004], lr: 0.000505, loss: 1.0357
2022-07-31 09:15:36 - train: epoch 0097, iter [04900, 05004], lr: 0.000499, loss: 1.2663
2022-07-31 09:16:30 - train: epoch 0097, iter [05000, 05004], lr: 0.000492, loss: 0.9769
2022-07-31 09:16:31 - train: epoch 097, train_loss: 1.0974
2022-07-31 09:18:37 - eval: epoch: 097, acc1: 72.266%, acc5: 90.656%, test_loss: 1.1272, per_image_load_time: 2.962ms, per_image_inference_time: 0.482ms
2022-07-31 09:18:37 - until epoch: 097, best_acc1: 72.266%
2022-07-31 09:18:37 - epoch 098 lr: 0.000492
2022-07-31 09:19:46 - train: epoch 0098, iter [00100, 05004], lr: 0.000485, loss: 1.0585
2022-07-31 09:20:42 - train: epoch 0098, iter [00200, 05004], lr: 0.000479, loss: 1.2613
2022-07-31 09:21:39 - train: epoch 0098, iter [00300, 05004], lr: 0.000472, loss: 1.3070
2022-07-31 09:22:36 - train: epoch 0098, iter [00400, 05004], lr: 0.000466, loss: 0.9546
2022-07-31 09:23:33 - train: epoch 0098, iter [00500, 05004], lr: 0.000460, loss: 1.2596
2022-07-31 09:24:29 - train: epoch 0098, iter [00600, 05004], lr: 0.000453, loss: 1.1496
2022-07-31 09:25:28 - train: epoch 0098, iter [00700, 05004], lr: 0.000447, loss: 1.1863
2022-07-31 09:26:25 - train: epoch 0098, iter [00800, 05004], lr: 0.000441, loss: 1.2047
2022-07-31 09:27:23 - train: epoch 0098, iter [00900, 05004], lr: 0.000435, loss: 1.2525
2022-07-31 09:28:22 - train: epoch 0098, iter [01000, 05004], lr: 0.000428, loss: 1.0391
2022-07-31 09:29:19 - train: epoch 0098, iter [01100, 05004], lr: 0.000422, loss: 1.0709
2022-07-31 09:30:18 - train: epoch 0098, iter [01200, 05004], lr: 0.000416, loss: 1.0759
2022-07-31 09:31:14 - train: epoch 0098, iter [01300, 05004], lr: 0.000410, loss: 1.1082
2022-07-31 09:32:11 - train: epoch 0098, iter [01400, 05004], lr: 0.000404, loss: 1.1579
2022-07-31 09:33:09 - train: epoch 0098, iter [01500, 05004], lr: 0.000398, loss: 1.0499
2022-07-31 09:34:07 - train: epoch 0098, iter [01600, 05004], lr: 0.000393, loss: 1.0728
2022-07-31 09:35:05 - train: epoch 0098, iter [01700, 05004], lr: 0.000387, loss: 1.0229
2022-07-31 09:36:02 - train: epoch 0098, iter [01800, 05004], lr: 0.000381, loss: 1.2291
2022-07-31 09:37:00 - train: epoch 0098, iter [01900, 05004], lr: 0.000375, loss: 1.0224
2022-07-31 09:37:57 - train: epoch 0098, iter [02000, 05004], lr: 0.000369, loss: 1.1088
2022-07-31 09:38:54 - train: epoch 0098, iter [02100, 05004], lr: 0.000364, loss: 1.0639
2022-07-31 09:39:51 - train: epoch 0098, iter [02200, 05004], lr: 0.000358, loss: 1.1174
2022-07-31 09:40:49 - train: epoch 0098, iter [02300, 05004], lr: 0.000353, loss: 0.9361
2022-07-31 09:41:47 - train: epoch 0098, iter [02400, 05004], lr: 0.000347, loss: 1.1655
2022-07-31 09:42:45 - train: epoch 0098, iter [02500, 05004], lr: 0.000342, loss: 0.9929
2022-07-31 09:43:43 - train: epoch 0098, iter [02600, 05004], lr: 0.000336, loss: 1.0794
2022-07-31 09:44:42 - train: epoch 0098, iter [02700, 05004], lr: 0.000331, loss: 1.0966
2022-07-31 09:45:39 - train: epoch 0098, iter [02800, 05004], lr: 0.000325, loss: 0.9309
2022-07-31 09:46:36 - train: epoch 0098, iter [02900, 05004], lr: 0.000320, loss: 1.2170
2022-07-31 09:47:36 - train: epoch 0098, iter [03000, 05004], lr: 0.000315, loss: 0.9906
2022-07-31 09:48:31 - train: epoch 0098, iter [03100, 05004], lr: 0.000310, loss: 1.1181
2022-07-31 09:49:30 - train: epoch 0098, iter [03200, 05004], lr: 0.000305, loss: 0.9245
2022-07-31 09:50:27 - train: epoch 0098, iter [03300, 05004], lr: 0.000299, loss: 0.8240
2022-07-31 09:51:24 - train: epoch 0098, iter [03400, 05004], lr: 0.000294, loss: 1.0994
2022-07-31 09:52:22 - train: epoch 0098, iter [03500, 05004], lr: 0.000289, loss: 1.0434
2022-07-31 09:53:18 - train: epoch 0098, iter [03600, 05004], lr: 0.000284, loss: 1.1659
2022-07-31 09:54:18 - train: epoch 0098, iter [03700, 05004], lr: 0.000279, loss: 0.9866
2022-07-31 09:55:15 - train: epoch 0098, iter [03800, 05004], lr: 0.000274, loss: 0.9774
2022-07-31 09:56:13 - train: epoch 0098, iter [03900, 05004], lr: 0.000270, loss: 1.1612
2022-07-31 09:57:09 - train: epoch 0098, iter [04000, 05004], lr: 0.000265, loss: 1.1227
2022-07-31 09:58:08 - train: epoch 0098, iter [04100, 05004], lr: 0.000260, loss: 1.0869
2022-07-31 09:59:05 - train: epoch 0098, iter [04200, 05004], lr: 0.000255, loss: 1.0920
2022-07-31 10:00:04 - train: epoch 0098, iter [04300, 05004], lr: 0.000250, loss: 0.9406
2022-07-31 10:01:01 - train: epoch 0098, iter [04400, 05004], lr: 0.000246, loss: 1.1869
2022-07-31 10:01:59 - train: epoch 0098, iter [04500, 05004], lr: 0.000241, loss: 1.0344
2022-07-31 10:02:57 - train: epoch 0098, iter [04600, 05004], lr: 0.000237, loss: 1.0449
2022-07-31 10:03:55 - train: epoch 0098, iter [04700, 05004], lr: 0.000232, loss: 1.1211
2022-07-31 10:04:54 - train: epoch 0098, iter [04800, 05004], lr: 0.000228, loss: 0.9029
2022-07-31 10:05:50 - train: epoch 0098, iter [04900, 05004], lr: 0.000223, loss: 0.9903
2022-07-31 10:06:44 - train: epoch 0098, iter [05000, 05004], lr: 0.000219, loss: 1.1487
2022-07-31 10:06:45 - train: epoch 098, train_loss: 1.0919
2022-07-31 10:08:52 - eval: epoch: 098, acc1: 72.326%, acc5: 90.728%, test_loss: 1.1262, per_image_load_time: 3.139ms, per_image_inference_time: 0.471ms
2022-07-31 10:08:52 - until epoch: 098, best_acc1: 72.326%
2022-07-31 10:08:52 - epoch 099 lr: 0.000219
2022-07-31 10:09:59 - train: epoch 0099, iter [00100, 05004], lr: 0.000214, loss: 1.1065
2022-07-31 10:10:56 - train: epoch 0099, iter [00200, 05004], lr: 0.000210, loss: 1.0161
2022-07-31 10:11:53 - train: epoch 0099, iter [00300, 05004], lr: 0.000206, loss: 0.9357
2022-07-31 10:12:51 - train: epoch 0099, iter [00400, 05004], lr: 0.000202, loss: 1.1641
2022-07-31 10:13:47 - train: epoch 0099, iter [00500, 05004], lr: 0.000197, loss: 1.0350
2022-07-31 10:14:44 - train: epoch 0099, iter [00600, 05004], lr: 0.000193, loss: 1.1028
2022-07-31 10:15:41 - train: epoch 0099, iter [00700, 05004], lr: 0.000189, loss: 1.0747
2022-07-31 10:16:38 - train: epoch 0099, iter [00800, 05004], lr: 0.000185, loss: 1.1131
2022-07-31 10:17:36 - train: epoch 0099, iter [00900, 05004], lr: 0.000181, loss: 1.0769
2022-07-31 10:18:33 - train: epoch 0099, iter [01000, 05004], lr: 0.000177, loss: 0.9387
2022-07-31 10:19:31 - train: epoch 0099, iter [01100, 05004], lr: 0.000173, loss: 1.1069
2022-07-31 10:20:29 - train: epoch 0099, iter [01200, 05004], lr: 0.000169, loss: 0.9072
2022-07-31 10:21:26 - train: epoch 0099, iter [01300, 05004], lr: 0.000166, loss: 1.0866
2022-07-31 10:22:24 - train: epoch 0099, iter [01400, 05004], lr: 0.000162, loss: 1.1149
2022-07-31 10:23:20 - train: epoch 0099, iter [01500, 05004], lr: 0.000158, loss: 1.0318
2022-07-31 10:24:19 - train: epoch 0099, iter [01600, 05004], lr: 0.000154, loss: 1.1843
2022-07-31 10:25:17 - train: epoch 0099, iter [01700, 05004], lr: 0.000151, loss: 1.0247
2022-07-31 10:26:14 - train: epoch 0099, iter [01800, 05004], lr: 0.000147, loss: 1.1041
2022-07-31 10:27:12 - train: epoch 0099, iter [01900, 05004], lr: 0.000144, loss: 1.1660
2022-07-31 10:28:11 - train: epoch 0099, iter [02000, 05004], lr: 0.000140, loss: 1.0131
2022-07-31 10:29:08 - train: epoch 0099, iter [02100, 05004], lr: 0.000137, loss: 1.0527
2022-07-31 10:30:05 - train: epoch 0099, iter [02200, 05004], lr: 0.000133, loss: 0.9400
2022-07-31 10:31:02 - train: epoch 0099, iter [02300, 05004], lr: 0.000130, loss: 1.0888
2022-07-31 10:32:00 - train: epoch 0099, iter [02400, 05004], lr: 0.000126, loss: 1.1647
2022-07-31 10:32:58 - train: epoch 0099, iter [02500, 05004], lr: 0.000123, loss: 0.9070
2022-07-31 10:33:57 - train: epoch 0099, iter [02600, 05004], lr: 0.000120, loss: 1.1343
2022-07-31 10:34:55 - train: epoch 0099, iter [02700, 05004], lr: 0.000117, loss: 1.1162
2022-07-31 10:35:53 - train: epoch 0099, iter [02800, 05004], lr: 0.000113, loss: 1.0881
2022-07-31 10:36:53 - train: epoch 0099, iter [02900, 05004], lr: 0.000110, loss: 0.9330
2022-07-31 10:37:50 - train: epoch 0099, iter [03000, 05004], lr: 0.000107, loss: 1.1490
2022-07-31 10:38:48 - train: epoch 0099, iter [03100, 05004], lr: 0.000104, loss: 0.8682
2022-07-31 10:39:46 - train: epoch 0099, iter [03200, 05004], lr: 0.000101, loss: 1.0704
2022-07-31 10:40:44 - train: epoch 0099, iter [03300, 05004], lr: 0.000098, loss: 1.0114
2022-07-31 10:41:43 - train: epoch 0099, iter [03400, 05004], lr: 0.000095, loss: 1.0987
2022-07-31 10:42:40 - train: epoch 0099, iter [03500, 05004], lr: 0.000092, loss: 1.1743
2022-07-31 10:43:38 - train: epoch 0099, iter [03600, 05004], lr: 0.000090, loss: 0.9714
2022-07-31 10:44:36 - train: epoch 0099, iter [03700, 05004], lr: 0.000087, loss: 1.0183
2022-07-31 10:45:34 - train: epoch 0099, iter [03800, 05004], lr: 0.000084, loss: 1.1160
2022-07-31 10:46:33 - train: epoch 0099, iter [03900, 05004], lr: 0.000081, loss: 1.0363
2022-07-31 10:47:32 - train: epoch 0099, iter [04000, 05004], lr: 0.000079, loss: 1.1161
2022-07-31 10:48:30 - train: epoch 0099, iter [04100, 05004], lr: 0.000076, loss: 1.2449
2022-07-31 10:49:29 - train: epoch 0099, iter [04200, 05004], lr: 0.000074, loss: 1.0998
2022-07-31 10:50:25 - train: epoch 0099, iter [04300, 05004], lr: 0.000071, loss: 1.3181
2022-07-31 10:51:23 - train: epoch 0099, iter [04400, 05004], lr: 0.000069, loss: 1.1697
2022-07-31 10:52:21 - train: epoch 0099, iter [04500, 05004], lr: 0.000066, loss: 1.1681
2022-07-31 10:53:18 - train: epoch 0099, iter [04600, 05004], lr: 0.000064, loss: 1.2273
2022-07-31 10:54:17 - train: epoch 0099, iter [04700, 05004], lr: 0.000062, loss: 1.1309
2022-07-31 10:55:13 - train: epoch 0099, iter [04800, 05004], lr: 0.000059, loss: 1.2043
2022-07-31 10:56:12 - train: epoch 0099, iter [04900, 05004], lr: 0.000057, loss: 1.1355
2022-07-31 10:57:06 - train: epoch 0099, iter [05000, 05004], lr: 0.000055, loss: 1.0171
2022-07-31 10:57:07 - train: epoch 099, train_loss: 1.0847
2022-07-31 10:59:12 - eval: epoch: 099, acc1: 72.372%, acc5: 90.788%, test_loss: 1.1260, per_image_load_time: 3.546ms, per_image_inference_time: 0.439ms
2022-07-31 10:59:12 - until epoch: 099, best_acc1: 72.372%
2022-07-31 10:59:12 - epoch 100 lr: 0.000055
2022-07-31 11:00:22 - train: epoch 0100, iter [00100, 05004], lr: 0.000053, loss: 1.1281
2022-07-31 11:01:18 - train: epoch 0100, iter [00200, 05004], lr: 0.000050, loss: 1.2148
2022-07-31 11:02:15 - train: epoch 0100, iter [00300, 05004], lr: 0.000048, loss: 1.1356
2022-07-31 11:03:11 - train: epoch 0100, iter [00400, 05004], lr: 0.000046, loss: 0.9270
2022-07-31 11:04:08 - train: epoch 0100, iter [00500, 05004], lr: 0.000044, loss: 1.1346
2022-07-31 11:05:08 - train: epoch 0100, iter [00600, 05004], lr: 0.000042, loss: 1.0656
2022-07-31 11:06:07 - train: epoch 0100, iter [00700, 05004], lr: 0.000040, loss: 0.9888
2022-07-31 11:07:04 - train: epoch 0100, iter [00800, 05004], lr: 0.000039, loss: 1.0902
2022-07-31 11:08:03 - train: epoch 0100, iter [00900, 05004], lr: 0.000037, loss: 1.1734
2022-07-31 11:09:02 - train: epoch 0100, iter [01000, 05004], lr: 0.000035, loss: 1.1045
2022-07-31 11:10:02 - train: epoch 0100, iter [01100, 05004], lr: 0.000033, loss: 1.0556
2022-07-31 11:11:01 - train: epoch 0100, iter [01200, 05004], lr: 0.000032, loss: 1.1673
2022-07-31 11:11:59 - train: epoch 0100, iter [01300, 05004], lr: 0.000030, loss: 1.0292
2022-07-31 11:12:58 - train: epoch 0100, iter [01400, 05004], lr: 0.000028, loss: 1.1052
2022-07-31 11:13:56 - train: epoch 0100, iter [01500, 05004], lr: 0.000027, loss: 1.0999
2022-07-31 11:14:55 - train: epoch 0100, iter [01600, 05004], lr: 0.000025, loss: 0.9565
2022-07-31 11:15:55 - train: epoch 0100, iter [01700, 05004], lr: 0.000024, loss: 0.8913
2022-07-31 11:16:52 - train: epoch 0100, iter [01800, 05004], lr: 0.000022, loss: 0.9947
2022-07-31 11:17:51 - train: epoch 0100, iter [01900, 05004], lr: 0.000021, loss: 1.1784
2022-07-31 11:18:50 - train: epoch 0100, iter [02000, 05004], lr: 0.000020, loss: 1.1819
2022-07-31 11:19:49 - train: epoch 0100, iter [02100, 05004], lr: 0.000018, loss: 1.1169
2022-07-31 11:20:49 - train: epoch 0100, iter [02200, 05004], lr: 0.000017, loss: 1.1441
2022-07-31 11:21:50 - train: epoch 0100, iter [02300, 05004], lr: 0.000016, loss: 1.0820
2022-07-31 11:22:48 - train: epoch 0100, iter [02400, 05004], lr: 0.000015, loss: 1.2336
2022-07-31 11:23:49 - train: epoch 0100, iter [02500, 05004], lr: 0.000014, loss: 0.9824
2022-07-31 11:24:47 - train: epoch 0100, iter [02600, 05004], lr: 0.000013, loss: 1.0868
2022-07-31 11:25:46 - train: epoch 0100, iter [02700, 05004], lr: 0.000012, loss: 1.0859
2022-07-31 11:26:45 - train: epoch 0100, iter [02800, 05004], lr: 0.000011, loss: 1.0345
2022-07-31 11:27:43 - train: epoch 0100, iter [02900, 05004], lr: 0.000010, loss: 1.0635
2022-07-31 11:28:43 - train: epoch 0100, iter [03000, 05004], lr: 0.000009, loss: 0.9836
2022-07-31 11:29:42 - train: epoch 0100, iter [03100, 05004], lr: 0.000008, loss: 1.2258
2022-07-31 11:30:39 - train: epoch 0100, iter [03200, 05004], lr: 0.000007, loss: 1.1533
2022-07-31 11:31:39 - train: epoch 0100, iter [03300, 05004], lr: 0.000006, loss: 1.0251
2022-07-31 11:32:39 - train: epoch 0100, iter [03400, 05004], lr: 0.000006, loss: 1.2107
2022-07-31 11:33:37 - train: epoch 0100, iter [03500, 05004], lr: 0.000005, loss: 0.9962
2022-07-31 11:34:37 - train: epoch 0100, iter [03600, 05004], lr: 0.000004, loss: 1.1903
2022-07-31 11:35:34 - train: epoch 0100, iter [03700, 05004], lr: 0.000004, loss: 0.9739
2022-07-31 11:36:34 - train: epoch 0100, iter [03800, 05004], lr: 0.000003, loss: 0.9822
2022-07-31 11:37:33 - train: epoch 0100, iter [03900, 05004], lr: 0.000003, loss: 1.0592
2022-07-31 11:38:32 - train: epoch 0100, iter [04000, 05004], lr: 0.000002, loss: 1.0660
2022-07-31 11:39:33 - train: epoch 0100, iter [04100, 05004], lr: 0.000002, loss: 0.9992
2022-07-31 11:40:31 - train: epoch 0100, iter [04200, 05004], lr: 0.000001, loss: 1.0441
2022-07-31 11:41:30 - train: epoch 0100, iter [04300, 05004], lr: 0.000001, loss: 1.0389
2022-07-31 11:42:28 - train: epoch 0100, iter [04400, 05004], lr: 0.000001, loss: 1.0903
2022-07-31 11:43:29 - train: epoch 0100, iter [04500, 05004], lr: 0.000001, loss: 0.7963
2022-07-31 11:44:27 - train: epoch 0100, iter [04600, 05004], lr: 0.000000, loss: 1.0878
2022-07-31 11:45:25 - train: epoch 0100, iter [04700, 05004], lr: 0.000000, loss: 1.1749
2022-07-31 11:46:23 - train: epoch 0100, iter [04800, 05004], lr: 0.000000, loss: 0.8753
2022-07-31 11:47:20 - train: epoch 0100, iter [04900, 05004], lr: 0.000000, loss: 1.0526
2022-07-31 11:48:15 - train: epoch 0100, iter [05000, 05004], lr: 0.000000, loss: 1.1932
2022-07-31 11:48:16 - train: epoch 100, train_loss: 1.0848
2022-07-31 11:50:26 - eval: epoch: 100, acc1: 72.376%, acc5: 90.730%, test_loss: 1.1249, per_image_load_time: 2.922ms, per_image_inference_time: 0.484ms
2022-07-31 11:50:26 - until epoch: 100, best_acc1: 72.376%
2022-07-31 11:50:26 - train done. model: RegNetX_400MF, train time: 167.792 hours, best_acc1: 72.376%
