2022-07-27 12:05:40 - train: epoch 0042, iter [01800, 05004], lr: 0.136013, loss: 1.9056
2022-07-27 12:07:41 - train: epoch 0042, iter [01900, 05004], lr: 0.135951, loss: 2.3293
2022-07-27 12:09:44 - train: epoch 0042, iter [02000, 05004], lr: 0.135889, loss: 2.1365
2022-07-27 12:11:45 - train: epoch 0042, iter [02100, 05004], lr: 0.135828, loss: 2.0600
2022-07-27 12:13:42 - train: epoch 0042, iter [02200, 05004], lr: 0.135766, loss: 2.1334
2022-07-27 12:15:43 - train: epoch 0042, iter [02300, 05004], lr: 0.135704, loss: 2.2128
2022-07-27 12:17:49 - train: epoch 0042, iter [02400, 05004], lr: 0.135642, loss: 2.1327
2022-07-27 12:19:46 - train: epoch 0042, iter [02500, 05004], lr: 0.135581, loss: 2.4466
2022-07-27 12:21:45 - train: epoch 0042, iter [02600, 05004], lr: 0.135519, loss: 2.1800
2022-07-27 12:23:41 - train: epoch 0042, iter [02700, 05004], lr: 0.135457, loss: 2.1072
2022-07-27 12:25:38 - train: epoch 0042, iter [02800, 05004], lr: 0.135395, loss: 2.1253
2022-07-27 12:27:40 - train: epoch 0042, iter [02900, 05004], lr: 0.135333, loss: 2.0886
2022-07-27 12:29:38 - train: epoch 0042, iter [03000, 05004], lr: 0.135272, loss: 2.1336
2022-07-27 12:31:35 - train: epoch 0042, iter [03100, 05004], lr: 0.135210, loss: 2.0291
2022-07-27 12:33:29 - train: epoch 0042, iter [03200, 05004], lr: 0.135148, loss: 2.1516
2022-07-27 12:35:38 - train: epoch 0042, iter [03300, 05004], lr: 0.135086, loss: 2.0915
2022-07-27 12:37:47 - train: epoch 0042, iter [03400, 05004], lr: 0.135024, loss: 1.9379
2022-07-27 12:39:45 - train: epoch 0042, iter [03500, 05004], lr: 0.134962, loss: 2.1012
2022-07-27 12:41:50 - train: epoch 0042, iter [03600, 05004], lr: 0.134900, loss: 2.4000
2022-07-27 12:43:48 - train: epoch 0042, iter [03700, 05004], lr: 0.134838, loss: 2.1512
2022-07-27 12:45:49 - train: epoch 0042, iter [03800, 05004], lr: 0.134776, loss: 1.8258
2022-07-27 12:47:50 - train: epoch 0042, iter [03900, 05004], lr: 0.134714, loss: 2.1842
2022-07-27 12:49:46 - train: epoch 0042, iter [04000, 05004], lr: 0.134652, loss: 2.0860
2022-07-27 12:51:47 - train: epoch 0042, iter [04100, 05004], lr: 0.134590, loss: 2.3117
2022-07-27 12:53:48 - train: epoch 0042, iter [04200, 05004], lr: 0.134528, loss: 2.2256
2022-07-27 12:55:44 - train: epoch 0042, iter [04300, 05004], lr: 0.134466, loss: 1.9790
2022-07-27 12:57:46 - train: epoch 0042, iter [04400, 05004], lr: 0.134404, loss: 1.9525
2022-07-27 12:59:47 - train: epoch 0042, iter [04500, 05004], lr: 0.134342, loss: 2.1283
2022-07-27 13:01:42 - train: epoch 0042, iter [04600, 05004], lr: 0.134280, loss: 2.2702
2022-07-27 13:03:46 - train: epoch 0042, iter [04700, 05004], lr: 0.134218, loss: 1.9785
2022-07-27 13:05:41 - train: epoch 0042, iter [04800, 05004], lr: 0.134156, loss: 2.2042
2022-07-27 13:07:43 - train: epoch 0042, iter [04900, 05004], lr: 0.134094, loss: 2.0369
2022-07-27 13:09:33 - train: epoch 0042, iter [05000, 05004], lr: 0.134032, loss: 1.9567
2022-07-27 13:09:37 - train: epoch 042, train_loss: 2.1013
2022-07-27 13:13:47 - eval: epoch: 042, acc1: 56.594%, acc5: 81.052%, test_loss: 1.8354, per_image_load_time: 8.841ms, per_image_inference_time: 0.707ms
2022-07-27 13:13:47 - until epoch: 042, best_acc1: 56.594%
2022-07-27 13:13:47 - epoch 043 lr: 0.134029
2022-07-27 13:16:02 - train: epoch 0043, iter [00100, 05004], lr: 0.133967, loss: 2.1685
2022-07-27 13:18:02 - train: epoch 0043, iter [00200, 05004], lr: 0.133905, loss: 2.0431
2022-07-27 13:19:59 - train: epoch 0043, iter [00300, 05004], lr: 0.133843, loss: 1.9200
2022-07-27 13:21:55 - train: epoch 0043, iter [00400, 05004], lr: 0.133781, loss: 1.9229
2022-07-27 13:23:52 - train: epoch 0043, iter [00500, 05004], lr: 0.133718, loss: 2.0030
2022-07-27 13:25:51 - train: epoch 0043, iter [00600, 05004], lr: 0.133656, loss: 2.0832
2022-07-27 13:27:53 - train: epoch 0043, iter [00700, 05004], lr: 0.133594, loss: 2.1023
2022-07-27 13:29:55 - train: epoch 0043, iter [00800, 05004], lr: 0.133532, loss: 2.2398
2022-07-27 13:31:50 - train: epoch 0043, iter [00900, 05004], lr: 0.133469, loss: 1.9078
2022-07-27 13:33:53 - train: epoch 0043, iter [01000, 05004], lr: 0.133407, loss: 2.1767
2022-07-27 13:35:52 - train: epoch 0043, iter [01100, 05004], lr: 0.133345, loss: 2.0930
2022-07-27 13:37:49 - train: epoch 0043, iter [01200, 05004], lr: 0.133283, loss: 1.7610
2022-07-27 13:39:49 - train: epoch 0043, iter [01300, 05004], lr: 0.133220, loss: 2.3729
2022-07-27 13:41:49 - train: epoch 0043, iter [01400, 05004], lr: 0.133158, loss: 2.0991
2022-07-27 13:43:46 - train: epoch 0043, iter [01500, 05004], lr: 0.133096, loss: 2.1395
2022-07-27 13:45:46 - train: epoch 0043, iter [01600, 05004], lr: 0.133033, loss: 2.2190
2022-07-27 13:47:46 - train: epoch 0043, iter [01700, 05004], lr: 0.132971, loss: 2.1829
2022-07-27 13:49:45 - train: epoch 0043, iter [01800, 05004], lr: 0.132908, loss: 2.3069
2022-07-27 13:51:42 - train: epoch 0043, iter [01900, 05004], lr: 0.132846, loss: 2.2842
2022-07-27 13:53:38 - train: epoch 0043, iter [02000, 05004], lr: 0.132784, loss: 1.9735
2022-07-27 13:55:36 - train: epoch 0043, iter [02100, 05004], lr: 0.132721, loss: 1.9577
2022-07-27 13:57:35 - train: epoch 0043, iter [02200, 05004], lr: 0.132659, loss: 2.1572
2022-07-27 13:59:31 - train: epoch 0043, iter [02300, 05004], lr: 0.132596, loss: 2.2492
2022-07-27 14:01:30 - train: epoch 0043, iter [02400, 05004], lr: 0.132534, loss: 1.9146
2022-07-27 14:03:32 - train: epoch 0043, iter [02500, 05004], lr: 0.132471, loss: 2.2676
2022-07-27 14:05:30 - train: epoch 0043, iter [02600, 05004], lr: 0.132409, loss: 1.8160
2022-07-27 14:07:21 - train: epoch 0043, iter [02700, 05004], lr: 0.132346, loss: 2.1829
2022-07-27 14:09:14 - train: epoch 0043, iter [02800, 05004], lr: 0.132284, loss: 1.8939
2022-07-27 14:11:08 - train: epoch 0043, iter [02900, 05004], lr: 0.132221, loss: 1.9744
2022-07-27 14:12:59 - train: epoch 0043, iter [03000, 05004], lr: 0.132158, loss: 2.1240
2022-07-27 14:14:58 - train: epoch 0043, iter [03100, 05004], lr: 0.132096, loss: 2.3527
2022-07-27 14:17:03 - train: epoch 0043, iter [03200, 05004], lr: 0.132033, loss: 2.1953
2022-07-27 14:19:21 - train: epoch 0043, iter [03300, 05004], lr: 0.131971, loss: 2.1686
2022-07-27 14:21:19 - train: epoch 0043, iter [03400, 05004], lr: 0.131908, loss: 2.1598
2022-07-27 14:23:21 - train: epoch 0043, iter [03500, 05004], lr: 0.131845, loss: 2.0105
2022-07-27 14:25:25 - train: epoch 0043, iter [03600, 05004], lr: 0.131783, loss: 2.2916
2022-07-27 14:27:26 - train: epoch 0043, iter [03700, 05004], lr: 0.131720, loss: 1.9403
2022-07-27 14:29:24 - train: epoch 0043, iter [03800, 05004], lr: 0.131657, loss: 2.2980
2022-07-27 14:31:34 - train: epoch 0043, iter [03900, 05004], lr: 0.131595, loss: 2.1274
2022-07-27 14:33:41 - train: epoch 0043, iter [04000, 05004], lr: 0.131532, loss: 2.1511
2022-07-27 14:35:44 - train: epoch 0043, iter [04100, 05004], lr: 0.131469, loss: 2.2387
2022-07-27 14:37:50 - train: epoch 0043, iter [04200, 05004], lr: 0.131407, loss: 2.1898
2022-07-27 14:39:59 - train: epoch 0043, iter [04300, 05004], lr: 0.131344, loss: 2.1435
2022-07-27 14:42:00 - train: epoch 0043, iter [04400, 05004], lr: 0.131281, loss: 2.0099
2022-07-27 14:44:02 - train: epoch 0043, iter [04500, 05004], lr: 0.131218, loss: 2.0841
2022-07-27 14:46:11 - train: epoch 0043, iter [04600, 05004], lr: 0.131156, loss: 2.3212
2022-07-27 14:48:14 - train: epoch 0043, iter [04700, 05004], lr: 0.131093, loss: 2.1557
2022-07-27 14:50:20 - train: epoch 0043, iter [04800, 05004], lr: 0.131030, loss: 2.0713
2022-07-27 14:52:21 - train: epoch 0043, iter [04900, 05004], lr: 0.130967, loss: 2.2585
2022-07-27 14:54:23 - train: epoch 0043, iter [05000, 05004], lr: 0.130904, loss: 2.0551
2022-07-27 14:54:27 - train: epoch 043, train_loss: 2.0907
2022-07-27 14:58:42 - eval: epoch: 043, acc1: 55.740%, acc5: 80.468%, test_loss: 1.8711, per_image_load_time: 8.639ms, per_image_inference_time: 0.725ms
2022-07-27 14:58:42 - until epoch: 043, best_acc1: 56.594%
2022-07-27 14:58:42 - epoch 044 lr: 0.130901
2022-07-27 15:01:07 - train: epoch 0044, iter [00100, 05004], lr: 0.130839, loss: 2.1132
2022-07-27 15:03:15 - train: epoch 0044, iter [00200, 05004], lr: 0.130776, loss: 2.2844
2022-07-27 15:05:20 - train: epoch 0044, iter [00300, 05004], lr: 0.130713, loss: 2.1066
2022-07-27 15:07:27 - train: epoch 0044, iter [00400, 05004], lr: 0.130650, loss: 1.7857
2022-07-27 15:09:30 - train: epoch 0044, iter [00500, 05004], lr: 0.130587, loss: 1.9778
2022-07-27 15:11:29 - train: epoch 0044, iter [00600, 05004], lr: 0.130524, loss: 1.8674
2022-07-27 15:13:38 - train: epoch 0044, iter [00700, 05004], lr: 0.130461, loss: 1.7208
2022-07-27 15:15:42 - train: epoch 0044, iter [00800, 05004], lr: 0.130398, loss: 1.9752
2022-07-27 15:17:40 - train: epoch 0044, iter [00900, 05004], lr: 0.130335, loss: 2.1242
2022-07-27 15:19:44 - train: epoch 0044, iter [01000, 05004], lr: 0.130273, loss: 2.0826
2022-07-27 15:21:55 - train: epoch 0044, iter [01100, 05004], lr: 0.130210, loss: 1.7284
2022-07-27 15:24:01 - train: epoch 0044, iter [01200, 05004], lr: 0.130147, loss: 1.9904
2022-07-27 15:25:49 - train: epoch 0044, iter [01300, 05004], lr: 0.130084, loss: 2.1491
2022-07-27 15:27:41 - train: epoch 0044, iter [01400, 05004], lr: 0.130020, loss: 1.9515
2022-07-27 15:29:44 - train: epoch 0044, iter [01500, 05004], lr: 0.129957, loss: 2.0644
2022-07-27 15:31:37 - train: epoch 0044, iter [01600, 05004], lr: 0.129894, loss: 2.1073
2022-07-27 15:33:26 - train: epoch 0044, iter [01700, 05004], lr: 0.129831, loss: 2.1421
2022-07-27 15:35:22 - train: epoch 0044, iter [01800, 05004], lr: 0.129768, loss: 1.9850
2022-07-27 15:37:24 - train: epoch 0044, iter [01900, 05004], lr: 0.129705, loss: 2.1776
2022-07-27 15:39:22 - train: epoch 0044, iter [02000, 05004], lr: 0.129642, loss: 2.1290
2022-07-27 15:41:23 - train: epoch 0044, iter [02100, 05004], lr: 0.129579, loss: 2.2774
2022-07-27 15:43:22 - train: epoch 0044, iter [02200, 05004], lr: 0.129516, loss: 2.0455
2022-07-27 15:45:27 - train: epoch 0044, iter [02300, 05004], lr: 0.129453, loss: 2.1471
2022-07-27 15:47:14 - train: epoch 0044, iter [02400, 05004], lr: 0.129389, loss: 2.0707
2022-07-27 15:49:03 - train: epoch 0044, iter [02500, 05004], lr: 0.129326, loss: 2.0385
2022-07-27 15:51:07 - train: epoch 0044, iter [02600, 05004], lr: 0.129263, loss: 2.1288
2022-07-27 15:53:11 - train: epoch 0044, iter [02700, 05004], lr: 0.129200, loss: 2.1085
2022-07-27 15:55:09 - train: epoch 0044, iter [02800, 05004], lr: 0.129137, loss: 1.9901
2022-07-27 15:57:18 - train: epoch 0044, iter [02900, 05004], lr: 0.129073, loss: 1.9037
2022-07-27 15:59:21 - train: epoch 0044, iter [03000, 05004], lr: 0.129010, loss: 1.9130
2022-07-27 16:01:17 - train: epoch 0044, iter [03100, 05004], lr: 0.128947, loss: 1.9894
2022-07-27 16:03:48 - train: epoch 0044, iter [03200, 05004], lr: 0.128884, loss: 1.8401
2022-07-27 16:05:55 - train: epoch 0044, iter [03300, 05004], lr: 0.128820, loss: 1.9992
2022-07-27 16:08:07 - train: epoch 0044, iter [03400, 05004], lr: 0.128757, loss: 2.4115
2022-07-27 16:10:17 - train: epoch 0044, iter [03500, 05004], lr: 0.128694, loss: 2.0625
2022-07-27 16:12:25 - train: epoch 0044, iter [03600, 05004], lr: 0.128631, loss: 2.2718
2022-07-27 16:14:33 - train: epoch 0044, iter [03700, 05004], lr: 0.128567, loss: 1.9926
2022-07-27 16:16:40 - train: epoch 0044, iter [03800, 05004], lr: 0.128504, loss: 1.7476
2022-07-27 16:18:45 - train: epoch 0044, iter [03900, 05004], lr: 0.128441, loss: 2.0518
2022-07-27 16:20:50 - train: epoch 0044, iter [04000, 05004], lr: 0.128377, loss: 2.0501
2022-07-27 16:22:54 - train: epoch 0044, iter [04100, 05004], lr: 0.128314, loss: 2.1111
2022-07-27 16:25:00 - train: epoch 0044, iter [04200, 05004], lr: 0.128250, loss: 2.0901
2022-07-27 16:27:03 - train: epoch 0044, iter [04300, 05004], lr: 0.128187, loss: 1.9808
2022-07-27 16:29:05 - train: epoch 0044, iter [04400, 05004], lr: 0.128124, loss: 2.2254
2022-07-27 16:31:11 - train: epoch 0044, iter [04500, 05004], lr: 0.128060, loss: 2.2653
2022-07-27 16:33:15 - train: epoch 0044, iter [04600, 05004], lr: 0.127997, loss: 2.2602
2022-07-27 16:35:12 - train: epoch 0044, iter [04700, 05004], lr: 0.127933, loss: 2.2766
2022-07-27 16:37:12 - train: epoch 0044, iter [04800, 05004], lr: 0.127870, loss: 2.1937
2022-07-27 16:39:14 - train: epoch 0044, iter [04900, 05004], lr: 0.127806, loss: 2.1044
2022-07-27 16:41:13 - train: epoch 0044, iter [05000, 05004], lr: 0.127743, loss: 2.2378
2022-07-27 16:41:18 - train: epoch 044, train_loss: 2.0785
2022-07-27 16:45:34 - eval: epoch: 044, acc1: 56.798%, acc5: 81.324%, test_loss: 1.8174, per_image_load_time: 8.875ms, per_image_inference_time: 0.725ms
2022-07-27 16:45:35 - until epoch: 044, best_acc1: 56.798%
2022-07-27 16:45:35 - epoch 045 lr: 0.127740
2022-07-27 16:47:59 - train: epoch 0045, iter [00100, 05004], lr: 0.127677, loss: 2.0591
2022-07-27 16:50:02 - train: epoch 0045, iter [00200, 05004], lr: 0.127613, loss: 1.9708
2022-07-27 16:52:05 - train: epoch 0045, iter [00300, 05004], lr: 0.127550, loss: 2.0221
2022-07-27 16:54:02 - train: epoch 0045, iter [00400, 05004], lr: 0.127486, loss: 2.0556
2022-07-27 16:56:11 - train: epoch 0045, iter [00500, 05004], lr: 0.127423, loss: 2.3336
2022-07-27 16:58:18 - train: epoch 0045, iter [00600, 05004], lr: 0.127359, loss: 2.0007
2022-07-27 17:00:21 - train: epoch 0045, iter [00700, 05004], lr: 0.127296, loss: 1.7234
2022-07-27 17:02:27 - train: epoch 0045, iter [00800, 05004], lr: 0.127232, loss: 2.0100
2022-07-27 17:04:31 - train: epoch 0045, iter [00900, 05004], lr: 0.127168, loss: 1.9119
2022-07-27 17:06:34 - train: epoch 0045, iter [01000, 05004], lr: 0.127105, loss: 1.9816
2022-07-27 17:08:43 - train: epoch 0045, iter [01100, 05004], lr: 0.127041, loss: 2.0361
2022-07-27 17:10:46 - train: epoch 0045, iter [01200, 05004], lr: 0.126978, loss: 2.0773
2022-07-27 17:12:47 - train: epoch 0045, iter [01300, 05004], lr: 0.126914, loss: 2.0473
2022-07-27 17:14:46 - train: epoch 0045, iter [01400, 05004], lr: 0.126850, loss: 2.1577
2022-07-27 17:16:46 - train: epoch 0045, iter [01500, 05004], lr: 0.126787, loss: 2.1343
2022-07-27 17:18:45 - train: epoch 0045, iter [01600, 05004], lr: 0.126723, loss: 2.0569
2022-07-27 17:20:46 - train: epoch 0045, iter [01700, 05004], lr: 0.126659, loss: 1.9558
2022-07-27 17:22:47 - train: epoch 0045, iter [01800, 05004], lr: 0.126595, loss: 2.0433
2022-07-27 17:24:44 - train: epoch 0045, iter [01900, 05004], lr: 0.126532, loss: 2.0777
2022-07-27 17:26:39 - train: epoch 0045, iter [02000, 05004], lr: 0.126468, loss: 2.1520
2022-07-27 17:28:37 - train: epoch 0045, iter [02100, 05004], lr: 0.126404, loss: 2.2698
2022-07-27 17:30:33 - train: epoch 0045, iter [02200, 05004], lr: 0.126341, loss: 1.9762
2022-07-27 17:32:29 - train: epoch 0045, iter [02300, 05004], lr: 0.126277, loss: 1.9301
2022-07-27 17:34:23 - train: epoch 0045, iter [02400, 05004], lr: 0.126213, loss: 2.1405
2022-07-27 17:36:16 - train: epoch 0045, iter [02500, 05004], lr: 0.126149, loss: 1.9546
2022-07-27 17:38:16 - train: epoch 0045, iter [02600, 05004], lr: 0.126085, loss: 2.0156
2022-07-27 17:40:09 - train: epoch 0045, iter [02700, 05004], lr: 0.126022, loss: 1.9779
2022-07-27 17:42:11 - train: epoch 0045, iter [02800, 05004], lr: 0.125958, loss: 1.9345
2022-07-27 17:44:04 - train: epoch 0045, iter [02900, 05004], lr: 0.125894, loss: 2.1516
2022-07-27 17:46:14 - train: epoch 0045, iter [03000, 05004], lr: 0.125830, loss: 2.1499
2022-07-27 17:48:19 - train: epoch 0045, iter [03100, 05004], lr: 0.125766, loss: 2.0310
2022-07-27 17:50:29 - train: epoch 0045, iter [03200, 05004], lr: 0.125702, loss: 2.3823
2022-07-27 17:52:35 - train: epoch 0045, iter [03300, 05004], lr: 0.125639, loss: 2.2674
2022-07-27 17:54:44 - train: epoch 0045, iter [03400, 05004], lr: 0.125575, loss: 2.0427
2022-07-27 17:56:46 - train: epoch 0045, iter [03500, 05004], lr: 0.125511, loss: 2.3664
2022-07-27 17:58:49 - train: epoch 0045, iter [03600, 05004], lr: 0.125447, loss: 2.0924
2022-07-27 18:00:46 - train: epoch 0045, iter [03700, 05004], lr: 0.125383, loss: 2.0276
2022-07-27 18:02:42 - train: epoch 0045, iter [03800, 05004], lr: 0.125319, loss: 2.0079
2022-07-27 18:04:35 - train: epoch 0045, iter [03900, 05004], lr: 0.125255, loss: 2.1927
2022-07-27 18:06:34 - train: epoch 0045, iter [04000, 05004], lr: 0.125191, loss: 2.1240
2022-07-27 18:08:27 - train: epoch 0045, iter [04100, 05004], lr: 0.125127, loss: 1.9899
2022-07-27 18:10:25 - train: epoch 0045, iter [04200, 05004], lr: 0.125063, loss: 2.2266
2022-07-27 18:12:26 - train: epoch 0045, iter [04300, 05004], lr: 0.124999, loss: 2.0870
2022-07-27 18:14:20 - train: epoch 0045, iter [04400, 05004], lr: 0.124935, loss: 2.1980
2022-07-27 18:16:15 - train: epoch 0045, iter [04500, 05004], lr: 0.124871, loss: 2.0900
2022-07-27 18:18:14 - train: epoch 0045, iter [04600, 05004], lr: 0.124807, loss: 2.0780
2022-07-27 18:20:10 - train: epoch 0045, iter [04700, 05004], lr: 0.124743, loss: 1.9976
2022-07-27 18:22:07 - train: epoch 0045, iter [04800, 05004], lr: 0.124679, loss: 1.8407
2022-07-27 18:24:04 - train: epoch 0045, iter [04900, 05004], lr: 0.124615, loss: 2.1646
2022-07-27 18:25:57 - train: epoch 0045, iter [05000, 05004], lr: 0.124551, loss: 1.9766
2022-07-27 18:26:01 - train: epoch 045, train_loss: 2.0653
2022-07-27 18:30:01 - eval: epoch: 045, acc1: 56.266%, acc5: 80.920%, test_loss: 1.8457, per_image_load_time: 8.273ms, per_image_inference_time: 0.685ms
2022-07-27 18:30:01 - until epoch: 045, best_acc1: 56.798%
2022-07-27 18:30:01 - epoch 046 lr: 0.124548
2022-07-27 18:32:12 - train: epoch 0046, iter [00100, 05004], lr: 0.124484, loss: 1.9191
2022-07-27 18:34:10 - train: epoch 0046, iter [00200, 05004], lr: 0.124420, loss: 1.9203
2022-07-27 18:36:03 - train: epoch 0046, iter [00300, 05004], lr: 0.124356, loss: 2.0907
2022-07-27 18:37:59 - train: epoch 0046, iter [00400, 05004], lr: 0.124292, loss: 2.0806
2022-07-27 18:39:54 - train: epoch 0046, iter [00500, 05004], lr: 0.124228, loss: 2.0890
2022-07-27 18:41:52 - train: epoch 0046, iter [00600, 05004], lr: 0.124164, loss: 2.3062
2022-07-27 18:43:59 - train: epoch 0046, iter [00700, 05004], lr: 0.124100, loss: 1.8045
2022-07-27 18:45:59 - train: epoch 0046, iter [00800, 05004], lr: 0.124036, loss: 2.1661
2022-07-27 18:47:53 - train: epoch 0046, iter [00900, 05004], lr: 0.123972, loss: 2.0936
2022-07-27 18:49:53 - train: epoch 0046, iter [01000, 05004], lr: 0.123907, loss: 1.8648
2022-07-27 18:51:49 - train: epoch 0046, iter [01100, 05004], lr: 0.123843, loss: 1.9426
2022-07-27 18:53:52 - train: epoch 0046, iter [01200, 05004], lr: 0.123779, loss: 1.9484
2022-07-27 18:55:56 - train: epoch 0046, iter [01300, 05004], lr: 0.123715, loss: 2.1084
2022-07-27 18:57:57 - train: epoch 0046, iter [01400, 05004], lr: 0.123651, loss: 1.9804
2022-07-27 18:59:57 - train: epoch 0046, iter [01500, 05004], lr: 0.123586, loss: 2.1012
2022-07-27 19:02:05 - train: epoch 0046, iter [01600, 05004], lr: 0.123522, loss: 2.2122
2022-07-27 19:04:06 - train: epoch 0046, iter [01700, 05004], lr: 0.123458, loss: 2.0396
2022-07-27 19:06:12 - train: epoch 0046, iter [01800, 05004], lr: 0.123394, loss: 2.3169
2022-07-27 19:08:13 - train: epoch 0046, iter [01900, 05004], lr: 0.123329, loss: 1.8445
2022-07-27 19:10:12 - train: epoch 0046, iter [02000, 05004], lr: 0.123265, loss: 1.9849
2022-07-27 19:12:14 - train: epoch 0046, iter [02100, 05004], lr: 0.123201, loss: 2.2184
2022-07-27 19:14:19 - train: epoch 0046, iter [02200, 05004], lr: 0.123137, loss: 1.8306
2022-07-27 19:16:21 - train: epoch 0046, iter [02300, 05004], lr: 0.123072, loss: 2.1469
2022-07-27 19:18:17 - train: epoch 0046, iter [02400, 05004], lr: 0.123008, loss: 1.9277
2022-07-27 19:20:18 - train: epoch 0046, iter [02500, 05004], lr: 0.122944, loss: 1.9817
2022-07-27 19:22:20 - train: epoch 0046, iter [02600, 05004], lr: 0.122879, loss: 2.2137
2022-07-27 19:24:21 - train: epoch 0046, iter [02700, 05004], lr: 0.122815, loss: 2.0349
2022-07-27 19:26:14 - train: epoch 0046, iter [02800, 05004], lr: 0.122751, loss: 1.8975
2022-07-27 19:28:46 - train: epoch 0046, iter [02900, 05004], lr: 0.122686, loss: 2.2454
2022-07-27 19:30:52 - train: epoch 0046, iter [03000, 05004], lr: 0.122622, loss: 1.9635
2022-07-27 19:33:05 - train: epoch 0046, iter [03100, 05004], lr: 0.122558, loss: 1.9189
2022-07-27 19:35:15 - train: epoch 0046, iter [03200, 05004], lr: 0.122493, loss: 2.0602
2022-07-27 19:37:17 - train: epoch 0046, iter [03300, 05004], lr: 0.122429, loss: 2.1484
2022-07-27 19:39:28 - train: epoch 0046, iter [03400, 05004], lr: 0.122364, loss: 2.1197
2022-07-27 19:41:34 - train: epoch 0046, iter [03500, 05004], lr: 0.122300, loss: 2.2466
2022-07-27 19:43:35 - train: epoch 0046, iter [03600, 05004], lr: 0.122236, loss: 2.1520
2022-07-27 19:45:39 - train: epoch 0046, iter [03700, 05004], lr: 0.122171, loss: 1.8700
2022-07-27 19:47:41 - train: epoch 0046, iter [03800, 05004], lr: 0.122107, loss: 1.9930
2022-07-27 19:49:42 - train: epoch 0046, iter [03900, 05004], lr: 0.122042, loss: 1.9977
2022-07-27 19:51:45 - train: epoch 0046, iter [04000, 05004], lr: 0.121978, loss: 1.8924
2022-07-27 19:53:49 - train: epoch 0046, iter [04100, 05004], lr: 0.121913, loss: 2.1746
2022-07-27 19:55:55 - train: epoch 0046, iter [04200, 05004], lr: 0.121849, loss: 1.9937
2022-07-27 19:57:59 - train: epoch 0046, iter [04300, 05004], lr: 0.121784, loss: 2.2765
2022-07-27 19:59:56 - train: epoch 0046, iter [04400, 05004], lr: 0.121720, loss: 2.1873
2022-07-27 20:01:59 - train: epoch 0046, iter [04500, 05004], lr: 0.121655, loss: 2.0654
2022-07-27 20:04:06 - train: epoch 0046, iter [04600, 05004], lr: 0.121591, loss: 2.0257
2022-07-27 20:06:09 - train: epoch 0046, iter [04700, 05004], lr: 0.121526, loss: 2.0020
2022-07-27 20:08:11 - train: epoch 0046, iter [04800, 05004], lr: 0.121462, loss: 1.7883
2022-07-27 20:10:08 - train: epoch 0046, iter [04900, 05004], lr: 0.121397, loss: 2.2412
2022-07-27 20:12:07 - train: epoch 0046, iter [05000, 05004], lr: 0.121333, loss: 2.0758
2022-07-27 20:12:11 - train: epoch 046, train_loss: 2.0536
2022-07-27 20:16:16 - eval: epoch: 046, acc1: 56.912%, acc5: 81.270%, test_loss: 1.8134, per_image_load_time: 8.488ms, per_image_inference_time: 0.688ms
2022-07-27 20:16:17 - until epoch: 046, best_acc1: 56.912%
2022-07-27 20:16:17 - epoch 047 lr: 0.121329
2022-07-27 20:18:38 - train: epoch 0047, iter [00100, 05004], lr: 0.121265, loss: 2.0004
2022-07-27 20:20:44 - train: epoch 0047, iter [00200, 05004], lr: 0.121201, loss: 2.1330
2022-07-27 20:22:45 - train: epoch 0047, iter [00300, 05004], lr: 0.121136, loss: 1.8115
2022-07-27 20:24:49 - train: epoch 0047, iter [00400, 05004], lr: 0.121072, loss: 1.8006
2022-07-27 20:26:48 - train: epoch 0047, iter [00500, 05004], lr: 0.121007, loss: 1.9207
2022-07-27 20:28:57 - train: epoch 0047, iter [00600, 05004], lr: 0.120942, loss: 2.0498
2022-07-27 20:31:00 - train: epoch 0047, iter [00700, 05004], lr: 0.120878, loss: 1.9098
2022-07-27 20:33:09 - train: epoch 0047, iter [00800, 05004], lr: 0.120813, loss: 2.0355
2022-07-27 20:35:08 - train: epoch 0047, iter [00900, 05004], lr: 0.120749, loss: 2.1448
2022-07-27 20:37:15 - train: epoch 0047, iter [01000, 05004], lr: 0.120684, loss: 1.9780
2022-07-27 20:39:17 - train: epoch 0047, iter [01100, 05004], lr: 0.120619, loss: 2.2528
2022-07-27 20:41:23 - train: epoch 0047, iter [01200, 05004], lr: 0.120555, loss: 2.0382
2022-07-27 20:43:21 - train: epoch 0047, iter [01300, 05004], lr: 0.120490, loss: 2.2074
2022-07-27 20:45:29 - train: epoch 0047, iter [01400, 05004], lr: 0.120425, loss: 1.8241
2022-07-27 20:47:32 - train: epoch 0047, iter [01500, 05004], lr: 0.120360, loss: 2.1583
2022-07-27 20:49:32 - train: epoch 0047, iter [01600, 05004], lr: 0.120296, loss: 2.0434
2022-07-27 20:51:31 - train: epoch 0047, iter [01700, 05004], lr: 0.120231, loss: 1.9808
2022-07-27 20:53:37 - train: epoch 0047, iter [01800, 05004], lr: 0.120166, loss: 1.9333
2022-07-27 20:55:41 - train: epoch 0047, iter [01900, 05004], lr: 0.120102, loss: 1.9997
2022-07-27 20:57:44 - train: epoch 0047, iter [02000, 05004], lr: 0.120037, loss: 1.9874
2022-07-27 20:59:40 - train: epoch 0047, iter [02100, 05004], lr: 0.119972, loss: 2.1864
2022-07-27 21:01:46 - train: epoch 0047, iter [02200, 05004], lr: 0.119907, loss: 2.1180
2022-07-27 21:03:48 - train: epoch 0047, iter [02300, 05004], lr: 0.119843, loss: 2.1218
2022-07-27 21:05:49 - train: epoch 0047, iter [02400, 05004], lr: 0.119778, loss: 2.0800
2022-07-27 21:07:52 - train: epoch 0047, iter [02500, 05004], lr: 0.119713, loss: 2.1651
2022-07-27 21:09:55 - train: epoch 0047, iter [02600, 05004], lr: 0.119648, loss: 2.2129
2022-07-27 21:11:50 - train: epoch 0047, iter [02700, 05004], lr: 0.119583, loss: 2.0122
2022-07-27 21:14:10 - train: epoch 0047, iter [02800, 05004], lr: 0.119519, loss: 2.0169
2022-07-27 21:16:23 - train: epoch 0047, iter [02900, 05004], lr: 0.119454, loss: 2.0612
2022-07-27 21:18:34 - train: epoch 0047, iter [03000, 05004], lr: 0.119389, loss: 2.2841
2022-07-27 21:20:44 - train: epoch 0047, iter [03100, 05004], lr: 0.119324, loss: 1.9153
2022-07-27 21:22:47 - train: epoch 0047, iter [03200, 05004], lr: 0.119259, loss: 2.2543
2022-07-27 21:24:55 - train: epoch 0047, iter [03300, 05004], lr: 0.119194, loss: 1.9235
2022-07-27 21:27:00 - train: epoch 0047, iter [03400, 05004], lr: 0.119130, loss: 1.8754
2022-07-27 21:29:04 - train: epoch 0047, iter [03500, 05004], lr: 0.119065, loss: 2.2184
2022-07-27 21:31:08 - train: epoch 0047, iter [03600, 05004], lr: 0.119000, loss: 1.9135
2022-07-27 21:33:10 - train: epoch 0047, iter [03700, 05004], lr: 0.118935, loss: 1.9501
2022-07-27 21:35:15 - train: epoch 0047, iter [03800, 05004], lr: 0.118870, loss: 1.9180
2022-07-27 21:37:18 - train: epoch 0047, iter [03900, 05004], lr: 0.118805, loss: 2.1466
2022-07-27 21:39:21 - train: epoch 0047, iter [04000, 05004], lr: 0.118740, loss: 2.1073
2022-07-27 21:41:22 - train: epoch 0047, iter [04100, 05004], lr: 0.118675, loss: 2.1906
2022-07-27 21:43:26 - train: epoch 0047, iter [04200, 05004], lr: 0.118610, loss: 1.9571
2022-07-27 21:45:25 - train: epoch 0047, iter [04300, 05004], lr: 0.118545, loss: 1.8797
2022-07-27 21:47:31 - train: epoch 0047, iter [04400, 05004], lr: 0.118480, loss: 2.1486
2022-07-27 21:49:33 - train: epoch 0047, iter [04500, 05004], lr: 0.118416, loss: 1.9206
2022-07-27 21:51:37 - train: epoch 0047, iter [04600, 05004], lr: 0.118351, loss: 2.0753
2022-07-27 21:53:37 - train: epoch 0047, iter [04700, 05004], lr: 0.118286, loss: 2.2013
2022-07-27 21:55:41 - train: epoch 0047, iter [04800, 05004], lr: 0.118221, loss: 1.7904
2022-07-27 21:57:41 - train: epoch 0047, iter [04900, 05004], lr: 0.118156, loss: 1.9811
2022-07-27 21:59:42 - train: epoch 0047, iter [05000, 05004], lr: 0.118091, loss: 1.9669
2022-07-27 21:59:47 - train: epoch 047, train_loss: 2.0423
2022-07-27 22:04:18 - eval: epoch: 047, acc1: 57.632%, acc5: 81.742%, test_loss: 1.7873, per_image_load_time: 8.480ms, per_image_inference_time: 0.804ms
2022-07-27 22:04:18 - until epoch: 047, best_acc1: 57.632%
2022-07-27 22:04:18 - epoch 048 lr: 0.118087
2022-07-27 22:06:36 - train: epoch 0048, iter [00100, 05004], lr: 0.118023, loss: 1.9319
2022-07-27 22:08:40 - train: epoch 0048, iter [00200, 05004], lr: 0.117958, loss: 2.3125
2022-07-27 22:10:40 - train: epoch 0048, iter [00300, 05004], lr: 0.117893, loss: 1.9841
2022-07-27 22:12:45 - train: epoch 0048, iter [00400, 05004], lr: 0.117828, loss: 1.8050
2022-07-27 22:14:44 - train: epoch 0048, iter [00500, 05004], lr: 0.117763, loss: 1.9820
2022-07-27 22:16:49 - train: epoch 0048, iter [00600, 05004], lr: 0.117698, loss: 2.0212
2022-07-27 22:18:47 - train: epoch 0048, iter [00700, 05004], lr: 0.117633, loss: 1.9492
2022-07-27 22:20:51 - train: epoch 0048, iter [00800, 05004], lr: 0.117568, loss: 2.0703
2022-07-27 22:22:54 - train: epoch 0048, iter [00900, 05004], lr: 0.117503, loss: 2.1276
2022-07-27 22:24:53 - train: epoch 0048, iter [01000, 05004], lr: 0.117438, loss: 1.9767
2022-07-27 22:26:58 - train: epoch 0048, iter [01100, 05004], lr: 0.117373, loss: 2.2894
2022-07-27 22:28:59 - train: epoch 0048, iter [01200, 05004], lr: 0.117308, loss: 1.8621
2022-07-27 22:31:05 - train: epoch 0048, iter [01300, 05004], lr: 0.117242, loss: 2.0121
2022-07-27 22:33:07 - train: epoch 0048, iter [01400, 05004], lr: 0.117177, loss: 2.0835
2022-07-27 22:35:12 - train: epoch 0048, iter [01500, 05004], lr: 0.117112, loss: 2.1679
2022-07-27 22:37:16 - train: epoch 0048, iter [01600, 05004], lr: 0.117047, loss: 2.0782
2022-07-27 22:39:18 - train: epoch 0048, iter [01700, 05004], lr: 0.116982, loss: 1.9470
2022-07-27 22:41:21 - train: epoch 0048, iter [01800, 05004], lr: 0.116917, loss: 1.9452
2022-07-27 22:43:30 - train: epoch 0048, iter [01900, 05004], lr: 0.116852, loss: 1.8662
2022-07-27 22:45:29 - train: epoch 0048, iter [02000, 05004], lr: 0.116787, loss: 2.1490
2022-07-27 22:47:31 - train: epoch 0048, iter [02100, 05004], lr: 0.116721, loss: 1.9922
2022-07-27 22:49:28 - train: epoch 0048, iter [02200, 05004], lr: 0.116656, loss: 2.2924
2022-07-27 22:51:30 - train: epoch 0048, iter [02300, 05004], lr: 0.116591, loss: 1.8421
2022-07-27 22:53:34 - train: epoch 0048, iter [02400, 05004], lr: 0.116526, loss: 2.0003
2022-07-27 22:55:35 - train: epoch 0048, iter [02500, 05004], lr: 0.116461, loss: 2.2469
2022-07-27 22:57:33 - train: epoch 0048, iter [02600, 05004], lr: 0.116396, loss: 2.0127
2022-07-27 22:59:54 - train: epoch 0048, iter [02700, 05004], lr: 0.116330, loss: 2.3447
2022-07-27 23:02:10 - train: epoch 0048, iter [02800, 05004], lr: 0.116265, loss: 1.9217
2022-07-27 23:04:21 - train: epoch 0048, iter [02900, 05004], lr: 0.116200, loss: 2.2353
2022-07-27 23:06:35 - train: epoch 0048, iter [03000, 05004], lr: 0.116135, loss: 1.8572
2022-07-27 23:08:47 - train: epoch 0048, iter [03100, 05004], lr: 0.116070, loss: 2.2387
2022-07-27 23:10:55 - train: epoch 0048, iter [03200, 05004], lr: 0.116004, loss: 1.9331
2022-07-27 23:12:58 - train: epoch 0048, iter [03300, 05004], lr: 0.115939, loss: 1.8849
2022-07-27 23:15:02 - train: epoch 0048, iter [03400, 05004], lr: 0.115874, loss: 2.0615
2022-07-27 23:17:07 - train: epoch 0048, iter [03500, 05004], lr: 0.115809, loss: 2.2324
2022-07-27 23:19:11 - train: epoch 0048, iter [03600, 05004], lr: 0.115743, loss: 1.9791
2022-07-27 23:21:16 - train: epoch 0048, iter [03700, 05004], lr: 0.115678, loss: 2.2388
2022-07-27 23:23:20 - train: epoch 0048, iter [03800, 05004], lr: 0.115613, loss: 2.0206
2022-07-27 23:25:26 - train: epoch 0048, iter [03900, 05004], lr: 0.115547, loss: 2.1862
2022-07-27 23:27:30 - train: epoch 0048, iter [04000, 05004], lr: 0.115482, loss: 1.8032
2022-07-27 23:29:31 - train: epoch 0048, iter [04100, 05004], lr: 0.115417, loss: 2.1177
2022-07-27 23:31:31 - train: epoch 0048, iter [04200, 05004], lr: 0.115352, loss: 1.8584
2022-07-27 23:33:38 - train: epoch 0048, iter [04300, 05004], lr: 0.115286, loss: 1.9804
2022-07-27 23:35:40 - train: epoch 0048, iter [04400, 05004], lr: 0.115221, loss: 1.9587
2022-07-27 23:37:42 - train: epoch 0048, iter [04500, 05004], lr: 0.115156, loss: 1.9529
2022-07-27 23:39:51 - train: epoch 0048, iter [04600, 05004], lr: 0.115090, loss: 1.8559
2022-07-27 23:41:57 - train: epoch 0048, iter [04700, 05004], lr: 0.115025, loss: 1.9543
2022-07-27 23:43:52 - train: epoch 0048, iter [04800, 05004], lr: 0.114960, loss: 2.3236
2022-07-27 23:45:52 - train: epoch 0048, iter [04900, 05004], lr: 0.114894, loss: 2.1735
2022-07-27 23:47:50 - train: epoch 0048, iter [05000, 05004], lr: 0.114829, loss: 1.9008
2022-07-27 23:47:54 - train: epoch 048, train_loss: 2.0307
2022-07-27 23:52:02 - eval: epoch: 048, acc1: 58.260%, acc5: 82.326%, test_loss: 1.7504, per_image_load_time: 8.385ms, per_image_inference_time: 0.773ms
2022-07-27 23:52:02 - until epoch: 048, best_acc1: 58.260%
2022-07-27 23:52:02 - epoch 049 lr: 0.114826
2022-07-27 23:54:26 - train: epoch 0049, iter [00100, 05004], lr: 0.114761, loss: 2.1332
2022-07-27 23:56:26 - train: epoch 0049, iter [00200, 05004], lr: 0.114696, loss: 1.8865
2022-07-27 23:58:36 - train: epoch 0049, iter [00300, 05004], lr: 0.114630, loss: 2.1101
2022-07-28 00:00:42 - train: epoch 0049, iter [00400, 05004], lr: 0.114565, loss: 1.9383
2022-07-28 00:02:40 - train: epoch 0049, iter [00500, 05004], lr: 0.114500, loss: 2.0014
2022-07-28 00:04:45 - train: epoch 0049, iter [00600, 05004], lr: 0.114434, loss: 1.8254
2022-07-28 00:06:48 - train: epoch 0049, iter [00700, 05004], lr: 0.114369, loss: 1.8385
2022-07-28 00:08:51 - train: epoch 0049, iter [00800, 05004], lr: 0.114303, loss: 2.4694
2022-07-28 00:10:52 - train: epoch 0049, iter [00900, 05004], lr: 0.114238, loss: 2.0058
2022-07-28 00:12:58 - train: epoch 0049, iter [01000, 05004], lr: 0.114172, loss: 1.9722
2022-07-28 00:14:59 - train: epoch 0049, iter [01100, 05004], lr: 0.114107, loss: 2.1267
2022-07-28 00:16:58 - train: epoch 0049, iter [01200, 05004], lr: 0.114042, loss: 1.9465
2022-07-28 00:18:59 - train: epoch 0049, iter [01300, 05004], lr: 0.113976, loss: 2.2778
2022-07-28 00:21:05 - train: epoch 0049, iter [01400, 05004], lr: 0.113911, loss: 2.3054
2022-07-28 00:23:05 - train: epoch 0049, iter [01500, 05004], lr: 0.113845, loss: 1.8951
2022-07-28 00:25:03 - train: epoch 0049, iter [01600, 05004], lr: 0.113780, loss: 2.0812
2022-07-28 00:27:10 - train: epoch 0049, iter [01700, 05004], lr: 0.113714, loss: 1.9459
2022-07-28 00:29:11 - train: epoch 0049, iter [01800, 05004], lr: 0.113649, loss: 2.1322
2022-07-28 00:31:12 - train: epoch 0049, iter [01900, 05004], lr: 0.113583, loss: 1.9962
2022-07-28 00:33:12 - train: epoch 0049, iter [02000, 05004], lr: 0.113518, loss: 2.0498
2022-07-28 00:35:14 - train: epoch 0049, iter [02100, 05004], lr: 0.113453, loss: 2.0172
2022-07-28 00:37:20 - train: epoch 0049, iter [02200, 05004], lr: 0.113387, loss: 1.8607
2022-07-28 00:39:21 - train: epoch 0049, iter [02300, 05004], lr: 0.113322, loss: 2.0009
2022-07-28 00:41:26 - train: epoch 0049, iter [02400, 05004], lr: 0.113256, loss: 2.1748
2022-07-28 00:43:29 - train: epoch 0049, iter [02500, 05004], lr: 0.113191, loss: 2.0599
2022-07-28 00:45:40 - train: epoch 0049, iter [02600, 05004], lr: 0.113125, loss: 2.1436
2022-07-28 00:47:57 - train: epoch 0049, iter [02700, 05004], lr: 0.113059, loss: 2.1432
2022-07-28 00:50:06 - train: epoch 0049, iter [02800, 05004], lr: 0.112994, loss: 2.0429
2022-07-28 00:52:15 - train: epoch 0049, iter [02900, 05004], lr: 0.112928, loss: 2.0128
2022-07-28 00:54:15 - train: epoch 0049, iter [03000, 05004], lr: 0.112863, loss: 2.0316
2022-07-28 00:56:23 - train: epoch 0049, iter [03100, 05004], lr: 0.112797, loss: 2.0832
2022-07-28 00:58:28 - train: epoch 0049, iter [03200, 05004], lr: 0.112732, loss: 2.0673
2022-07-28 01:00:34 - train: epoch 0049, iter [03300, 05004], lr: 0.112666, loss: 1.9155
2022-07-28 01:02:42 - train: epoch 0049, iter [03400, 05004], lr: 0.112601, loss: 2.1583
2022-07-28 01:04:45 - train: epoch 0049, iter [03500, 05004], lr: 0.112535, loss: 2.1203
2022-07-28 01:06:50 - train: epoch 0049, iter [03600, 05004], lr: 0.112470, loss: 2.1035
2022-07-28 01:08:56 - train: epoch 0049, iter [03700, 05004], lr: 0.112404, loss: 2.0058
2022-07-28 01:11:00 - train: epoch 0049, iter [03800, 05004], lr: 0.112338, loss: 2.2260
2022-07-28 01:13:07 - train: epoch 0049, iter [03900, 05004], lr: 0.112273, loss: 2.0974
2022-07-28 01:15:11 - train: epoch 0049, iter [04000, 05004], lr: 0.112207, loss: 1.7895
2022-07-28 01:17:13 - train: epoch 0049, iter [04100, 05004], lr: 0.112142, loss: 1.9702
2022-07-28 01:19:11 - train: epoch 0049, iter [04200, 05004], lr: 0.112076, loss: 2.0506
2022-07-28 01:21:16 - train: epoch 0049, iter [04300, 05004], lr: 0.112010, loss: 2.3182
2022-07-28 01:23:17 - train: epoch 0049, iter [04400, 05004], lr: 0.111945, loss: 2.0051
2022-07-28 01:25:15 - train: epoch 0049, iter [04500, 05004], lr: 0.111879, loss: 1.9836
2022-07-28 01:27:24 - train: epoch 0049, iter [04600, 05004], lr: 0.111814, loss: 1.8638
2022-07-28 01:29:25 - train: epoch 0049, iter [04700, 05004], lr: 0.111748, loss: 2.1787
2022-07-28 01:31:29 - train: epoch 0049, iter [04800, 05004], lr: 0.111682, loss: 1.9636
2022-07-28 01:33:34 - train: epoch 0049, iter [04900, 05004], lr: 0.111617, loss: 1.9199
2022-07-28 01:35:35 - train: epoch 0049, iter [05000, 05004], lr: 0.111551, loss: 1.9917
2022-07-28 01:35:38 - train: epoch 049, train_loss: 2.0171
2022-07-28 01:39:48 - eval: epoch: 049, acc1: 58.440%, acc5: 82.474%, test_loss: 1.7370, per_image_load_time: 8.423ms, per_image_inference_time: 0.735ms
2022-07-28 01:39:49 - until epoch: 049, best_acc1: 58.440%
2022-07-28 01:39:49 - epoch 050 lr: 0.111548
2022-07-28 01:42:07 - train: epoch 0050, iter [00100, 05004], lr: 0.111483, loss: 1.9340
2022-07-28 01:44:05 - train: epoch 0050, iter [00200, 05004], lr: 0.111417, loss: 1.9206
2022-07-28 01:46:09 - train: epoch 0050, iter [00300, 05004], lr: 0.111352, loss: 1.8941
2022-07-28 01:48:09 - train: epoch 0050, iter [00400, 05004], lr: 0.111286, loss: 1.8864
2022-07-28 01:50:13 - train: epoch 0050, iter [00500, 05004], lr: 0.111220, loss: 2.0151
2022-07-28 01:52:14 - train: epoch 0050, iter [00600, 05004], lr: 0.111155, loss: 2.1791
2022-07-28 01:54:18 - train: epoch 0050, iter [00700, 05004], lr: 0.111089, loss: 1.6926
2022-07-28 01:56:23 - train: epoch 0050, iter [00800, 05004], lr: 0.111023, loss: 1.6889
2022-07-28 01:58:24 - train: epoch 0050, iter [00900, 05004], lr: 0.110957, loss: 2.0324
2022-07-28 02:00:31 - train: epoch 0050, iter [01000, 05004], lr: 0.110892, loss: 2.0672
2022-07-28 02:02:36 - train: epoch 0050, iter [01100, 05004], lr: 0.110826, loss: 2.0140
2022-07-28 02:04:33 - train: epoch 0050, iter [01200, 05004], lr: 0.110760, loss: 1.8798
2022-07-28 02:06:39 - train: epoch 0050, iter [01300, 05004], lr: 0.110695, loss: 1.9720
2022-07-28 02:08:48 - train: epoch 0050, iter [01400, 05004], lr: 0.110629, loss: 1.9715
2022-07-28 02:10:47 - train: epoch 0050, iter [01500, 05004], lr: 0.110563, loss: 1.9742
2022-07-28 02:12:52 - train: epoch 0050, iter [01600, 05004], lr: 0.110498, loss: 1.9886
2022-07-28 02:14:55 - train: epoch 0050, iter [01700, 05004], lr: 0.110432, loss: 2.0273
2022-07-28 02:16:59 - train: epoch 0050, iter [01800, 05004], lr: 0.110366, loss: 2.2264
2022-07-28 02:19:01 - train: epoch 0050, iter [01900, 05004], lr: 0.110300, loss: 1.8773
2022-07-28 02:21:02 - train: epoch 0050, iter [02000, 05004], lr: 0.110235, loss: 1.9288
2022-07-28 02:23:07 - train: epoch 0050, iter [02100, 05004], lr: 0.110169, loss: 1.8223
2022-07-28 02:25:12 - train: epoch 0050, iter [02200, 05004], lr: 0.110103, loss: 1.8409
2022-07-28 02:27:15 - train: epoch 0050, iter [02300, 05004], lr: 0.110037, loss: 1.7170
2022-07-28 02:29:16 - train: epoch 0050, iter [02400, 05004], lr: 0.109972, loss: 2.0247
2022-07-28 02:31:12 - train: epoch 0050, iter [02500, 05004], lr: 0.109906, loss: 1.9863
2022-07-28 02:33:29 - train: epoch 0050, iter [02600, 05004], lr: 0.109840, loss: 1.8774
2022-07-28 02:35:33 - train: epoch 0050, iter [02700, 05004], lr: 0.109774, loss: 1.9943
2022-07-28 02:37:37 - train: epoch 0050, iter [02800, 05004], lr: 0.109709, loss: 2.0191
2022-07-28 02:39:49 - train: epoch 0050, iter [02900, 05004], lr: 0.109643, loss: 2.3785
2022-07-28 02:41:57 - train: epoch 0050, iter [03000, 05004], lr: 0.109577, loss: 2.0148
2022-07-28 02:44:01 - train: epoch 0050, iter [03100, 05004], lr: 0.109511, loss: 2.1640
2022-07-28 02:46:04 - train: epoch 0050, iter [03200, 05004], lr: 0.109445, loss: 2.0871
2022-07-28 02:48:09 - train: epoch 0050, iter [03300, 05004], lr: 0.109380, loss: 1.8869
2022-07-28 02:50:15 - train: epoch 0050, iter [03400, 05004], lr: 0.109314, loss: 2.0791
2022-07-28 02:52:24 - train: epoch 0050, iter [03500, 05004], lr: 0.109248, loss: 2.0695
2022-07-28 02:54:24 - train: epoch 0050, iter [03600, 05004], lr: 0.109182, loss: 2.0044
2022-07-28 02:56:30 - train: epoch 0050, iter [03700, 05004], lr: 0.109116, loss: 2.2383
2022-07-28 02:58:33 - train: epoch 0050, iter [03800, 05004], lr: 0.109051, loss: 1.8585
2022-07-28 03:00:42 - train: epoch 0050, iter [03900, 05004], lr: 0.108985, loss: 1.6322
2022-07-28 03:02:48 - train: epoch 0050, iter [04000, 05004], lr: 0.108919, loss: 2.0344
2022-07-28 03:04:54 - train: epoch 0050, iter [04100, 05004], lr: 0.108853, loss: 2.1566
2022-07-28 03:06:59 - train: epoch 0050, iter [04200, 05004], lr: 0.108787, loss: 2.0675
2022-07-28 03:09:09 - train: epoch 0050, iter [04300, 05004], lr: 0.108721, loss: 2.1168
2022-07-28 03:11:11 - train: epoch 0050, iter [04400, 05004], lr: 0.108656, loss: 2.0592
2022-07-28 03:13:08 - train: epoch 0050, iter [04500, 05004], lr: 0.108590, loss: 2.0342
2022-07-28 03:15:09 - train: epoch 0050, iter [04600, 05004], lr: 0.108524, loss: 1.9734
2022-07-28 03:17:15 - train: epoch 0050, iter [04700, 05004], lr: 0.108458, loss: 2.1477
2022-07-28 03:19:14 - train: epoch 0050, iter [04800, 05004], lr: 0.108392, loss: 2.0236
2022-07-28 03:21:16 - train: epoch 0050, iter [04900, 05004], lr: 0.108326, loss: 1.9363
2022-07-28 03:23:14 - train: epoch 0050, iter [05000, 05004], lr: 0.108261, loss: 1.8782
2022-07-28 03:23:18 - train: epoch 050, train_loss: 2.0056
2022-07-28 03:27:26 - eval: epoch: 050, acc1: 57.804%, acc5: 81.858%, test_loss: 1.7747, per_image_load_time: 1.733ms, per_image_inference_time: 0.645ms
2022-07-28 03:27:26 - until epoch: 050, best_acc1: 58.440%
2022-07-28 03:27:26 - epoch 051 lr: 0.108257
2022-07-28 03:29:51 - train: epoch 0051, iter [00100, 05004], lr: 0.108192, loss: 2.1624
2022-07-28 03:31:56 - train: epoch 0051, iter [00200, 05004], lr: 0.108126, loss: 2.2041
2022-07-28 03:33:57 - train: epoch 0051, iter [00300, 05004], lr: 0.108060, loss: 2.0479
2022-07-28 03:36:01 - train: epoch 0051, iter [00400, 05004], lr: 0.107994, loss: 1.7390
2022-07-28 03:38:14 - train: epoch 0051, iter [00500, 05004], lr: 0.107929, loss: 1.9920
2022-07-28 03:40:18 - train: epoch 0051, iter [00600, 05004], lr: 0.107863, loss: 1.7660
2022-07-28 03:42:19 - train: epoch 0051, iter [00700, 05004], lr: 0.107797, loss: 1.9804
2022-07-28 03:44:22 - train: epoch 0051, iter [00800, 05004], lr: 0.107731, loss: 2.2273
2022-07-28 03:46:29 - train: epoch 0051, iter [00900, 05004], lr: 0.107665, loss: 1.8883
2022-07-28 03:48:32 - train: epoch 0051, iter [01000, 05004], lr: 0.107599, loss: 2.3423
2022-07-28 03:50:40 - train: epoch 0051, iter [01100, 05004], lr: 0.107533, loss: 1.9314
2022-07-28 03:52:47 - train: epoch 0051, iter [01200, 05004], lr: 0.107467, loss: 1.8404
2022-07-28 03:54:50 - train: epoch 0051, iter [01300, 05004], lr: 0.107401, loss: 1.7692
2022-07-28 03:56:57 - train: epoch 0051, iter [01400, 05004], lr: 0.107336, loss: 1.8965
2022-07-28 03:59:00 - train: epoch 0051, iter [01500, 05004], lr: 0.107270, loss: 2.0502
2022-07-28 04:01:06 - train: epoch 0051, iter [01600, 05004], lr: 0.107204, loss: 1.9059
2022-07-28 04:03:07 - train: epoch 0051, iter [01700, 05004], lr: 0.107138, loss: 2.0323
2022-07-28 04:05:12 - train: epoch 0051, iter [01800, 05004], lr: 0.107072, loss: 1.9665
2022-07-28 04:07:10 - train: epoch 0051, iter [01900, 05004], lr: 0.107006, loss: 1.8028
2022-07-28 04:09:13 - train: epoch 0051, iter [02000, 05004], lr: 0.106940, loss: 2.2057
2022-07-28 04:11:17 - train: epoch 0051, iter [02100, 05004], lr: 0.106874, loss: 2.0378
2022-07-28 04:13:23 - train: epoch 0051, iter [02200, 05004], lr: 0.106808, loss: 1.9496
2022-07-28 04:15:25 - train: epoch 0051, iter [02300, 05004], lr: 0.106742, loss: 2.1145
2022-07-28 04:17:22 - train: epoch 0051, iter [02400, 05004], lr: 0.106676, loss: 1.8999
2022-07-28 04:19:47 - train: epoch 0051, iter [02500, 05004], lr: 0.106610, loss: 2.0985
2022-07-28 04:21:50 - train: epoch 0051, iter [02600, 05004], lr: 0.106544, loss: 1.8443
2022-07-28 04:23:55 - train: epoch 0051, iter [02700, 05004], lr: 0.106478, loss: 2.0513
2022-07-28 04:25:58 - train: epoch 0051, iter [02800, 05004], lr: 0.106413, loss: 1.9128
2022-07-28 04:28:01 - train: epoch 0051, iter [02900, 05004], lr: 0.106347, loss: 2.1539
2022-07-28 04:29:59 - train: epoch 0051, iter [03000, 05004], lr: 0.106281, loss: 1.6768
2022-07-28 04:32:04 - train: epoch 0051, iter [03100, 05004], lr: 0.106215, loss: 1.8099
2022-07-28 04:34:06 - train: epoch 0051, iter [03200, 05004], lr: 0.106149, loss: 1.9087
2022-07-28 04:36:10 - train: epoch 0051, iter [03300, 05004], lr: 0.106083, loss: 2.0433
2022-07-28 04:38:07 - train: epoch 0051, iter [03400, 05004], lr: 0.106017, loss: 2.0424
2022-07-28 04:40:11 - train: epoch 0051, iter [03500, 05004], lr: 0.105951, loss: 2.0405
2022-07-28 04:42:07 - train: epoch 0051, iter [03600, 05004], lr: 0.105885, loss: 1.9950
2022-07-28 04:44:09 - train: epoch 0051, iter [03700, 05004], lr: 0.105819, loss: 2.1465
2022-07-28 04:46:12 - train: epoch 0051, iter [03800, 05004], lr: 0.105753, loss: 2.0433
2022-07-28 04:48:13 - train: epoch 0051, iter [03900, 05004], lr: 0.105687, loss: 2.0481
2022-07-28 04:50:17 - train: epoch 0051, iter [04000, 05004], lr: 0.105621, loss: 2.1508
2022-07-28 04:52:21 - train: epoch 0051, iter [04100, 05004], lr: 0.105555, loss: 1.9248
2022-07-28 04:54:27 - train: epoch 0051, iter [04200, 05004], lr: 0.105489, loss: 2.1295
2022-07-28 04:56:29 - train: epoch 0051, iter [04300, 05004], lr: 0.105423, loss: 1.8374
2022-07-28 04:58:32 - train: epoch 0051, iter [04400, 05004], lr: 0.105357, loss: 2.1827
2022-07-28 05:00:31 - train: epoch 0051, iter [04500, 05004], lr: 0.105291, loss: 1.8373
2022-07-28 05:02:40 - train: epoch 0051, iter [04600, 05004], lr: 0.105225, loss: 2.1067
2022-07-28 05:04:45 - train: epoch 0051, iter [04700, 05004], lr: 0.105159, loss: 1.7809
2022-07-28 05:06:45 - train: epoch 0051, iter [04800, 05004], lr: 0.105093, loss: 1.9356
2022-07-28 05:08:41 - train: epoch 0051, iter [04900, 05004], lr: 0.105027, loss: 2.0212
2022-07-28 05:10:34 - train: epoch 0051, iter [05000, 05004], lr: 0.104961, loss: 2.0428
2022-07-28 05:10:38 - train: epoch 051, train_loss: 1.9915
2022-07-28 05:14:42 - eval: epoch: 051, acc1: 59.096%, acc5: 82.760%, test_loss: 1.7093, per_image_load_time: 1.583ms, per_image_inference_time: 0.584ms
2022-07-28 05:14:42 - until epoch: 051, best_acc1: 59.096%
2022-07-28 05:14:42 - epoch 052 lr: 0.104958
2022-07-28 05:17:04 - train: epoch 0052, iter [00100, 05004], lr: 0.104892, loss: 1.8003
2022-07-28 05:18:59 - train: epoch 0052, iter [00200, 05004], lr: 0.104826, loss: 2.0806
2022-07-28 05:21:01 - train: epoch 0052, iter [00300, 05004], lr: 0.104760, loss: 1.7303
2022-07-28 05:23:01 - train: epoch 0052, iter [00400, 05004], lr: 0.104694, loss: 2.0967
2022-07-28 05:25:04 - train: epoch 0052, iter [00500, 05004], lr: 0.104628, loss: 2.0828
2022-07-28 05:27:04 - train: epoch 0052, iter [00600, 05004], lr: 0.104562, loss: 1.8657
2022-07-28 05:29:08 - train: epoch 0052, iter [00700, 05004], lr: 0.104496, loss: 1.9093
2022-07-28 05:31:12 - train: epoch 0052, iter [00800, 05004], lr: 0.104430, loss: 1.8456
2022-07-28 05:33:10 - train: epoch 0052, iter [00900, 05004], lr: 0.104364, loss: 1.8946
2022-07-28 05:35:12 - train: epoch 0052, iter [01000, 05004], lr: 0.104298, loss: 2.0840
2022-07-28 05:37:19 - train: epoch 0052, iter [01100, 05004], lr: 0.104232, loss: 1.8976
2022-07-28 05:39:26 - train: epoch 0052, iter [01200, 05004], lr: 0.104166, loss: 1.9327
2022-07-28 05:41:30 - train: epoch 0052, iter [01300, 05004], lr: 0.104100, loss: 1.8874
2022-07-28 05:43:30 - train: epoch 0052, iter [01400, 05004], lr: 0.104034, loss: 2.1375
2022-07-28 05:45:36 - train: epoch 0052, iter [01500, 05004], lr: 0.103968, loss: 1.9000
2022-07-28 05:47:45 - train: epoch 0052, iter [01600, 05004], lr: 0.103902, loss: 1.5729
2022-07-28 05:49:43 - train: epoch 0052, iter [01700, 05004], lr: 0.103836, loss: 1.7650
2022-07-28 05:51:47 - train: epoch 0052, iter [01800, 05004], lr: 0.103770, loss: 1.8846
2022-07-28 05:53:51 - train: epoch 0052, iter [01900, 05004], lr: 0.103704, loss: 1.7928
2022-07-28 05:55:57 - train: epoch 0052, iter [02000, 05004], lr: 0.103638, loss: 2.1562
2022-07-28 05:57:56 - train: epoch 0052, iter [02100, 05004], lr: 0.103572, loss: 1.9061
2022-07-28 05:59:53 - train: epoch 0052, iter [02200, 05004], lr: 0.103506, loss: 2.1201
2022-07-28 06:01:58 - train: epoch 0052, iter [02300, 05004], lr: 0.103440, loss: 1.8999
2022-07-28 06:03:42 - train: epoch 0052, iter [02400, 05004], lr: 0.103374, loss: 1.7321
2022-07-28 06:05:56 - train: epoch 0052, iter [02500, 05004], lr: 0.103308, loss: 1.8089
2022-07-28 06:08:00 - train: epoch 0052, iter [02600, 05004], lr: 0.103242, loss: 1.8226
2022-07-28 06:10:05 - train: epoch 0052, iter [02700, 05004], lr: 0.103176, loss: 2.0437
2022-07-28 06:12:11 - train: epoch 0052, iter [02800, 05004], lr: 0.103110, loss: 2.0209
2022-07-28 06:14:18 - train: epoch 0052, iter [02900, 05004], lr: 0.103043, loss: 1.8464
2022-07-28 06:16:19 - train: epoch 0052, iter [03000, 05004], lr: 0.102977, loss: 1.8582
2022-07-28 06:18:25 - train: epoch 0052, iter [03100, 05004], lr: 0.102911, loss: 2.0367
2022-07-28 06:20:27 - train: epoch 0052, iter [03200, 05004], lr: 0.102845, loss: 1.8538
2022-07-28 06:22:31 - train: epoch 0052, iter [03300, 05004], lr: 0.102779, loss: 2.0233
2022-07-28 06:24:31 - train: epoch 0052, iter [03400, 05004], lr: 0.102713, loss: 2.1431
2022-07-28 06:26:31 - train: epoch 0052, iter [03500, 05004], lr: 0.102647, loss: 1.9772
2022-07-28 06:28:42 - train: epoch 0052, iter [03600, 05004], lr: 0.102581, loss: 2.0853
2022-07-28 06:30:54 - train: epoch 0052, iter [03700, 05004], lr: 0.102515, loss: 2.0826
2022-07-28 06:33:06 - train: epoch 0052, iter [03800, 05004], lr: 0.102449, loss: 1.8325
2022-07-28 06:35:20 - train: epoch 0052, iter [03900, 05004], lr: 0.102383, loss: 1.7885
2022-07-28 06:37:31 - train: epoch 0052, iter [04000, 05004], lr: 0.102317, loss: 2.2832
2022-07-28 06:39:40 - train: epoch 0052, iter [04100, 05004], lr: 0.102251, loss: 1.7823
2022-07-28 06:41:52 - train: epoch 0052, iter [04200, 05004], lr: 0.102185, loss: 2.0322
2022-07-28 06:44:04 - train: epoch 0052, iter [04300, 05004], lr: 0.102119, loss: 1.8867
2022-07-28 06:46:16 - train: epoch 0052, iter [04400, 05004], lr: 0.102052, loss: 1.8611
2022-07-28 06:48:26 - train: epoch 0052, iter [04500, 05004], lr: 0.101986, loss: 1.9378
2022-07-28 06:50:35 - train: epoch 0052, iter [04600, 05004], lr: 0.101920, loss: 1.9327
2022-07-28 06:52:41 - train: epoch 0052, iter [04700, 05004], lr: 0.101854, loss: 1.9389
2022-07-28 06:54:55 - train: epoch 0052, iter [04800, 05004], lr: 0.101788, loss: 1.8784
2022-07-28 06:57:06 - train: epoch 0052, iter [04900, 05004], lr: 0.101722, loss: 1.9672
2022-07-28 06:59:11 - train: epoch 0052, iter [05000, 05004], lr: 0.101656, loss: 1.9977
2022-07-28 06:59:15 - train: epoch 052, train_loss: 1.9794
2022-07-28 07:03:54 - eval: epoch: 052, acc1: 59.062%, acc5: 82.648%, test_loss: 1.7168, per_image_load_time: 9.636ms, per_image_inference_time: 0.865ms
2022-07-28 07:03:55 - until epoch: 052, best_acc1: 59.096%
2022-07-28 07:03:55 - epoch 053 lr: 0.101653
2022-07-28 07:06:20 - train: epoch 0053, iter [00100, 05004], lr: 0.101587, loss: 1.9145
2022-07-28 07:08:30 - train: epoch 0053, iter [00200, 05004], lr: 0.101521, loss: 1.9655
2022-07-28 07:10:38 - train: epoch 0053, iter [00300, 05004], lr: 0.101455, loss: 1.8810
2022-07-28 07:12:46 - train: epoch 0053, iter [00400, 05004], lr: 0.101389, loss: 2.2172
2022-07-28 07:15:01 - train: epoch 0053, iter [00500, 05004], lr: 0.101323, loss: 1.9841
2022-07-28 07:17:15 - train: epoch 0053, iter [00600, 05004], lr: 0.101257, loss: 1.8116
2022-07-28 07:19:19 - train: epoch 0053, iter [00700, 05004], lr: 0.101191, loss: 1.9888
2022-07-28 07:21:29 - train: epoch 0053, iter [00800, 05004], lr: 0.101125, loss: 1.9811
2022-07-28 07:23:41 - train: epoch 0053, iter [00900, 05004], lr: 0.101059, loss: 1.9259
2022-07-28 07:25:56 - train: epoch 0053, iter [01000, 05004], lr: 0.100993, loss: 2.0628
2022-07-28 07:28:08 - train: epoch 0053, iter [01100, 05004], lr: 0.100927, loss: 1.9558
2022-07-28 07:30:11 - train: epoch 0053, iter [01200, 05004], lr: 0.100860, loss: 1.8919
2022-07-28 07:32:19 - train: epoch 0053, iter [01300, 05004], lr: 0.100794, loss: 1.9719
2022-07-28 07:34:22 - train: epoch 0053, iter [01400, 05004], lr: 0.100728, loss: 2.0229
2022-07-28 07:36:24 - train: epoch 0053, iter [01500, 05004], lr: 0.100662, loss: 1.6634
2022-07-28 07:38:28 - train: epoch 0053, iter [01600, 05004], lr: 0.100596, loss: 2.2343
2022-07-28 07:40:31 - train: epoch 0053, iter [01700, 05004], lr: 0.100530, loss: 2.2260
2022-07-28 07:42:31 - train: epoch 0053, iter [01800, 05004], lr: 0.100464, loss: 2.0347
2022-07-28 07:44:34 - train: epoch 0053, iter [01900, 05004], lr: 0.100398, loss: 2.0577
2022-07-28 07:46:34 - train: epoch 0053, iter [02000, 05004], lr: 0.100332, loss: 1.9793
2022-07-28 07:48:37 - train: epoch 0053, iter [02100, 05004], lr: 0.100266, loss: 2.1126
2022-07-28 07:50:38 - train: epoch 0053, iter [02200, 05004], lr: 0.100200, loss: 1.6702
2022-07-28 07:52:26 - train: epoch 0053, iter [02300, 05004], lr: 0.100133, loss: 2.1493
2022-07-28 07:54:56 - train: epoch 0053, iter [02400, 05004], lr: 0.100067, loss: 1.8653
2022-07-28 07:57:04 - train: epoch 0053, iter [02500, 05004], lr: 0.100001, loss: 2.1354
2022-07-28 07:59:11 - train: epoch 0053, iter [02600, 05004], lr: 0.099935, loss: 2.0550
2022-07-28 08:01:21 - train: epoch 0053, iter [02700, 05004], lr: 0.099869, loss: 2.1782
2022-07-28 08:03:20 - train: epoch 0053, iter [02800, 05004], lr: 0.099803, loss: 2.0811
2022-07-28 08:05:28 - train: epoch 0053, iter [02900, 05004], lr: 0.099737, loss: 2.0057
2022-07-28 08:07:26 - train: epoch 0053, iter [03000, 05004], lr: 0.099671, loss: 1.6921
2022-07-28 08:09:29 - train: epoch 0053, iter [03100, 05004], lr: 0.099605, loss: 2.0668
2022-07-28 08:11:31 - train: epoch 0053, iter [03200, 05004], lr: 0.099539, loss: 2.1319
2022-07-28 08:13:31 - train: epoch 0053, iter [03300, 05004], lr: 0.099473, loss: 1.9718
2022-07-28 08:15:26 - train: epoch 0053, iter [03400, 05004], lr: 0.099407, loss: 1.9031
2022-07-28 08:17:31 - train: epoch 0053, iter [03500, 05004], lr: 0.099340, loss: 2.0048
2022-07-28 08:19:37 - train: epoch 0053, iter [03600, 05004], lr: 0.099274, loss: 1.9107
2022-07-28 08:21:38 - train: epoch 0053, iter [03700, 05004], lr: 0.099208, loss: 2.0002
2022-07-28 08:23:42 - train: epoch 0053, iter [03800, 05004], lr: 0.099142, loss: 1.8205
2022-07-28 08:25:37 - train: epoch 0053, iter [03900, 05004], lr: 0.099076, loss: 1.9906
2022-07-28 08:27:40 - train: epoch 0053, iter [04000, 05004], lr: 0.099010, loss: 1.8401
2022-07-28 08:29:52 - train: epoch 0053, iter [04100, 05004], lr: 0.098944, loss: 2.1057
2022-07-28 08:32:04 - train: epoch 0053, iter [04200, 05004], lr: 0.098878, loss: 2.0841
2022-07-28 08:34:11 - train: epoch 0053, iter [04300, 05004], lr: 0.098812, loss: 2.1805
2022-07-28 08:36:20 - train: epoch 0053, iter [04400, 05004], lr: 0.098746, loss: 1.9739
2022-07-28 08:38:27 - train: epoch 0053, iter [04500, 05004], lr: 0.098680, loss: 2.3530
2022-07-28 08:40:37 - train: epoch 0053, iter [04600, 05004], lr: 0.098614, loss: 1.9976
2022-07-28 08:42:46 - train: epoch 0053, iter [04700, 05004], lr: 0.098547, loss: 1.9884
2022-07-28 08:45:00 - train: epoch 0053, iter [04800, 05004], lr: 0.098481, loss: 2.1618
2022-07-28 08:47:13 - train: epoch 0053, iter [04900, 05004], lr: 0.098415, loss: 2.1892
2022-07-28 08:49:17 - train: epoch 0053, iter [05000, 05004], lr: 0.098349, loss: 1.6663
2022-07-28 08:49:21 - train: epoch 053, train_loss: 1.9646
2022-07-28 08:53:52 - eval: epoch: 053, acc1: 59.302%, acc5: 83.344%, test_loss: 1.6858, per_image_load_time: 4.732ms, per_image_inference_time: 0.837ms
2022-07-28 08:53:53 - until epoch: 053, best_acc1: 59.302%
2022-07-28 08:53:53 - epoch 054 lr: 0.098346
2022-07-28 08:56:30 - train: epoch 0054, iter [00100, 05004], lr: 0.098281, loss: 1.8612
2022-07-28 08:58:42 - train: epoch 0054, iter [00200, 05004], lr: 0.098214, loss: 2.0474
2022-07-28 09:00:53 - train: epoch 0054, iter [00300, 05004], lr: 0.098148, loss: 1.9159
2022-07-28 09:03:05 - train: epoch 0054, iter [00400, 05004], lr: 0.098082, loss: 1.7546
2022-07-28 09:05:21 - train: epoch 0054, iter [00500, 05004], lr: 0.098016, loss: 1.8534
2022-07-28 09:07:29 - train: epoch 0054, iter [00600, 05004], lr: 0.097950, loss: 2.0887
2022-07-28 09:09:45 - train: epoch 0054, iter [00700, 05004], lr: 0.097884, loss: 2.2671
2022-07-28 09:11:54 - train: epoch 0054, iter [00800, 05004], lr: 0.097818, loss: 1.9757
2022-07-28 09:13:56 - train: epoch 0054, iter [00900, 05004], lr: 0.097752, loss: 1.6860
2022-07-28 09:16:00 - train: epoch 0054, iter [01000, 05004], lr: 0.097686, loss: 1.7836
2022-07-28 09:17:57 - train: epoch 0054, iter [01100, 05004], lr: 0.097620, loss: 1.7403
2022-07-28 09:19:58 - train: epoch 0054, iter [01200, 05004], lr: 0.097554, loss: 2.1389
2022-07-28 09:21:58 - train: epoch 0054, iter [01300, 05004], lr: 0.097488, loss: 1.9595
2022-07-28 09:23:59 - train: epoch 0054, iter [01400, 05004], lr: 0.097422, loss: 2.1493
2022-07-28 09:25:58 - train: epoch 0054, iter [01500, 05004], lr: 0.097356, loss: 1.9149
2022-07-28 09:28:05 - train: epoch 0054, iter [01600, 05004], lr: 0.097289, loss: 1.7709
2022-07-28 09:30:09 - train: epoch 0054, iter [01700, 05004], lr: 0.097223, loss: 1.8862
2022-07-28 09:32:05 - train: epoch 0054, iter [01800, 05004], lr: 0.097157, loss: 1.7261
2022-07-28 09:34:06 - train: epoch 0054, iter [01900, 05004], lr: 0.097091, loss: 2.1166
2022-07-28 09:36:10 - train: epoch 0054, iter [02000, 05004], lr: 0.097025, loss: 2.0925
2022-07-28 09:38:14 - train: epoch 0054, iter [02100, 05004], lr: 0.096959, loss: 1.7733
2022-07-28 09:40:16 - train: epoch 0054, iter [02200, 05004], lr: 0.096893, loss: 2.0705
2022-07-28 09:42:39 - train: epoch 0054, iter [02300, 05004], lr: 0.096827, loss: 1.8773
2022-07-28 09:44:45 - train: epoch 0054, iter [02400, 05004], lr: 0.096761, loss: 2.1348
2022-07-28 09:46:55 - train: epoch 0054, iter [02500, 05004], lr: 0.096695, loss: 1.9627
2022-07-28 09:48:55 - train: epoch 0054, iter [02600, 05004], lr: 0.096629, loss: 1.6959
2022-07-28 09:51:00 - train: epoch 0054, iter [02700, 05004], lr: 0.096563, loss: 1.7089
2022-07-28 09:53:07 - train: epoch 0054, iter [02800, 05004], lr: 0.096497, loss: 2.1962
2022-07-28 09:55:14 - train: epoch 0054, iter [02900, 05004], lr: 0.096431, loss: 1.8523
2022-07-28 09:57:14 - train: epoch 0054, iter [03000, 05004], lr: 0.096365, loss: 2.0043
2022-07-28 09:59:20 - train: epoch 0054, iter [03100, 05004], lr: 0.096299, loss: 2.1485
2022-07-28 10:01:20 - train: epoch 0054, iter [03200, 05004], lr: 0.096233, loss: 2.0207
2022-07-28 10:03:18 - train: epoch 0054, iter [03300, 05004], lr: 0.096167, loss: 1.9016
2022-07-28 10:05:18 - train: epoch 0054, iter [03400, 05004], lr: 0.096101, loss: 1.8252
2022-07-28 10:07:15 - train: epoch 0054, iter [03500, 05004], lr: 0.096035, loss: 1.9696
2022-07-28 10:09:21 - train: epoch 0054, iter [03600, 05004], lr: 0.095969, loss: 1.8817
2022-07-28 10:11:26 - train: epoch 0054, iter [03700, 05004], lr: 0.095902, loss: 1.9181
2022-07-28 10:13:27 - train: epoch 0054, iter [03800, 05004], lr: 0.095836, loss: 1.8594
2022-07-28 10:15:30 - train: epoch 0054, iter [03900, 05004], lr: 0.095770, loss: 1.9543
2022-07-28 10:17:31 - train: epoch 0054, iter [04000, 05004], lr: 0.095704, loss: 1.9252
2022-07-28 10:19:30 - train: epoch 0054, iter [04100, 05004], lr: 0.095638, loss: 1.7888
2022-07-28 10:21:31 - train: epoch 0054, iter [04200, 05004], lr: 0.095572, loss: 1.9189
2022-07-28 10:23:23 - train: epoch 0054, iter [04300, 05004], lr: 0.095506, loss: 2.0458
2022-07-28 10:25:20 - train: epoch 0054, iter [04400, 05004], lr: 0.095440, loss: 1.6738
2022-07-28 10:27:11 - train: epoch 0054, iter [04500, 05004], lr: 0.095374, loss: 1.9327
2022-07-28 10:29:07 - train: epoch 0054, iter [04600, 05004], lr: 0.095308, loss: 1.9771
2022-07-28 10:31:04 - train: epoch 0054, iter [04700, 05004], lr: 0.095242, loss: 2.2075
2022-07-28 10:32:59 - train: epoch 0054, iter [04800, 05004], lr: 0.095176, loss: 2.0739
2022-07-28 10:34:56 - train: epoch 0054, iter [04900, 05004], lr: 0.095110, loss: 1.8987
2022-07-28 10:36:55 - train: epoch 0054, iter [05000, 05004], lr: 0.095044, loss: 1.9338
2022-07-28 10:36:59 - train: epoch 054, train_loss: 1.9492
2022-07-28 10:41:11 - eval: epoch: 054, acc1: 59.514%, acc5: 83.216%, test_loss: 1.6823, per_image_load_time: 8.404ms, per_image_inference_time: 0.641ms
2022-07-28 10:41:12 - until epoch: 054, best_acc1: 59.514%
2022-07-28 10:41:12 - epoch 055 lr: 0.095041
2022-07-28 10:43:27 - train: epoch 0055, iter [00100, 05004], lr: 0.094976, loss: 1.6318
2022-07-28 10:45:23 - train: epoch 0055, iter [00200, 05004], lr: 0.094910, loss: 1.9267
2022-07-28 10:47:21 - train: epoch 0055, iter [00300, 05004], lr: 0.094844, loss: 1.9217
2022-07-28 10:49:25 - train: epoch 0055, iter [00400, 05004], lr: 0.094778, loss: 1.7596
2022-07-28 10:51:26 - train: epoch 0055, iter [00500, 05004], lr: 0.094712, loss: 1.6080
2022-07-28 10:53:30 - train: epoch 0055, iter [00600, 05004], lr: 0.094646, loss: 1.9596
2022-07-28 10:55:39 - train: epoch 0055, iter [00700, 05004], lr: 0.094580, loss: 1.9126
2022-07-28 10:57:36 - train: epoch 0055, iter [00800, 05004], lr: 0.094514, loss: 1.8602
2022-07-28 10:59:34 - train: epoch 0055, iter [00900, 05004], lr: 0.094448, loss: 2.0047
2022-07-28 11:01:25 - train: epoch 0055, iter [01000, 05004], lr: 0.094382, loss: 1.9871
2022-07-28 11:03:24 - train: epoch 0055, iter [01100, 05004], lr: 0.094316, loss: 1.9273
2022-07-28 11:05:26 - train: epoch 0055, iter [01200, 05004], lr: 0.094250, loss: 1.9286
2022-07-28 11:07:28 - train: epoch 0055, iter [01300, 05004], lr: 0.094184, loss: 1.9024
2022-07-28 11:09:30 - train: epoch 0055, iter [01400, 05004], lr: 0.094118, loss: 2.0250
2022-07-28 11:11:35 - train: epoch 0055, iter [01500, 05004], lr: 0.094052, loss: 1.9493
2022-07-28 11:13:33 - train: epoch 0055, iter [01600, 05004], lr: 0.093986, loss: 2.0971
2022-07-28 11:15:29 - train: epoch 0055, iter [01700, 05004], lr: 0.093920, loss: 2.2044
2022-07-28 11:17:29 - train: epoch 0055, iter [01800, 05004], lr: 0.093854, loss: 2.0613
2022-07-28 11:19:19 - train: epoch 0055, iter [01900, 05004], lr: 0.093788, loss: 2.0560
2022-07-28 11:21:16 - train: epoch 0055, iter [02000, 05004], lr: 0.093722, loss: 1.9098
2022-07-28 11:23:16 - train: epoch 0055, iter [02100, 05004], lr: 0.093656, loss: 1.5674
2022-07-28 11:25:26 - train: epoch 0055, iter [02200, 05004], lr: 0.093590, loss: 2.0800
2022-07-28 11:27:21 - train: epoch 0055, iter [02300, 05004], lr: 0.093524, loss: 1.7150
2022-07-28 11:29:21 - train: epoch 0055, iter [02400, 05004], lr: 0.093458, loss: 1.6998
2022-07-28 11:31:24 - train: epoch 0055, iter [02500, 05004], lr: 0.093392, loss: 1.9677
2022-07-28 11:33:24 - train: epoch 0055, iter [02600, 05004], lr: 0.093326, loss: 1.7980
2022-07-28 11:35:21 - train: epoch 0055, iter [02700, 05004], lr: 0.093260, loss: 1.8986
2022-07-28 11:37:19 - train: epoch 0055, iter [02800, 05004], lr: 0.093194, loss: 2.1069
2022-07-28 11:39:12 - train: epoch 0055, iter [02900, 05004], lr: 0.093129, loss: 1.9279
2022-07-28 11:41:08 - train: epoch 0055, iter [03000, 05004], lr: 0.093063, loss: 2.0716
2022-07-28 11:43:05 - train: epoch 0055, iter [03100, 05004], lr: 0.092997, loss: 1.8777
2022-07-28 11:45:04 - train: epoch 0055, iter [03200, 05004], lr: 0.092931, loss: 1.9975
2022-07-28 11:46:57 - train: epoch 0055, iter [03300, 05004], lr: 0.092865, loss: 1.6470
2022-07-28 11:48:51 - train: epoch 0055, iter [03400, 05004], lr: 0.092799, loss: 1.8780
2022-07-28 11:50:45 - train: epoch 0055, iter [03500, 05004], lr: 0.092733, loss: 1.9181
2022-07-28 11:52:40 - train: epoch 0055, iter [03600, 05004], lr: 0.092667, loss: 2.0280
2022-07-28 11:54:33 - train: epoch 0055, iter [03700, 05004], lr: 0.092601, loss: 1.5806
2022-07-28 11:56:29 - train: epoch 0055, iter [03800, 05004], lr: 0.092535, loss: 1.6779
2022-07-28 11:58:30 - train: epoch 0055, iter [03900, 05004], lr: 0.092469, loss: 2.1399
2022-07-28 12:00:30 - train: epoch 0055, iter [04000, 05004], lr: 0.092403, loss: 1.8159
2022-07-28 12:02:29 - train: epoch 0055, iter [04100, 05004], lr: 0.092338, loss: 1.8398
2022-07-28 12:04:30 - train: epoch 0055, iter [04200, 05004], lr: 0.092272, loss: 1.9618
