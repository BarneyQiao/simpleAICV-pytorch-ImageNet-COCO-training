2022-07-28 12:06:22 - train: epoch 0055, iter [04300, 05004], lr: 0.092206, loss: 2.1026
2022-07-28 12:08:24 - train: epoch 0055, iter [04400, 05004], lr: 0.092140, loss: 2.0426
2022-07-28 12:10:25 - train: epoch 0055, iter [04500, 05004], lr: 0.092074, loss: 1.9983
2022-07-28 12:12:21 - train: epoch 0055, iter [04600, 05004], lr: 0.092008, loss: 1.8346
2022-07-28 12:14:17 - train: epoch 0055, iter [04700, 05004], lr: 0.091942, loss: 1.9325
2022-07-28 12:16:18 - train: epoch 0055, iter [04800, 05004], lr: 0.091876, loss: 1.9336
2022-07-28 12:18:12 - train: epoch 0055, iter [04900, 05004], lr: 0.091811, loss: 2.0415
2022-07-28 12:20:04 - train: epoch 0055, iter [05000, 05004], lr: 0.091745, loss: 2.0672
2022-07-28 12:20:08 - train: epoch 055, train_loss: 1.9370
2022-07-28 12:24:10 - eval: epoch: 055, acc1: 59.762%, acc5: 83.552%, test_loss: 1.6651, per_image_load_time: 6.605ms, per_image_inference_time: 0.634ms
2022-07-28 12:24:10 - until epoch: 055, best_acc1: 59.762%
2022-07-28 12:24:10 - epoch 056 lr: 0.091741
2022-07-28 12:26:20 - train: epoch 0056, iter [00100, 05004], lr: 0.091676, loss: 1.8775
2022-07-28 12:28:09 - train: epoch 0056, iter [00200, 05004], lr: 0.091610, loss: 1.8705
2022-07-28 12:30:05 - train: epoch 0056, iter [00300, 05004], lr: 0.091545, loss: 1.8796
2022-07-28 12:31:59 - train: epoch 0056, iter [00400, 05004], lr: 0.091479, loss: 1.8179
2022-07-28 12:34:00 - train: epoch 0056, iter [00500, 05004], lr: 0.091413, loss: 1.9992
2022-07-28 12:36:01 - train: epoch 0056, iter [00600, 05004], lr: 0.091347, loss: 2.0404
2022-07-28 12:38:01 - train: epoch 0056, iter [00700, 05004], lr: 0.091281, loss: 1.9189
2022-07-28 12:39:56 - train: epoch 0056, iter [00800, 05004], lr: 0.091215, loss: 2.0170
2022-07-28 12:41:50 - train: epoch 0056, iter [00900, 05004], lr: 0.091149, loss: 1.9520
2022-07-28 12:43:53 - train: epoch 0056, iter [01000, 05004], lr: 0.091084, loss: 2.1562
2022-07-28 12:45:57 - train: epoch 0056, iter [01100, 05004], lr: 0.091018, loss: 1.9003
2022-07-28 12:48:02 - train: epoch 0056, iter [01200, 05004], lr: 0.090952, loss: 1.8719
2022-07-28 12:50:06 - train: epoch 0056, iter [01300, 05004], lr: 0.090886, loss: 2.0585
2022-07-28 12:52:08 - train: epoch 0056, iter [01400, 05004], lr: 0.090820, loss: 1.9025
2022-07-28 12:54:16 - train: epoch 0056, iter [01500, 05004], lr: 0.090755, loss: 2.2035
2022-07-28 12:56:18 - train: epoch 0056, iter [01600, 05004], lr: 0.090689, loss: 1.8095
2022-07-28 12:58:21 - train: epoch 0056, iter [01700, 05004], lr: 0.090623, loss: 1.9191
2022-07-28 13:00:16 - train: epoch 0056, iter [01800, 05004], lr: 0.090557, loss: 2.1823
2022-07-28 13:02:16 - train: epoch 0056, iter [01900, 05004], lr: 0.090491, loss: 1.9932
2022-07-28 13:04:03 - train: epoch 0056, iter [02000, 05004], lr: 0.090426, loss: 1.8580
2022-07-28 13:06:20 - train: epoch 0056, iter [02100, 05004], lr: 0.090360, loss: 2.0601
2022-07-28 13:08:22 - train: epoch 0056, iter [02200, 05004], lr: 0.090294, loss: 2.1424
2022-07-28 13:10:21 - train: epoch 0056, iter [02300, 05004], lr: 0.090228, loss: 1.8286
2022-07-28 13:12:15 - train: epoch 0056, iter [02400, 05004], lr: 0.090163, loss: 1.8530
2022-07-28 13:14:19 - train: epoch 0056, iter [02500, 05004], lr: 0.090097, loss: 2.1096
2022-07-28 13:16:20 - train: epoch 0056, iter [02600, 05004], lr: 0.090031, loss: 1.8731
2022-07-28 13:18:33 - train: epoch 0056, iter [02700, 05004], lr: 0.089965, loss: 1.6194
2022-07-28 13:20:33 - train: epoch 0056, iter [02800, 05004], lr: 0.089899, loss: 1.7310
2022-07-28 13:22:35 - train: epoch 0056, iter [02900, 05004], lr: 0.089834, loss: 1.9523
2022-07-28 13:24:39 - train: epoch 0056, iter [03000, 05004], lr: 0.089768, loss: 2.0689
2022-07-28 13:26:40 - train: epoch 0056, iter [03100, 05004], lr: 0.089702, loss: 1.8879
2022-07-28 13:28:42 - train: epoch 0056, iter [03200, 05004], lr: 0.089637, loss: 1.9833
2022-07-28 13:30:41 - train: epoch 0056, iter [03300, 05004], lr: 0.089571, loss: 1.9551
2022-07-28 13:32:34 - train: epoch 0056, iter [03400, 05004], lr: 0.089505, loss: 1.8787
2022-07-28 13:34:35 - train: epoch 0056, iter [03500, 05004], lr: 0.089439, loss: 1.8049
2022-07-28 13:36:28 - train: epoch 0056, iter [03600, 05004], lr: 0.089374, loss: 1.7730
2022-07-28 13:38:21 - train: epoch 0056, iter [03700, 05004], lr: 0.089308, loss: 1.8675
2022-07-28 13:40:19 - train: epoch 0056, iter [03800, 05004], lr: 0.089242, loss: 1.8852
2022-07-28 13:42:20 - train: epoch 0056, iter [03900, 05004], lr: 0.089177, loss: 2.0992
2022-07-28 13:44:25 - train: epoch 0056, iter [04000, 05004], lr: 0.089111, loss: 1.9754
2022-07-28 13:46:30 - train: epoch 0056, iter [04100, 05004], lr: 0.089045, loss: 2.0126
2022-07-28 13:48:31 - train: epoch 0056, iter [04200, 05004], lr: 0.088979, loss: 1.8706
2022-07-28 13:50:36 - train: epoch 0056, iter [04300, 05004], lr: 0.088914, loss: 1.8680
2022-07-28 13:52:39 - train: epoch 0056, iter [04400, 05004], lr: 0.088848, loss: 2.1292
2022-07-28 13:54:49 - train: epoch 0056, iter [04500, 05004], lr: 0.088782, loss: 1.7994
2022-07-28 13:56:54 - train: epoch 0056, iter [04600, 05004], lr: 0.088717, loss: 1.8925
2022-07-28 13:59:05 - train: epoch 0056, iter [04700, 05004], lr: 0.088651, loss: 1.9811
2022-07-28 14:01:05 - train: epoch 0056, iter [04800, 05004], lr: 0.088585, loss: 1.8796
2022-07-28 14:03:08 - train: epoch 0056, iter [04900, 05004], lr: 0.088520, loss: 2.1138
2022-07-28 14:05:14 - train: epoch 0056, iter [05000, 05004], lr: 0.088454, loss: 1.9020
2022-07-28 14:05:18 - train: epoch 056, train_loss: 1.9202
2022-07-28 14:09:41 - eval: epoch: 056, acc1: 59.710%, acc5: 83.340%, test_loss: 1.6791, per_image_load_time: 9.055ms, per_image_inference_time: 0.794ms
2022-07-28 14:09:41 - until epoch: 056, best_acc1: 59.762%
2022-07-28 14:09:41 - epoch 057 lr: 0.088451
2022-07-28 14:11:56 - train: epoch 0057, iter [00100, 05004], lr: 0.088386, loss: 1.9106
2022-07-28 14:13:50 - train: epoch 0057, iter [00200, 05004], lr: 0.088320, loss: 1.8188
2022-07-28 14:15:48 - train: epoch 0057, iter [00300, 05004], lr: 0.088255, loss: 1.8544
2022-07-28 14:17:58 - train: epoch 0057, iter [00400, 05004], lr: 0.088189, loss: 1.8565
2022-07-28 14:20:08 - train: epoch 0057, iter [00500, 05004], lr: 0.088123, loss: 1.7444
2022-07-28 14:22:06 - train: epoch 0057, iter [00600, 05004], lr: 0.088058, loss: 1.9772
2022-07-28 14:24:07 - train: epoch 0057, iter [00700, 05004], lr: 0.087992, loss: 1.7275
2022-07-28 14:26:04 - train: epoch 0057, iter [00800, 05004], lr: 0.087927, loss: 1.8599
2022-07-28 14:28:00 - train: epoch 0057, iter [00900, 05004], lr: 0.087861, loss: 1.9424
2022-07-28 14:30:04 - train: epoch 0057, iter [01000, 05004], lr: 0.087795, loss: 1.8973
2022-07-28 14:32:05 - train: epoch 0057, iter [01100, 05004], lr: 0.087730, loss: 1.8961
2022-07-28 14:34:12 - train: epoch 0057, iter [01200, 05004], lr: 0.087664, loss: 1.7987
2022-07-28 14:36:17 - train: epoch 0057, iter [01300, 05004], lr: 0.087599, loss: 1.8979
2022-07-28 14:38:20 - train: epoch 0057, iter [01400, 05004], lr: 0.087533, loss: 1.9407
2022-07-28 14:40:24 - train: epoch 0057, iter [01500, 05004], lr: 0.087467, loss: 2.1519
2022-07-28 14:42:24 - train: epoch 0057, iter [01600, 05004], lr: 0.087402, loss: 2.0460
2022-07-28 14:44:30 - train: epoch 0057, iter [01700, 05004], lr: 0.087336, loss: 1.9387
2022-07-28 14:46:28 - train: epoch 0057, iter [01800, 05004], lr: 0.087271, loss: 2.1188
2022-07-28 14:48:28 - train: epoch 0057, iter [01900, 05004], lr: 0.087205, loss: 1.9304
2022-07-28 14:50:38 - train: epoch 0057, iter [02000, 05004], lr: 0.087140, loss: 1.8707
2022-07-28 14:53:02 - train: epoch 0057, iter [02100, 05004], lr: 0.087074, loss: 1.9248
2022-07-28 14:55:01 - train: epoch 0057, iter [02200, 05004], lr: 0.087009, loss: 1.9428
2022-07-28 14:57:11 - train: epoch 0057, iter [02300, 05004], lr: 0.086943, loss: 2.0372
2022-07-28 14:59:19 - train: epoch 0057, iter [02400, 05004], lr: 0.086878, loss: 1.7886
2022-07-28 15:01:20 - train: epoch 0057, iter [02500, 05004], lr: 0.086812, loss: 2.1127
2022-07-28 15:03:33 - train: epoch 0057, iter [02600, 05004], lr: 0.086747, loss: 1.7627
2022-07-28 15:05:36 - train: epoch 0057, iter [02700, 05004], lr: 0.086681, loss: 1.7474
2022-07-28 15:07:38 - train: epoch 0057, iter [02800, 05004], lr: 0.086616, loss: 1.6404
2022-07-28 15:09:44 - train: epoch 0057, iter [02900, 05004], lr: 0.086550, loss: 1.8580
2022-07-28 15:11:50 - train: epoch 0057, iter [03000, 05004], lr: 0.086485, loss: 1.9928
2022-07-28 15:13:45 - train: epoch 0057, iter [03100, 05004], lr: 0.086419, loss: 2.0331
2022-07-28 15:15:46 - train: epoch 0057, iter [03200, 05004], lr: 0.086354, loss: 2.0697
2022-07-28 15:17:43 - train: epoch 0057, iter [03300, 05004], lr: 0.086288, loss: 1.9108
2022-07-28 15:19:48 - train: epoch 0057, iter [03400, 05004], lr: 0.086223, loss: 2.0614
2022-07-28 15:21:54 - train: epoch 0057, iter [03500, 05004], lr: 0.086157, loss: 1.9025
2022-07-28 15:23:55 - train: epoch 0057, iter [03600, 05004], lr: 0.086092, loss: 1.9783
2022-07-28 15:25:48 - train: epoch 0057, iter [03700, 05004], lr: 0.086026, loss: 1.8001
2022-07-28 15:27:43 - train: epoch 0057, iter [03800, 05004], lr: 0.085961, loss: 1.8809
2022-07-28 15:29:43 - train: epoch 0057, iter [03900, 05004], lr: 0.085896, loss: 2.2485
2022-07-28 15:31:37 - train: epoch 0057, iter [04000, 05004], lr: 0.085830, loss: 1.7575
2022-07-28 15:33:36 - train: epoch 0057, iter [04100, 05004], lr: 0.085765, loss: 1.9343
2022-07-28 15:35:30 - train: epoch 0057, iter [04200, 05004], lr: 0.085699, loss: 1.9779
2022-07-28 15:37:27 - train: epoch 0057, iter [04300, 05004], lr: 0.085634, loss: 1.6328
2022-07-28 15:39:27 - train: epoch 0057, iter [04400, 05004], lr: 0.085568, loss: 2.1188
2022-07-28 15:41:24 - train: epoch 0057, iter [04500, 05004], lr: 0.085503, loss: 1.8381
2022-07-28 15:43:20 - train: epoch 0057, iter [04600, 05004], lr: 0.085438, loss: 1.8387
2022-07-28 15:45:12 - train: epoch 0057, iter [04700, 05004], lr: 0.085372, loss: 2.0258
2022-07-28 15:47:10 - train: epoch 0057, iter [04800, 05004], lr: 0.085307, loss: 2.1474
2022-07-28 15:49:06 - train: epoch 0057, iter [04900, 05004], lr: 0.085242, loss: 2.1343
2022-07-28 15:50:55 - train: epoch 0057, iter [05000, 05004], lr: 0.085176, loss: 1.8607
2022-07-28 15:50:58 - train: epoch 057, train_loss: 1.9046
2022-07-28 15:55:12 - eval: epoch: 057, acc1: 60.628%, acc5: 83.810%, test_loss: 1.6405, per_image_load_time: 8.817ms, per_image_inference_time: 0.667ms
2022-07-28 15:55:13 - until epoch: 057, best_acc1: 60.628%
2022-07-28 15:55:13 - epoch 058 lr: 0.085173
2022-07-28 15:57:33 - train: epoch 0058, iter [00100, 05004], lr: 0.085108, loss: 1.8875
2022-07-28 15:59:32 - train: epoch 0058, iter [00200, 05004], lr: 0.085043, loss: 1.7988
2022-07-28 16:01:32 - train: epoch 0058, iter [00300, 05004], lr: 0.084978, loss: 2.0436
2022-07-28 16:03:29 - train: epoch 0058, iter [00400, 05004], lr: 0.084912, loss: 1.8160
2022-07-28 16:05:24 - train: epoch 0058, iter [00500, 05004], lr: 0.084847, loss: 1.7839
2022-07-28 16:07:19 - train: epoch 0058, iter [00600, 05004], lr: 0.084782, loss: 2.2730
2022-07-28 16:09:21 - train: epoch 0058, iter [00700, 05004], lr: 0.084716, loss: 2.0952
2022-07-28 16:11:15 - train: epoch 0058, iter [00800, 05004], lr: 0.084651, loss: 1.7909
2022-07-28 16:13:07 - train: epoch 0058, iter [00900, 05004], lr: 0.084586, loss: 1.7939
2022-07-28 16:15:04 - train: epoch 0058, iter [01000, 05004], lr: 0.084520, loss: 1.8518
2022-07-28 16:16:59 - train: epoch 0058, iter [01100, 05004], lr: 0.084455, loss: 1.6868
2022-07-28 16:18:53 - train: epoch 0058, iter [01200, 05004], lr: 0.084390, loss: 1.7574
2022-07-28 16:20:54 - train: epoch 0058, iter [01300, 05004], lr: 0.084325, loss: 2.1733
2022-07-28 16:22:53 - train: epoch 0058, iter [01400, 05004], lr: 0.084259, loss: 1.9968
2022-07-28 16:24:53 - train: epoch 0058, iter [01500, 05004], lr: 0.084194, loss: 1.7445
2022-07-28 16:26:49 - train: epoch 0058, iter [01600, 05004], lr: 0.084129, loss: 1.9137
2022-07-28 16:28:45 - train: epoch 0058, iter [01700, 05004], lr: 0.084064, loss: 1.9260
2022-07-28 16:30:40 - train: epoch 0058, iter [01800, 05004], lr: 0.083998, loss: 2.1763
2022-07-28 16:32:35 - train: epoch 0058, iter [01900, 05004], lr: 0.083933, loss: 2.0012
2022-07-28 16:34:35 - train: epoch 0058, iter [02000, 05004], lr: 0.083868, loss: 2.1400
2022-07-28 16:36:35 - train: epoch 0058, iter [02100, 05004], lr: 0.083803, loss: 1.8425
2022-07-28 16:38:30 - train: epoch 0058, iter [02200, 05004], lr: 0.083737, loss: 1.9473
2022-07-28 16:40:29 - train: epoch 0058, iter [02300, 05004], lr: 0.083672, loss: 1.8180
2022-07-28 16:42:27 - train: epoch 0058, iter [02400, 05004], lr: 0.083607, loss: 2.1452
2022-07-28 16:44:21 - train: epoch 0058, iter [02500, 05004], lr: 0.083542, loss: 1.8879
2022-07-28 16:46:11 - train: epoch 0058, iter [02600, 05004], lr: 0.083477, loss: 1.8416
2022-07-28 16:48:07 - train: epoch 0058, iter [02700, 05004], lr: 0.083411, loss: 2.0875
2022-07-28 16:50:03 - train: epoch 0058, iter [02800, 05004], lr: 0.083346, loss: 1.9149
2022-07-28 16:51:56 - train: epoch 0058, iter [02900, 05004], lr: 0.083281, loss: 1.7336
2022-07-28 16:53:51 - train: epoch 0058, iter [03000, 05004], lr: 0.083216, loss: 2.1809
2022-07-28 16:55:48 - train: epoch 0058, iter [03100, 05004], lr: 0.083151, loss: 1.9382
2022-07-28 16:57:46 - train: epoch 0058, iter [03200, 05004], lr: 0.083086, loss: 1.8367
2022-07-28 16:59:48 - train: epoch 0058, iter [03300, 05004], lr: 0.083021, loss: 1.8908
2022-07-28 17:01:51 - train: epoch 0058, iter [03400, 05004], lr: 0.082955, loss: 1.9981
2022-07-28 17:03:57 - train: epoch 0058, iter [03500, 05004], lr: 0.082890, loss: 1.7447
2022-07-28 17:05:55 - train: epoch 0058, iter [03600, 05004], lr: 0.082825, loss: 1.9210
2022-07-28 17:07:55 - train: epoch 0058, iter [03700, 05004], lr: 0.082760, loss: 2.0014
2022-07-28 17:09:54 - train: epoch 0058, iter [03800, 05004], lr: 0.082695, loss: 1.9969
2022-07-28 17:11:56 - train: epoch 0058, iter [03900, 05004], lr: 0.082630, loss: 2.0318
2022-07-28 17:13:56 - train: epoch 0058, iter [04000, 05004], lr: 0.082565, loss: 1.8423
2022-07-28 17:15:55 - train: epoch 0058, iter [04100, 05004], lr: 0.082500, loss: 1.8436
2022-07-28 17:17:54 - train: epoch 0058, iter [04200, 05004], lr: 0.082435, loss: 1.6686
2022-07-28 17:19:56 - train: epoch 0058, iter [04300, 05004], lr: 0.082370, loss: 1.9141
2022-07-28 17:21:48 - train: epoch 0058, iter [04400, 05004], lr: 0.082305, loss: 1.7153
2022-07-28 17:23:47 - train: epoch 0058, iter [04500, 05004], lr: 0.082240, loss: 1.7173
2022-07-28 17:25:48 - train: epoch 0058, iter [04600, 05004], lr: 0.082175, loss: 1.7801
2022-07-28 17:27:45 - train: epoch 0058, iter [04700, 05004], lr: 0.082110, loss: 1.8710
2022-07-28 17:29:50 - train: epoch 0058, iter [04800, 05004], lr: 0.082045, loss: 1.9427
2022-07-28 17:31:49 - train: epoch 0058, iter [04900, 05004], lr: 0.081980, loss: 1.8463
2022-07-28 17:33:43 - train: epoch 0058, iter [05000, 05004], lr: 0.081915, loss: 1.7174
2022-07-28 17:33:47 - train: epoch 058, train_loss: 1.8935
2022-07-28 17:38:00 - eval: epoch: 058, acc1: 60.698%, acc5: 83.968%, test_loss: 1.6302, per_image_load_time: 6.429ms, per_image_inference_time: 0.709ms
2022-07-28 17:38:01 - until epoch: 058, best_acc1: 60.698%
2022-07-28 17:38:01 - epoch 059 lr: 0.081911
2022-07-28 17:40:21 - train: epoch 0059, iter [00100, 05004], lr: 0.081847, loss: 1.9206
2022-07-28 17:42:21 - train: epoch 0059, iter [00200, 05004], lr: 0.081782, loss: 1.7649
2022-07-28 17:44:17 - train: epoch 0059, iter [00300, 05004], lr: 0.081717, loss: 1.6687
2022-07-28 17:46:16 - train: epoch 0059, iter [00400, 05004], lr: 0.081652, loss: 1.9454
2022-07-28 17:48:17 - train: epoch 0059, iter [00500, 05004], lr: 0.081587, loss: 1.9575
2022-07-28 17:50:14 - train: epoch 0059, iter [00600, 05004], lr: 0.081522, loss: 1.7208
2022-07-28 17:52:11 - train: epoch 0059, iter [00700, 05004], lr: 0.081457, loss: 1.7215
2022-07-28 17:54:17 - train: epoch 0059, iter [00800, 05004], lr: 0.081392, loss: 1.8025
2022-07-28 17:56:18 - train: epoch 0059, iter [00900, 05004], lr: 0.081327, loss: 1.7883
2022-07-28 17:58:19 - train: epoch 0059, iter [01000, 05004], lr: 0.081262, loss: 1.9952
2022-07-28 18:00:14 - train: epoch 0059, iter [01100, 05004], lr: 0.081197, loss: 2.2576
2022-07-28 18:02:11 - train: epoch 0059, iter [01200, 05004], lr: 0.081133, loss: 1.6681
2022-07-28 18:04:01 - train: epoch 0059, iter [01300, 05004], lr: 0.081068, loss: 2.0829
2022-07-28 18:06:00 - train: epoch 0059, iter [01400, 05004], lr: 0.081003, loss: 2.2413
2022-07-28 18:07:57 - train: epoch 0059, iter [01500, 05004], lr: 0.080938, loss: 1.9067
2022-07-28 18:09:55 - train: epoch 0059, iter [01600, 05004], lr: 0.080873, loss: 1.8303
2022-07-28 18:11:54 - train: epoch 0059, iter [01700, 05004], lr: 0.080808, loss: 1.8897
2022-07-28 18:13:51 - train: epoch 0059, iter [01800, 05004], lr: 0.080743, loss: 1.8022
2022-07-28 18:16:04 - train: epoch 0059, iter [01900, 05004], lr: 0.080678, loss: 1.8081
2022-07-28 18:18:03 - train: epoch 0059, iter [02000, 05004], lr: 0.080614, loss: 1.6914
2022-07-28 18:20:18 - train: epoch 0059, iter [02100, 05004], lr: 0.080549, loss: 2.1153
2022-07-28 18:22:16 - train: epoch 0059, iter [02200, 05004], lr: 0.080484, loss: 1.9415
2022-07-28 18:24:22 - train: epoch 0059, iter [02300, 05004], lr: 0.080419, loss: 1.9454
2022-07-28 18:26:23 - train: epoch 0059, iter [02400, 05004], lr: 0.080354, loss: 2.0516
2022-07-28 18:28:23 - train: epoch 0059, iter [02500, 05004], lr: 0.080290, loss: 2.0436
2022-07-28 18:30:24 - train: epoch 0059, iter [02600, 05004], lr: 0.080225, loss: 1.7911
2022-07-28 18:32:19 - train: epoch 0059, iter [02700, 05004], lr: 0.080160, loss: 1.8156
2022-07-28 18:34:16 - train: epoch 0059, iter [02800, 05004], lr: 0.080095, loss: 2.0982
2022-07-28 18:36:10 - train: epoch 0059, iter [02900, 05004], lr: 0.080031, loss: 1.7119
2022-07-28 18:38:07 - train: epoch 0059, iter [03000, 05004], lr: 0.079966, loss: 2.0396
2022-07-28 18:40:06 - train: epoch 0059, iter [03100, 05004], lr: 0.079901, loss: 1.7764
2022-07-28 18:42:01 - train: epoch 0059, iter [03200, 05004], lr: 0.079836, loss: 1.9289
2022-07-28 18:43:55 - train: epoch 0059, iter [03300, 05004], lr: 0.079772, loss: 2.0289
2022-07-28 18:45:50 - train: epoch 0059, iter [03400, 05004], lr: 0.079707, loss: 2.1343
2022-07-28 18:47:49 - train: epoch 0059, iter [03500, 05004], lr: 0.079642, loss: 1.8983
2022-07-28 18:49:47 - train: epoch 0059, iter [03600, 05004], lr: 0.079577, loss: 1.9166
2022-07-28 18:51:46 - train: epoch 0059, iter [03700, 05004], lr: 0.079513, loss: 1.8521
2022-07-28 18:53:48 - train: epoch 0059, iter [03800, 05004], lr: 0.079448, loss: 2.0142
2022-07-28 18:55:43 - train: epoch 0059, iter [03900, 05004], lr: 0.079383, loss: 1.6211
2022-07-28 18:57:44 - train: epoch 0059, iter [04000, 05004], lr: 0.079319, loss: 1.9941
2022-07-28 18:59:51 - train: epoch 0059, iter [04100, 05004], lr: 0.079254, loss: 1.6639
2022-07-28 19:01:55 - train: epoch 0059, iter [04200, 05004], lr: 0.079189, loss: 2.0279
2022-07-28 19:03:54 - train: epoch 0059, iter [04300, 05004], lr: 0.079125, loss: 2.1068
2022-07-28 19:05:54 - train: epoch 0059, iter [04400, 05004], lr: 0.079060, loss: 2.0015
2022-07-28 19:07:54 - train: epoch 0059, iter [04500, 05004], lr: 0.078996, loss: 2.1107
2022-07-28 19:09:46 - train: epoch 0059, iter [04600, 05004], lr: 0.078931, loss: 2.0662
2022-07-28 19:11:43 - train: epoch 0059, iter [04700, 05004], lr: 0.078866, loss: 1.9210
2022-07-28 19:13:35 - train: epoch 0059, iter [04800, 05004], lr: 0.078802, loss: 1.9625
2022-07-28 19:15:40 - train: epoch 0059, iter [04900, 05004], lr: 0.078737, loss: 1.7622
2022-07-28 19:17:42 - train: epoch 0059, iter [05000, 05004], lr: 0.078673, loss: 2.0652
2022-07-28 19:17:47 - train: epoch 059, train_loss: 1.8767
2022-07-28 19:22:18 - eval: epoch: 059, acc1: 60.974%, acc5: 84.244%, test_loss: 1.6149, per_image_load_time: 8.855ms, per_image_inference_time: 0.718ms
2022-07-28 19:22:18 - until epoch: 059, best_acc1: 60.974%
2022-07-28 19:22:18 - epoch 060 lr: 0.078669
2022-07-28 19:24:45 - train: epoch 0060, iter [00100, 05004], lr: 0.078605, loss: 1.6967
2022-07-28 19:26:55 - train: epoch 0060, iter [00200, 05004], lr: 0.078541, loss: 1.5965
2022-07-28 19:29:02 - train: epoch 0060, iter [00300, 05004], lr: 0.078476, loss: 1.6620
2022-07-28 19:30:58 - train: epoch 0060, iter [00400, 05004], lr: 0.078412, loss: 1.8396
2022-07-28 19:32:59 - train: epoch 0060, iter [00500, 05004], lr: 0.078347, loss: 2.0306
2022-07-28 19:35:02 - train: epoch 0060, iter [00600, 05004], lr: 0.078283, loss: 2.1909
2022-07-28 19:36:59 - train: epoch 0060, iter [00700, 05004], lr: 0.078218, loss: 1.9093
2022-07-28 19:38:56 - train: epoch 0060, iter [00800, 05004], lr: 0.078154, loss: 2.0681
2022-07-28 19:40:56 - train: epoch 0060, iter [00900, 05004], lr: 0.078089, loss: 1.6992
2022-07-28 19:42:49 - train: epoch 0060, iter [01000, 05004], lr: 0.078025, loss: 1.6326
2022-07-28 19:44:46 - train: epoch 0060, iter [01100, 05004], lr: 0.077960, loss: 1.7226
2022-07-28 19:46:43 - train: epoch 0060, iter [01200, 05004], lr: 0.077896, loss: 1.8639
2022-07-28 19:48:38 - train: epoch 0060, iter [01300, 05004], lr: 0.077831, loss: 1.7493
2022-07-28 19:50:29 - train: epoch 0060, iter [01400, 05004], lr: 0.077767, loss: 1.9400
2022-07-28 19:52:23 - train: epoch 0060, iter [01500, 05004], lr: 0.077703, loss: 1.9368
2022-07-28 19:54:07 - train: epoch 0060, iter [01600, 05004], lr: 0.077638, loss: 1.7125
2022-07-28 19:56:42 - train: epoch 0060, iter [01700, 05004], lr: 0.077574, loss: 1.8640
2022-07-28 19:58:48 - train: epoch 0060, iter [01800, 05004], lr: 0.077509, loss: 1.9361
2022-07-28 20:00:47 - train: epoch 0060, iter [01900, 05004], lr: 0.077445, loss: 2.1531
2022-07-28 20:02:45 - train: epoch 0060, iter [02000, 05004], lr: 0.077381, loss: 1.7807
2022-07-28 20:04:41 - train: epoch 0060, iter [02100, 05004], lr: 0.077316, loss: 2.0737
2022-07-28 20:06:39 - train: epoch 0060, iter [02200, 05004], lr: 0.077252, loss: 2.0981
2022-07-28 20:08:35 - train: epoch 0060, iter [02300, 05004], lr: 0.077188, loss: 1.6518
2022-07-28 20:10:27 - train: epoch 0060, iter [02400, 05004], lr: 0.077123, loss: 1.7542
2022-07-28 20:12:22 - train: epoch 0060, iter [02500, 05004], lr: 0.077059, loss: 1.9042
2022-07-28 20:14:16 - train: epoch 0060, iter [02600, 05004], lr: 0.076995, loss: 1.9129
2022-07-28 20:16:08 - train: epoch 0060, iter [02700, 05004], lr: 0.076930, loss: 1.8335
2022-07-28 20:18:02 - train: epoch 0060, iter [02800, 05004], lr: 0.076866, loss: 1.7639
2022-07-28 20:19:52 - train: epoch 0060, iter [02900, 05004], lr: 0.076802, loss: 1.9239
2022-07-28 20:21:47 - train: epoch 0060, iter [03000, 05004], lr: 0.076737, loss: 2.0079
2022-07-28 20:23:41 - train: epoch 0060, iter [03100, 05004], lr: 0.076673, loss: 1.9747
2022-07-28 20:25:39 - train: epoch 0060, iter [03200, 05004], lr: 0.076609, loss: 1.7466
2022-07-28 20:27:37 - train: epoch 0060, iter [03300, 05004], lr: 0.076545, loss: 1.5845
2022-07-28 20:29:26 - train: epoch 0060, iter [03400, 05004], lr: 0.076480, loss: 1.7252
2022-07-28 20:31:20 - train: epoch 0060, iter [03500, 05004], lr: 0.076416, loss: 1.9502
2022-07-28 20:33:09 - train: epoch 0060, iter [03600, 05004], lr: 0.076352, loss: 1.8973
2022-07-28 20:35:07 - train: epoch 0060, iter [03700, 05004], lr: 0.076288, loss: 1.7636
2022-07-28 20:37:02 - train: epoch 0060, iter [03800, 05004], lr: 0.076224, loss: 1.7794
2022-07-28 20:38:52 - train: epoch 0060, iter [03900, 05004], lr: 0.076159, loss: 1.9551
2022-07-28 20:40:50 - train: epoch 0060, iter [04000, 05004], lr: 0.076095, loss: 1.8763
2022-07-28 20:42:42 - train: epoch 0060, iter [04100, 05004], lr: 0.076031, loss: 1.9844
2022-07-28 20:44:34 - train: epoch 0060, iter [04200, 05004], lr: 0.075967, loss: 1.9478
2022-07-28 20:46:27 - train: epoch 0060, iter [04300, 05004], lr: 0.075903, loss: 1.8211
2022-07-28 20:48:20 - train: epoch 0060, iter [04400, 05004], lr: 0.075839, loss: 2.1140
2022-07-28 20:50:12 - train: epoch 0060, iter [04500, 05004], lr: 0.075774, loss: 2.1277
2022-07-28 20:52:03 - train: epoch 0060, iter [04600, 05004], lr: 0.075710, loss: 1.7422
2022-07-28 20:53:53 - train: epoch 0060, iter [04700, 05004], lr: 0.075646, loss: 1.6165
2022-07-28 20:55:49 - train: epoch 0060, iter [04800, 05004], lr: 0.075582, loss: 1.8415
2022-07-28 20:57:42 - train: epoch 0060, iter [04900, 05004], lr: 0.075518, loss: 1.7784
2022-07-28 20:59:34 - train: epoch 0060, iter [05000, 05004], lr: 0.075454, loss: 1.9857
2022-07-28 20:59:39 - train: epoch 060, train_loss: 1.8618
2022-07-28 21:03:56 - eval: epoch: 060, acc1: 60.430%, acc5: 83.876%, test_loss: 1.6357, per_image_load_time: 8.084ms, per_image_inference_time: 0.711ms
2022-07-28 21:03:57 - until epoch: 060, best_acc1: 60.974%
2022-07-28 21:03:57 - epoch 061 lr: 0.075451
2022-07-28 21:06:20 - train: epoch 0061, iter [00100, 05004], lr: 0.075387, loss: 1.4821
2022-07-28 21:08:22 - train: epoch 0061, iter [00200, 05004], lr: 0.075323, loss: 1.7778
2022-07-28 21:10:17 - train: epoch 0061, iter [00300, 05004], lr: 0.075259, loss: 1.8879
2022-07-28 21:12:10 - train: epoch 0061, iter [00400, 05004], lr: 0.075195, loss: 1.9603
2022-07-28 21:14:05 - train: epoch 0061, iter [00500, 05004], lr: 0.075131, loss: 2.1054
2022-07-28 21:15:58 - train: epoch 0061, iter [00600, 05004], lr: 0.075067, loss: 1.8499
2022-07-28 21:17:53 - train: epoch 0061, iter [00700, 05004], lr: 0.075003, loss: 1.6932
2022-07-28 21:19:44 - train: epoch 0061, iter [00800, 05004], lr: 0.074939, loss: 1.8150
2022-07-28 21:21:36 - train: epoch 0061, iter [00900, 05004], lr: 0.074875, loss: 1.7436
2022-07-28 21:23:30 - train: epoch 0061, iter [01000, 05004], lr: 0.074811, loss: 1.8804
2022-07-28 21:25:16 - train: epoch 0061, iter [01100, 05004], lr: 0.074747, loss: 1.5533
2022-07-28 21:27:12 - train: epoch 0061, iter [01200, 05004], lr: 0.074683, loss: 1.8652
2022-07-28 21:28:59 - train: epoch 0061, iter [01300, 05004], lr: 0.074620, loss: 1.8463
2022-07-28 21:30:48 - train: epoch 0061, iter [01400, 05004], lr: 0.074556, loss: 1.7835
2022-07-28 21:32:30 - train: epoch 0061, iter [01500, 05004], lr: 0.074492, loss: 2.1032
2022-07-28 21:34:37 - train: epoch 0061, iter [01600, 05004], lr: 0.074428, loss: 1.5654
2022-07-28 21:36:50 - train: epoch 0061, iter [01700, 05004], lr: 0.074364, loss: 1.9627
2022-07-28 21:38:59 - train: epoch 0061, iter [01800, 05004], lr: 0.074300, loss: 1.8331
2022-07-28 21:41:04 - train: epoch 0061, iter [01900, 05004], lr: 0.074236, loss: 2.1772
2022-07-28 21:43:03 - train: epoch 0061, iter [02000, 05004], lr: 0.074172, loss: 1.8714
2022-07-28 21:45:04 - train: epoch 0061, iter [02100, 05004], lr: 0.074109, loss: 2.0890
2022-07-28 21:47:03 - train: epoch 0061, iter [02200, 05004], lr: 0.074045, loss: 1.7517
2022-07-28 21:49:02 - train: epoch 0061, iter [02300, 05004], lr: 0.073981, loss: 1.8662
2022-07-28 21:51:01 - train: epoch 0061, iter [02400, 05004], lr: 0.073917, loss: 1.8109
2022-07-28 21:52:59 - train: epoch 0061, iter [02500, 05004], lr: 0.073853, loss: 1.8219
2022-07-28 21:54:56 - train: epoch 0061, iter [02600, 05004], lr: 0.073790, loss: 2.1205
2022-07-28 21:56:51 - train: epoch 0061, iter [02700, 05004], lr: 0.073726, loss: 2.0621
2022-07-28 21:58:51 - train: epoch 0061, iter [02800, 05004], lr: 0.073662, loss: 1.8478
2022-07-28 22:00:51 - train: epoch 0061, iter [02900, 05004], lr: 0.073598, loss: 2.0081
2022-07-28 22:02:48 - train: epoch 0061, iter [03000, 05004], lr: 0.073534, loss: 1.8040
2022-07-28 22:04:47 - train: epoch 0061, iter [03100, 05004], lr: 0.073471, loss: 1.8090
2022-07-28 22:06:45 - train: epoch 0061, iter [03200, 05004], lr: 0.073407, loss: 1.7871
2022-07-28 22:08:41 - train: epoch 0061, iter [03300, 05004], lr: 0.073343, loss: 1.8108
2022-07-28 22:10:36 - train: epoch 0061, iter [03400, 05004], lr: 0.073280, loss: 1.8481
2022-07-28 22:12:32 - train: epoch 0061, iter [03500, 05004], lr: 0.073216, loss: 2.0599
2022-07-28 22:14:28 - train: epoch 0061, iter [03600, 05004], lr: 0.073152, loss: 1.7127
2022-07-28 22:16:24 - train: epoch 0061, iter [03700, 05004], lr: 0.073089, loss: 1.7827
2022-07-28 22:18:21 - train: epoch 0061, iter [03800, 05004], lr: 0.073025, loss: 1.8159
2022-07-28 22:20:20 - train: epoch 0061, iter [03900, 05004], lr: 0.072961, loss: 1.9131
2022-07-28 22:22:18 - train: epoch 0061, iter [04000, 05004], lr: 0.072898, loss: 1.9609
2022-07-28 22:24:18 - train: epoch 0061, iter [04100, 05004], lr: 0.072834, loss: 2.2117
2022-07-28 22:26:13 - train: epoch 0061, iter [04200, 05004], lr: 0.072771, loss: 1.8870
2022-07-28 22:28:09 - train: epoch 0061, iter [04300, 05004], lr: 0.072707, loss: 1.7686
2022-07-28 22:30:06 - train: epoch 0061, iter [04400, 05004], lr: 0.072643, loss: 1.7482
2022-07-28 22:31:56 - train: epoch 0061, iter [04500, 05004], lr: 0.072580, loss: 1.9868
2022-07-28 22:33:56 - train: epoch 0061, iter [04600, 05004], lr: 0.072516, loss: 1.7932
2022-07-28 22:35:52 - train: epoch 0061, iter [04700, 05004], lr: 0.072453, loss: 1.8535
2022-07-28 22:37:46 - train: epoch 0061, iter [04800, 05004], lr: 0.072389, loss: 1.8453
2022-07-28 22:39:45 - train: epoch 0061, iter [04900, 05004], lr: 0.072326, loss: 1.9864
2022-07-28 22:41:34 - train: epoch 0061, iter [05000, 05004], lr: 0.072262, loss: 2.0699
2022-07-28 22:41:38 - train: epoch 061, train_loss: 1.8441
2022-07-28 22:46:07 - eval: epoch: 061, acc1: 61.134%, acc5: 83.938%, test_loss: 1.6177, per_image_load_time: 5.200ms, per_image_inference_time: 0.712ms
2022-07-28 22:46:08 - until epoch: 061, best_acc1: 61.134%
2022-07-28 22:46:08 - epoch 062 lr: 0.072259
2022-07-28 22:48:30 - train: epoch 0062, iter [00100, 05004], lr: 0.072196, loss: 2.0112
2022-07-28 22:50:29 - train: epoch 0062, iter [00200, 05004], lr: 0.072133, loss: 1.9190
2022-07-28 22:52:34 - train: epoch 0062, iter [00300, 05004], lr: 0.072069, loss: 1.8281
2022-07-28 22:54:38 - train: epoch 0062, iter [00400, 05004], lr: 0.072006, loss: 1.9086
2022-07-28 22:56:37 - train: epoch 0062, iter [00500, 05004], lr: 0.071942, loss: 1.7653
2022-07-28 22:58:40 - train: epoch 0062, iter [00600, 05004], lr: 0.071879, loss: 1.4624
2022-07-28 23:00:36 - train: epoch 0062, iter [00700, 05004], lr: 0.071816, loss: 1.8341
2022-07-28 23:02:32 - train: epoch 0062, iter [00800, 05004], lr: 0.071752, loss: 2.1230
2022-07-28 23:04:31 - train: epoch 0062, iter [00900, 05004], lr: 0.071689, loss: 1.8448
2022-07-28 23:06:32 - train: epoch 0062, iter [01000, 05004], lr: 0.071625, loss: 1.9246
2022-07-28 23:08:30 - train: epoch 0062, iter [01100, 05004], lr: 0.071562, loss: 1.8137
2022-07-28 23:10:29 - train: epoch 0062, iter [01200, 05004], lr: 0.071499, loss: 1.8673
2022-07-28 23:12:33 - train: epoch 0062, iter [01300, 05004], lr: 0.071435, loss: 1.7544
2022-07-28 23:14:50 - train: epoch 0062, iter [01400, 05004], lr: 0.071372, loss: 1.8084
2022-07-28 23:16:51 - train: epoch 0062, iter [01500, 05004], lr: 0.071309, loss: 1.9606
2022-07-28 23:18:55 - train: epoch 0062, iter [01600, 05004], lr: 0.071245, loss: 1.9798
2022-07-28 23:20:58 - train: epoch 0062, iter [01700, 05004], lr: 0.071182, loss: 1.8353
2022-07-28 23:23:00 - train: epoch 0062, iter [01800, 05004], lr: 0.071119, loss: 1.6594
2022-07-28 23:25:00 - train: epoch 0062, iter [01900, 05004], lr: 0.071056, loss: 1.7484
2022-07-28 23:27:04 - train: epoch 0062, iter [02000, 05004], lr: 0.070992, loss: 1.7907
2022-07-28 23:29:05 - train: epoch 0062, iter [02100, 05004], lr: 0.070929, loss: 1.9635
2022-07-28 23:31:07 - train: epoch 0062, iter [02200, 05004], lr: 0.070866, loss: 1.4909
2022-07-28 23:33:06 - train: epoch 0062, iter [02300, 05004], lr: 0.070803, loss: 1.7703
2022-07-28 23:35:02 - train: epoch 0062, iter [02400, 05004], lr: 0.070739, loss: 1.7586
2022-07-28 23:36:56 - train: epoch 0062, iter [02500, 05004], lr: 0.070676, loss: 1.9666
2022-07-28 23:38:50 - train: epoch 0062, iter [02600, 05004], lr: 0.070613, loss: 1.7356
2022-07-28 23:40:42 - train: epoch 0062, iter [02700, 05004], lr: 0.070550, loss: 1.8748
2022-07-28 23:42:40 - train: epoch 0062, iter [02800, 05004], lr: 0.070487, loss: 2.0353
2022-07-28 23:44:34 - train: epoch 0062, iter [02900, 05004], lr: 0.070424, loss: 1.9624
2022-07-28 23:46:33 - train: epoch 0062, iter [03000, 05004], lr: 0.070361, loss: 1.8472
2022-07-28 23:48:28 - train: epoch 0062, iter [03100, 05004], lr: 0.070297, loss: 1.8771
2022-07-28 23:50:21 - train: epoch 0062, iter [03200, 05004], lr: 0.070234, loss: 1.6931
2022-07-28 23:52:15 - train: epoch 0062, iter [03300, 05004], lr: 0.070171, loss: 1.8153
2022-07-28 23:54:06 - train: epoch 0062, iter [03400, 05004], lr: 0.070108, loss: 1.8213
2022-07-28 23:56:05 - train: epoch 0062, iter [03500, 05004], lr: 0.070045, loss: 1.8864
2022-07-28 23:57:54 - train: epoch 0062, iter [03600, 05004], lr: 0.069982, loss: 1.8185
2022-07-28 23:59:47 - train: epoch 0062, iter [03700, 05004], lr: 0.069919, loss: 1.7023
2022-07-29 00:01:41 - train: epoch 0062, iter [03800, 05004], lr: 0.069856, loss: 1.8352
2022-07-29 00:03:35 - train: epoch 0062, iter [03900, 05004], lr: 0.069793, loss: 2.1881
2022-07-29 00:05:26 - train: epoch 0062, iter [04000, 05004], lr: 0.069730, loss: 1.7878
2022-07-29 00:07:18 - train: epoch 0062, iter [04100, 05004], lr: 0.069667, loss: 1.9751
2022-07-29 00:09:12 - train: epoch 0062, iter [04200, 05004], lr: 0.069604, loss: 1.6402
2022-07-29 00:11:03 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.6468
2022-07-29 00:12:55 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.7530
2022-07-29 00:14:51 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.7813
2022-07-29 00:16:43 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.6377
2022-07-29 00:18:34 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.7063
2022-07-29 00:20:26 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.6784
2022-07-29 00:22:19 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.6650
2022-07-29 00:24:11 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.9950
2022-07-29 00:24:15 - train: epoch 062, train_loss: 1.8286
2022-07-29 00:28:17 - eval: epoch: 062, acc1: 61.590%, acc5: 84.446%, test_loss: 1.5921, per_image_load_time: 6.426ms, per_image_inference_time: 0.703ms
2022-07-29 00:28:18 - until epoch: 062, best_acc1: 61.590%
2022-07-29 00:28:18 - epoch 063 lr: 0.069098
2022-07-29 00:30:31 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.7592
2022-07-29 00:32:26 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.8342
2022-07-29 00:34:20 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.7873
2022-07-29 00:36:12 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.7349
2022-07-29 00:38:07 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.6554
2022-07-29 00:39:56 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.8328
2022-07-29 00:41:49 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.9464
2022-07-29 00:43:41 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.8806
2022-07-29 00:45:26 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.8082
2022-07-29 00:47:18 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 2.1382
2022-07-29 00:49:01 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.6789
2022-07-29 00:51:22 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.8561
2022-07-29 00:53:23 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.6319
2022-07-29 00:55:27 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 2.0413
2022-07-29 00:57:25 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.8134
2022-07-29 00:59:15 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.8553
2022-07-29 01:01:09 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.5340
2022-07-29 01:03:00 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.8668
2022-07-29 01:04:46 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.9662
2022-07-29 01:06:39 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.7674
2022-07-29 01:08:24 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.7876
2022-07-29 01:10:13 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 2.1282
2022-07-29 01:12:02 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.7632
2022-07-29 01:13:50 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.8084
2022-07-29 01:15:36 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.7787
2022-07-29 01:17:22 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.6544
2022-07-29 01:19:11 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.9653
2022-07-29 01:20:57 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.9345
2022-07-29 01:22:47 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.8875
2022-07-29 01:24:29 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 2.2854
2022-07-29 01:26:16 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 2.0244
2022-07-29 01:28:03 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 2.0024
2022-07-29 01:29:50 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.7013
2022-07-29 01:31:38 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.8179
2022-07-29 01:33:25 - train: epoch 0063, iter [03500, 05004], lr: 0.066907, loss: 2.0523
2022-07-29 01:35:13 - train: epoch 0063, iter [03600, 05004], lr: 0.066845, loss: 1.7835
2022-07-29 01:36:57 - train: epoch 0063, iter [03700, 05004], lr: 0.066782, loss: 1.7925
2022-07-29 01:38:44 - train: epoch 0063, iter [03800, 05004], lr: 0.066720, loss: 1.9307
2022-07-29 01:40:30 - train: epoch 0063, iter [03900, 05004], lr: 0.066658, loss: 1.7309
2022-07-29 01:42:19 - train: epoch 0063, iter [04000, 05004], lr: 0.066595, loss: 1.7001
2022-07-29 01:44:07 - train: epoch 0063, iter [04100, 05004], lr: 0.066533, loss: 1.8857
2022-07-29 01:45:53 - train: epoch 0063, iter [04200, 05004], lr: 0.066471, loss: 1.7456
2022-07-29 01:47:40 - train: epoch 0063, iter [04300, 05004], lr: 0.066409, loss: 1.7289
2022-07-29 01:49:26 - train: epoch 0063, iter [04400, 05004], lr: 0.066346, loss: 1.7501
2022-07-29 01:51:15 - train: epoch 0063, iter [04500, 05004], lr: 0.066284, loss: 1.7935
2022-07-29 01:52:58 - train: epoch 0063, iter [04600, 05004], lr: 0.066222, loss: 1.7911
2022-07-29 01:54:46 - train: epoch 0063, iter [04700, 05004], lr: 0.066160, loss: 1.8637
2022-07-29 01:56:34 - train: epoch 0063, iter [04800, 05004], lr: 0.066097, loss: 1.8059
2022-07-29 01:58:20 - train: epoch 0063, iter [04900, 05004], lr: 0.066035, loss: 1.9331
2022-07-29 02:00:03 - train: epoch 0063, iter [05000, 05004], lr: 0.065973, loss: 1.6863
2022-07-29 02:00:07 - train: epoch 063, train_loss: 1.8110
2022-07-29 02:04:05 - eval: epoch: 063, acc1: 62.212%, acc5: 84.934%, test_loss: 1.5627, per_image_load_time: 6.253ms, per_image_inference_time: 0.531ms
2022-07-29 02:04:05 - until epoch: 063, best_acc1: 62.212%
2022-07-29 02:04:05 - epoch 064 lr: 0.065970
2022-07-29 02:06:14 - train: epoch 0064, iter [00100, 05004], lr: 0.065909, loss: 1.5456
2022-07-29 02:08:05 - train: epoch 0064, iter [00200, 05004], lr: 0.065846, loss: 1.7563
2022-07-29 02:09:49 - train: epoch 0064, iter [00300, 05004], lr: 0.065784, loss: 1.6247
2022-07-29 02:11:37 - train: epoch 0064, iter [00400, 05004], lr: 0.065722, loss: 1.5810
2022-07-29 02:13:26 - train: epoch 0064, iter [00500, 05004], lr: 0.065660, loss: 1.7437
2022-07-29 02:15:15 - train: epoch 0064, iter [00600, 05004], lr: 0.065598, loss: 1.8431
2022-07-29 02:17:00 - train: epoch 0064, iter [00700, 05004], lr: 0.065536, loss: 1.8866
2022-07-29 02:18:48 - train: epoch 0064, iter [00800, 05004], lr: 0.065474, loss: 1.4125
2022-07-29 02:20:34 - train: epoch 0064, iter [00900, 05004], lr: 0.065412, loss: 1.5563
2022-07-29 02:22:19 - train: epoch 0064, iter [01000, 05004], lr: 0.065350, loss: 1.8793
2022-07-29 02:24:14 - train: epoch 0064, iter [01100, 05004], lr: 0.065288, loss: 1.5589
2022-07-29 02:26:26 - train: epoch 0064, iter [01200, 05004], lr: 0.065226, loss: 1.8074
2022-07-29 02:28:24 - train: epoch 0064, iter [01300, 05004], lr: 0.065164, loss: 1.8892
2022-07-29 02:30:22 - train: epoch 0064, iter [01400, 05004], lr: 0.065102, loss: 1.9930
2022-07-29 02:32:15 - train: epoch 0064, iter [01500, 05004], lr: 0.065040, loss: 1.7776
2022-07-29 02:34:09 - train: epoch 0064, iter [01600, 05004], lr: 0.064978, loss: 1.7593
2022-07-29 02:36:00 - train: epoch 0064, iter [01700, 05004], lr: 0.064916, loss: 1.5922
2022-07-29 02:37:49 - train: epoch 0064, iter [01800, 05004], lr: 0.064855, loss: 1.5786
2022-07-29 02:39:44 - train: epoch 0064, iter [01900, 05004], lr: 0.064793, loss: 1.9031
2022-07-29 02:41:29 - train: epoch 0064, iter [02000, 05004], lr: 0.064731, loss: 1.7105
2022-07-29 02:43:19 - train: epoch 0064, iter [02100, 05004], lr: 0.064669, loss: 1.9724
2022-07-29 02:45:03 - train: epoch 0064, iter [02200, 05004], lr: 0.064607, loss: 1.9694
2022-07-29 02:46:54 - train: epoch 0064, iter [02300, 05004], lr: 0.064545, loss: 1.9004
2022-07-29 02:48:41 - train: epoch 0064, iter [02400, 05004], lr: 0.064484, loss: 1.8862
2022-07-29 02:50:25 - train: epoch 0064, iter [02500, 05004], lr: 0.064422, loss: 1.8792
2022-07-29 02:52:15 - train: epoch 0064, iter [02600, 05004], lr: 0.064360, loss: 1.7081
2022-07-29 02:54:01 - train: epoch 0064, iter [02700, 05004], lr: 0.064298, loss: 1.6855
2022-07-29 02:55:54 - train: epoch 0064, iter [02800, 05004], lr: 0.064237, loss: 1.8585
2022-07-29 02:57:46 - train: epoch 0064, iter [02900, 05004], lr: 0.064175, loss: 2.1564
2022-07-29 02:59:40 - train: epoch 0064, iter [03000, 05004], lr: 0.064113, loss: 1.7508
2022-07-29 03:01:30 - train: epoch 0064, iter [03100, 05004], lr: 0.064052, loss: 1.9059
2022-07-29 03:03:26 - train: epoch 0064, iter [03200, 05004], lr: 0.063990, loss: 1.8967
2022-07-29 03:05:16 - train: epoch 0064, iter [03300, 05004], lr: 0.063928, loss: 1.8088
2022-07-29 03:07:20 - train: epoch 0064, iter [03400, 05004], lr: 0.063867, loss: 1.6844
2022-07-29 03:09:19 - train: epoch 0064, iter [03500, 05004], lr: 0.063805, loss: 1.7515
2022-07-29 03:11:22 - train: epoch 0064, iter [03600, 05004], lr: 0.063743, loss: 1.7909
2022-07-29 03:13:21 - train: epoch 0064, iter [03700, 05004], lr: 0.063682, loss: 1.5795
2022-07-29 03:15:24 - train: epoch 0064, iter [03800, 05004], lr: 0.063620, loss: 1.9527
2022-07-29 03:17:25 - train: epoch 0064, iter [03900, 05004], lr: 0.063559, loss: 1.7190
2022-07-29 03:19:25 - train: epoch 0064, iter [04000, 05004], lr: 0.063497, loss: 1.7829
2022-07-29 03:21:26 - train: epoch 0064, iter [04100, 05004], lr: 0.063436, loss: 1.5697
2022-07-29 03:23:24 - train: epoch 0064, iter [04200, 05004], lr: 0.063374, loss: 1.6972
2022-07-29 03:25:24 - train: epoch 0064, iter [04300, 05004], lr: 0.063313, loss: 1.7910
2022-07-29 03:27:22 - train: epoch 0064, iter [04400, 05004], lr: 0.063251, loss: 1.6973
2022-07-29 03:29:26 - train: epoch 0064, iter [04500, 05004], lr: 0.063190, loss: 1.7326
2022-07-29 03:31:24 - train: epoch 0064, iter [04600, 05004], lr: 0.063128, loss: 2.0584
2022-07-29 03:33:24 - train: epoch 0064, iter [04700, 05004], lr: 0.063067, loss: 2.1458
2022-07-29 03:35:26 - train: epoch 0064, iter [04800, 05004], lr: 0.063005, loss: 1.7524
2022-07-29 03:37:23 - train: epoch 0064, iter [04900, 05004], lr: 0.062944, loss: 1.9372
2022-07-29 03:39:20 - train: epoch 0064, iter [05000, 05004], lr: 0.062883, loss: 1.4938
2022-07-29 03:39:24 - train: epoch 064, train_loss: 1.7981
2022-07-29 03:43:28 - eval: epoch: 064, acc1: 62.062%, acc5: 84.714%, test_loss: 1.5708, per_image_load_time: 4.877ms, per_image_inference_time: 0.591ms
2022-07-29 03:43:28 - until epoch: 064, best_acc1: 62.212%
2022-07-29 03:43:28 - epoch 065 lr: 0.062880
2022-07-29 03:45:37 - train: epoch 0065, iter [00100, 05004], lr: 0.062819, loss: 1.6321
2022-07-29 03:47:31 - train: epoch 0065, iter [00200, 05004], lr: 0.062758, loss: 1.6923
2022-07-29 03:49:26 - train: epoch 0065, iter [00300, 05004], lr: 0.062696, loss: 1.7929
2022-07-29 03:51:25 - train: epoch 0065, iter [00400, 05004], lr: 0.062635, loss: 1.9138
2022-07-29 03:53:18 - train: epoch 0065, iter [00500, 05004], lr: 0.062574, loss: 1.7603
2022-07-29 03:55:11 - train: epoch 0065, iter [00600, 05004], lr: 0.062512, loss: 1.7156
2022-07-29 03:57:05 - train: epoch 0065, iter [00700, 05004], lr: 0.062451, loss: 1.7846
2022-07-29 03:59:04 - train: epoch 0065, iter [00800, 05004], lr: 0.062390, loss: 1.6948
2022-07-29 04:00:59 - train: epoch 0065, iter [00900, 05004], lr: 0.062329, loss: 1.8566
2022-07-29 04:02:56 - train: epoch 0065, iter [01000, 05004], lr: 0.062267, loss: 1.8211
2022-07-29 04:05:00 - train: epoch 0065, iter [01100, 05004], lr: 0.062206, loss: 1.7612
2022-07-29 04:07:02 - train: epoch 0065, iter [01200, 05004], lr: 0.062145, loss: 1.8830
2022-07-29 04:09:06 - train: epoch 0065, iter [01300, 05004], lr: 0.062084, loss: 1.7757
2022-07-29 04:11:01 - train: epoch 0065, iter [01400, 05004], lr: 0.062023, loss: 1.6148
2022-07-29 04:12:57 - train: epoch 0065, iter [01500, 05004], lr: 0.061962, loss: 1.7485
2022-07-29 04:14:57 - train: epoch 0065, iter [01600, 05004], lr: 0.061901, loss: 1.9456
2022-07-29 04:16:57 - train: epoch 0065, iter [01700, 05004], lr: 0.061839, loss: 1.6876
2022-07-29 04:18:55 - train: epoch 0065, iter [01800, 05004], lr: 0.061778, loss: 1.5263
2022-07-29 04:20:54 - train: epoch 0065, iter [01900, 05004], lr: 0.061717, loss: 1.5285
2022-07-29 04:22:54 - train: epoch 0065, iter [02000, 05004], lr: 0.061656, loss: 1.8096
2022-07-29 04:24:52 - train: epoch 0065, iter [02100, 05004], lr: 0.061595, loss: 1.9526
2022-07-29 04:26:45 - train: epoch 0065, iter [02200, 05004], lr: 0.061534, loss: 1.7473
2022-07-29 04:28:40 - train: epoch 0065, iter [02300, 05004], lr: 0.061473, loss: 1.5850
2022-07-29 04:30:34 - train: epoch 0065, iter [02400, 05004], lr: 0.061412, loss: 1.6956
2022-07-29 04:32:29 - train: epoch 0065, iter [02500, 05004], lr: 0.061351, loss: 1.8388
2022-07-29 04:34:30 - train: epoch 0065, iter [02600, 05004], lr: 0.061290, loss: 1.9618
2022-07-29 04:36:24 - train: epoch 0065, iter [02700, 05004], lr: 0.061229, loss: 1.8894
2022-07-29 04:38:16 - train: epoch 0065, iter [02800, 05004], lr: 0.061169, loss: 1.7341
2022-07-29 04:40:14 - train: epoch 0065, iter [02900, 05004], lr: 0.061108, loss: 1.7274
2022-07-29 04:42:08 - train: epoch 0065, iter [03000, 05004], lr: 0.061047, loss: 1.7666
2022-07-29 04:44:03 - train: epoch 0065, iter [03100, 05004], lr: 0.060986, loss: 1.8487
2022-07-29 04:45:57 - train: epoch 0065, iter [03200, 05004], lr: 0.060925, loss: 2.1261
2022-07-29 04:47:55 - train: epoch 0065, iter [03300, 05004], lr: 0.060864, loss: 1.8655
2022-07-29 04:49:47 - train: epoch 0065, iter [03400, 05004], lr: 0.060803, loss: 1.8619
2022-07-29 04:51:45 - train: epoch 0065, iter [03500, 05004], lr: 0.060743, loss: 1.9843
2022-07-29 04:53:42 - train: epoch 0065, iter [03600, 05004], lr: 0.060682, loss: 1.7436
2022-07-29 04:55:36 - train: epoch 0065, iter [03700, 05004], lr: 0.060621, loss: 1.6751
2022-07-29 04:57:29 - train: epoch 0065, iter [03800, 05004], lr: 0.060560, loss: 1.8133
2022-07-29 04:59:25 - train: epoch 0065, iter [03900, 05004], lr: 0.060500, loss: 1.9629
2022-07-29 05:01:19 - train: epoch 0065, iter [04000, 05004], lr: 0.060439, loss: 1.9944
2022-07-29 05:03:13 - train: epoch 0065, iter [04100, 05004], lr: 0.060378, loss: 1.9483
2022-07-29 05:05:07 - train: epoch 0065, iter [04200, 05004], lr: 0.060318, loss: 1.8502
2022-07-29 05:07:08 - train: epoch 0065, iter [04300, 05004], lr: 0.060257, loss: 1.6829
2022-07-29 05:09:03 - train: epoch 0065, iter [04400, 05004], lr: 0.060196, loss: 1.7109
2022-07-29 05:10:54 - train: epoch 0065, iter [04500, 05004], lr: 0.060136, loss: 1.5686
2022-07-29 05:12:52 - train: epoch 0065, iter [04600, 05004], lr: 0.060075, loss: 1.7774
2022-07-29 05:14:48 - train: epoch 0065, iter [04700, 05004], lr: 0.060015, loss: 1.8989
2022-07-29 05:16:46 - train: epoch 0065, iter [04800, 05004], lr: 0.059954, loss: 1.6158
2022-07-29 05:18:40 - train: epoch 0065, iter [04900, 05004], lr: 0.059893, loss: 1.8896
2022-07-29 05:20:30 - train: epoch 0065, iter [05000, 05004], lr: 0.059833, loss: 1.7986
2022-07-29 05:20:34 - train: epoch 065, train_loss: 1.7802
2022-07-29 05:24:33 - eval: epoch: 065, acc1: 63.060%, acc5: 85.314%, test_loss: 1.5342, per_image_load_time: 3.001ms, per_image_inference_time: 0.607ms
2022-07-29 05:24:33 - until epoch: 065, best_acc1: 63.060%
2022-07-29 05:24:33 - epoch 066 lr: 0.059830
2022-07-29 05:26:47 - train: epoch 0066, iter [00100, 05004], lr: 0.059770, loss: 1.6145
2022-07-29 05:28:36 - train: epoch 0066, iter [00200, 05004], lr: 0.059709, loss: 2.0023
2022-07-29 05:30:35 - train: epoch 0066, iter [00300, 05004], lr: 0.059649, loss: 1.8533
2022-07-29 05:32:27 - train: epoch 0066, iter [00400, 05004], lr: 0.059589, loss: 1.5556
2022-07-29 05:34:26 - train: epoch 0066, iter [00500, 05004], lr: 0.059528, loss: 1.8346
2022-07-29 05:36:20 - train: epoch 0066, iter [00600, 05004], lr: 0.059468, loss: 1.7163
2022-07-29 05:38:13 - train: epoch 0066, iter [00700, 05004], lr: 0.059407, loss: 1.8006
2022-07-29 05:40:09 - train: epoch 0066, iter [00800, 05004], lr: 0.059347, loss: 1.8103
2022-07-29 05:42:05 - train: epoch 0066, iter [00900, 05004], lr: 0.059286, loss: 1.8729
2022-07-29 05:44:06 - train: epoch 0066, iter [01000, 05004], lr: 0.059226, loss: 1.7830
2022-07-29 05:46:25 - train: epoch 0066, iter [01100, 05004], lr: 0.059166, loss: 1.7802
2022-07-29 05:48:19 - train: epoch 0066, iter [01200, 05004], lr: 0.059105, loss: 1.8829
2022-07-29 05:50:21 - train: epoch 0066, iter [01300, 05004], lr: 0.059045, loss: 1.7997
2022-07-29 05:52:20 - train: epoch 0066, iter [01400, 05004], lr: 0.058985, loss: 1.8107
2022-07-29 05:54:28 - train: epoch 0066, iter [01500, 05004], lr: 0.058925, loss: 1.7715
2022-07-29 05:56:22 - train: epoch 0066, iter [01600, 05004], lr: 0.058864, loss: 1.7200
2022-07-29 05:58:21 - train: epoch 0066, iter [01700, 05004], lr: 0.058804, loss: 1.5594
2022-07-29 06:00:20 - train: epoch 0066, iter [01800, 05004], lr: 0.058744, loss: 1.3557
2022-07-29 06:02:19 - train: epoch 0066, iter [01900, 05004], lr: 0.058684, loss: 1.9378
2022-07-29 06:04:09 - train: epoch 0066, iter [02000, 05004], lr: 0.058624, loss: 1.6583
2022-07-29 06:06:03 - train: epoch 0066, iter [02100, 05004], lr: 0.058563, loss: 1.7695
2022-07-29 06:08:01 - train: epoch 0066, iter [02200, 05004], lr: 0.058503, loss: 1.6377
2022-07-29 06:09:54 - train: epoch 0066, iter [02300, 05004], lr: 0.058443, loss: 1.9304
2022-07-29 06:11:53 - train: epoch 0066, iter [02400, 05004], lr: 0.058383, loss: 1.8403
2022-07-29 06:13:52 - train: epoch 0066, iter [02500, 05004], lr: 0.058323, loss: 1.9302
2022-07-29 06:15:47 - train: epoch 0066, iter [02600, 05004], lr: 0.058263, loss: 1.8959
2022-07-29 06:17:41 - train: epoch 0066, iter [02700, 05004], lr: 0.058203, loss: 1.8052
2022-07-29 06:19:35 - train: epoch 0066, iter [02800, 05004], lr: 0.058143, loss: 1.6293
2022-07-29 06:21:30 - train: epoch 0066, iter [02900, 05004], lr: 0.058083, loss: 1.7504
2022-07-29 06:23:22 - train: epoch 0066, iter [03000, 05004], lr: 0.058023, loss: 1.7649
2022-07-29 06:25:19 - train: epoch 0066, iter [03100, 05004], lr: 0.057963, loss: 1.9362
2022-07-29 06:27:15 - train: epoch 0066, iter [03200, 05004], lr: 0.057903, loss: 1.6125
2022-07-29 06:29:07 - train: epoch 0066, iter [03300, 05004], lr: 0.057843, loss: 1.6307
2022-07-29 06:31:04 - train: epoch 0066, iter [03400, 05004], lr: 0.057783, loss: 1.8992
2022-07-29 06:32:56 - train: epoch 0066, iter [03500, 05004], lr: 0.057723, loss: 1.8213
2022-07-29 06:34:53 - train: epoch 0066, iter [03600, 05004], lr: 0.057663, loss: 1.8621
2022-07-29 06:36:44 - train: epoch 0066, iter [03700, 05004], lr: 0.057603, loss: 1.7950
2022-07-29 06:38:40 - train: epoch 0066, iter [03800, 05004], lr: 0.057544, loss: 1.7303
2022-07-29 06:40:33 - train: epoch 0066, iter [03900, 05004], lr: 0.057484, loss: 1.5689
2022-07-29 06:42:31 - train: epoch 0066, iter [04000, 05004], lr: 0.057424, loss: 1.7953
2022-07-29 06:44:27 - train: epoch 0066, iter [04100, 05004], lr: 0.057364, loss: 1.6142
2022-07-29 06:46:15 - train: epoch 0066, iter [04200, 05004], lr: 0.057304, loss: 1.6775
2022-07-29 06:48:14 - train: epoch 0066, iter [04300, 05004], lr: 0.057245, loss: 1.4580
2022-07-29 06:50:10 - train: epoch 0066, iter [04400, 05004], lr: 0.057185, loss: 1.8066
2022-07-29 06:52:00 - train: epoch 0066, iter [04500, 05004], lr: 0.057125, loss: 1.8672
2022-07-29 06:53:59 - train: epoch 0066, iter [04600, 05004], lr: 0.057066, loss: 1.8211
2022-07-29 06:55:48 - train: epoch 0066, iter [04700, 05004], lr: 0.057006, loss: 1.7305
2022-07-29 06:57:45 - train: epoch 0066, iter [04800, 05004], lr: 0.056946, loss: 1.8875
2022-07-29 06:59:40 - train: epoch 0066, iter [04900, 05004], lr: 0.056887, loss: 1.5844
2022-07-29 07:01:35 - train: epoch 0066, iter [05000, 05004], lr: 0.056827, loss: 1.7028
2022-07-29 07:01:39 - train: epoch 066, train_loss: 1.7589
2022-07-29 07:05:45 - eval: epoch: 066, acc1: 63.304%, acc5: 85.720%, test_loss: 1.5091, per_image_load_time: 8.722ms, per_image_inference_time: 0.566ms
2022-07-29 07:05:46 - until epoch: 066, best_acc1: 63.304%
2022-07-29 07:05:46 - epoch 067 lr: 0.056824
2022-07-29 07:07:56 - train: epoch 0067, iter [00100, 05004], lr: 0.056765, loss: 1.8669
2022-07-29 07:09:47 - train: epoch 0067, iter [00200, 05004], lr: 0.056705, loss: 1.8629
2022-07-29 07:11:39 - train: epoch 0067, iter [00300, 05004], lr: 0.056646, loss: 1.7108
2022-07-29 07:13:31 - train: epoch 0067, iter [00400, 05004], lr: 0.056586, loss: 1.5308
2022-07-29 07:15:25 - train: epoch 0067, iter [00500, 05004], lr: 0.056527, loss: 1.5106
2022-07-29 07:17:20 - train: epoch 0067, iter [00600, 05004], lr: 0.056467, loss: 1.4945
2022-07-29 07:19:17 - train: epoch 0067, iter [00700, 05004], lr: 0.056408, loss: 1.8042
2022-07-29 07:21:11 - train: epoch 0067, iter [00800, 05004], lr: 0.056348, loss: 1.8271
2022-07-29 07:22:51 - train: epoch 0067, iter [00900, 05004], lr: 0.056289, loss: 2.0316
2022-07-29 07:25:08 - train: epoch 0067, iter [01000, 05004], lr: 0.056229, loss: 1.5992
2022-07-29 07:27:04 - train: epoch 0067, iter [01100, 05004], lr: 0.056170, loss: 1.8470
2022-07-29 07:29:10 - train: epoch 0067, iter [01200, 05004], lr: 0.056111, loss: 1.6955
2022-07-29 07:31:07 - train: epoch 0067, iter [01300, 05004], lr: 0.056051, loss: 1.9895
2022-07-29 07:33:08 - train: epoch 0067, iter [01400, 05004], lr: 0.055992, loss: 1.8033
2022-07-29 07:34:58 - train: epoch 0067, iter [01500, 05004], lr: 0.055933, loss: 1.6025
2022-07-29 07:36:54 - train: epoch 0067, iter [01600, 05004], lr: 0.055873, loss: 1.8383
2022-07-29 07:38:47 - train: epoch 0067, iter [01700, 05004], lr: 0.055814, loss: 1.7269
2022-07-29 07:40:40 - train: epoch 0067, iter [01800, 05004], lr: 0.055755, loss: 1.8441
2022-07-29 07:42:34 - train: epoch 0067, iter [01900, 05004], lr: 0.055695, loss: 1.7359
2022-07-29 07:44:27 - train: epoch 0067, iter [02000, 05004], lr: 0.055636, loss: 1.8010
2022-07-29 07:46:25 - train: epoch 0067, iter [02100, 05004], lr: 0.055577, loss: 1.4090
2022-07-29 07:48:15 - train: epoch 0067, iter [02200, 05004], lr: 0.055518, loss: 1.5968
2022-07-29 07:50:17 - train: epoch 0067, iter [02300, 05004], lr: 0.055459, loss: 1.6997
2022-07-29 07:52:16 - train: epoch 0067, iter [02400, 05004], lr: 0.055399, loss: 1.6367
2022-07-29 07:54:08 - train: epoch 0067, iter [02500, 05004], lr: 0.055340, loss: 1.7496
2022-07-29 07:56:01 - train: epoch 0067, iter [02600, 05004], lr: 0.055281, loss: 1.7727
2022-07-29 07:57:51 - train: epoch 0067, iter [02700, 05004], lr: 0.055222, loss: 1.5182
2022-07-29 07:59:50 - train: epoch 0067, iter [02800, 05004], lr: 0.055163, loss: 1.8674
2022-07-29 08:01:47 - train: epoch 0067, iter [02900, 05004], lr: 0.055104, loss: 1.6730
2022-07-29 08:03:42 - train: epoch 0067, iter [03000, 05004], lr: 0.055045, loss: 1.7206
2022-07-29 08:05:39 - train: epoch 0067, iter [03100, 05004], lr: 0.054986, loss: 1.7783
2022-07-29 08:07:32 - train: epoch 0067, iter [03200, 05004], lr: 0.054927, loss: 1.6793
2022-07-29 08:09:26 - train: epoch 0067, iter [03300, 05004], lr: 0.054868, loss: 1.6292
2022-07-29 08:11:24 - train: epoch 0067, iter [03400, 05004], lr: 0.054809, loss: 1.8180
2022-07-29 08:13:19 - train: epoch 0067, iter [03500, 05004], lr: 0.054750, loss: 1.9888
2022-07-29 08:15:15 - train: epoch 0067, iter [03600, 05004], lr: 0.054691, loss: 1.6384
2022-07-29 08:17:02 - train: epoch 0067, iter [03700, 05004], lr: 0.054632, loss: 1.8518
2022-07-29 08:18:56 - train: epoch 0067, iter [03800, 05004], lr: 0.054573, loss: 1.9743
2022-07-29 08:20:45 - train: epoch 0067, iter [03900, 05004], lr: 0.054514, loss: 1.9523
2022-07-29 08:22:35 - train: epoch 0067, iter [04000, 05004], lr: 0.054456, loss: 1.7548
2022-07-29 08:24:29 - train: epoch 0067, iter [04100, 05004], lr: 0.054397, loss: 1.7477
2022-07-29 08:26:25 - train: epoch 0067, iter [04200, 05004], lr: 0.054338, loss: 1.7442
2022-07-29 08:28:22 - train: epoch 0067, iter [04300, 05004], lr: 0.054279, loss: 1.4821
2022-07-29 08:30:20 - train: epoch 0067, iter [04400, 05004], lr: 0.054220, loss: 1.7523
2022-07-29 08:32:24 - train: epoch 0067, iter [04500, 05004], lr: 0.054162, loss: 1.7945
2022-07-29 08:34:26 - train: epoch 0067, iter [04600, 05004], lr: 0.054103, loss: 1.5869
2022-07-29 08:36:21 - train: epoch 0067, iter [04700, 05004], lr: 0.054044, loss: 1.7119
2022-07-29 08:38:23 - train: epoch 0067, iter [04800, 05004], lr: 0.053986, loss: 1.6825
2022-07-29 08:40:23 - train: epoch 0067, iter [04900, 05004], lr: 0.053927, loss: 1.5953
2022-07-29 08:42:20 - train: epoch 0067, iter [05000, 05004], lr: 0.053868, loss: 1.8513
2022-07-29 08:42:24 - train: epoch 067, train_loss: 1.7457
2022-07-29 08:46:34 - eval: epoch: 067, acc1: 63.096%, acc5: 85.792%, test_loss: 1.5081, per_image_load_time: 6.892ms, per_image_inference_time: 0.728ms
2022-07-29 08:46:34 - until epoch: 067, best_acc1: 63.304%
2022-07-29 08:46:34 - epoch 068 lr: 0.053865
2022-07-29 08:48:50 - train: epoch 0068, iter [00100, 05004], lr: 0.053807, loss: 1.9185
2022-07-29 08:50:49 - train: epoch 0068, iter [00200, 05004], lr: 0.053749, loss: 1.7938
2022-07-29 08:52:41 - train: epoch 0068, iter [00300, 05004], lr: 0.053690, loss: 1.6596
2022-07-29 08:54:40 - train: epoch 0068, iter [00400, 05004], lr: 0.053632, loss: 1.6319
2022-07-29 08:56:42 - train: epoch 0068, iter [00500, 05004], lr: 0.053573, loss: 1.6403
2022-07-29 08:58:40 - train: epoch 0068, iter [00600, 05004], lr: 0.053514, loss: 1.8909
2022-07-29 09:00:42 - train: epoch 0068, iter [00700, 05004], lr: 0.053456, loss: 1.6716
2022-07-29 09:02:43 - train: epoch 0068, iter [00800, 05004], lr: 0.053397, loss: 1.6257
2022-07-29 09:04:43 - train: epoch 0068, iter [00900, 05004], lr: 0.053339, loss: 1.8144
2022-07-29 09:06:53 - train: epoch 0068, iter [01000, 05004], lr: 0.053281, loss: 1.9320
2022-07-29 09:08:48 - train: epoch 0068, iter [01100, 05004], lr: 0.053222, loss: 1.9608
2022-07-29 09:10:48 - train: epoch 0068, iter [01200, 05004], lr: 0.053164, loss: 1.5503
2022-07-29 09:12:47 - train: epoch 0068, iter [01300, 05004], lr: 0.053105, loss: 1.7328
2022-07-29 09:14:47 - train: epoch 0068, iter [01400, 05004], lr: 0.053047, loss: 1.6347
2022-07-29 09:16:45 - train: epoch 0068, iter [01500, 05004], lr: 0.052989, loss: 1.8458
2022-07-29 09:18:43 - train: epoch 0068, iter [01600, 05004], lr: 0.052930, loss: 1.7202
2022-07-29 09:20:39 - train: epoch 0068, iter [01700, 05004], lr: 0.052872, loss: 1.6524
2022-07-29 09:22:38 - train: epoch 0068, iter [01800, 05004], lr: 0.052814, loss: 1.5472
2022-07-29 09:24:36 - train: epoch 0068, iter [01900, 05004], lr: 0.052756, loss: 1.8663
2022-07-29 09:26:33 - train: epoch 0068, iter [02000, 05004], lr: 0.052697, loss: 1.7887
2022-07-29 09:28:29 - train: epoch 0068, iter [02100, 05004], lr: 0.052639, loss: 1.9145
2022-07-29 09:30:29 - train: epoch 0068, iter [02200, 05004], lr: 0.052581, loss: 1.7114
2022-07-29 09:32:26 - train: epoch 0068, iter [02300, 05004], lr: 0.052523, loss: 1.7616
2022-07-29 09:34:25 - train: epoch 0068, iter [02400, 05004], lr: 0.052465, loss: 1.7968
2022-07-29 09:36:25 - train: epoch 0068, iter [02500, 05004], lr: 0.052406, loss: 1.7360
2022-07-29 09:38:22 - train: epoch 0068, iter [02600, 05004], lr: 0.052348, loss: 1.7003
2022-07-29 09:40:17 - train: epoch 0068, iter [02700, 05004], lr: 0.052290, loss: 1.6648
2022-07-29 09:42:09 - train: epoch 0068, iter [02800, 05004], lr: 0.052232, loss: 1.9954
2022-07-29 09:44:00 - train: epoch 0068, iter [02900, 05004], lr: 0.052174, loss: 1.7295
2022-07-29 09:45:52 - train: epoch 0068, iter [03000, 05004], lr: 0.052116, loss: 1.7326
2022-07-29 09:47:44 - train: epoch 0068, iter [03100, 05004], lr: 0.052058, loss: 1.4646
2022-07-29 09:49:38 - train: epoch 0068, iter [03200, 05004], lr: 0.052000, loss: 1.8882
2022-07-29 09:51:34 - train: epoch 0068, iter [03300, 05004], lr: 0.051942, loss: 1.6288
2022-07-29 09:53:23 - train: epoch 0068, iter [03400, 05004], lr: 0.051884, loss: 1.6840
2022-07-29 09:55:15 - train: epoch 0068, iter [03500, 05004], lr: 0.051826, loss: 1.7451
2022-07-29 09:57:07 - train: epoch 0068, iter [03600, 05004], lr: 0.051768, loss: 1.7308
2022-07-29 09:59:00 - train: epoch 0068, iter [03700, 05004], lr: 0.051710, loss: 1.7255
2022-07-29 10:00:56 - train: epoch 0068, iter [03800, 05004], lr: 0.051653, loss: 1.8396
2022-07-29 10:02:48 - train: epoch 0068, iter [03900, 05004], lr: 0.051595, loss: 1.8383
2022-07-29 10:04:40 - train: epoch 0068, iter [04000, 05004], lr: 0.051537, loss: 1.7263
2022-07-29 10:06:33 - train: epoch 0068, iter [04100, 05004], lr: 0.051479, loss: 1.5431
2022-07-29 10:08:24 - train: epoch 0068, iter [04200, 05004], lr: 0.051421, loss: 1.9605
2022-07-29 10:10:16 - train: epoch 0068, iter [04300, 05004], lr: 0.051364, loss: 1.7433
2022-07-29 10:12:07 - train: epoch 0068, iter [04400, 05004], lr: 0.051306, loss: 1.7933
2022-07-29 10:14:01 - train: epoch 0068, iter [04500, 05004], lr: 0.051248, loss: 1.6883
2022-07-29 10:15:55 - train: epoch 0068, iter [04600, 05004], lr: 0.051190, loss: 1.7762
2022-07-29 10:17:45 - train: epoch 0068, iter [04700, 05004], lr: 0.051133, loss: 1.7845
2022-07-29 10:19:32 - train: epoch 0068, iter [04800, 05004], lr: 0.051075, loss: 1.9281
2022-07-29 10:21:28 - train: epoch 0068, iter [04900, 05004], lr: 0.051018, loss: 1.7985
2022-07-29 10:23:18 - train: epoch 0068, iter [05000, 05004], lr: 0.050960, loss: 1.7696
2022-07-29 10:23:21 - train: epoch 068, train_loss: 1.7273
2022-07-29 10:27:21 - eval: epoch: 068, acc1: 63.330%, acc5: 85.716%, test_loss: 1.5077, per_image_load_time: 3.294ms, per_image_inference_time: 0.673ms
2022-07-29 10:27:21 - until epoch: 068, best_acc1: 63.330%
2022-07-29 10:27:21 - epoch 069 lr: 0.050957
2022-07-29 10:29:35 - train: epoch 0069, iter [00100, 05004], lr: 0.050900, loss: 1.7484
2022-07-29 10:31:33 - train: epoch 0069, iter [00200, 05004], lr: 0.050843, loss: 1.6505
2022-07-29 10:33:26 - train: epoch 0069, iter [00300, 05004], lr: 0.050785, loss: 1.6477
2022-07-29 10:35:30 - train: epoch 0069, iter [00400, 05004], lr: 0.050727, loss: 1.5903
2022-07-29 10:37:24 - train: epoch 0069, iter [00500, 05004], lr: 0.050670, loss: 1.8808
2022-07-29 10:39:26 - train: epoch 0069, iter [00600, 05004], lr: 0.050612, loss: 1.5582
2022-07-29 10:41:28 - train: epoch 0069, iter [00700, 05004], lr: 0.050555, loss: 1.4831
2022-07-29 10:43:31 - train: epoch 0069, iter [00800, 05004], lr: 0.050498, loss: 1.8427
2022-07-29 10:45:29 - train: epoch 0069, iter [00900, 05004], lr: 0.050440, loss: 1.5025
2022-07-29 10:48:05 - train: epoch 0069, iter [01000, 05004], lr: 0.050383, loss: 1.5934
2022-07-29 10:50:07 - train: epoch 0069, iter [01100, 05004], lr: 0.050325, loss: 1.6992
2022-07-29 10:52:17 - train: epoch 0069, iter [01200, 05004], lr: 0.050268, loss: 1.5914
2022-07-29 10:54:23 - train: epoch 0069, iter [01300, 05004], lr: 0.050211, loss: 1.8483
2022-07-29 10:56:25 - train: epoch 0069, iter [01400, 05004], lr: 0.050153, loss: 1.7116
2022-07-29 10:58:27 - train: epoch 0069, iter [01500, 05004], lr: 0.050096, loss: 1.7793
2022-07-29 11:00:34 - train: epoch 0069, iter [01600, 05004], lr: 0.050039, loss: 1.8775
2022-07-29 11:02:43 - train: epoch 0069, iter [01700, 05004], lr: 0.049982, loss: 1.6592
2022-07-29 11:04:43 - train: epoch 0069, iter [01800, 05004], lr: 0.049924, loss: 1.5513
2022-07-29 11:06:40 - train: epoch 0069, iter [01900, 05004], lr: 0.049867, loss: 1.6170
2022-07-29 11:08:46 - train: epoch 0069, iter [02000, 05004], lr: 0.049810, loss: 1.7010
2022-07-29 11:10:48 - train: epoch 0069, iter [02100, 05004], lr: 0.049753, loss: 1.6668
2022-07-29 11:12:56 - train: epoch 0069, iter [02200, 05004], lr: 0.049696, loss: 1.7505
2022-07-29 11:15:01 - train: epoch 0069, iter [02300, 05004], lr: 0.049639, loss: 1.6740
2022-07-29 11:17:01 - train: epoch 0069, iter [02400, 05004], lr: 0.049582, loss: 1.9531
2022-07-29 11:19:01 - train: epoch 0069, iter [02500, 05004], lr: 0.049525, loss: 1.6854
2022-07-29 11:21:04 - train: epoch 0069, iter [02600, 05004], lr: 0.049468, loss: 1.7583
2022-07-29 11:23:07 - train: epoch 0069, iter [02700, 05004], lr: 0.049411, loss: 1.9861
2022-07-29 11:25:08 - train: epoch 0069, iter [02800, 05004], lr: 0.049354, loss: 1.8336
2022-07-29 11:27:11 - train: epoch 0069, iter [02900, 05004], lr: 0.049297, loss: 1.6629
2022-07-29 11:29:14 - train: epoch 0069, iter [03000, 05004], lr: 0.049240, loss: 1.6543
2022-07-29 11:31:14 - train: epoch 0069, iter [03100, 05004], lr: 0.049183, loss: 1.6918
2022-07-29 11:33:19 - train: epoch 0069, iter [03200, 05004], lr: 0.049126, loss: 1.5522
2022-07-29 11:35:19 - train: epoch 0069, iter [03300, 05004], lr: 0.049069, loss: 1.6110
2022-07-29 11:37:17 - train: epoch 0069, iter [03400, 05004], lr: 0.049012, loss: 1.6818
2022-07-29 11:39:18 - train: epoch 0069, iter [03500, 05004], lr: 0.048955, loss: 1.5904
2022-07-29 11:41:15 - train: epoch 0069, iter [03600, 05004], lr: 0.048898, loss: 1.5019
2022-07-29 11:43:12 - train: epoch 0069, iter [03700, 05004], lr: 0.048842, loss: 1.9341
2022-07-29 11:45:09 - train: epoch 0069, iter [03800, 05004], lr: 0.048785, loss: 1.7414
2022-07-29 11:47:10 - train: epoch 0069, iter [03900, 05004], lr: 0.048728, loss: 1.7095
2022-07-29 11:49:14 - train: epoch 0069, iter [04000, 05004], lr: 0.048671, loss: 1.8346
2022-07-29 11:51:16 - train: epoch 0069, iter [04100, 05004], lr: 0.048615, loss: 1.7075
2022-07-29 11:53:18 - train: epoch 0069, iter [04200, 05004], lr: 0.048558, loss: 1.7854
2022-07-29 11:55:17 - train: epoch 0069, iter [04300, 05004], lr: 0.048501, loss: 1.6849
2022-07-29 11:57:16 - train: epoch 0069, iter [04400, 05004], lr: 0.048445, loss: 1.8858
2022-07-29 11:59:15 - train: epoch 0069, iter [04500, 05004], lr: 0.048388, loss: 1.7377
2022-07-29 12:01:17 - train: epoch 0069, iter [04600, 05004], lr: 0.048331, loss: 1.9099
2022-07-29 12:03:22 - train: epoch 0069, iter [04700, 05004], lr: 0.048275, loss: 1.7229
2022-07-29 12:05:19 - train: epoch 0069, iter [04800, 05004], lr: 0.048218, loss: 1.7478
