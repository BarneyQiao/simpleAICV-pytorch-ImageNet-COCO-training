2022-07-25 12:03:28 - train: epoch 0014, iter [03800, 05004], lr: 0.195834, loss: 2.6003
2022-07-25 12:05:26 - train: epoch 0014, iter [03900, 05004], lr: 0.195815, loss: 2.5103
2022-07-25 12:07:23 - train: epoch 0014, iter [04000, 05004], lr: 0.195796, loss: 2.4867
2022-07-25 12:09:20 - train: epoch 0014, iter [04100, 05004], lr: 0.195777, loss: 2.4191
2022-07-25 12:11:19 - train: epoch 0014, iter [04200, 05004], lr: 0.195758, loss: 2.2249
2022-07-25 12:13:16 - train: epoch 0014, iter [04300, 05004], lr: 0.195739, loss: 2.2599
2022-07-25 12:15:12 - train: epoch 0014, iter [04400, 05004], lr: 0.195720, loss: 2.4793
2022-07-25 12:17:09 - train: epoch 0014, iter [04500, 05004], lr: 0.195701, loss: 2.5152
2022-07-25 12:19:05 - train: epoch 0014, iter [04600, 05004], lr: 0.195682, loss: 2.3318
2022-07-25 12:21:02 - train: epoch 0014, iter [04700, 05004], lr: 0.195662, loss: 2.3123
2022-07-25 12:22:58 - train: epoch 0014, iter [04800, 05004], lr: 0.195643, loss: 2.4995
2022-07-25 12:24:53 - train: epoch 0014, iter [04900, 05004], lr: 0.195624, loss: 2.3723
2022-07-25 12:26:45 - train: epoch 0014, iter [05000, 05004], lr: 0.195604, loss: 2.5661
2022-07-25 12:26:49 - train: epoch 014, train_loss: 2.4261
2022-07-25 12:30:46 - eval: epoch: 014, acc1: 49.190%, acc5: 75.182%, test_loss: 2.2141, per_image_load_time: 5.466ms, per_image_inference_time: 0.664ms
2022-07-25 12:30:46 - until epoch: 014, best_acc1: 50.126%
2022-07-25 12:30:46 - epoch 015 lr: 0.195603
2022-07-25 12:33:09 - train: epoch 0015, iter [00100, 05004], lr: 0.195584, loss: 2.2615
2022-07-25 12:35:03 - train: epoch 0015, iter [00200, 05004], lr: 0.195565, loss: 2.5469
2022-07-25 12:37:01 - train: epoch 0015, iter [00300, 05004], lr: 0.195545, loss: 2.5316
2022-07-25 12:39:00 - train: epoch 0015, iter [00400, 05004], lr: 0.195526, loss: 2.4945
2022-07-25 12:40:57 - train: epoch 0015, iter [00500, 05004], lr: 0.195506, loss: 2.2714
2022-07-25 12:42:56 - train: epoch 0015, iter [00600, 05004], lr: 0.195487, loss: 2.5135
2022-07-25 12:44:48 - train: epoch 0015, iter [00700, 05004], lr: 0.195467, loss: 2.4460
2022-07-25 12:46:45 - train: epoch 0015, iter [00800, 05004], lr: 0.195447, loss: 2.4533
2022-07-25 12:48:41 - train: epoch 0015, iter [00900, 05004], lr: 0.195427, loss: 2.4240
2022-07-25 12:50:37 - train: epoch 0015, iter [01000, 05004], lr: 0.195408, loss: 2.4873
2022-07-25 12:52:26 - train: epoch 0015, iter [01100, 05004], lr: 0.195388, loss: 2.1059
2022-07-25 12:54:16 - train: epoch 0015, iter [01200, 05004], lr: 0.195368, loss: 2.5549
2022-07-25 12:56:07 - train: epoch 0015, iter [01300, 05004], lr: 0.195348, loss: 2.6229
2022-07-25 12:57:58 - train: epoch 0015, iter [01400, 05004], lr: 0.195328, loss: 2.3399
2022-07-25 12:59:52 - train: epoch 0015, iter [01500, 05004], lr: 0.195308, loss: 2.3783
2022-07-25 13:01:44 - train: epoch 0015, iter [01600, 05004], lr: 0.195288, loss: 2.4829
2022-07-25 13:03:36 - train: epoch 0015, iter [01700, 05004], lr: 0.195268, loss: 2.5805
2022-07-25 13:05:26 - train: epoch 0015, iter [01800, 05004], lr: 0.195248, loss: 2.3326
2022-07-25 13:07:18 - train: epoch 0015, iter [01900, 05004], lr: 0.195228, loss: 2.2547
2022-07-25 13:09:07 - train: epoch 0015, iter [02000, 05004], lr: 0.195208, loss: 2.2984
2022-07-25 13:11:00 - train: epoch 0015, iter [02100, 05004], lr: 0.195187, loss: 2.2628
2022-07-25 13:12:52 - train: epoch 0015, iter [02200, 05004], lr: 0.195167, loss: 2.3021
2022-07-25 13:14:39 - train: epoch 0015, iter [02300, 05004], lr: 0.195147, loss: 2.4161
2022-07-25 13:16:27 - train: epoch 0015, iter [02400, 05004], lr: 0.195126, loss: 2.4060
2022-07-25 13:18:21 - train: epoch 0015, iter [02500, 05004], lr: 0.195106, loss: 2.4520
2022-07-25 13:20:14 - train: epoch 0015, iter [02600, 05004], lr: 0.195086, loss: 2.2765
2022-07-25 13:22:06 - train: epoch 0015, iter [02700, 05004], lr: 0.195065, loss: 2.5723
2022-07-25 13:23:58 - train: epoch 0015, iter [02800, 05004], lr: 0.195045, loss: 2.4772
2022-07-25 13:25:54 - train: epoch 0015, iter [02900, 05004], lr: 0.195024, loss: 2.2165
2022-07-25 13:27:43 - train: epoch 0015, iter [03000, 05004], lr: 0.195003, loss: 2.2124
2022-07-25 13:29:35 - train: epoch 0015, iter [03100, 05004], lr: 0.194983, loss: 2.3325
2022-07-25 13:31:30 - train: epoch 0015, iter [03200, 05004], lr: 0.194962, loss: 2.2184
2022-07-25 13:33:21 - train: epoch 0015, iter [03300, 05004], lr: 0.194941, loss: 2.2005
2022-07-25 13:35:12 - train: epoch 0015, iter [03400, 05004], lr: 0.194921, loss: 2.5267
2022-07-25 13:37:01 - train: epoch 0015, iter [03500, 05004], lr: 0.194900, loss: 2.7361
2022-07-25 13:38:51 - train: epoch 0015, iter [03600, 05004], lr: 0.194879, loss: 2.4085
2022-07-25 13:40:41 - train: epoch 0015, iter [03700, 05004], lr: 0.194858, loss: 2.2619
2022-07-25 13:42:32 - train: epoch 0015, iter [03800, 05004], lr: 0.194837, loss: 2.1541
2022-07-25 13:44:21 - train: epoch 0015, iter [03900, 05004], lr: 0.194816, loss: 2.4307
2022-07-25 13:46:11 - train: epoch 0015, iter [04000, 05004], lr: 0.194795, loss: 2.5153
2022-07-25 13:47:59 - train: epoch 0015, iter [04100, 05004], lr: 0.194774, loss: 2.3196
2022-07-25 13:49:48 - train: epoch 0015, iter [04200, 05004], lr: 0.194753, loss: 2.0688
2022-07-25 13:51:48 - train: epoch 0015, iter [04300, 05004], lr: 0.194732, loss: 2.1006
2022-07-25 13:53:42 - train: epoch 0015, iter [04400, 05004], lr: 0.194711, loss: 2.2659
2022-07-25 13:55:41 - train: epoch 0015, iter [04500, 05004], lr: 0.194689, loss: 2.3903
2022-07-25 13:57:39 - train: epoch 0015, iter [04600, 05004], lr: 0.194668, loss: 2.4073
2022-07-25 13:59:35 - train: epoch 0015, iter [04700, 05004], lr: 0.194647, loss: 2.4851
2022-07-25 14:01:32 - train: epoch 0015, iter [04800, 05004], lr: 0.194625, loss: 2.4166
2022-07-25 14:03:26 - train: epoch 0015, iter [04900, 05004], lr: 0.194604, loss: 2.2359
2022-07-25 14:05:20 - train: epoch 0015, iter [05000, 05004], lr: 0.194583, loss: 2.6829
2022-07-25 14:05:24 - train: epoch 015, train_loss: 2.4052
2022-07-25 14:09:45 - eval: epoch: 015, acc1: 49.300%, acc5: 75.112%, test_loss: 2.2081, per_image_load_time: 1.566ms, per_image_inference_time: 0.621ms
2022-07-25 14:09:45 - until epoch: 015, best_acc1: 50.126%
2022-07-25 14:09:45 - epoch 016 lr: 0.194582
2022-07-25 14:11:53 - train: epoch 0016, iter [00100, 05004], lr: 0.194560, loss: 2.3242
2022-07-25 14:13:52 - train: epoch 0016, iter [00200, 05004], lr: 0.194539, loss: 2.2482
2022-07-25 14:15:57 - train: epoch 0016, iter [00300, 05004], lr: 0.194517, loss: 2.2685
2022-07-25 14:17:54 - train: epoch 0016, iter [00400, 05004], lr: 0.194496, loss: 2.5446
2022-07-25 14:19:52 - train: epoch 0016, iter [00500, 05004], lr: 0.194474, loss: 2.1154
2022-07-25 14:21:54 - train: epoch 0016, iter [00600, 05004], lr: 0.194452, loss: 2.2881
2022-07-25 14:23:50 - train: epoch 0016, iter [00700, 05004], lr: 0.194431, loss: 2.0404
2022-07-25 14:25:52 - train: epoch 0016, iter [00800, 05004], lr: 0.194409, loss: 2.4711
2022-07-25 14:27:48 - train: epoch 0016, iter [00900, 05004], lr: 0.194387, loss: 2.5664
2022-07-25 14:29:46 - train: epoch 0016, iter [01000, 05004], lr: 0.194365, loss: 2.4390
2022-07-25 14:31:45 - train: epoch 0016, iter [01100, 05004], lr: 0.194343, loss: 2.2815
2022-07-25 14:33:43 - train: epoch 0016, iter [01200, 05004], lr: 0.194321, loss: 2.2409
2022-07-25 14:35:44 - train: epoch 0016, iter [01300, 05004], lr: 0.194299, loss: 2.2568
2022-07-25 14:37:43 - train: epoch 0016, iter [01400, 05004], lr: 0.194277, loss: 2.1881
2022-07-25 14:39:40 - train: epoch 0016, iter [01500, 05004], lr: 0.194255, loss: 2.5188
2022-07-25 14:41:34 - train: epoch 0016, iter [01600, 05004], lr: 0.194233, loss: 2.5300
2022-07-25 14:43:35 - train: epoch 0016, iter [01700, 05004], lr: 0.194211, loss: 2.5917
2022-07-25 14:45:32 - train: epoch 0016, iter [01800, 05004], lr: 0.194189, loss: 2.6249
2022-07-25 14:47:36 - train: epoch 0016, iter [01900, 05004], lr: 0.194167, loss: 2.3647
2022-07-25 14:49:32 - train: epoch 0016, iter [02000, 05004], lr: 0.194144, loss: 1.9600
2022-07-25 14:51:28 - train: epoch 0016, iter [02100, 05004], lr: 0.194122, loss: 2.5230
2022-07-25 14:53:29 - train: epoch 0016, iter [02200, 05004], lr: 0.194100, loss: 2.5614
2022-07-25 14:55:21 - train: epoch 0016, iter [02300, 05004], lr: 0.194077, loss: 2.5397
2022-07-25 14:57:20 - train: epoch 0016, iter [02400, 05004], lr: 0.194055, loss: 2.4749
2022-07-25 14:59:14 - train: epoch 0016, iter [02500, 05004], lr: 0.194032, loss: 2.4058
2022-07-25 15:01:10 - train: epoch 0016, iter [02600, 05004], lr: 0.194010, loss: 2.5710
2022-07-25 15:03:06 - train: epoch 0016, iter [02700, 05004], lr: 0.193987, loss: 2.2866
2022-07-25 15:05:01 - train: epoch 0016, iter [02800, 05004], lr: 0.193965, loss: 2.2518
2022-07-25 15:07:02 - train: epoch 0016, iter [02900, 05004], lr: 0.193942, loss: 2.3620
2022-07-25 15:08:55 - train: epoch 0016, iter [03000, 05004], lr: 0.193919, loss: 2.5624
2022-07-25 15:10:55 - train: epoch 0016, iter [03100, 05004], lr: 0.193897, loss: 2.4578
2022-07-25 15:12:54 - train: epoch 0016, iter [03200, 05004], lr: 0.193874, loss: 2.4037
2022-07-25 15:14:54 - train: epoch 0016, iter [03300, 05004], lr: 0.193851, loss: 2.5500
2022-07-25 15:16:47 - train: epoch 0016, iter [03400, 05004], lr: 0.193828, loss: 2.3299
2022-07-25 15:18:43 - train: epoch 0016, iter [03500, 05004], lr: 0.193805, loss: 2.4010
2022-07-25 15:20:43 - train: epoch 0016, iter [03600, 05004], lr: 0.193783, loss: 2.1305
2022-07-25 15:22:43 - train: epoch 0016, iter [03700, 05004], lr: 0.193760, loss: 2.5323
2022-07-25 15:24:36 - train: epoch 0016, iter [03800, 05004], lr: 0.193737, loss: 2.5683
2022-07-25 15:26:32 - train: epoch 0016, iter [03900, 05004], lr: 0.193714, loss: 2.4102
2022-07-25 15:28:23 - train: epoch 0016, iter [04000, 05004], lr: 0.193690, loss: 2.4987
2022-07-25 15:30:19 - train: epoch 0016, iter [04100, 05004], lr: 0.193667, loss: 2.2748
2022-07-25 15:32:12 - train: epoch 0016, iter [04200, 05004], lr: 0.193644, loss: 2.2065
2022-07-25 15:34:10 - train: epoch 0016, iter [04300, 05004], lr: 0.193621, loss: 2.3772
2022-07-25 15:36:07 - train: epoch 0016, iter [04400, 05004], lr: 0.193598, loss: 2.3848
2022-07-25 15:38:06 - train: epoch 0016, iter [04500, 05004], lr: 0.193574, loss: 2.3895
2022-07-25 15:40:01 - train: epoch 0016, iter [04600, 05004], lr: 0.193551, loss: 2.3587
2022-07-25 15:41:57 - train: epoch 0016, iter [04700, 05004], lr: 0.193528, loss: 2.6740
2022-07-25 15:43:53 - train: epoch 0016, iter [04800, 05004], lr: 0.193504, loss: 2.3120
2022-07-25 15:45:52 - train: epoch 0016, iter [04900, 05004], lr: 0.193481, loss: 2.4152
2022-07-25 15:47:39 - train: epoch 0016, iter [05000, 05004], lr: 0.193457, loss: 2.5062
2022-07-25 15:47:44 - train: epoch 016, train_loss: 2.3923
2022-07-25 15:50:58 - eval: epoch: 016, acc1: 51.550%, acc5: 76.982%, test_loss: 2.0889, per_image_load_time: 2.286ms, per_image_inference_time: 0.561ms
2022-07-25 15:50:59 - until epoch: 016, best_acc1: 51.550%
2022-07-25 15:50:59 - epoch 017 lr: 0.193456
2022-07-25 15:53:04 - train: epoch 0017, iter [00100, 05004], lr: 0.193433, loss: 2.4808
2022-07-25 15:54:59 - train: epoch 0017, iter [00200, 05004], lr: 0.193409, loss: 2.5561
2022-07-25 15:56:53 - train: epoch 0017, iter [00300, 05004], lr: 0.193386, loss: 2.6019
2022-07-25 15:58:48 - train: epoch 0017, iter [00400, 05004], lr: 0.193362, loss: 2.0300
2022-07-25 16:00:40 - train: epoch 0017, iter [00500, 05004], lr: 0.193338, loss: 2.2646
2022-07-25 16:02:35 - train: epoch 0017, iter [00600, 05004], lr: 0.193315, loss: 2.7777
2022-07-25 16:04:26 - train: epoch 0017, iter [00700, 05004], lr: 0.193291, loss: 2.5117
2022-07-25 16:06:19 - train: epoch 0017, iter [00800, 05004], lr: 0.193267, loss: 2.4589
2022-07-25 16:08:14 - train: epoch 0017, iter [00900, 05004], lr: 0.193243, loss: 2.2349
2022-07-25 16:10:05 - train: epoch 0017, iter [01000, 05004], lr: 0.193219, loss: 2.3413
2022-07-25 16:12:05 - train: epoch 0017, iter [01100, 05004], lr: 0.193195, loss: 2.6735
2022-07-25 16:13:58 - train: epoch 0017, iter [01200, 05004], lr: 0.193171, loss: 2.2584
2022-07-25 16:15:51 - train: epoch 0017, iter [01300, 05004], lr: 0.193147, loss: 2.5090
2022-07-25 16:17:50 - train: epoch 0017, iter [01400, 05004], lr: 0.193123, loss: 2.2857
2022-07-25 16:19:44 - train: epoch 0017, iter [01500, 05004], lr: 0.193099, loss: 2.2517
2022-07-25 16:21:44 - train: epoch 0017, iter [01600, 05004], lr: 0.193075, loss: 2.1394
2022-07-25 16:24:02 - train: epoch 0017, iter [01700, 05004], lr: 0.193051, loss: 2.2195
2022-07-25 16:26:02 - train: epoch 0017, iter [01800, 05004], lr: 0.193027, loss: 2.4256
2022-07-25 16:28:03 - train: epoch 0017, iter [01900, 05004], lr: 0.193002, loss: 2.1154
2022-07-25 16:30:13 - train: epoch 0017, iter [02000, 05004], lr: 0.192978, loss: 2.1660
2022-07-25 16:32:09 - train: epoch 0017, iter [02100, 05004], lr: 0.192954, loss: 2.2466
2022-07-25 16:34:06 - train: epoch 0017, iter [02200, 05004], lr: 0.192929, loss: 2.1083
2022-07-25 16:36:05 - train: epoch 0017, iter [02300, 05004], lr: 0.192905, loss: 2.3988
2022-07-25 16:38:00 - train: epoch 0017, iter [02400, 05004], lr: 0.192880, loss: 2.1952
2022-07-25 16:39:59 - train: epoch 0017, iter [02500, 05004], lr: 0.192856, loss: 2.6630
2022-07-25 16:41:54 - train: epoch 0017, iter [02600, 05004], lr: 0.192831, loss: 2.3380
2022-07-25 16:43:56 - train: epoch 0017, iter [02700, 05004], lr: 0.192807, loss: 2.2976
2022-07-25 16:45:56 - train: epoch 0017, iter [02800, 05004], lr: 0.192782, loss: 2.3752
2022-07-25 16:47:53 - train: epoch 0017, iter [02900, 05004], lr: 0.192757, loss: 2.6029
2022-07-25 16:49:46 - train: epoch 0017, iter [03000, 05004], lr: 0.192733, loss: 2.0943
2022-07-25 16:51:48 - train: epoch 0017, iter [03100, 05004], lr: 0.192708, loss: 2.3998
2022-07-25 16:53:50 - train: epoch 0017, iter [03200, 05004], lr: 0.192683, loss: 2.1410
2022-07-25 16:55:44 - train: epoch 0017, iter [03300, 05004], lr: 0.192658, loss: 2.4415
2022-07-25 16:57:40 - train: epoch 0017, iter [03400, 05004], lr: 0.192633, loss: 2.2769
2022-07-25 16:59:41 - train: epoch 0017, iter [03500, 05004], lr: 0.192609, loss: 2.4611
2022-07-25 17:01:32 - train: epoch 0017, iter [03600, 05004], lr: 0.192584, loss: 2.2722
2022-07-25 17:03:31 - train: epoch 0017, iter [03700, 05004], lr: 0.192559, loss: 2.2584
2022-07-25 17:05:33 - train: epoch 0017, iter [03800, 05004], lr: 0.192534, loss: 2.4589
2022-07-25 17:07:29 - train: epoch 0017, iter [03900, 05004], lr: 0.192509, loss: 2.2385
2022-07-25 17:09:29 - train: epoch 0017, iter [04000, 05004], lr: 0.192483, loss: 2.4443
2022-07-25 17:11:23 - train: epoch 0017, iter [04100, 05004], lr: 0.192458, loss: 2.4806
2022-07-25 17:13:22 - train: epoch 0017, iter [04200, 05004], lr: 0.192433, loss: 2.4527
2022-07-25 17:15:19 - train: epoch 0017, iter [04300, 05004], lr: 0.192408, loss: 2.3476
2022-07-25 17:17:19 - train: epoch 0017, iter [04400, 05004], lr: 0.192383, loss: 2.5414
2022-07-25 17:19:18 - train: epoch 0017, iter [04500, 05004], lr: 0.192357, loss: 2.6296
2022-07-25 17:21:14 - train: epoch 0017, iter [04600, 05004], lr: 0.192332, loss: 2.1552
2022-07-25 17:23:05 - train: epoch 0017, iter [04700, 05004], lr: 0.192306, loss: 2.6270
2022-07-25 17:25:05 - train: epoch 0017, iter [04800, 05004], lr: 0.192281, loss: 2.4350
2022-07-25 17:26:50 - train: epoch 0017, iter [04900, 05004], lr: 0.192256, loss: 2.2979
2022-07-25 17:28:54 - train: epoch 0017, iter [05000, 05004], lr: 0.192230, loss: 1.9793
2022-07-25 17:29:00 - train: epoch 017, train_loss: 2.3766
2022-07-25 17:32:36 - eval: epoch: 017, acc1: 51.068%, acc5: 76.816%, test_loss: 2.1042, per_image_load_time: 2.312ms, per_image_inference_time: 0.623ms
2022-07-25 17:32:37 - until epoch: 017, best_acc1: 51.550%
2022-07-25 17:32:37 - epoch 018 lr: 0.192229
2022-07-25 17:34:58 - train: epoch 0018, iter [00100, 05004], lr: 0.192203, loss: 2.4551
2022-07-25 17:36:59 - train: epoch 0018, iter [00200, 05004], lr: 0.192178, loss: 2.5114
2022-07-25 17:38:54 - train: epoch 0018, iter [00300, 05004], lr: 0.192152, loss: 2.4589
2022-07-25 17:41:01 - train: epoch 0018, iter [00400, 05004], lr: 0.192126, loss: 2.4570
2022-07-25 17:42:54 - train: epoch 0018, iter [00500, 05004], lr: 0.192101, loss: 2.3787
2022-07-25 17:44:56 - train: epoch 0018, iter [00600, 05004], lr: 0.192075, loss: 2.3424
2022-07-25 17:46:54 - train: epoch 0018, iter [00700, 05004], lr: 0.192049, loss: 2.4183
2022-07-25 17:48:58 - train: epoch 0018, iter [00800, 05004], lr: 0.192023, loss: 2.3291
2022-07-25 17:50:57 - train: epoch 0018, iter [00900, 05004], lr: 0.191997, loss: 2.4724
2022-07-25 17:53:01 - train: epoch 0018, iter [01000, 05004], lr: 0.191972, loss: 2.3093
2022-07-25 17:55:04 - train: epoch 0018, iter [01100, 05004], lr: 0.191946, loss: 2.6666
2022-07-25 17:57:15 - train: epoch 0018, iter [01200, 05004], lr: 0.191920, loss: 2.2169
2022-07-25 17:59:16 - train: epoch 0018, iter [01300, 05004], lr: 0.191894, loss: 2.5695
2022-07-25 18:01:20 - train: epoch 0018, iter [01400, 05004], lr: 0.191867, loss: 2.4557
2022-07-25 18:03:17 - train: epoch 0018, iter [01500, 05004], lr: 0.191841, loss: 2.4681
2022-07-25 18:05:17 - train: epoch 0018, iter [01600, 05004], lr: 0.191815, loss: 2.3831
2022-07-25 18:07:19 - train: epoch 0018, iter [01700, 05004], lr: 0.191789, loss: 2.4257
2022-07-25 18:09:16 - train: epoch 0018, iter [01800, 05004], lr: 0.191763, loss: 2.2981
2022-07-25 18:11:13 - train: epoch 0018, iter [01900, 05004], lr: 0.191736, loss: 2.4320
2022-07-25 18:13:15 - train: epoch 0018, iter [02000, 05004], lr: 0.191710, loss: 2.7169
2022-07-25 18:15:07 - train: epoch 0018, iter [02100, 05004], lr: 0.191684, loss: 2.4548
2022-07-25 18:17:07 - train: epoch 0018, iter [02200, 05004], lr: 0.191657, loss: 2.3371
2022-07-25 18:18:59 - train: epoch 0018, iter [02300, 05004], lr: 0.191631, loss: 2.3384
2022-07-25 18:20:59 - train: epoch 0018, iter [02400, 05004], lr: 0.191604, loss: 2.1007
2022-07-25 18:22:55 - train: epoch 0018, iter [02500, 05004], lr: 0.191578, loss: 2.1490
2022-07-25 18:24:54 - train: epoch 0018, iter [02600, 05004], lr: 0.191551, loss: 2.0833
2022-07-25 18:26:48 - train: epoch 0018, iter [02700, 05004], lr: 0.191525, loss: 2.4288
2022-07-25 18:28:49 - train: epoch 0018, iter [02800, 05004], lr: 0.191498, loss: 2.3164
2022-07-25 18:30:45 - train: epoch 0018, iter [02900, 05004], lr: 0.191471, loss: 2.4727
2022-07-25 18:32:38 - train: epoch 0018, iter [03000, 05004], lr: 0.191445, loss: 2.1934
2022-07-25 18:34:40 - train: epoch 0018, iter [03100, 05004], lr: 0.191418, loss: 2.8279
2022-07-25 18:36:38 - train: epoch 0018, iter [03200, 05004], lr: 0.191391, loss: 2.4347
2022-07-25 18:38:34 - train: epoch 0018, iter [03300, 05004], lr: 0.191364, loss: 2.3375
2022-07-25 18:40:33 - train: epoch 0018, iter [03400, 05004], lr: 0.191337, loss: 2.3897
2022-07-25 18:42:27 - train: epoch 0018, iter [03500, 05004], lr: 0.191310, loss: 2.3945
2022-07-25 18:44:25 - train: epoch 0018, iter [03600, 05004], lr: 0.191283, loss: 2.5208
2022-07-25 18:46:15 - train: epoch 0018, iter [03700, 05004], lr: 0.191256, loss: 2.6626
2022-07-25 18:48:13 - train: epoch 0018, iter [03800, 05004], lr: 0.191229, loss: 2.4743
2022-07-25 18:50:11 - train: epoch 0018, iter [03900, 05004], lr: 0.191202, loss: 2.3469
2022-07-25 18:52:02 - train: epoch 0018, iter [04000, 05004], lr: 0.191175, loss: 2.2771
2022-07-25 18:54:02 - train: epoch 0018, iter [04100, 05004], lr: 0.191148, loss: 2.4000
2022-07-25 18:56:01 - train: epoch 0018, iter [04200, 05004], lr: 0.191121, loss: 2.2771
2022-07-25 18:57:57 - train: epoch 0018, iter [04300, 05004], lr: 0.191094, loss: 2.3004
2022-07-25 18:59:56 - train: epoch 0018, iter [04400, 05004], lr: 0.191066, loss: 2.2873
2022-07-25 19:01:54 - train: epoch 0018, iter [04500, 05004], lr: 0.191039, loss: 2.5075
2022-07-25 19:03:53 - train: epoch 0018, iter [04600, 05004], lr: 0.191012, loss: 2.3755
2022-07-25 19:05:48 - train: epoch 0018, iter [04700, 05004], lr: 0.190984, loss: 2.6058
2022-07-25 19:07:44 - train: epoch 0018, iter [04800, 05004], lr: 0.190957, loss: 2.4807
2022-07-25 19:09:45 - train: epoch 0018, iter [04900, 05004], lr: 0.190929, loss: 2.4005
2022-07-25 19:11:57 - train: epoch 0018, iter [05000, 05004], lr: 0.190902, loss: 2.3600
2022-07-25 19:12:02 - train: epoch 018, train_loss: 2.3623
2022-07-25 19:15:35 - eval: epoch: 018, acc1: 50.204%, acc5: 75.920%, test_loss: 2.1649, per_image_load_time: 1.712ms, per_image_inference_time: 0.571ms
2022-07-25 19:15:35 - until epoch: 018, best_acc1: 51.550%
2022-07-25 19:15:35 - epoch 019 lr: 0.190900
2022-07-25 19:17:53 - train: epoch 0019, iter [00100, 05004], lr: 0.190873, loss: 2.2438
2022-07-25 19:19:53 - train: epoch 0019, iter [00200, 05004], lr: 0.190845, loss: 2.3792
2022-07-25 19:21:47 - train: epoch 0019, iter [00300, 05004], lr: 0.190818, loss: 2.3164
2022-07-25 19:23:47 - train: epoch 0019, iter [00400, 05004], lr: 0.190790, loss: 2.2575
2022-07-25 19:25:47 - train: epoch 0019, iter [00500, 05004], lr: 0.190762, loss: 2.2400
2022-07-25 19:27:48 - train: epoch 0019, iter [00600, 05004], lr: 0.190735, loss: 2.3355
2022-07-25 19:29:47 - train: epoch 0019, iter [00700, 05004], lr: 0.190707, loss: 2.2124
2022-07-25 19:31:55 - train: epoch 0019, iter [00800, 05004], lr: 0.190679, loss: 2.5037
2022-07-25 19:33:46 - train: epoch 0019, iter [00900, 05004], lr: 0.190651, loss: 2.3838
2022-07-25 19:35:45 - train: epoch 0019, iter [01000, 05004], lr: 0.190623, loss: 2.2924
2022-07-25 19:37:39 - train: epoch 0019, iter [01100, 05004], lr: 0.190595, loss: 2.2995
2022-07-25 19:39:43 - train: epoch 0019, iter [01200, 05004], lr: 0.190567, loss: 2.2098
2022-07-25 19:41:43 - train: epoch 0019, iter [01300, 05004], lr: 0.190539, loss: 2.4931
2022-07-25 19:43:39 - train: epoch 0019, iter [01400, 05004], lr: 0.190511, loss: 2.0016
2022-07-25 19:45:33 - train: epoch 0019, iter [01500, 05004], lr: 0.190483, loss: 2.6903
2022-07-25 19:47:28 - train: epoch 0019, iter [01600, 05004], lr: 0.190455, loss: 2.2448
2022-07-25 19:49:27 - train: epoch 0019, iter [01700, 05004], lr: 0.190427, loss: 2.5636
2022-07-25 19:51:24 - train: epoch 0019, iter [01800, 05004], lr: 0.190398, loss: 2.4332
2022-07-25 19:53:22 - train: epoch 0019, iter [01900, 05004], lr: 0.190370, loss: 2.6369
2022-07-25 19:55:17 - train: epoch 0019, iter [02000, 05004], lr: 0.190342, loss: 2.4511
2022-07-25 19:57:12 - train: epoch 0019, iter [02100, 05004], lr: 0.190314, loss: 2.2562
2022-07-25 19:59:08 - train: epoch 0019, iter [02200, 05004], lr: 0.190285, loss: 2.4526
2022-07-25 20:01:07 - train: epoch 0019, iter [02300, 05004], lr: 0.190257, loss: 2.5093
2022-07-25 20:03:01 - train: epoch 0019, iter [02400, 05004], lr: 0.190228, loss: 2.4305
2022-07-25 20:04:59 - train: epoch 0019, iter [02500, 05004], lr: 0.190200, loss: 2.2669
2022-07-25 20:06:55 - train: epoch 0019, iter [02600, 05004], lr: 0.190171, loss: 2.1736
2022-07-25 20:08:51 - train: epoch 0019, iter [02700, 05004], lr: 0.190143, loss: 2.4805
2022-07-25 20:10:43 - train: epoch 0019, iter [02800, 05004], lr: 0.190114, loss: 2.4755
2022-07-25 20:12:39 - train: epoch 0019, iter [02900, 05004], lr: 0.190085, loss: 2.4137
2022-07-25 20:14:35 - train: epoch 0019, iter [03000, 05004], lr: 0.190057, loss: 2.4924
2022-07-25 20:16:27 - train: epoch 0019, iter [03100, 05004], lr: 0.190028, loss: 2.3385
2022-07-25 20:18:19 - train: epoch 0019, iter [03200, 05004], lr: 0.189999, loss: 1.8862
2022-07-25 20:20:17 - train: epoch 0019, iter [03300, 05004], lr: 0.189970, loss: 2.2901
2022-07-25 20:22:11 - train: epoch 0019, iter [03400, 05004], lr: 0.189941, loss: 2.3793
2022-07-25 20:24:09 - train: epoch 0019, iter [03500, 05004], lr: 0.189912, loss: 2.7189
2022-07-25 20:26:06 - train: epoch 0019, iter [03600, 05004], lr: 0.189883, loss: 2.0473
2022-07-25 20:28:03 - train: epoch 0019, iter [03700, 05004], lr: 0.189854, loss: 2.5404
2022-07-25 20:30:02 - train: epoch 0019, iter [03800, 05004], lr: 0.189825, loss: 2.5465
2022-07-25 20:31:58 - train: epoch 0019, iter [03900, 05004], lr: 0.189796, loss: 2.2387
2022-07-25 20:33:52 - train: epoch 0019, iter [04000, 05004], lr: 0.189767, loss: 2.2645
2022-07-25 20:35:51 - train: epoch 0019, iter [04100, 05004], lr: 0.189738, loss: 2.3528
2022-07-25 20:37:51 - train: epoch 0019, iter [04200, 05004], lr: 0.189709, loss: 2.5243
2022-07-25 20:39:54 - train: epoch 0019, iter [04300, 05004], lr: 0.189680, loss: 2.3817
2022-07-25 20:41:54 - train: epoch 0019, iter [04400, 05004], lr: 0.189650, loss: 2.5897
2022-07-25 20:43:48 - train: epoch 0019, iter [04500, 05004], lr: 0.189621, loss: 2.5484
2022-07-25 20:45:47 - train: epoch 0019, iter [04600, 05004], lr: 0.189592, loss: 2.3052
2022-07-25 20:47:39 - train: epoch 0019, iter [04700, 05004], lr: 0.189562, loss: 2.4384
2022-07-25 20:49:39 - train: epoch 0019, iter [04800, 05004], lr: 0.189533, loss: 2.3672
2022-07-25 20:51:29 - train: epoch 0019, iter [04900, 05004], lr: 0.189504, loss: 2.1582
2022-07-25 20:53:37 - train: epoch 0019, iter [05000, 05004], lr: 0.189474, loss: 2.2975
2022-07-25 20:53:43 - train: epoch 019, train_loss: 2.3493
2022-07-25 20:57:25 - eval: epoch: 019, acc1: 51.188%, acc5: 77.034%, test_loss: 2.1004, per_image_load_time: 4.359ms, per_image_inference_time: 0.650ms
2022-07-25 20:57:25 - until epoch: 019, best_acc1: 51.550%
2022-07-25 20:57:25 - epoch 020 lr: 0.189473
2022-07-25 20:59:49 - train: epoch 0020, iter [00100, 05004], lr: 0.189443, loss: 2.2772
2022-07-25 21:01:51 - train: epoch 0020, iter [00200, 05004], lr: 0.189414, loss: 2.2424
2022-07-25 21:03:50 - train: epoch 0020, iter [00300, 05004], lr: 0.189384, loss: 2.2428
2022-07-25 21:05:50 - train: epoch 0020, iter [00400, 05004], lr: 0.189355, loss: 2.2869
2022-07-25 21:07:52 - train: epoch 0020, iter [00500, 05004], lr: 0.189325, loss: 2.1598
2022-07-25 21:09:48 - train: epoch 0020, iter [00600, 05004], lr: 0.189295, loss: 2.3001
2022-07-25 21:11:44 - train: epoch 0020, iter [00700, 05004], lr: 0.189265, loss: 2.2866
2022-07-25 21:13:39 - train: epoch 0020, iter [00800, 05004], lr: 0.189236, loss: 2.3838
2022-07-25 21:15:38 - train: epoch 0020, iter [00900, 05004], lr: 0.189206, loss: 2.5582
2022-07-25 21:17:31 - train: epoch 0020, iter [01000, 05004], lr: 0.189176, loss: 2.2569
2022-07-25 21:19:32 - train: epoch 0020, iter [01100, 05004], lr: 0.189146, loss: 2.3795
2022-07-25 21:21:31 - train: epoch 0020, iter [01200, 05004], lr: 0.189116, loss: 2.1825
2022-07-25 21:23:37 - train: epoch 0020, iter [01300, 05004], lr: 0.189086, loss: 2.1922
2022-07-25 21:25:38 - train: epoch 0020, iter [01400, 05004], lr: 0.189056, loss: 2.3917
2022-07-25 21:27:45 - train: epoch 0020, iter [01500, 05004], lr: 0.189026, loss: 2.2690
2022-07-25 21:29:49 - train: epoch 0020, iter [01600, 05004], lr: 0.188996, loss: 2.4432
2022-07-25 21:31:54 - train: epoch 0020, iter [01700, 05004], lr: 0.188966, loss: 2.3982
2022-07-25 21:33:50 - train: epoch 0020, iter [01800, 05004], lr: 0.188935, loss: 2.3106
2022-07-25 21:35:53 - train: epoch 0020, iter [01900, 05004], lr: 0.188905, loss: 2.3288
2022-07-25 21:37:53 - train: epoch 0020, iter [02000, 05004], lr: 0.188875, loss: 2.3114
2022-07-25 21:39:57 - train: epoch 0020, iter [02100, 05004], lr: 0.188845, loss: 2.6445
2022-07-25 21:41:59 - train: epoch 0020, iter [02200, 05004], lr: 0.188814, loss: 2.1298
2022-07-25 21:44:02 - train: epoch 0020, iter [02300, 05004], lr: 0.188784, loss: 2.1947
2022-07-25 21:46:05 - train: epoch 0020, iter [02400, 05004], lr: 0.188753, loss: 2.3810
2022-07-25 21:48:10 - train: epoch 0020, iter [02500, 05004], lr: 0.188723, loss: 2.2374
2022-07-25 21:50:11 - train: epoch 0020, iter [02600, 05004], lr: 0.188692, loss: 2.3286
2022-07-25 21:52:11 - train: epoch 0020, iter [02700, 05004], lr: 0.188662, loss: 2.3107
2022-07-25 21:54:15 - train: epoch 0020, iter [02800, 05004], lr: 0.188631, loss: 2.5184
2022-07-25 21:56:24 - train: epoch 0020, iter [02900, 05004], lr: 0.188601, loss: 2.5101
2022-07-25 21:58:28 - train: epoch 0020, iter [03000, 05004], lr: 0.188570, loss: 2.4091
2022-07-25 22:00:26 - train: epoch 0020, iter [03100, 05004], lr: 0.188539, loss: 2.3206
2022-07-25 22:02:26 - train: epoch 0020, iter [03200, 05004], lr: 0.188509, loss: 2.3729
2022-07-25 22:04:29 - train: epoch 0020, iter [03300, 05004], lr: 0.188478, loss: 2.0674
2022-07-25 22:06:28 - train: epoch 0020, iter [03400, 05004], lr: 0.188447, loss: 2.1751
2022-07-25 22:08:26 - train: epoch 0020, iter [03500, 05004], lr: 0.188416, loss: 2.1264
2022-07-25 22:10:23 - train: epoch 0020, iter [03600, 05004], lr: 0.188385, loss: 2.5003
2022-07-25 22:12:20 - train: epoch 0020, iter [03700, 05004], lr: 0.188354, loss: 2.2949
2022-07-25 22:14:13 - train: epoch 0020, iter [03800, 05004], lr: 0.188323, loss: 2.3424
2022-07-25 22:16:14 - train: epoch 0020, iter [03900, 05004], lr: 0.188292, loss: 2.3927
2022-07-25 22:18:13 - train: epoch 0020, iter [04000, 05004], lr: 0.188261, loss: 2.2402
2022-07-25 22:20:13 - train: epoch 0020, iter [04100, 05004], lr: 0.188230, loss: 2.4757
2022-07-25 22:22:15 - train: epoch 0020, iter [04200, 05004], lr: 0.188199, loss: 2.2535
2022-07-25 22:24:16 - train: epoch 0020, iter [04300, 05004], lr: 0.188168, loss: 2.3190
2022-07-25 22:26:16 - train: epoch 0020, iter [04400, 05004], lr: 0.188137, loss: 2.3664
2022-07-25 22:28:15 - train: epoch 0020, iter [04500, 05004], lr: 0.188105, loss: 2.2962
2022-07-25 22:30:13 - train: epoch 0020, iter [04600, 05004], lr: 0.188074, loss: 2.3794
2022-07-25 22:32:18 - train: epoch 0020, iter [04700, 05004], lr: 0.188043, loss: 2.2972
2022-07-25 22:34:22 - train: epoch 0020, iter [04800, 05004], lr: 0.188011, loss: 2.1512
2022-07-25 22:36:16 - train: epoch 0020, iter [04900, 05004], lr: 0.187980, loss: 2.2827
2022-07-25 22:38:39 - train: epoch 0020, iter [05000, 05004], lr: 0.187949, loss: 2.0055
2022-07-25 22:38:43 - train: epoch 020, train_loss: 2.3410
2022-07-25 22:42:31 - eval: epoch: 020, acc1: 52.682%, acc5: 77.792%, test_loss: 2.0364, per_image_load_time: 5.814ms, per_image_inference_time: 0.662ms
2022-07-25 22:42:32 - until epoch: 020, best_acc1: 52.682%
2022-07-25 22:42:32 - epoch 021 lr: 0.187947
2022-07-25 22:44:50 - train: epoch 0021, iter [00100, 05004], lr: 0.187916, loss: 2.2846
2022-07-25 22:46:53 - train: epoch 0021, iter [00200, 05004], lr: 0.187884, loss: 2.4514
2022-07-25 22:48:53 - train: epoch 0021, iter [00300, 05004], lr: 0.187853, loss: 1.9947
2022-07-25 22:50:54 - train: epoch 0021, iter [00400, 05004], lr: 0.187821, loss: 2.3177
2022-07-25 22:52:51 - train: epoch 0021, iter [00500, 05004], lr: 0.187790, loss: 2.1220
2022-07-25 22:54:53 - train: epoch 0021, iter [00600, 05004], lr: 0.187758, loss: 2.1842
2022-07-25 22:56:49 - train: epoch 0021, iter [00700, 05004], lr: 0.187726, loss: 2.0556
2022-07-25 22:58:47 - train: epoch 0021, iter [00800, 05004], lr: 0.187695, loss: 2.3233
2022-07-25 23:00:49 - train: epoch 0021, iter [00900, 05004], lr: 0.187663, loss: 2.4022
2022-07-25 23:02:49 - train: epoch 0021, iter [01000, 05004], lr: 0.187631, loss: 2.1584
2022-07-25 23:04:51 - train: epoch 0021, iter [01100, 05004], lr: 0.187599, loss: 2.2370
2022-07-25 23:06:46 - train: epoch 0021, iter [01200, 05004], lr: 0.187567, loss: 2.4118
2022-07-25 23:08:47 - train: epoch 0021, iter [01300, 05004], lr: 0.187535, loss: 2.2910
2022-07-25 23:10:44 - train: epoch 0021, iter [01400, 05004], lr: 0.187503, loss: 2.0519
2022-07-25 23:12:43 - train: epoch 0021, iter [01500, 05004], lr: 0.187471, loss: 2.0500
2022-07-25 23:14:46 - train: epoch 0021, iter [01600, 05004], lr: 0.187439, loss: 2.2054
2022-07-25 23:16:50 - train: epoch 0021, iter [01700, 05004], lr: 0.187407, loss: 2.2796
2022-07-25 23:18:45 - train: epoch 0021, iter [01800, 05004], lr: 0.187375, loss: 2.1290
2022-07-25 23:20:50 - train: epoch 0021, iter [01900, 05004], lr: 0.187343, loss: 2.5160
2022-07-25 23:22:46 - train: epoch 0021, iter [02000, 05004], lr: 0.187311, loss: 2.4793
2022-07-25 23:24:46 - train: epoch 0021, iter [02100, 05004], lr: 0.187278, loss: 2.2157
2022-07-25 23:26:45 - train: epoch 0021, iter [02200, 05004], lr: 0.187246, loss: 2.4952
2022-07-25 23:28:42 - train: epoch 0021, iter [02300, 05004], lr: 0.187214, loss: 2.2980
2022-07-25 23:30:44 - train: epoch 0021, iter [02400, 05004], lr: 0.187181, loss: 2.1920
2022-07-25 23:32:42 - train: epoch 0021, iter [02500, 05004], lr: 0.187149, loss: 2.2382
2022-07-25 23:34:42 - train: epoch 0021, iter [02600, 05004], lr: 0.187117, loss: 2.3226
2022-07-25 23:36:40 - train: epoch 0021, iter [02700, 05004], lr: 0.187084, loss: 2.2901
2022-07-25 23:38:42 - train: epoch 0021, iter [02800, 05004], lr: 0.187052, loss: 2.3358
2022-07-25 23:40:46 - train: epoch 0021, iter [02900, 05004], lr: 0.187019, loss: 2.1691
2022-07-25 23:42:49 - train: epoch 0021, iter [03000, 05004], lr: 0.186987, loss: 2.4389
2022-07-25 23:44:54 - train: epoch 0021, iter [03100, 05004], lr: 0.186954, loss: 2.2130
2022-07-25 23:47:06 - train: epoch 0021, iter [03200, 05004], lr: 0.186921, loss: 2.2152
2022-07-25 23:49:11 - train: epoch 0021, iter [03300, 05004], lr: 0.186889, loss: 2.7079
2022-07-25 23:51:15 - train: epoch 0021, iter [03400, 05004], lr: 0.186856, loss: 2.4762
2022-07-25 23:53:22 - train: epoch 0021, iter [03500, 05004], lr: 0.186823, loss: 2.4687
2022-07-25 23:55:30 - train: epoch 0021, iter [03600, 05004], lr: 0.186790, loss: 2.4162
2022-07-25 23:57:38 - train: epoch 0021, iter [03700, 05004], lr: 0.186757, loss: 2.4149
2022-07-25 23:59:50 - train: epoch 0021, iter [03800, 05004], lr: 0.186725, loss: 2.3426
2022-07-26 00:01:58 - train: epoch 0021, iter [03900, 05004], lr: 0.186692, loss: 2.3316
2022-07-26 00:04:00 - train: epoch 0021, iter [04000, 05004], lr: 0.186659, loss: 2.4188
2022-07-26 00:06:09 - train: epoch 0021, iter [04100, 05004], lr: 0.186626, loss: 2.1842
2022-07-26 00:08:19 - train: epoch 0021, iter [04200, 05004], lr: 0.186593, loss: 2.2331
2022-07-26 00:10:23 - train: epoch 0021, iter [04300, 05004], lr: 0.186560, loss: 2.3971
2022-07-26 00:12:36 - train: epoch 0021, iter [04400, 05004], lr: 0.186526, loss: 2.3616
2022-07-26 00:14:45 - train: epoch 0021, iter [04500, 05004], lr: 0.186493, loss: 2.5385
2022-07-26 00:16:51 - train: epoch 0021, iter [04600, 05004], lr: 0.186460, loss: 2.2882
2022-07-26 00:19:00 - train: epoch 0021, iter [04700, 05004], lr: 0.186427, loss: 2.4980
2022-07-26 00:21:11 - train: epoch 0021, iter [04800, 05004], lr: 0.186394, loss: 2.3761
2022-07-26 00:23:08 - train: epoch 0021, iter [04900, 05004], lr: 0.186360, loss: 2.0756
2022-07-26 00:25:40 - train: epoch 0021, iter [05000, 05004], lr: 0.186327, loss: 2.2527
2022-07-26 00:25:46 - train: epoch 021, train_loss: 2.3274
2022-07-26 00:29:43 - eval: epoch: 021, acc1: 50.500%, acc5: 76.444%, test_loss: 2.1312, per_image_load_time: 3.292ms, per_image_inference_time: 0.718ms
2022-07-26 00:29:43 - until epoch: 021, best_acc1: 52.682%
2022-07-26 00:29:43 - epoch 022 lr: 0.186325
2022-07-26 00:32:08 - train: epoch 0022, iter [00100, 05004], lr: 0.186292, loss: 2.1189
2022-07-26 00:34:06 - train: epoch 0022, iter [00200, 05004], lr: 0.186259, loss: 2.3092
2022-07-26 00:36:12 - train: epoch 0022, iter [00300, 05004], lr: 0.186225, loss: 2.2044
2022-07-26 00:38:07 - train: epoch 0022, iter [00400, 05004], lr: 0.186192, loss: 2.3019
2022-07-26 00:40:12 - train: epoch 0022, iter [00500, 05004], lr: 0.186158, loss: 2.1596
2022-07-26 00:42:11 - train: epoch 0022, iter [00600, 05004], lr: 0.186125, loss: 2.4510
2022-07-26 00:44:06 - train: epoch 0022, iter [00700, 05004], lr: 0.186091, loss: 2.5407
2022-07-26 00:46:05 - train: epoch 0022, iter [00800, 05004], lr: 0.186058, loss: 2.3952
2022-07-26 00:48:06 - train: epoch 0022, iter [00900, 05004], lr: 0.186024, loss: 2.3866
2022-07-26 00:50:03 - train: epoch 0022, iter [01000, 05004], lr: 0.185990, loss: 2.2557
2022-07-26 00:52:02 - train: epoch 0022, iter [01100, 05004], lr: 0.185956, loss: 2.1876
2022-07-26 00:54:00 - train: epoch 0022, iter [01200, 05004], lr: 0.185923, loss: 2.0135
2022-07-26 00:55:54 - train: epoch 0022, iter [01300, 05004], lr: 0.185889, loss: 2.4431
2022-07-26 00:57:54 - train: epoch 0022, iter [01400, 05004], lr: 0.185855, loss: 2.3574
2022-07-26 00:59:55 - train: epoch 0022, iter [01500, 05004], lr: 0.185821, loss: 2.5131
2022-07-26 01:01:58 - train: epoch 0022, iter [01600, 05004], lr: 0.185787, loss: 2.0783
2022-07-26 01:04:03 - train: epoch 0022, iter [01700, 05004], lr: 0.185753, loss: 2.3280
2022-07-26 01:06:08 - train: epoch 0022, iter [01800, 05004], lr: 0.185719, loss: 2.6796
2022-07-26 01:08:07 - train: epoch 0022, iter [01900, 05004], lr: 0.185685, loss: 2.2160
2022-07-26 01:10:10 - train: epoch 0022, iter [02000, 05004], lr: 0.185651, loss: 2.1635
2022-07-26 01:12:12 - train: epoch 0022, iter [02100, 05004], lr: 0.185617, loss: 2.3573
2022-07-26 01:14:13 - train: epoch 0022, iter [02200, 05004], lr: 0.185583, loss: 1.9432
2022-07-26 01:16:16 - train: epoch 0022, iter [02300, 05004], lr: 0.185548, loss: 2.3037
2022-07-26 01:18:13 - train: epoch 0022, iter [02400, 05004], lr: 0.185514, loss: 2.5129
2022-07-26 01:20:14 - train: epoch 0022, iter [02500, 05004], lr: 0.185480, loss: 2.3987
2022-07-26 01:22:14 - train: epoch 0022, iter [02600, 05004], lr: 0.185446, loss: 2.4918
2022-07-26 01:24:15 - train: epoch 0022, iter [02700, 05004], lr: 0.185411, loss: 2.1526
2022-07-26 01:26:20 - train: epoch 0022, iter [02800, 05004], lr: 0.185377, loss: 2.3402
2022-07-26 01:28:19 - train: epoch 0022, iter [02900, 05004], lr: 0.185342, loss: 2.2060
2022-07-26 01:30:17 - train: epoch 0022, iter [03000, 05004], lr: 0.185308, loss: 2.5308
2022-07-26 01:32:19 - train: epoch 0022, iter [03100, 05004], lr: 0.185274, loss: 2.5349
2022-07-26 01:34:25 - train: epoch 0022, iter [03200, 05004], lr: 0.185239, loss: 2.3712
2022-07-26 01:36:25 - train: epoch 0022, iter [03300, 05004], lr: 0.185204, loss: 2.3067
2022-07-26 01:38:23 - train: epoch 0022, iter [03400, 05004], lr: 0.185170, loss: 2.2509
2022-07-26 01:40:23 - train: epoch 0022, iter [03500, 05004], lr: 0.185135, loss: 2.3249
2022-07-26 01:42:32 - train: epoch 0022, iter [03600, 05004], lr: 0.185100, loss: 2.5537
2022-07-26 01:44:31 - train: epoch 0022, iter [03700, 05004], lr: 0.185066, loss: 2.4546
2022-07-26 01:46:33 - train: epoch 0022, iter [03800, 05004], lr: 0.185031, loss: 2.5670
2022-07-26 01:48:36 - train: epoch 0022, iter [03900, 05004], lr: 0.184996, loss: 2.2619
2022-07-26 01:50:31 - train: epoch 0022, iter [04000, 05004], lr: 0.184961, loss: 2.3603
2022-07-26 01:52:35 - train: epoch 0022, iter [04100, 05004], lr: 0.184926, loss: 2.3044
2022-07-26 01:54:36 - train: epoch 0022, iter [04200, 05004], lr: 0.184892, loss: 2.4229
2022-07-26 01:56:34 - train: epoch 0022, iter [04300, 05004], lr: 0.184857, loss: 2.3862
2022-07-26 01:58:34 - train: epoch 0022, iter [04400, 05004], lr: 0.184822, loss: 2.0776
2022-07-26 02:00:39 - train: epoch 0022, iter [04500, 05004], lr: 0.184787, loss: 2.2472
2022-07-26 02:02:37 - train: epoch 0022, iter [04600, 05004], lr: 0.184752, loss: 2.6698
2022-07-26 02:04:40 - train: epoch 0022, iter [04700, 05004], lr: 0.184716, loss: 2.4748
2022-07-26 02:06:33 - train: epoch 0022, iter [04800, 05004], lr: 0.184681, loss: 2.1547
2022-07-26 02:08:50 - train: epoch 0022, iter [04900, 05004], lr: 0.184646, loss: 2.3150
2022-07-26 02:10:57 - train: epoch 0022, iter [05000, 05004], lr: 0.184611, loss: 2.1800
2022-07-26 02:11:02 - train: epoch 022, train_loss: 2.3131
2022-07-26 02:14:54 - eval: epoch: 022, acc1: 53.142%, acc5: 78.528%, test_loss: 1.9952, per_image_load_time: 7.930ms, per_image_inference_time: 0.776ms
2022-07-26 02:14:55 - until epoch: 022, best_acc1: 53.142%
2022-07-26 02:14:55 - epoch 023 lr: 0.184609
2022-07-26 02:17:16 - train: epoch 0023, iter [00100, 05004], lr: 0.184574, loss: 2.1091
2022-07-26 02:19:26 - train: epoch 0023, iter [00200, 05004], lr: 0.184539, loss: 2.1327
2022-07-26 02:21:27 - train: epoch 0023, iter [00300, 05004], lr: 0.184504, loss: 2.4215
2022-07-26 02:23:34 - train: epoch 0023, iter [00400, 05004], lr: 0.184468, loss: 2.2720
2022-07-26 02:25:38 - train: epoch 0023, iter [00500, 05004], lr: 0.184433, loss: 2.2816
2022-07-26 02:27:41 - train: epoch 0023, iter [00600, 05004], lr: 0.184398, loss: 2.1645
2022-07-26 02:29:47 - train: epoch 0023, iter [00700, 05004], lr: 0.184362, loss: 2.1853
2022-07-26 02:31:47 - train: epoch 0023, iter [00800, 05004], lr: 0.184327, loss: 2.4694
2022-07-26 02:33:48 - train: epoch 0023, iter [00900, 05004], lr: 0.184291, loss: 2.3929
2022-07-26 02:35:47 - train: epoch 0023, iter [01000, 05004], lr: 0.184255, loss: 2.1203
2022-07-26 02:37:49 - train: epoch 0023, iter [01100, 05004], lr: 0.184220, loss: 2.1302
2022-07-26 02:39:49 - train: epoch 0023, iter [01200, 05004], lr: 0.184184, loss: 1.9893
2022-07-26 02:41:47 - train: epoch 0023, iter [01300, 05004], lr: 0.184148, loss: 2.0892
2022-07-26 02:43:48 - train: epoch 0023, iter [01400, 05004], lr: 0.184113, loss: 2.4424
2022-07-26 02:45:50 - train: epoch 0023, iter [01500, 05004], lr: 0.184077, loss: 2.3714
2022-07-26 02:47:55 - train: epoch 0023, iter [01600, 05004], lr: 0.184041, loss: 2.1608
2022-07-26 02:49:57 - train: epoch 0023, iter [01700, 05004], lr: 0.184005, loss: 2.3777
2022-07-26 02:51:56 - train: epoch 0023, iter [01800, 05004], lr: 0.183969, loss: 2.1934
2022-07-26 02:53:59 - train: epoch 0023, iter [01900, 05004], lr: 0.183934, loss: 2.4554
2022-07-26 02:56:01 - train: epoch 0023, iter [02000, 05004], lr: 0.183898, loss: 2.0392
2022-07-26 02:58:02 - train: epoch 0023, iter [02100, 05004], lr: 0.183862, loss: 2.3073
2022-07-26 03:00:05 - train: epoch 0023, iter [02200, 05004], lr: 0.183826, loss: 2.1411
2022-07-26 03:02:07 - train: epoch 0023, iter [02300, 05004], lr: 0.183790, loss: 2.1895
2022-07-26 03:04:12 - train: epoch 0023, iter [02400, 05004], lr: 0.183753, loss: 2.4210
2022-07-26 03:06:19 - train: epoch 0023, iter [02500, 05004], lr: 0.183717, loss: 2.4914
2022-07-26 03:08:19 - train: epoch 0023, iter [02600, 05004], lr: 0.183681, loss: 2.3042
2022-07-26 03:10:18 - train: epoch 0023, iter [02700, 05004], lr: 0.183645, loss: 2.4035
2022-07-26 03:12:19 - train: epoch 0023, iter [02800, 05004], lr: 0.183609, loss: 2.5558
2022-07-26 03:14:22 - train: epoch 0023, iter [02900, 05004], lr: 0.183572, loss: 2.1731
2022-07-26 03:16:24 - train: epoch 0023, iter [03000, 05004], lr: 0.183536, loss: 2.2729
2022-07-26 03:18:25 - train: epoch 0023, iter [03100, 05004], lr: 0.183500, loss: 2.4635
2022-07-26 03:20:23 - train: epoch 0023, iter [03200, 05004], lr: 0.183463, loss: 2.4111
2022-07-26 03:22:21 - train: epoch 0023, iter [03300, 05004], lr: 0.183427, loss: 2.3414
2022-07-26 03:24:27 - train: epoch 0023, iter [03400, 05004], lr: 0.183391, loss: 2.4482
2022-07-26 03:26:27 - train: epoch 0023, iter [03500, 05004], lr: 0.183354, loss: 2.0696
2022-07-26 03:28:26 - train: epoch 0023, iter [03600, 05004], lr: 0.183318, loss: 2.3969
2022-07-26 03:30:33 - train: epoch 0023, iter [03700, 05004], lr: 0.183281, loss: 2.4107
2022-07-26 03:32:32 - train: epoch 0023, iter [03800, 05004], lr: 0.183244, loss: 2.2430
2022-07-26 03:34:30 - train: epoch 0023, iter [03900, 05004], lr: 0.183208, loss: 2.4575
2022-07-26 03:36:34 - train: epoch 0023, iter [04000, 05004], lr: 0.183171, loss: 2.0216
2022-07-26 03:38:34 - train: epoch 0023, iter [04100, 05004], lr: 0.183134, loss: 2.0610
2022-07-26 03:40:34 - train: epoch 0023, iter [04200, 05004], lr: 0.183098, loss: 2.1340
2022-07-26 03:42:37 - train: epoch 0023, iter [04300, 05004], lr: 0.183061, loss: 2.1648
2022-07-26 03:44:41 - train: epoch 0023, iter [04400, 05004], lr: 0.183024, loss: 2.1783
2022-07-26 03:46:46 - train: epoch 0023, iter [04500, 05004], lr: 0.182987, loss: 2.2637
2022-07-26 03:48:44 - train: epoch 0023, iter [04600, 05004], lr: 0.182950, loss: 2.5171
2022-07-26 03:50:44 - train: epoch 0023, iter [04700, 05004], lr: 0.182913, loss: 2.1422
2022-07-26 03:52:59 - train: epoch 0023, iter [04800, 05004], lr: 0.182876, loss: 2.2820
2022-07-26 03:55:21 - train: epoch 0023, iter [04900, 05004], lr: 0.182839, loss: 2.1114
2022-07-26 03:57:25 - train: epoch 0023, iter [05000, 05004], lr: 0.182802, loss: 2.4334
2022-07-26 03:57:29 - train: epoch 023, train_loss: 2.3047
2022-07-26 04:01:19 - eval: epoch: 023, acc1: 53.554%, acc5: 78.528%, test_loss: 2.0016, per_image_load_time: 7.781ms, per_image_inference_time: 0.798ms
2022-07-26 04:01:19 - until epoch: 023, best_acc1: 53.554%
2022-07-26 04:01:19 - epoch 024 lr: 0.182801
2022-07-26 04:03:39 - train: epoch 0024, iter [00100, 05004], lr: 0.182764, loss: 2.3610
2022-07-26 04:05:37 - train: epoch 0024, iter [00200, 05004], lr: 0.182727, loss: 2.2324
2022-07-26 04:07:36 - train: epoch 0024, iter [00300, 05004], lr: 0.182690, loss: 2.2536
2022-07-26 04:09:33 - train: epoch 0024, iter [00400, 05004], lr: 0.182652, loss: 2.2502
2022-07-26 04:11:28 - train: epoch 0024, iter [00500, 05004], lr: 0.182615, loss: 2.2883
2022-07-26 04:13:27 - train: epoch 0024, iter [00600, 05004], lr: 0.182578, loss: 2.3433
2022-07-26 04:15:28 - train: epoch 0024, iter [00700, 05004], lr: 0.182541, loss: 2.2143
2022-07-26 04:17:25 - train: epoch 0024, iter [00800, 05004], lr: 0.182503, loss: 2.2751
2022-07-26 04:19:23 - train: epoch 0024, iter [00900, 05004], lr: 0.182466, loss: 2.2106
2022-07-26 04:21:20 - train: epoch 0024, iter [01000, 05004], lr: 0.182429, loss: 2.4314
2022-07-26 04:23:23 - train: epoch 0024, iter [01100, 05004], lr: 0.182391, loss: 2.0862
2022-07-26 04:25:19 - train: epoch 0024, iter [01200, 05004], lr: 0.182354, loss: 2.0538
2022-07-26 04:27:18 - train: epoch 0024, iter [01300, 05004], lr: 0.182316, loss: 2.4969
2022-07-26 04:29:13 - train: epoch 0024, iter [01400, 05004], lr: 0.182279, loss: 2.2088
2022-07-26 04:31:11 - train: epoch 0024, iter [01500, 05004], lr: 0.182241, loss: 2.3405
2022-07-26 04:33:13 - train: epoch 0024, iter [01600, 05004], lr: 0.182203, loss: 2.2286
2022-07-26 04:35:07 - train: epoch 0024, iter [01700, 05004], lr: 0.182166, loss: 2.2808
2022-07-26 04:37:05 - train: epoch 0024, iter [01800, 05004], lr: 0.182128, loss: 2.5243
2022-07-26 04:39:04 - train: epoch 0024, iter [01900, 05004], lr: 0.182090, loss: 2.2177
2022-07-26 04:40:57 - train: epoch 0024, iter [02000, 05004], lr: 0.182053, loss: 2.3504
2022-07-26 04:42:55 - train: epoch 0024, iter [02100, 05004], lr: 0.182015, loss: 2.3944
2022-07-26 04:44:53 - train: epoch 0024, iter [02200, 05004], lr: 0.181977, loss: 2.3869
2022-07-26 04:46:51 - train: epoch 0024, iter [02300, 05004], lr: 0.181939, loss: 2.3826
2022-07-26 04:48:46 - train: epoch 0024, iter [02400, 05004], lr: 0.181901, loss: 2.3592
2022-07-26 04:50:46 - train: epoch 0024, iter [02500, 05004], lr: 0.181863, loss: 2.4520
2022-07-26 04:52:46 - train: epoch 0024, iter [02600, 05004], lr: 0.181825, loss: 2.5814
2022-07-26 04:54:41 - train: epoch 0024, iter [02700, 05004], lr: 0.181787, loss: 2.4378
2022-07-26 04:56:40 - train: epoch 0024, iter [02800, 05004], lr: 0.181749, loss: 2.2869
2022-07-26 04:58:41 - train: epoch 0024, iter [02900, 05004], lr: 0.181711, loss: 2.4069
2022-07-26 05:00:37 - train: epoch 0024, iter [03000, 05004], lr: 0.181673, loss: 2.1253
2022-07-26 05:02:33 - train: epoch 0024, iter [03100, 05004], lr: 0.181635, loss: 2.0673
2022-07-26 05:04:27 - train: epoch 0024, iter [03200, 05004], lr: 0.181597, loss: 2.4300
2022-07-26 05:06:28 - train: epoch 0024, iter [03300, 05004], lr: 0.181558, loss: 2.0885
2022-07-26 05:08:23 - train: epoch 0024, iter [03400, 05004], lr: 0.181520, loss: 2.1779
2022-07-26 05:10:20 - train: epoch 0024, iter [03500, 05004], lr: 0.181482, loss: 2.4214
2022-07-26 05:12:19 - train: epoch 0024, iter [03600, 05004], lr: 0.181444, loss: 2.3255
2022-07-26 05:14:16 - train: epoch 0024, iter [03700, 05004], lr: 0.181405, loss: 2.2699
2022-07-26 05:16:14 - train: epoch 0024, iter [03800, 05004], lr: 0.181367, loss: 2.5026
2022-07-26 05:18:11 - train: epoch 0024, iter [03900, 05004], lr: 0.181328, loss: 2.1445
2022-07-26 05:20:08 - train: epoch 0024, iter [04000, 05004], lr: 0.181290, loss: 2.0617
2022-07-26 05:22:05 - train: epoch 0024, iter [04100, 05004], lr: 0.181251, loss: 2.1161
2022-07-26 05:24:02 - train: epoch 0024, iter [04200, 05004], lr: 0.181213, loss: 2.2361
2022-07-26 05:26:06 - train: epoch 0024, iter [04300, 05004], lr: 0.181174, loss: 2.2302
2022-07-26 05:27:57 - train: epoch 0024, iter [04400, 05004], lr: 0.181136, loss: 2.3635
2022-07-26 05:29:51 - train: epoch 0024, iter [04500, 05004], lr: 0.181097, loss: 2.1974
2022-07-26 05:31:51 - train: epoch 0024, iter [04600, 05004], lr: 0.181058, loss: 2.3278
2022-07-26 05:33:48 - train: epoch 0024, iter [04700, 05004], lr: 0.181020, loss: 2.1997
2022-07-26 05:36:13 - train: epoch 0024, iter [04800, 05004], lr: 0.180981, loss: 2.3098
2022-07-26 05:38:07 - train: epoch 0024, iter [04900, 05004], lr: 0.180942, loss: 2.2769
2022-07-26 05:40:00 - train: epoch 0024, iter [05000, 05004], lr: 0.180903, loss: 2.3131
2022-07-26 05:40:03 - train: epoch 024, train_loss: 2.2942
2022-07-26 05:43:54 - eval: epoch: 024, acc1: 54.440%, acc5: 79.358%, test_loss: 1.9523, per_image_load_time: 8.034ms, per_image_inference_time: 0.750ms
2022-07-26 05:43:54 - until epoch: 024, best_acc1: 54.440%
2022-07-26 05:43:54 - epoch 025 lr: 0.180901
2022-07-26 05:46:13 - train: epoch 0025, iter [00100, 05004], lr: 0.180863, loss: 2.1038
2022-07-26 05:48:14 - train: epoch 0025, iter [00200, 05004], lr: 0.180824, loss: 2.1006
2022-07-26 05:50:08 - train: epoch 0025, iter [00300, 05004], lr: 0.180785, loss: 2.1306
2022-07-26 05:52:06 - train: epoch 0025, iter [00400, 05004], lr: 0.180746, loss: 2.5309
2022-07-26 05:54:05 - train: epoch 0025, iter [00500, 05004], lr: 0.180707, loss: 2.1762
2022-07-26 05:56:03 - train: epoch 0025, iter [00600, 05004], lr: 0.180668, loss: 2.1726
2022-07-26 05:57:55 - train: epoch 0025, iter [00700, 05004], lr: 0.180629, loss: 2.3659
2022-07-26 05:59:51 - train: epoch 0025, iter [00800, 05004], lr: 0.180590, loss: 2.5249
2022-07-26 06:01:51 - train: epoch 0025, iter [00900, 05004], lr: 0.180551, loss: 2.1317
2022-07-26 06:03:50 - train: epoch 0025, iter [01000, 05004], lr: 0.180511, loss: 2.1051
2022-07-26 06:05:50 - train: epoch 0025, iter [01100, 05004], lr: 0.180472, loss: 2.3938
2022-07-26 06:07:45 - train: epoch 0025, iter [01200, 05004], lr: 0.180433, loss: 2.1965
2022-07-26 06:09:42 - train: epoch 0025, iter [01300, 05004], lr: 0.180394, loss: 2.2507
2022-07-26 06:11:40 - train: epoch 0025, iter [01400, 05004], lr: 0.180354, loss: 2.1006
2022-07-26 06:13:39 - train: epoch 0025, iter [01500, 05004], lr: 0.180315, loss: 2.5198
2022-07-26 06:15:35 - train: epoch 0025, iter [01600, 05004], lr: 0.180276, loss: 2.1786
2022-07-26 06:17:31 - train: epoch 0025, iter [01700, 05004], lr: 0.180236, loss: 2.5358
2022-07-26 06:19:23 - train: epoch 0025, iter [01800, 05004], lr: 0.180197, loss: 2.0870
2022-07-26 06:21:22 - train: epoch 0025, iter [01900, 05004], lr: 0.180157, loss: 2.2530
2022-07-26 06:23:22 - train: epoch 0025, iter [02000, 05004], lr: 0.180118, loss: 2.1953
2022-07-26 06:25:19 - train: epoch 0025, iter [02100, 05004], lr: 0.180078, loss: 2.0028
2022-07-26 06:27:18 - train: epoch 0025, iter [02200, 05004], lr: 0.180039, loss: 2.1203
2022-07-26 06:29:18 - train: epoch 0025, iter [02300, 05004], lr: 0.179999, loss: 2.3355
2022-07-26 06:31:19 - train: epoch 0025, iter [02400, 05004], lr: 0.179959, loss: 1.9097
2022-07-26 06:33:15 - train: epoch 0025, iter [02500, 05004], lr: 0.179920, loss: 2.3726
2022-07-26 06:35:14 - train: epoch 0025, iter [02600, 05004], lr: 0.179880, loss: 2.3153
2022-07-26 06:37:14 - train: epoch 0025, iter [02700, 05004], lr: 0.179840, loss: 2.4233
2022-07-26 06:39:18 - train: epoch 0025, iter [02800, 05004], lr: 0.179800, loss: 2.3577
2022-07-26 06:41:17 - train: epoch 0025, iter [02900, 05004], lr: 0.179760, loss: 2.3049
2022-07-26 06:43:19 - train: epoch 0025, iter [03000, 05004], lr: 0.179721, loss: 2.3159
2022-07-26 06:45:17 - train: epoch 0025, iter [03100, 05004], lr: 0.179681, loss: 2.2166
2022-07-26 06:47:18 - train: epoch 0025, iter [03200, 05004], lr: 0.179641, loss: 2.2525
2022-07-26 06:49:15 - train: epoch 0025, iter [03300, 05004], lr: 0.179601, loss: 1.9996
2022-07-26 06:51:10 - train: epoch 0025, iter [03400, 05004], lr: 0.179561, loss: 2.2665
2022-07-26 06:53:10 - train: epoch 0025, iter [03500, 05004], lr: 0.179521, loss: 2.2227
2022-07-26 06:55:01 - train: epoch 0025, iter [03600, 05004], lr: 0.179481, loss: 2.3275
2022-07-26 06:57:02 - train: epoch 0025, iter [03700, 05004], lr: 0.179440, loss: 2.3061
2022-07-26 06:58:59 - train: epoch 0025, iter [03800, 05004], lr: 0.179400, loss: 2.3198
2022-07-26 07:00:54 - train: epoch 0025, iter [03900, 05004], lr: 0.179360, loss: 2.4361
2022-07-26 07:02:50 - train: epoch 0025, iter [04000, 05004], lr: 0.179320, loss: 2.1876
2022-07-26 07:04:43 - train: epoch 0025, iter [04100, 05004], lr: 0.179280, loss: 2.5014
2022-07-26 07:06:42 - train: epoch 0025, iter [04200, 05004], lr: 0.179239, loss: 2.4478
2022-07-26 07:08:44 - train: epoch 0025, iter [04300, 05004], lr: 0.179199, loss: 2.1059
2022-07-26 07:10:42 - train: epoch 0025, iter [04400, 05004], lr: 0.179159, loss: 1.9784
2022-07-26 07:12:43 - train: epoch 0025, iter [04500, 05004], lr: 0.179118, loss: 2.1582
2022-07-26 07:14:24 - train: epoch 0025, iter [04600, 05004], lr: 0.179078, loss: 2.1640
2022-07-26 07:16:54 - train: epoch 0025, iter [04700, 05004], lr: 0.179037, loss: 2.2290
2022-07-26 07:19:02 - train: epoch 0025, iter [04800, 05004], lr: 0.178997, loss: 2.1731
2022-07-26 07:21:15 - train: epoch 0025, iter [04900, 05004], lr: 0.178956, loss: 2.2526
2022-07-26 07:23:19 - train: epoch 0025, iter [05000, 05004], lr: 0.178916, loss: 2.5351
2022-07-26 07:23:23 - train: epoch 025, train_loss: 2.2838
2022-07-26 07:27:18 - eval: epoch: 025, acc1: 51.966%, acc5: 77.502%, test_loss: 2.0636, per_image_load_time: 8.222ms, per_image_inference_time: 0.725ms
2022-07-26 07:27:18 - until epoch: 025, best_acc1: 54.440%
2022-07-26 07:27:18 - epoch 026 lr: 0.178914
2022-07-26 07:29:38 - train: epoch 0026, iter [00100, 05004], lr: 0.178873, loss: 2.5584
2022-07-26 07:31:41 - train: epoch 0026, iter [00200, 05004], lr: 0.178833, loss: 2.2308
2022-07-26 07:33:42 - train: epoch 0026, iter [00300, 05004], lr: 0.178792, loss: 2.2886
2022-07-26 07:35:38 - train: epoch 0026, iter [00400, 05004], lr: 0.178751, loss: 2.1495
2022-07-26 07:37:36 - train: epoch 0026, iter [00500, 05004], lr: 0.178711, loss: 2.1772
2022-07-26 07:39:35 - train: epoch 0026, iter [00600, 05004], lr: 0.178670, loss: 2.5946
2022-07-26 07:41:35 - train: epoch 0026, iter [00700, 05004], lr: 0.178629, loss: 2.2652
2022-07-26 07:43:35 - train: epoch 0026, iter [00800, 05004], lr: 0.178588, loss: 2.0586
2022-07-26 07:45:29 - train: epoch 0026, iter [00900, 05004], lr: 0.178547, loss: 2.2766
2022-07-26 07:47:18 - train: epoch 0026, iter [01000, 05004], lr: 0.178506, loss: 2.1516
2022-07-26 07:49:14 - train: epoch 0026, iter [01100, 05004], lr: 0.178465, loss: 2.2500
2022-07-26 07:51:04 - train: epoch 0026, iter [01200, 05004], lr: 0.178424, loss: 2.3873
2022-07-26 07:52:55 - train: epoch 0026, iter [01300, 05004], lr: 0.178383, loss: 2.1893
2022-07-26 07:54:46 - train: epoch 0026, iter [01400, 05004], lr: 0.178342, loss: 2.4253
2022-07-26 07:56:37 - train: epoch 0026, iter [01500, 05004], lr: 0.178301, loss: 2.1529
2022-07-26 07:58:30 - train: epoch 0026, iter [01600, 05004], lr: 0.178260, loss: 2.2907
2022-07-26 08:00:23 - train: epoch 0026, iter [01700, 05004], lr: 0.178219, loss: 2.2626
2022-07-26 08:02:16 - train: epoch 0026, iter [01800, 05004], lr: 0.178178, loss: 2.5201
2022-07-26 08:04:08 - train: epoch 0026, iter [01900, 05004], lr: 0.178137, loss: 2.5371
2022-07-26 08:06:03 - train: epoch 0026, iter [02000, 05004], lr: 0.178095, loss: 2.3605
2022-07-26 08:07:53 - train: epoch 0026, iter [02100, 05004], lr: 0.178054, loss: 2.3698
2022-07-26 08:09:50 - train: epoch 0026, iter [02200, 05004], lr: 0.178013, loss: 2.3802
2022-07-26 08:11:40 - train: epoch 0026, iter [02300, 05004], lr: 0.177971, loss: 2.2026
2022-07-26 08:13:33 - train: epoch 0026, iter [02400, 05004], lr: 0.177930, loss: 2.5750
2022-07-26 08:15:26 - train: epoch 0026, iter [02500, 05004], lr: 0.177889, loss: 2.2555
2022-07-26 08:17:16 - train: epoch 0026, iter [02600, 05004], lr: 0.177847, loss: 2.2016
2022-07-26 08:19:07 - train: epoch 0026, iter [02700, 05004], lr: 0.177806, loss: 2.3927
2022-07-26 08:20:56 - train: epoch 0026, iter [02800, 05004], lr: 0.177764, loss: 2.2982
2022-07-26 08:22:47 - train: epoch 0026, iter [02900, 05004], lr: 0.177722, loss: 2.5133
2022-07-26 08:24:38 - train: epoch 0026, iter [03000, 05004], lr: 0.177681, loss: 2.2736
2022-07-26 08:26:27 - train: epoch 0026, iter [03100, 05004], lr: 0.177639, loss: 2.1451
2022-07-26 08:28:24 - train: epoch 0026, iter [03200, 05004], lr: 0.177598, loss: 2.1891
2022-07-26 08:30:10 - train: epoch 0026, iter [03300, 05004], lr: 0.177556, loss: 2.1761
2022-07-26 08:32:02 - train: epoch 0026, iter [03400, 05004], lr: 0.177514, loss: 2.4639
2022-07-26 08:33:51 - train: epoch 0026, iter [03500, 05004], lr: 0.177472, loss: 2.3189
2022-07-26 08:35:47 - train: epoch 0026, iter [03600, 05004], lr: 0.177431, loss: 2.3331
2022-07-26 08:37:32 - train: epoch 0026, iter [03700, 05004], lr: 0.177389, loss: 2.3124
2022-07-26 08:39:26 - train: epoch 0026, iter [03800, 05004], lr: 0.177347, loss: 2.4655
2022-07-26 08:41:15 - train: epoch 0026, iter [03900, 05004], lr: 0.177305, loss: 2.5202
2022-07-26 08:43:09 - train: epoch 0026, iter [04000, 05004], lr: 0.177263, loss: 2.2628
2022-07-26 08:45:08 - train: epoch 0026, iter [04100, 05004], lr: 0.177221, loss: 2.4169
2022-07-26 08:47:04 - train: epoch 0026, iter [04200, 05004], lr: 0.177179, loss: 2.3373
2022-07-26 08:49:01 - train: epoch 0026, iter [04300, 05004], lr: 0.177137, loss: 2.2639
2022-07-26 08:51:09 - train: epoch 0026, iter [04400, 05004], lr: 0.177095, loss: 2.3603
2022-07-26 08:53:08 - train: epoch 0026, iter [04500, 05004], lr: 0.177053, loss: 2.2704
2022-07-26 08:55:38 - train: epoch 0026, iter [04600, 05004], lr: 0.177011, loss: 2.2261
2022-07-26 08:57:46 - train: epoch 0026, iter [04700, 05004], lr: 0.176969, loss: 2.0054
2022-07-26 09:00:01 - train: epoch 0026, iter [04800, 05004], lr: 0.176926, loss: 2.1541
2022-07-26 09:02:05 - train: epoch 0026, iter [04900, 05004], lr: 0.176884, loss: 2.3454
2022-07-26 09:04:00 - train: epoch 0026, iter [05000, 05004], lr: 0.176842, loss: 2.3069
2022-07-26 09:04:04 - train: epoch 026, train_loss: 2.2726
2022-07-26 09:07:54 - eval: epoch: 026, acc1: 52.708%, acc5: 78.160%, test_loss: 2.0168, per_image_load_time: 7.972ms, per_image_inference_time: 0.719ms
2022-07-26 09:07:54 - until epoch: 026, best_acc1: 54.440%
2022-07-26 09:07:54 - epoch 027 lr: 0.176840
2022-07-26 09:10:11 - train: epoch 0027, iter [00100, 05004], lr: 0.176798, loss: 2.2701
2022-07-26 09:12:06 - train: epoch 0027, iter [00200, 05004], lr: 0.176755, loss: 2.2920
2022-07-26 09:14:02 - train: epoch 0027, iter [00300, 05004], lr: 0.176713, loss: 2.4197
2022-07-26 09:16:03 - train: epoch 0027, iter [00400, 05004], lr: 0.176671, loss: 2.2512
2022-07-26 09:17:57 - train: epoch 0027, iter [00500, 05004], lr: 0.176628, loss: 2.1264
2022-07-26 09:19:53 - train: epoch 0027, iter [00600, 05004], lr: 0.176586, loss: 2.5080
2022-07-26 09:21:51 - train: epoch 0027, iter [00700, 05004], lr: 0.176543, loss: 2.3997
2022-07-26 09:23:49 - train: epoch 0027, iter [00800, 05004], lr: 0.176501, loss: 2.2523
2022-07-26 09:25:46 - train: epoch 0027, iter [00900, 05004], lr: 0.176458, loss: 2.1376
2022-07-26 09:27:44 - train: epoch 0027, iter [01000, 05004], lr: 0.176416, loss: 2.3881
2022-07-26 09:29:42 - train: epoch 0027, iter [01100, 05004], lr: 0.176373, loss: 2.1173
2022-07-26 09:31:42 - train: epoch 0027, iter [01200, 05004], lr: 0.176330, loss: 2.2870
2022-07-26 09:33:45 - train: epoch 0027, iter [01300, 05004], lr: 0.176287, loss: 2.3545
2022-07-26 09:35:45 - train: epoch 0027, iter [01400, 05004], lr: 0.176245, loss: 2.3814
2022-07-26 09:37:47 - train: epoch 0027, iter [01500, 05004], lr: 0.176202, loss: 2.4859
2022-07-26 09:39:39 - train: epoch 0027, iter [01600, 05004], lr: 0.176159, loss: 2.4351
2022-07-26 09:41:35 - train: epoch 0027, iter [01700, 05004], lr: 0.176116, loss: 2.0995
2022-07-26 09:43:29 - train: epoch 0027, iter [01800, 05004], lr: 0.176073, loss: 2.1109
2022-07-26 09:45:24 - train: epoch 0027, iter [01900, 05004], lr: 0.176031, loss: 2.3115
2022-07-26 09:47:22 - train: epoch 0027, iter [02000, 05004], lr: 0.175988, loss: 2.1832
2022-07-26 09:49:19 - train: epoch 0027, iter [02100, 05004], lr: 0.175945, loss: 2.4142
2022-07-26 09:51:15 - train: epoch 0027, iter [02200, 05004], lr: 0.175902, loss: 2.3582
2022-07-26 09:53:17 - train: epoch 0027, iter [02300, 05004], lr: 0.175859, loss: 2.4414
2022-07-26 09:55:14 - train: epoch 0027, iter [02400, 05004], lr: 0.175815, loss: 2.2711
2022-07-26 09:57:11 - train: epoch 0027, iter [02500, 05004], lr: 0.175772, loss: 2.1566
2022-07-26 09:59:13 - train: epoch 0027, iter [02600, 05004], lr: 0.175729, loss: 2.3541
2022-07-26 10:01:09 - train: epoch 0027, iter [02700, 05004], lr: 0.175686, loss: 2.4593
2022-07-26 10:03:10 - train: epoch 0027, iter [02800, 05004], lr: 0.175643, loss: 2.2818
2022-07-26 10:05:05 - train: epoch 0027, iter [02900, 05004], lr: 0.175600, loss: 2.5348
2022-07-26 10:07:05 - train: epoch 0027, iter [03000, 05004], lr: 0.175556, loss: 1.9794
2022-07-26 10:09:03 - train: epoch 0027, iter [03100, 05004], lr: 0.175513, loss: 2.0799
2022-07-26 10:11:03 - train: epoch 0027, iter [03200, 05004], lr: 0.175470, loss: 2.1393
2022-07-26 10:12:57 - train: epoch 0027, iter [03300, 05004], lr: 0.175426, loss: 2.3030
2022-07-26 10:14:53 - train: epoch 0027, iter [03400, 05004], lr: 0.175383, loss: 2.1532
2022-07-26 10:16:53 - train: epoch 0027, iter [03500, 05004], lr: 0.175339, loss: 2.5030
2022-07-26 10:18:47 - train: epoch 0027, iter [03600, 05004], lr: 0.175296, loss: 2.5055
2022-07-26 10:20:47 - train: epoch 0027, iter [03700, 05004], lr: 0.175252, loss: 2.2161
2022-07-26 10:22:41 - train: epoch 0027, iter [03800, 05004], lr: 0.175209, loss: 2.2272
2022-07-26 10:24:42 - train: epoch 0027, iter [03900, 05004], lr: 0.175165, loss: 2.0819
2022-07-26 10:26:38 - train: epoch 0027, iter [04000, 05004], lr: 0.175122, loss: 2.3792
2022-07-26 10:28:33 - train: epoch 0027, iter [04100, 05004], lr: 0.175078, loss: 2.3416
2022-07-26 10:30:34 - train: epoch 0027, iter [04200, 05004], lr: 0.175034, loss: 2.2892
2022-07-26 10:32:35 - train: epoch 0027, iter [04300, 05004], lr: 0.174991, loss: 2.3807
2022-07-26 10:34:19 - train: epoch 0027, iter [04400, 05004], lr: 0.174947, loss: 2.3350
2022-07-26 10:36:46 - train: epoch 0027, iter [04500, 05004], lr: 0.174903, loss: 1.9423
2022-07-26 10:38:51 - train: epoch 0027, iter [04600, 05004], lr: 0.174859, loss: 2.0499
2022-07-26 10:40:49 - train: epoch 0027, iter [04700, 05004], lr: 0.174816, loss: 2.3356
2022-07-26 10:42:52 - train: epoch 0027, iter [04800, 05004], lr: 0.174772, loss: 2.5221
2022-07-26 10:44:52 - train: epoch 0027, iter [04900, 05004], lr: 0.174728, loss: 2.2269
2022-07-26 10:46:45 - train: epoch 0027, iter [05000, 05004], lr: 0.174684, loss: 1.8659
2022-07-26 10:46:48 - train: epoch 027, train_loss: 2.2640
2022-07-26 10:50:57 - eval: epoch: 027, acc1: 54.254%, acc5: 79.250%, test_loss: 1.9443, per_image_load_time: 8.809ms, per_image_inference_time: 0.668ms
2022-07-26 10:50:57 - until epoch: 027, best_acc1: 54.440%
2022-07-26 10:50:57 - epoch 028 lr: 0.174682
2022-07-26 10:53:12 - train: epoch 0028, iter [00100, 05004], lr: 0.174638, loss: 2.0965
2022-07-26 10:55:09 - train: epoch 0028, iter [00200, 05004], lr: 0.174594, loss: 2.1544
2022-07-26 10:57:06 - train: epoch 0028, iter [00300, 05004], lr: 0.174550, loss: 2.2002
2022-07-26 10:59:01 - train: epoch 0028, iter [00400, 05004], lr: 0.174506, loss: 2.3196
2022-07-26 11:01:00 - train: epoch 0028, iter [00500, 05004], lr: 0.174462, loss: 2.1116
2022-07-26 11:03:03 - train: epoch 0028, iter [00600, 05004], lr: 0.174418, loss: 2.3381
2022-07-26 11:05:01 - train: epoch 0028, iter [00700, 05004], lr: 0.174374, loss: 2.5373
2022-07-26 11:06:56 - train: epoch 0028, iter [00800, 05004], lr: 0.174330, loss: 2.0275
2022-07-26 11:08:51 - train: epoch 0028, iter [00900, 05004], lr: 0.174285, loss: 2.1144
2022-07-26 11:10:49 - train: epoch 0028, iter [01000, 05004], lr: 0.174241, loss: 2.4270
2022-07-26 11:12:52 - train: epoch 0028, iter [01100, 05004], lr: 0.174197, loss: 2.3342
2022-07-26 11:14:47 - train: epoch 0028, iter [01200, 05004], lr: 0.174152, loss: 2.1929
2022-07-26 11:16:45 - train: epoch 0028, iter [01300, 05004], lr: 0.174108, loss: 2.2795
2022-07-26 11:18:43 - train: epoch 0028, iter [01400, 05004], lr: 0.174064, loss: 2.5945
2022-07-26 11:20:42 - train: epoch 0028, iter [01500, 05004], lr: 0.174019, loss: 2.2181
2022-07-26 11:22:40 - train: epoch 0028, iter [01600, 05004], lr: 0.173975, loss: 2.2426
2022-07-26 11:24:41 - train: epoch 0028, iter [01700, 05004], lr: 0.173930, loss: 2.2748
2022-07-26 11:26:41 - train: epoch 0028, iter [01800, 05004], lr: 0.173886, loss: 2.1941
2022-07-26 11:28:39 - train: epoch 0028, iter [01900, 05004], lr: 0.173841, loss: 2.3123
2022-07-26 11:30:37 - train: epoch 0028, iter [02000, 05004], lr: 0.173797, loss: 2.2345
2022-07-26 11:32:35 - train: epoch 0028, iter [02100, 05004], lr: 0.173752, loss: 2.3194
2022-07-26 11:34:32 - train: epoch 0028, iter [02200, 05004], lr: 0.173707, loss: 2.3220
2022-07-26 11:36:28 - train: epoch 0028, iter [02300, 05004], lr: 0.173663, loss: 2.4457
2022-07-26 11:38:31 - train: epoch 0028, iter [02400, 05004], lr: 0.173618, loss: 2.4485
2022-07-26 11:40:31 - train: epoch 0028, iter [02500, 05004], lr: 0.173573, loss: 2.2398
2022-07-26 11:42:27 - train: epoch 0028, iter [02600, 05004], lr: 0.173529, loss: 2.3424
2022-07-26 11:44:30 - train: epoch 0028, iter [02700, 05004], lr: 0.173484, loss: 2.2191
2022-07-26 11:46:29 - train: epoch 0028, iter [02800, 05004], lr: 0.173439, loss: 2.2801
2022-07-26 11:48:32 - train: epoch 0028, iter [02900, 05004], lr: 0.173394, loss: 2.1741
2022-07-26 11:50:32 - train: epoch 0028, iter [03000, 05004], lr: 0.173349, loss: 2.2327
2022-07-26 11:52:31 - train: epoch 0028, iter [03100, 05004], lr: 0.173304, loss: 2.5155
2022-07-26 11:54:30 - train: epoch 0028, iter [03200, 05004], lr: 0.173259, loss: 2.0797
2022-07-26 11:56:26 - train: epoch 0028, iter [03300, 05004], lr: 0.173214, loss: 2.3682
2022-07-26 11:58:22 - train: epoch 0028, iter [03400, 05004], lr: 0.173169, loss: 2.3477
2022-07-26 12:00:22 - train: epoch 0028, iter [03500, 05004], lr: 0.173124, loss: 2.2371
2022-07-26 12:02:17 - train: epoch 0028, iter [03600, 05004], lr: 0.173079, loss: 2.2746
