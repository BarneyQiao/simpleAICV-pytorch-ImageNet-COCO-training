2022-03-18 00:00:07 - teacher: resnet152
2022-03-18 00:00:07 - student: resnet50
2022-03-18 00:00:07 - num_classes: 1000
2022-03-18 00:00:07 - input_image_size: 224
2022-03-18 00:00:07 - scale: 1.1428571428571428
2022-03-18 00:00:07 - teacher_pretrained_model_path: /root/code/simpleAICV-pytorch-ImageNet-COCO-training/pretrained_models/resnet/resnet152-acc78.372.pth
2022-03-18 00:00:07 - student_pretrained_model_path: 
2022-03-18 00:00:07 - freeze_teacher: False
2022-03-18 00:00:07 - loss_list: ['CELoss', 'DMLLoss']
2022-03-18 00:00:07 - T: 1
2022-03-18 00:00:07 - train_criterion: {'CELoss': CELoss(
  (loss): CrossEntropyLoss()
), 'DMLLoss': DMLLoss()}
2022-03-18 00:00:07 - loss_name: DMLLoss
2022-03-18 00:00:07 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-18 00:00:07 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8805208c10>
2022-03-18 00:00:07 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f8805208940>
2022-03-18 00:00:07 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f8805208910>
2022-03-18 00:00:07 - seed: 0
2022-03-18 00:00:07 - batch_size: 256
2022-03-18 00:00:07 - num_workers: 16
2022-03-18 00:00:07 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-18 00:00:07 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-18 00:00:07 - epochs: 100
2022-03-18 00:00:07 - print_interval: 100
2022-03-18 00:00:07 - distributed: True
2022-03-18 00:00:07 - sync_bn: False
2022-03-18 00:00:07 - apex: True
2022-03-18 00:00:07 - gpus_type: NVIDIA GeForce RTX 3090
2022-03-18 00:00:07 - gpus_num: 2
2022-03-18 00:00:07 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f87fabbd8f0>
2022-03-18 00:00:12 - --------------------parameters--------------------
2022-03-18 00:00:12 - name: teacher.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: teacher.fc.weight, grad: True
2022-03-18 00:00:12 - name: teacher.fc.bias, grad: True
2022-03-18 00:00:12 - name: student.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.0.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.1.weight, grad: True
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.1.bias, grad: True
2022-03-18 00:00:12 - name: student.fc.weight, grad: True
2022-03-18 00:00:12 - name: student.fc.bias, grad: True
2022-03-18 00:00:12 - --------------------buffers--------------------
2022-03-18 00:00:12 - name: teacher.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.6.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer2.7.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.23.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.24.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.25.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.26.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.27.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.28.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.29.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.30.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.31.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.32.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.33.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.34.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer3.35.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: teacher.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.1.running_var, grad: False
2022-03-18 00:00:12 - name: student.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-18 00:00:12 - epoch 001 lr: 0.1
2022-03-18 00:01:14 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 13.7614, tea_CELoss: 6.7134 stu_CELoss: 6.9210 DMLLoss: 0.1270 
2022-03-18 00:02:10 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 13.4954, tea_CELoss: 6.4000 stu_CELoss: 6.9088 DMLLoss: 0.1867 
2022-03-18 00:03:06 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 13.1446, tea_CELoss: 6.0040 stu_CELoss: 6.8748 DMLLoss: 0.2659 
2022-03-18 00:04:02 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 12.7341, tea_CELoss: 5.5907 stu_CELoss: 6.8276 DMLLoss: 0.3158 
2022-03-18 00:04:58 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 12.5987, tea_CELoss: 5.3825 stu_CELoss: 6.8191 DMLLoss: 0.3971 
2022-03-18 00:05:54 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 11.9550, tea_CELoss: 4.8117 stu_CELoss: 6.6746 DMLLoss: 0.4687 
2022-03-18 00:06:51 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 11.7129, tea_CELoss: 4.3821 stu_CELoss: 6.7134 DMLLoss: 0.6174 
2022-03-18 00:07:48 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 11.5318, tea_CELoss: 4.2071 stu_CELoss: 6.6660 DMLLoss: 0.6588 
2022-03-18 00:08:44 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 11.2723, tea_CELoss: 4.0120 stu_CELoss: 6.5656 DMLLoss: 0.6947 
2022-03-18 00:09:41 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 11.0949, tea_CELoss: 3.7580 stu_CELoss: 6.5728 DMLLoss: 0.7641 
2022-03-18 00:10:37 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 11.1460, tea_CELoss: 3.8681 stu_CELoss: 6.5443 DMLLoss: 0.7335 
2022-03-18 00:11:33 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 10.6179, tea_CELoss: 3.4063 stu_CELoss: 6.3609 DMLLoss: 0.8507 
2022-03-18 00:12:29 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 10.6304, tea_CELoss: 3.3618 stu_CELoss: 6.3727 DMLLoss: 0.8960 
2022-03-18 00:13:26 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 10.4509, tea_CELoss: 3.3483 stu_CELoss: 6.2719 DMLLoss: 0.8307 
2022-03-18 00:14:22 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 10.4075, tea_CELoss: 3.4685 stu_CELoss: 6.2124 DMLLoss: 0.7266 
2022-03-18 00:15:18 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 10.5642, tea_CELoss: 3.5523 stu_CELoss: 6.2216 DMLLoss: 0.7903 
2022-03-18 00:16:15 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 10.0711, tea_CELoss: 3.2985 stu_CELoss: 6.0161 DMLLoss: 0.7565 
2022-03-18 00:17:11 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 10.0502, tea_CELoss: 3.2551 stu_CELoss: 5.9667 DMLLoss: 0.8284 
2022-03-18 00:18:07 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 9.9244, tea_CELoss: 3.1492 stu_CELoss: 5.9613 DMLLoss: 0.8139 
2022-03-18 00:19:03 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 9.8552, tea_CELoss: 3.2081 stu_CELoss: 5.8679 DMLLoss: 0.7792 
2022-03-18 00:20:00 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 9.8641, tea_CELoss: 3.2325 stu_CELoss: 5.8038 DMLLoss: 0.8278 
2022-03-18 00:20:56 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 9.5900, tea_CELoss: 3.0867 stu_CELoss: 5.6849 DMLLoss: 0.8183 
2022-03-18 00:21:53 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 9.5972, tea_CELoss: 3.1010 stu_CELoss: 5.6809 DMLLoss: 0.8153 
2022-03-18 00:22:49 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 9.6146, tea_CELoss: 3.1500 stu_CELoss: 5.6389 DMLLoss: 0.8257 
2022-03-18 00:23:46 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 9.3948, tea_CELoss: 3.1519 stu_CELoss: 5.4731 DMLLoss: 0.7698 
2022-03-18 00:24:42 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 9.7322, tea_CELoss: 3.3336 stu_CELoss: 5.6440 DMLLoss: 0.7546 
2022-03-18 00:25:39 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 9.4945, tea_CELoss: 3.1823 stu_CELoss: 5.5350 DMLLoss: 0.7772 
2022-03-18 00:26:35 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 9.2762, tea_CELoss: 2.8995 stu_CELoss: 5.4729 DMLLoss: 0.9039 
2022-03-18 00:27:32 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 9.1403, tea_CELoss: 3.0292 stu_CELoss: 5.3549 DMLLoss: 0.7562 
2022-03-18 00:28:28 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 9.2436, tea_CELoss: 3.0816 stu_CELoss: 5.4047 DMLLoss: 0.7574 
2022-03-18 00:29:24 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 9.2361, tea_CELoss: 3.0888 stu_CELoss: 5.3958 DMLLoss: 0.7515 
2022-03-18 00:30:20 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 9.1207, tea_CELoss: 3.0177 stu_CELoss: 5.2978 DMLLoss: 0.8053 
2022-03-18 00:31:16 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 9.0581, tea_CELoss: 2.8799 stu_CELoss: 5.2734 DMLLoss: 0.9048 
2022-03-18 00:32:12 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 9.0221, tea_CELoss: 3.0579 stu_CELoss: 5.1988 DMLLoss: 0.7654 
2022-03-18 00:33:09 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 8.8860, tea_CELoss: 2.8572 stu_CELoss: 5.1455 DMLLoss: 0.8833 
2022-03-18 00:34:05 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 8.9377, tea_CELoss: 2.9034 stu_CELoss: 5.2217 DMLLoss: 0.8125 
2022-03-18 00:35:01 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 9.0280, tea_CELoss: 3.0830 stu_CELoss: 5.1584 DMLLoss: 0.7865 
2022-03-18 00:35:57 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 8.6989, tea_CELoss: 2.8118 stu_CELoss: 5.0106 DMLLoss: 0.8765 
2022-03-18 00:36:54 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 8.8052, tea_CELoss: 3.0248 stu_CELoss: 5.0176 DMLLoss: 0.7628 
2022-03-18 00:37:50 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 8.6639, tea_CELoss: 2.8337 stu_CELoss: 5.0268 DMLLoss: 0.8034 
2022-03-18 00:38:46 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 8.8109, tea_CELoss: 3.0177 stu_CELoss: 5.0181 DMLLoss: 0.7751 
2022-03-18 00:39:43 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 8.4438, tea_CELoss: 2.6733 stu_CELoss: 4.8891 DMLLoss: 0.8814 
2022-03-18 00:40:39 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 8.3872, tea_CELoss: 2.6862 stu_CELoss: 4.8842 DMLLoss: 0.8168 
2022-03-18 00:41:35 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 8.1467, tea_CELoss: 2.6292 stu_CELoss: 4.6887 DMLLoss: 0.8287 
2022-03-18 00:42:31 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 8.7126, tea_CELoss: 2.9038 stu_CELoss: 5.0379 DMLLoss: 0.7710 
2022-03-18 00:43:27 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 8.7277, tea_CELoss: 2.8965 stu_CELoss: 4.9919 DMLLoss: 0.8393 
2022-03-18 00:44:24 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 8.3103, tea_CELoss: 2.8068 stu_CELoss: 4.6773 DMLLoss: 0.8262 
2022-03-18 00:45:20 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 8.8164, tea_CELoss: 2.9984 stu_CELoss: 4.9608 DMLLoss: 0.8572 
2022-03-18 00:46:16 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 8.2819, tea_CELoss: 2.6254 stu_CELoss: 4.7902 DMLLoss: 0.8664 
2022-03-18 00:47:13 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 8.1754, tea_CELoss: 2.6782 stu_CELoss: 4.6612 DMLLoss: 0.8360 
2022-03-18 00:47:16 - train: epoch 001, train_loss: 9.9559
2022-03-18 00:49:42 - eval: epoch: 001, tea_acc1: 50.358%, tea_acc5: 75.630%, tea_test_loss: 2.5179, stu_acc1: 12.838%, stu_acc5: 30.698%, stu_test_loss: 4.6012
2022-03-18 00:49:43 - until epoch: 001, tea_best_acc1: 50.358%, stu_best_acc1: 12.838%
2022-03-18 00:49:43 - epoch 002 lr: 0.1
2022-03-18 00:50:45 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 7.8580, tea_CELoss: 2.5307 stu_CELoss: 4.4669 DMLLoss: 0.8604 
2022-03-18 00:51:41 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 7.8524, tea_CELoss: 2.6792 stu_CELoss: 4.4256 DMLLoss: 0.7476 
2022-03-18 00:52:37 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 8.1732, tea_CELoss: 2.7357 stu_CELoss: 4.6107 DMLLoss: 0.8268 
2022-03-18 00:53:34 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 8.2567, tea_CELoss: 2.8829 stu_CELoss: 4.6332 DMLLoss: 0.7405 
2022-03-18 00:54:30 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 7.8026, tea_CELoss: 2.6962 stu_CELoss: 4.3588 DMLLoss: 0.7476 
2022-03-18 00:55:26 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 7.7094, tea_CELoss: 2.5838 stu_CELoss: 4.3746 DMLLoss: 0.7510 
2022-03-18 00:56:23 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 8.1092, tea_CELoss: 2.7205 stu_CELoss: 4.6057 DMLLoss: 0.7830 
2022-03-18 00:57:19 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 7.8881, tea_CELoss: 2.6634 stu_CELoss: 4.4113 DMLLoss: 0.8134 
2022-03-18 00:58:15 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 7.5902, tea_CELoss: 2.5406 stu_CELoss: 4.2753 DMLLoss: 0.7742 
2022-03-18 00:59:12 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 7.8351, tea_CELoss: 2.6474 stu_CELoss: 4.3681 DMLLoss: 0.8197 
2022-03-18 01:00:08 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 8.2276, tea_CELoss: 2.8292 stu_CELoss: 4.5746 DMLLoss: 0.8237 
2022-03-18 01:01:04 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 7.5343, tea_CELoss: 2.4912 stu_CELoss: 4.2258 DMLLoss: 0.8174 
2022-03-18 01:02:01 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 7.6281, tea_CELoss: 2.5157 stu_CELoss: 4.3158 DMLLoss: 0.7965 
2022-03-18 01:02:57 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 7.9854, tea_CELoss: 2.7323 stu_CELoss: 4.4977 DMLLoss: 0.7555 
2022-03-18 01:03:54 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 7.7748, tea_CELoss: 2.6260 stu_CELoss: 4.3433 DMLLoss: 0.8055 
2022-03-18 01:04:50 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 7.3816, tea_CELoss: 2.5261 stu_CELoss: 4.0816 DMLLoss: 0.7739 
2022-03-18 01:05:47 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 7.4747, tea_CELoss: 2.6238 stu_CELoss: 4.1034 DMLLoss: 0.7475 
2022-03-18 01:06:43 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 7.5472, tea_CELoss: 2.5768 stu_CELoss: 4.1806 DMLLoss: 0.7898 
2022-03-18 01:07:39 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 7.0393, tea_CELoss: 2.3408 stu_CELoss: 3.9252 DMLLoss: 0.7733 
2022-03-18 01:08:36 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 6.9675, tea_CELoss: 2.3343 stu_CELoss: 3.8490 DMLLoss: 0.7842 
2022-03-18 01:09:33 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 7.4967, tea_CELoss: 2.5338 stu_CELoss: 4.1946 DMLLoss: 0.7683 
2022-03-18 01:10:29 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 7.3661, tea_CELoss: 2.5500 stu_CELoss: 4.0579 DMLLoss: 0.7581 
2022-03-18 01:11:25 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 7.4860, tea_CELoss: 2.5638 stu_CELoss: 4.1535 DMLLoss: 0.7687 
2022-03-18 01:12:22 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 7.0749, tea_CELoss: 2.3625 stu_CELoss: 3.9492 DMLLoss: 0.7632 
2022-03-18 01:13:18 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 7.0227, tea_CELoss: 2.4170 stu_CELoss: 3.8522 DMLLoss: 0.7535 
2022-03-18 01:14:15 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 7.0694, tea_CELoss: 2.4389 stu_CELoss: 3.8758 DMLLoss: 0.7548 
2022-03-18 01:15:11 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 7.3939, tea_CELoss: 2.5437 stu_CELoss: 4.0622 DMLLoss: 0.7880 
2022-03-18 01:16:07 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 7.4167, tea_CELoss: 2.5511 stu_CELoss: 4.0901 DMLLoss: 0.7754 
2022-03-18 01:17:04 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 7.1563, tea_CELoss: 2.4935 stu_CELoss: 3.9125 DMLLoss: 0.7503 
2022-03-18 01:18:01 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 6.8301, tea_CELoss: 2.3629 stu_CELoss: 3.7250 DMLLoss: 0.7422 
2022-03-18 01:18:57 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 7.3199, tea_CELoss: 2.5223 stu_CELoss: 4.0240 DMLLoss: 0.7736 
2022-03-18 01:19:54 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 7.1481, tea_CELoss: 2.5520 stu_CELoss: 3.9106 DMLLoss: 0.6855 
2022-03-18 01:20:50 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 7.3234, tea_CELoss: 2.6350 stu_CELoss: 3.9760 DMLLoss: 0.7124 
2022-03-18 01:21:46 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 7.2473, tea_CELoss: 2.5119 stu_CELoss: 3.9727 DMLLoss: 0.7628 
2022-03-18 01:22:43 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 6.6792, tea_CELoss: 2.3452 stu_CELoss: 3.6438 DMLLoss: 0.6902 
2022-03-18 01:23:39 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 7.0894, tea_CELoss: 2.5915 stu_CELoss: 3.8399 DMLLoss: 0.6581 
2022-03-18 01:24:36 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 7.2595, tea_CELoss: 2.6013 stu_CELoss: 3.9508 DMLLoss: 0.7074 
2022-03-18 01:25:32 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 6.7883, tea_CELoss: 2.4609 stu_CELoss: 3.6589 DMLLoss: 0.6685 
2022-03-18 01:26:29 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 6.9233, tea_CELoss: 2.4907 stu_CELoss: 3.7248 DMLLoss: 0.7077 
2022-03-18 01:27:25 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 6.5835, tea_CELoss: 2.3229 stu_CELoss: 3.5567 DMLLoss: 0.7039 
2022-03-18 01:28:22 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 6.9540, tea_CELoss: 2.4763 stu_CELoss: 3.7630 DMLLoss: 0.7148 
2022-03-18 01:29:18 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 6.7864, tea_CELoss: 2.4714 stu_CELoss: 3.6351 DMLLoss: 0.6799 
2022-03-18 01:30:15 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 6.7108, tea_CELoss: 2.4271 stu_CELoss: 3.6357 DMLLoss: 0.6481 
2022-03-18 01:31:11 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 6.5322, tea_CELoss: 2.2872 stu_CELoss: 3.5755 DMLLoss: 0.6695 
2022-03-18 01:32:08 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 6.3012, tea_CELoss: 2.2121 stu_CELoss: 3.3838 DMLLoss: 0.7053 
2022-03-18 01:33:04 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 6.6554, tea_CELoss: 2.3376 stu_CELoss: 3.6401 DMLLoss: 0.6776 
2022-03-18 01:34:01 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 6.4071, tea_CELoss: 2.2310 stu_CELoss: 3.4863 DMLLoss: 0.6897 
2022-03-18 01:34:58 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 6.9257, tea_CELoss: 2.4912 stu_CELoss: 3.7250 DMLLoss: 0.7095 
2022-03-18 01:35:54 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 6.5219, tea_CELoss: 2.2988 stu_CELoss: 3.5154 DMLLoss: 0.7077 
2022-03-18 01:36:51 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 6.7087, tea_CELoss: 2.4458 stu_CELoss: 3.6153 DMLLoss: 0.6476 
2022-03-18 01:36:54 - train: epoch 002, train_loss: 7.3126
2022-03-18 01:39:20 - eval: epoch: 002, tea_acc1: 52.734%, tea_acc5: 78.288%, tea_test_loss: 2.1368, stu_acc1: 28.298%, stu_acc5: 53.952%, stu_test_loss: 3.4060
2022-03-18 01:39:23 - until epoch: 002, tea_best_acc1: 52.734%, stu_best_acc1: 28.298%
2022-03-18 01:39:23 - epoch 003 lr: 0.1
2022-03-18 01:40:22 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 6.6665, tea_CELoss: 2.4971 stu_CELoss: 3.5226 DMLLoss: 0.6467 
2022-03-18 01:41:18 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 6.6524, tea_CELoss: 2.3302 stu_CELoss: 3.6276 DMLLoss: 0.6946 
2022-03-18 01:42:15 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 6.7396, tea_CELoss: 2.4095 stu_CELoss: 3.6271 DMLLoss: 0.7029 
2022-03-18 01:43:11 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 6.6188, tea_CELoss: 2.4012 stu_CELoss: 3.5399 DMLLoss: 0.6776 
2022-03-18 01:44:07 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 6.7261, tea_CELoss: 2.5467 stu_CELoss: 3.5514 DMLLoss: 0.6280 
2022-03-18 01:45:03 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 6.1322, tea_CELoss: 2.2509 stu_CELoss: 3.2723 DMLLoss: 0.6090 
2022-03-18 01:46:00 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 6.7016, tea_CELoss: 2.4955 stu_CELoss: 3.5660 DMLLoss: 0.6401 
2022-03-18 01:46:56 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 6.6681, tea_CELoss: 2.4089 stu_CELoss: 3.5679 DMLLoss: 0.6913 
2022-03-18 01:47:53 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 6.2701, tea_CELoss: 2.3659 stu_CELoss: 3.2998 DMLLoss: 0.6044 
2022-03-18 01:48:49 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 6.5912, tea_CELoss: 2.4315 stu_CELoss: 3.4616 DMLLoss: 0.6981 
2022-03-18 01:49:45 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 6.4216, tea_CELoss: 2.3433 stu_CELoss: 3.3930 DMLLoss: 0.6854 
2022-03-18 01:50:42 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 6.3419, tea_CELoss: 2.3136 stu_CELoss: 3.3956 DMLLoss: 0.6327 
2022-03-18 01:51:38 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 6.0238, tea_CELoss: 2.2701 stu_CELoss: 3.1637 DMLLoss: 0.5900 
2022-03-18 01:52:34 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 6.0198, tea_CELoss: 2.1958 stu_CELoss: 3.2023 DMLLoss: 0.6216 
2022-03-18 01:53:31 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 6.6859, tea_CELoss: 2.4560 stu_CELoss: 3.6170 DMLLoss: 0.6129 
2022-03-18 01:54:27 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 5.9839, tea_CELoss: 2.1828 stu_CELoss: 3.1534 DMLLoss: 0.6477 
2022-03-18 01:55:24 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 6.2212, tea_CELoss: 2.2581 stu_CELoss: 3.2986 DMLLoss: 0.6645 
2022-03-18 01:56:20 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 6.2058, tea_CELoss: 2.4125 stu_CELoss: 3.2249 DMLLoss: 0.5684 
2022-03-18 01:57:16 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 6.3859, tea_CELoss: 2.3200 stu_CELoss: 3.3936 DMLLoss: 0.6723 
2022-03-18 01:58:13 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 6.6485, tea_CELoss: 2.5562 stu_CELoss: 3.4746 DMLLoss: 0.6176 
2022-03-18 01:59:09 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 6.3291, tea_CELoss: 2.3662 stu_CELoss: 3.3394 DMLLoss: 0.6235 
2022-03-18 02:00:06 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 6.8318, tea_CELoss: 2.6218 stu_CELoss: 3.5751 DMLLoss: 0.6349 
2022-03-18 02:01:02 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 6.1135, tea_CELoss: 2.3350 stu_CELoss: 3.1969 DMLLoss: 0.5816 
2022-03-18 02:01:58 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 5.9997, tea_CELoss: 2.2285 stu_CELoss: 3.1576 DMLLoss: 0.6136 
2022-03-18 02:02:55 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 6.2200, tea_CELoss: 2.3767 stu_CELoss: 3.2673 DMLLoss: 0.5760 
2022-03-18 02:03:51 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 6.2133, tea_CELoss: 2.3462 stu_CELoss: 3.2480 DMLLoss: 0.6191 
2022-03-18 02:04:47 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 6.5008, tea_CELoss: 2.5139 stu_CELoss: 3.4080 DMLLoss: 0.5788 
2022-03-18 02:05:44 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 6.0040, tea_CELoss: 2.2567 stu_CELoss: 3.1135 DMLLoss: 0.6339 
2022-03-18 02:06:40 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 6.2202, tea_CELoss: 2.2480 stu_CELoss: 3.3461 DMLLoss: 0.6261 
2022-03-18 02:07:36 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 5.9931, tea_CELoss: 2.2578 stu_CELoss: 3.1565 DMLLoss: 0.5787 
2022-03-18 02:08:33 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 6.7097, tea_CELoss: 2.4901 stu_CELoss: 3.5561 DMLLoss: 0.6635 
2022-03-18 02:09:29 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 6.2959, tea_CELoss: 2.4257 stu_CELoss: 3.3093 DMLLoss: 0.5609 
2022-03-18 02:10:26 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 5.8818, tea_CELoss: 2.2197 stu_CELoss: 3.0668 DMLLoss: 0.5953 
2022-03-18 02:11:23 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 6.4888, tea_CELoss: 2.5039 stu_CELoss: 3.3970 DMLLoss: 0.5880 
2022-03-18 02:12:19 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 5.9999, tea_CELoss: 2.2913 stu_CELoss: 3.1322 DMLLoss: 0.5764 
2022-03-18 02:13:16 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 5.7973, tea_CELoss: 2.1537 stu_CELoss: 3.0705 DMLLoss: 0.5731 
2022-03-18 02:14:12 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 6.0439, tea_CELoss: 2.3195 stu_CELoss: 3.1297 DMLLoss: 0.5947 
2022-03-18 02:15:09 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 6.4089, tea_CELoss: 2.4579 stu_CELoss: 3.3447 DMLLoss: 0.6063 
2022-03-18 02:16:05 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 6.5991, tea_CELoss: 2.5883 stu_CELoss: 3.4207 DMLLoss: 0.5901 
2022-03-18 02:17:02 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 5.5145, tea_CELoss: 2.1129 stu_CELoss: 2.8686 DMLLoss: 0.5331 
2022-03-18 02:17:59 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 6.1394, tea_CELoss: 2.4035 stu_CELoss: 3.2082 DMLLoss: 0.5277 
2022-03-18 02:18:55 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 6.0039, tea_CELoss: 2.3624 stu_CELoss: 3.1071 DMLLoss: 0.5344 
2022-03-18 02:19:52 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 5.4378, tea_CELoss: 2.0397 stu_CELoss: 2.8333 DMLLoss: 0.5648 
2022-03-18 02:20:48 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 5.8790, tea_CELoss: 2.2540 stu_CELoss: 3.0214 DMLLoss: 0.6036 
2022-03-18 02:21:45 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 5.6754, tea_CELoss: 2.1299 stu_CELoss: 2.9609 DMLLoss: 0.5846 
2022-03-18 02:22:42 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 5.7034, tea_CELoss: 2.2426 stu_CELoss: 2.8955 DMLLoss: 0.5653 
2022-03-18 02:23:39 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 5.9177, tea_CELoss: 2.2825 stu_CELoss: 3.0748 DMLLoss: 0.5604 
2022-03-18 02:24:35 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 6.5020, tea_CELoss: 2.5232 stu_CELoss: 3.3655 DMLLoss: 0.6133 
2022-03-18 02:25:32 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 6.0001, tea_CELoss: 2.3377 stu_CELoss: 3.1127 DMLLoss: 0.5497 
2022-03-18 02:26:28 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 5.9546, tea_CELoss: 2.2916 stu_CELoss: 3.1066 DMLLoss: 0.5564 
2022-03-18 02:26:31 - train: epoch 003, train_loss: 6.2024
2022-03-18 02:28:57 - eval: epoch: 003, tea_acc1: 52.540%, tea_acc5: 78.148%, tea_test_loss: 2.0982, stu_acc1: 37.308%, stu_acc5: 63.816%, stu_test_loss: 2.8659
2022-03-18 02:29:00 - until epoch: 003, tea_best_acc1: 52.734%, stu_best_acc1: 37.308%
2022-03-18 02:29:00 - epoch 004 lr: 0.1
2022-03-18 02:30:01 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 6.3587, tea_CELoss: 2.5482 stu_CELoss: 3.2346 DMLLoss: 0.5759 
2022-03-18 02:30:57 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 5.4686, tea_CELoss: 2.0962 stu_CELoss: 2.8325 DMLLoss: 0.5399 
2022-03-18 02:31:54 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 5.7722, tea_CELoss: 2.1883 stu_CELoss: 2.9897 DMLLoss: 0.5942 
2022-03-18 02:32:50 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 5.6752, tea_CELoss: 2.2195 stu_CELoss: 2.8881 DMLLoss: 0.5675 
2022-03-18 02:33:46 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 5.6107, tea_CELoss: 2.1028 stu_CELoss: 2.9282 DMLLoss: 0.5796 
2022-03-18 02:34:43 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 6.2655, tea_CELoss: 2.4592 stu_CELoss: 3.2134 DMLLoss: 0.5929 
2022-03-18 02:35:39 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 6.1712, tea_CELoss: 2.3943 stu_CELoss: 3.2014 DMLLoss: 0.5755 
2022-03-18 02:36:36 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 5.6262, tea_CELoss: 2.1442 stu_CELoss: 2.9689 DMLLoss: 0.5132 
2022-03-18 02:37:32 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 5.2120, tea_CELoss: 1.9413 stu_CELoss: 2.7265 DMLLoss: 0.5442 
2022-03-18 02:38:29 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 5.7367, tea_CELoss: 2.1785 stu_CELoss: 2.9604 DMLLoss: 0.5978 
2022-03-18 02:39:25 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 6.0070, tea_CELoss: 2.3612 stu_CELoss: 3.1080 DMLLoss: 0.5378 
2022-03-18 02:40:21 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 5.4764, tea_CELoss: 2.0733 stu_CELoss: 2.8301 DMLLoss: 0.5731 
2022-03-18 02:41:18 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 5.7779, tea_CELoss: 2.2503 stu_CELoss: 2.9551 DMLLoss: 0.5725 
2022-03-18 02:42:14 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 5.8041, tea_CELoss: 2.3670 stu_CELoss: 2.9405 DMLLoss: 0.4965 
2022-03-18 02:43:11 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 5.4836, tea_CELoss: 2.0576 stu_CELoss: 2.8769 DMLLoss: 0.5491 
2022-03-18 02:44:07 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 5.9144, tea_CELoss: 2.2994 stu_CELoss: 3.0790 DMLLoss: 0.5361 
2022-03-18 02:45:03 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 5.6965, tea_CELoss: 2.2076 stu_CELoss: 2.9204 DMLLoss: 0.5685 
2022-03-18 02:46:00 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 5.9692, tea_CELoss: 2.4274 stu_CELoss: 3.0307 DMLLoss: 0.5111 
2022-03-18 02:46:56 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 5.7242, tea_CELoss: 2.1914 stu_CELoss: 2.9925 DMLLoss: 0.5403 
2022-03-18 02:47:53 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 5.9152, tea_CELoss: 2.3394 stu_CELoss: 3.0087 DMLLoss: 0.5672 
2022-03-18 02:48:49 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 5.5148, tea_CELoss: 2.1474 stu_CELoss: 2.8659 DMLLoss: 0.5015 
2022-03-18 02:49:45 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 5.8362, tea_CELoss: 2.3578 stu_CELoss: 2.9393 DMLLoss: 0.5392 
2022-03-18 02:50:42 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 5.1709, tea_CELoss: 2.0168 stu_CELoss: 2.6447 DMLLoss: 0.5095 
2022-03-18 02:51:38 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 5.2570, tea_CELoss: 2.1288 stu_CELoss: 2.6533 DMLLoss: 0.4749 
2022-03-18 02:52:34 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 5.3513, tea_CELoss: 2.1873 stu_CELoss: 2.6589 DMLLoss: 0.5050 
2022-03-18 02:53:30 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 5.6250, tea_CELoss: 2.2380 stu_CELoss: 2.8632 DMLLoss: 0.5238 
2022-03-18 02:54:27 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 5.3973, tea_CELoss: 2.0612 stu_CELoss: 2.7719 DMLLoss: 0.5642 
2022-03-18 02:55:23 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 5.4058, tea_CELoss: 2.1266 stu_CELoss: 2.7514 DMLLoss: 0.5279 
2022-03-18 02:56:19 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 5.6003, tea_CELoss: 2.1186 stu_CELoss: 2.9326 DMLLoss: 0.5491 
2022-03-18 02:57:16 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 5.7806, tea_CELoss: 2.2921 stu_CELoss: 2.9764 DMLLoss: 0.5122 
2022-03-18 02:58:12 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 5.7099, tea_CELoss: 2.2123 stu_CELoss: 2.9082 DMLLoss: 0.5893 
2022-03-18 02:59:08 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 5.7653, tea_CELoss: 2.2847 stu_CELoss: 2.9422 DMLLoss: 0.5384 
2022-03-18 03:00:04 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 5.5734, tea_CELoss: 2.1925 stu_CELoss: 2.9025 DMLLoss: 0.4785 
2022-03-18 03:01:00 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 5.4978, tea_CELoss: 2.2088 stu_CELoss: 2.7859 DMLLoss: 0.5031 
2022-03-18 03:01:57 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 5.6317, tea_CELoss: 2.2797 stu_CELoss: 2.8514 DMLLoss: 0.5005 
2022-03-18 03:02:53 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 5.4627, tea_CELoss: 2.1620 stu_CELoss: 2.7666 DMLLoss: 0.5342 
2022-03-18 03:03:50 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 5.5138, tea_CELoss: 2.2115 stu_CELoss: 2.8166 DMLLoss: 0.4857 
2022-03-18 03:04:46 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 5.2648, tea_CELoss: 2.0376 stu_CELoss: 2.7136 DMLLoss: 0.5136 
2022-03-18 03:05:42 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 5.3035, tea_CELoss: 2.0443 stu_CELoss: 2.7170 DMLLoss: 0.5421 
2022-03-18 03:06:39 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 5.2907, tea_CELoss: 2.0872 stu_CELoss: 2.6817 DMLLoss: 0.5219 
2022-03-18 03:07:35 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 5.3183, tea_CELoss: 2.0923 stu_CELoss: 2.7112 DMLLoss: 0.5148 
2022-03-18 03:08:31 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 5.2992, tea_CELoss: 2.1053 stu_CELoss: 2.6824 DMLLoss: 0.5114 
2022-03-18 03:09:27 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 5.2900, tea_CELoss: 2.0303 stu_CELoss: 2.7188 DMLLoss: 0.5409 
2022-03-18 03:10:24 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 5.2855, tea_CELoss: 2.0633 stu_CELoss: 2.7110 DMLLoss: 0.5112 
2022-03-18 03:11:20 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 4.6723, tea_CELoss: 1.8457 stu_CELoss: 2.3799 DMLLoss: 0.4467 
2022-03-18 03:12:16 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 5.0894, tea_CELoss: 2.0097 stu_CELoss: 2.6373 DMLLoss: 0.4425 
2022-03-18 03:13:13 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 5.2373, tea_CELoss: 2.1275 stu_CELoss: 2.6315 DMLLoss: 0.4783 
2022-03-18 03:14:09 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 4.8109, tea_CELoss: 1.9286 stu_CELoss: 2.4151 DMLLoss: 0.4673 
2022-03-18 03:15:06 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 5.5761, tea_CELoss: 2.2405 stu_CELoss: 2.8569 DMLLoss: 0.4787 
2022-03-18 03:16:02 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 5.3380, tea_CELoss: 2.1469 stu_CELoss: 2.6863 DMLLoss: 0.5048 
2022-03-18 03:16:05 - train: epoch 004, train_loss: 5.6073
2022-03-18 03:18:30 - eval: epoch: 004, tea_acc1: 55.670%, tea_acc5: 80.894%, tea_test_loss: 1.9004, stu_acc1: 43.262%, stu_acc5: 70.168%, stu_test_loss: 2.5046
2022-03-18 03:18:33 - until epoch: 004, tea_best_acc1: 55.670%, stu_best_acc1: 43.262%
2022-03-18 03:18:33 - epoch 005 lr: 0.1
2022-03-18 03:19:33 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 5.5406, tea_CELoss: 2.2312 stu_CELoss: 2.8077 DMLLoss: 0.5016 
2022-03-18 03:20:30 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 5.4503, tea_CELoss: 2.2181 stu_CELoss: 2.7609 DMLLoss: 0.4712 
2022-03-18 03:21:26 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 5.7645, tea_CELoss: 2.2937 stu_CELoss: 2.9695 DMLLoss: 0.5013 
2022-03-18 03:22:22 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 5.2336, tea_CELoss: 2.1178 stu_CELoss: 2.6445 DMLLoss: 0.4712 
2022-03-18 03:23:19 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 5.0949, tea_CELoss: 2.0552 stu_CELoss: 2.5622 DMLLoss: 0.4775 
2022-03-18 03:24:15 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 5.5042, tea_CELoss: 2.2182 stu_CELoss: 2.7879 DMLLoss: 0.4981 
2022-03-18 03:25:12 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 5.4885, tea_CELoss: 2.2400 stu_CELoss: 2.7459 DMLLoss: 0.5026 
2022-03-18 03:26:08 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 5.8579, tea_CELoss: 2.3619 stu_CELoss: 2.9649 DMLLoss: 0.5311 
2022-03-18 03:27:05 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 5.2988, tea_CELoss: 2.1208 stu_CELoss: 2.6805 DMLLoss: 0.4975 
2022-03-18 03:28:01 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 5.4659, tea_CELoss: 2.2645 stu_CELoss: 2.7278 DMLLoss: 0.4736 
2022-03-18 03:28:57 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 5.3404, tea_CELoss: 2.1228 stu_CELoss: 2.7212 DMLLoss: 0.4965 
2022-03-18 03:29:53 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 5.4812, tea_CELoss: 2.1796 stu_CELoss: 2.7894 DMLLoss: 0.5123 
2022-03-18 03:30:50 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 4.9697, tea_CELoss: 2.0322 stu_CELoss: 2.5016 DMLLoss: 0.4358 
2022-03-18 03:31:47 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 5.3240, tea_CELoss: 2.2381 stu_CELoss: 2.6616 DMLLoss: 0.4243 
2022-03-18 03:32:43 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 5.1447, tea_CELoss: 2.0858 stu_CELoss: 2.5756 DMLLoss: 0.4833 
2022-03-18 03:33:39 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 4.6905, tea_CELoss: 1.8964 stu_CELoss: 2.3879 DMLLoss: 0.4062 
2022-03-18 03:34:36 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 5.5573, tea_CELoss: 2.2692 stu_CELoss: 2.7957 DMLLoss: 0.4925 
2022-03-18 03:35:32 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 5.0213, tea_CELoss: 2.0597 stu_CELoss: 2.4602 DMLLoss: 0.5015 
2022-03-18 03:36:29 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 5.1369, tea_CELoss: 2.0992 stu_CELoss: 2.5631 DMLLoss: 0.4745 
2022-03-18 03:37:25 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 5.2665, tea_CELoss: 2.1189 stu_CELoss: 2.6616 DMLLoss: 0.4860 
2022-03-18 03:38:22 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 4.9895, tea_CELoss: 2.0129 stu_CELoss: 2.4951 DMLLoss: 0.4816 
2022-03-18 03:39:18 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 5.0987, tea_CELoss: 2.0586 stu_CELoss: 2.5760 DMLLoss: 0.4641 
2022-03-18 03:40:15 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 4.7133, tea_CELoss: 1.9206 stu_CELoss: 2.3183 DMLLoss: 0.4744 
2022-03-18 03:41:11 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 5.1558, tea_CELoss: 2.1504 stu_CELoss: 2.5556 DMLLoss: 0.4497 
2022-03-18 03:42:08 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 5.4377, tea_CELoss: 2.2117 stu_CELoss: 2.7452 DMLLoss: 0.4809 
2022-03-18 03:43:05 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 5.0492, tea_CELoss: 2.0394 stu_CELoss: 2.5298 DMLLoss: 0.4800 
2022-03-18 03:44:01 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 5.5661, tea_CELoss: 2.2077 stu_CELoss: 2.8005 DMLLoss: 0.5579 
2022-03-18 03:44:58 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 5.1970, tea_CELoss: 2.1096 stu_CELoss: 2.6035 DMLLoss: 0.4838 
2022-03-18 03:45:54 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 5.2908, tea_CELoss: 2.1520 stu_CELoss: 2.6568 DMLLoss: 0.4820 
2022-03-18 03:46:51 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 5.2852, tea_CELoss: 2.1603 stu_CELoss: 2.6591 DMLLoss: 0.4658 
2022-03-18 03:47:47 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 5.3353, tea_CELoss: 2.1498 stu_CELoss: 2.7377 DMLLoss: 0.4478 
2022-03-18 03:48:43 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 5.3205, tea_CELoss: 2.2043 stu_CELoss: 2.6711 DMLLoss: 0.4452 
2022-03-18 03:49:40 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 5.0150, tea_CELoss: 2.1086 stu_CELoss: 2.4493 DMLLoss: 0.4571 
2022-03-18 03:50:36 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 5.2040, tea_CELoss: 2.1286 stu_CELoss: 2.6032 DMLLoss: 0.4722 
2022-03-18 03:51:32 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 5.5540, tea_CELoss: 2.2820 stu_CELoss: 2.7962 DMLLoss: 0.4758 
2022-03-18 03:52:29 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 5.1879, tea_CELoss: 2.1462 stu_CELoss: 2.5949 DMLLoss: 0.4468 
2022-03-18 03:53:25 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 5.2094, tea_CELoss: 2.1756 stu_CELoss: 2.6194 DMLLoss: 0.4144 
2022-03-18 03:54:21 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 5.0764, tea_CELoss: 2.1033 stu_CELoss: 2.5171 DMLLoss: 0.4561 
2022-03-18 03:55:18 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 5.3688, tea_CELoss: 2.1924 stu_CELoss: 2.7219 DMLLoss: 0.4545 
2022-03-18 03:56:14 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 5.3279, tea_CELoss: 2.2082 stu_CELoss: 2.6206 DMLLoss: 0.4991 
2022-03-18 03:57:10 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 5.3202, tea_CELoss: 2.1481 stu_CELoss: 2.7032 DMLLoss: 0.4689 
2022-03-18 03:58:06 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 5.4804, tea_CELoss: 2.3193 stu_CELoss: 2.7173 DMLLoss: 0.4438 
2022-03-18 03:59:02 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 5.3725, tea_CELoss: 2.1881 stu_CELoss: 2.7129 DMLLoss: 0.4715 
2022-03-18 03:59:59 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 5.4955, tea_CELoss: 2.3194 stu_CELoss: 2.7444 DMLLoss: 0.4318 
2022-03-18 04:00:55 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 5.2551, tea_CELoss: 2.1899 stu_CELoss: 2.6225 DMLLoss: 0.4427 
2022-03-18 04:01:51 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 5.0413, tea_CELoss: 2.1213 stu_CELoss: 2.5028 DMLLoss: 0.4171 
2022-03-18 04:02:47 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 4.8769, tea_CELoss: 1.9627 stu_CELoss: 2.4854 DMLLoss: 0.4288 
2022-03-18 04:03:43 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 5.0123, tea_CELoss: 2.0661 stu_CELoss: 2.5008 DMLLoss: 0.4454 
2022-03-18 04:04:40 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 5.6628, tea_CELoss: 2.3258 stu_CELoss: 2.8358 DMLLoss: 0.5013 
2022-03-18 04:05:36 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 4.9402, tea_CELoss: 2.0702 stu_CELoss: 2.4282 DMLLoss: 0.4418 
2022-03-18 04:05:39 - train: epoch 005, train_loss: 5.2521
2022-03-18 04:08:05 - eval: epoch: 005, tea_acc1: 56.094%, tea_acc5: 80.938%, tea_test_loss: 1.8622, stu_acc1: 45.548%, stu_acc5: 72.092%, stu_test_loss: 2.3777
2022-03-18 04:08:08 - until epoch: 005, tea_best_acc1: 56.094%, stu_best_acc1: 45.548%
2022-03-18 04:08:08 - epoch 006 lr: 0.1
2022-03-18 04:09:08 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 4.8236, tea_CELoss: 2.0179 stu_CELoss: 2.3856 DMLLoss: 0.4201 
2022-03-18 04:10:05 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 5.0639, tea_CELoss: 2.1356 stu_CELoss: 2.5254 DMLLoss: 0.4029 
2022-03-18 04:11:01 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 4.7642, tea_CELoss: 1.8801 stu_CELoss: 2.4076 DMLLoss: 0.4766 
2022-03-18 04:11:58 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 5.1434, tea_CELoss: 2.1383 stu_CELoss: 2.5463 DMLLoss: 0.4587 
2022-03-18 04:12:55 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 5.0839, tea_CELoss: 2.0387 stu_CELoss: 2.5339 DMLLoss: 0.5113 
2022-03-18 04:13:51 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 4.7539, tea_CELoss: 1.9801 stu_CELoss: 2.3568 DMLLoss: 0.4171 
2022-03-18 04:14:47 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 5.4863, tea_CELoss: 2.1953 stu_CELoss: 2.8161 DMLLoss: 0.4749 
2022-03-18 04:15:44 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 4.8778, tea_CELoss: 1.9432 stu_CELoss: 2.4365 DMLLoss: 0.4981 
2022-03-18 04:16:40 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 4.7520, tea_CELoss: 1.9542 stu_CELoss: 2.3929 DMLLoss: 0.4049 
2022-03-18 04:17:37 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 5.0957, tea_CELoss: 2.0618 stu_CELoss: 2.5717 DMLLoss: 0.4622 
2022-03-18 04:18:33 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 5.2460, tea_CELoss: 2.1061 stu_CELoss: 2.6258 DMLLoss: 0.5141 
2022-03-18 04:19:30 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 5.2333, tea_CELoss: 2.1680 stu_CELoss: 2.5794 DMLLoss: 0.4859 
2022-03-18 04:20:26 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 5.1625, tea_CELoss: 2.1294 stu_CELoss: 2.5775 DMLLoss: 0.4557 
2022-03-18 04:21:23 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 5.3890, tea_CELoss: 2.2676 stu_CELoss: 2.6469 DMLLoss: 0.4745 
2022-03-18 04:22:19 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 5.4110, tea_CELoss: 2.2081 stu_CELoss: 2.7295 DMLLoss: 0.4734 
2022-03-18 04:23:16 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 4.8314, tea_CELoss: 2.0004 stu_CELoss: 2.3843 DMLLoss: 0.4467 
2022-03-18 04:24:13 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 5.3565, tea_CELoss: 2.2191 stu_CELoss: 2.6762 DMLLoss: 0.4611 
2022-03-18 04:25:09 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 5.0114, tea_CELoss: 2.0849 stu_CELoss: 2.4822 DMLLoss: 0.4444 
2022-03-18 04:26:06 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 4.8255, tea_CELoss: 1.9985 stu_CELoss: 2.4076 DMLLoss: 0.4193 
2022-03-18 04:27:02 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 5.3059, tea_CELoss: 2.1858 stu_CELoss: 2.6780 DMLLoss: 0.4421 
2022-03-18 04:27:59 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 5.1454, tea_CELoss: 2.1729 stu_CELoss: 2.5382 DMLLoss: 0.4343 
2022-03-18 04:28:55 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 4.6572, tea_CELoss: 1.9016 stu_CELoss: 2.3099 DMLLoss: 0.4457 
2022-03-18 04:29:52 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 4.9652, tea_CELoss: 2.0693 stu_CELoss: 2.4773 DMLLoss: 0.4186 
2022-03-18 04:30:49 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 4.9284, tea_CELoss: 2.0300 stu_CELoss: 2.4767 DMLLoss: 0.4217 
2022-03-18 04:31:45 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 5.0013, tea_CELoss: 2.0412 stu_CELoss: 2.4807 DMLLoss: 0.4794 
2022-03-18 04:32:41 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 5.0470, tea_CELoss: 2.1023 stu_CELoss: 2.4932 DMLLoss: 0.4515 
2022-03-18 04:33:38 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 4.9853, tea_CELoss: 2.1116 stu_CELoss: 2.4523 DMLLoss: 0.4214 
2022-03-18 04:34:34 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 4.5915, tea_CELoss: 1.8691 stu_CELoss: 2.2711 DMLLoss: 0.4513 
2022-03-18 04:35:30 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 4.9877, tea_CELoss: 2.0647 stu_CELoss: 2.4760 DMLLoss: 0.4470 
2022-03-18 04:36:26 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 4.9116, tea_CELoss: 2.0239 stu_CELoss: 2.4426 DMLLoss: 0.4451 
2022-03-18 04:37:23 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 4.9078, tea_CELoss: 2.0285 stu_CELoss: 2.4070 DMLLoss: 0.4723 
2022-03-18 04:38:19 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 4.8654, tea_CELoss: 2.0122 stu_CELoss: 2.4168 DMLLoss: 0.4364 
2022-03-18 04:39:16 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 4.8316, tea_CELoss: 1.9487 stu_CELoss: 2.4216 DMLLoss: 0.4614 
2022-03-18 04:40:12 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 4.9837, tea_CELoss: 2.1126 stu_CELoss: 2.4389 DMLLoss: 0.4322 
2022-03-18 04:41:09 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 4.9289, tea_CELoss: 2.0623 stu_CELoss: 2.4448 DMLLoss: 0.4218 
2022-03-18 04:42:05 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 4.9404, tea_CELoss: 2.0080 stu_CELoss: 2.5168 DMLLoss: 0.4157 
2022-03-18 04:43:02 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 4.6721, tea_CELoss: 1.9893 stu_CELoss: 2.2950 DMLLoss: 0.3878 
2022-03-18 04:43:58 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 4.6078, tea_CELoss: 1.8779 stu_CELoss: 2.3020 DMLLoss: 0.4280 
2022-03-18 04:44:55 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 4.9425, tea_CELoss: 2.0245 stu_CELoss: 2.5118 DMLLoss: 0.4062 
2022-03-18 04:45:51 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 4.8939, tea_CELoss: 2.0494 stu_CELoss: 2.4304 DMLLoss: 0.4141 
2022-03-18 04:46:48 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 5.0357, tea_CELoss: 2.0918 stu_CELoss: 2.5046 DMLLoss: 0.4393 
2022-03-18 04:47:45 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 4.5743, tea_CELoss: 1.9423 stu_CELoss: 2.2445 DMLLoss: 0.3875 
2022-03-18 04:48:41 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 5.0845, tea_CELoss: 2.0671 stu_CELoss: 2.5979 DMLLoss: 0.4195 
2022-03-18 04:49:38 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 4.9555, tea_CELoss: 2.0106 stu_CELoss: 2.4744 DMLLoss: 0.4706 
2022-03-18 04:50:34 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 5.1498, tea_CELoss: 2.2035 stu_CELoss: 2.5216 DMLLoss: 0.4247 
2022-03-18 04:51:31 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 4.8599, tea_CELoss: 1.9783 stu_CELoss: 2.4351 DMLLoss: 0.4465 
2022-03-18 04:52:28 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 4.9078, tea_CELoss: 2.0161 stu_CELoss: 2.4410 DMLLoss: 0.4507 
2022-03-18 04:53:25 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 4.9017, tea_CELoss: 2.0333 stu_CELoss: 2.4497 DMLLoss: 0.4187 
2022-03-18 04:54:21 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 5.0772, tea_CELoss: 2.1685 stu_CELoss: 2.5089 DMLLoss: 0.3998 
2022-03-18 04:55:18 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 4.7934, tea_CELoss: 2.0024 stu_CELoss: 2.3218 DMLLoss: 0.4692 
2022-03-18 04:55:21 - train: epoch 006, train_loss: 5.0323
2022-03-18 04:57:47 - eval: epoch: 006, tea_acc1: 56.554%, tea_acc5: 81.130%, tea_test_loss: 1.8347, stu_acc1: 50.008%, stu_acc5: 75.616%, stu_test_loss: 2.1698
2022-03-18 04:57:50 - until epoch: 006, tea_best_acc1: 56.554%, stu_best_acc1: 50.008%
2022-03-18 04:57:50 - epoch 007 lr: 0.1
2022-03-18 04:58:50 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 4.5535, tea_CELoss: 1.9174 stu_CELoss: 2.2123 DMLLoss: 0.4238 
2022-03-18 04:59:46 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 4.9131, tea_CELoss: 2.0636 stu_CELoss: 2.4324 DMLLoss: 0.4170 
2022-03-18 05:00:43 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 5.2259, tea_CELoss: 2.1960 stu_CELoss: 2.5693 DMLLoss: 0.4606 
2022-03-18 05:01:39 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 4.9762, tea_CELoss: 2.0845 stu_CELoss: 2.4644 DMLLoss: 0.4273 
2022-03-18 05:02:35 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 4.5397, tea_CELoss: 1.9251 stu_CELoss: 2.2348 DMLLoss: 0.3798 
2022-03-18 05:03:32 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 5.3697, tea_CELoss: 2.2560 stu_CELoss: 2.6500 DMLLoss: 0.4637 
2022-03-18 05:04:28 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 4.8160, tea_CELoss: 2.0513 stu_CELoss: 2.3685 DMLLoss: 0.3962 
2022-03-18 05:05:25 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 5.0331, tea_CELoss: 2.0666 stu_CELoss: 2.5318 DMLLoss: 0.4347 
2022-03-18 05:06:21 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 5.0112, tea_CELoss: 2.0705 stu_CELoss: 2.4778 DMLLoss: 0.4629 
2022-03-18 05:07:17 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 4.7534, tea_CELoss: 1.9700 stu_CELoss: 2.3793 DMLLoss: 0.4042 
2022-03-18 05:08:13 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 5.0338, tea_CELoss: 2.0837 stu_CELoss: 2.5098 DMLLoss: 0.4403 
2022-03-18 05:09:10 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 4.8363, tea_CELoss: 2.0634 stu_CELoss: 2.3253 DMLLoss: 0.4477 
2022-03-18 05:10:06 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 4.3255, tea_CELoss: 1.8169 stu_CELoss: 2.1134 DMLLoss: 0.3952 
2022-03-18 05:11:03 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 4.9856, tea_CELoss: 2.0816 stu_CELoss: 2.4761 DMLLoss: 0.4279 
2022-03-18 05:11:59 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 5.2135, tea_CELoss: 2.2172 stu_CELoss: 2.5613 DMLLoss: 0.4350 
2022-03-18 05:12:55 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 5.0535, tea_CELoss: 2.0443 stu_CELoss: 2.5063 DMLLoss: 0.5028 
2022-03-18 05:13:52 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 4.5724, tea_CELoss: 1.9604 stu_CELoss: 2.2158 DMLLoss: 0.3962 
2022-03-18 05:14:48 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 4.9964, tea_CELoss: 2.1093 stu_CELoss: 2.4884 DMLLoss: 0.3987 
2022-03-18 05:15:45 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 4.8087, tea_CELoss: 2.0050 stu_CELoss: 2.3833 DMLLoss: 0.4204 
2022-03-18 05:16:41 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 4.3137, tea_CELoss: 1.7844 stu_CELoss: 2.1150 DMLLoss: 0.4143 
2022-03-18 05:17:38 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 5.0662, tea_CELoss: 2.1412 stu_CELoss: 2.5192 DMLLoss: 0.4058 
2022-03-18 05:18:34 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 4.6045, tea_CELoss: 1.9227 stu_CELoss: 2.2773 DMLLoss: 0.4045 
2022-03-18 05:19:31 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 5.2270, tea_CELoss: 2.1835 stu_CELoss: 2.5759 DMLLoss: 0.4676 
2022-03-18 05:20:27 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 5.2348, tea_CELoss: 2.2164 stu_CELoss: 2.6030 DMLLoss: 0.4154 
2022-03-18 05:21:24 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 4.8928, tea_CELoss: 2.0820 stu_CELoss: 2.4158 DMLLoss: 0.3950 
2022-03-18 05:22:20 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 4.5451, tea_CELoss: 1.8855 stu_CELoss: 2.2903 DMLLoss: 0.3693 
2022-03-18 05:23:17 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 4.6778, tea_CELoss: 1.9166 stu_CELoss: 2.3332 DMLLoss: 0.4279 
2022-03-18 05:24:13 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 4.8748, tea_CELoss: 2.0189 stu_CELoss: 2.4148 DMLLoss: 0.4411 
2022-03-18 05:25:09 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 4.7555, tea_CELoss: 1.9854 stu_CELoss: 2.3117 DMLLoss: 0.4584 
2022-03-18 05:26:06 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 5.2809, tea_CELoss: 2.2605 stu_CELoss: 2.5658 DMLLoss: 0.4546 
2022-03-18 05:27:02 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 4.5225, tea_CELoss: 1.9506 stu_CELoss: 2.1551 DMLLoss: 0.4167 
2022-03-18 05:27:58 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 4.4562, tea_CELoss: 1.8156 stu_CELoss: 2.2023 DMLLoss: 0.4383 
2022-03-18 05:28:55 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 5.2149, tea_CELoss: 2.1942 stu_CELoss: 2.5604 DMLLoss: 0.4603 
2022-03-18 05:29:51 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 5.2546, tea_CELoss: 2.2151 stu_CELoss: 2.6062 DMLLoss: 0.4333 
2022-03-18 05:30:47 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 5.0899, tea_CELoss: 2.1419 stu_CELoss: 2.5450 DMLLoss: 0.4030 
2022-03-18 05:31:44 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 4.7058, tea_CELoss: 2.0390 stu_CELoss: 2.2333 DMLLoss: 0.4335 
2022-03-18 05:32:40 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 4.6094, tea_CELoss: 1.9336 stu_CELoss: 2.2288 DMLLoss: 0.4470 
2022-03-18 05:33:37 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 5.4354, tea_CELoss: 2.3261 stu_CELoss: 2.6322 DMLLoss: 0.4770 
2022-03-18 05:34:33 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 4.9874, tea_CELoss: 2.1531 stu_CELoss: 2.4179 DMLLoss: 0.4165 
2022-03-18 05:35:30 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 5.0015, tea_CELoss: 2.1684 stu_CELoss: 2.4160 DMLLoss: 0.4171 
2022-03-18 05:36:26 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 4.7881, tea_CELoss: 1.9212 stu_CELoss: 2.4219 DMLLoss: 0.4451 
2022-03-18 05:37:23 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 4.6195, tea_CELoss: 1.9199 stu_CELoss: 2.2377 DMLLoss: 0.4619 
2022-03-18 05:38:19 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 5.0107, tea_CELoss: 2.1223 stu_CELoss: 2.4960 DMLLoss: 0.3924 
2022-03-18 05:39:15 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 4.7469, tea_CELoss: 1.9159 stu_CELoss: 2.3708 DMLLoss: 0.4602 
2022-03-18 05:40:12 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 4.9387, tea_CELoss: 2.1191 stu_CELoss: 2.4329 DMLLoss: 0.3867 
2022-03-18 05:41:08 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 4.9208, tea_CELoss: 1.9661 stu_CELoss: 2.4785 DMLLoss: 0.4763 
2022-03-18 05:42:05 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 4.4204, tea_CELoss: 1.8499 stu_CELoss: 2.1583 DMLLoss: 0.4123 
2022-03-18 05:43:01 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 5.2336, tea_CELoss: 2.2828 stu_CELoss: 2.5474 DMLLoss: 0.4033 
2022-03-18 05:43:58 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 4.6778, tea_CELoss: 1.9208 stu_CELoss: 2.3236 DMLLoss: 0.4334 
2022-03-18 05:44:54 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 4.6496, tea_CELoss: 1.9795 stu_CELoss: 2.2716 DMLLoss: 0.3984 
2022-03-18 05:44:57 - train: epoch 007, train_loss: 4.8874
2022-03-18 05:47:23 - eval: epoch: 007, tea_acc1: 57.170%, tea_acc5: 81.826%, tea_test_loss: 1.8006, stu_acc1: 51.150%, stu_acc5: 77.022%, stu_test_loss: 2.0863
2022-03-18 05:47:26 - until epoch: 007, tea_best_acc1: 57.170%, stu_best_acc1: 51.150%
2022-03-18 05:47:26 - epoch 008 lr: 0.1
2022-03-18 05:48:26 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 4.6808, tea_CELoss: 1.9949 stu_CELoss: 2.2804 DMLLoss: 0.4054 
2022-03-18 05:49:22 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 4.9684, tea_CELoss: 2.0899 stu_CELoss: 2.4465 DMLLoss: 0.4320 
2022-03-18 05:50:19 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 4.7417, tea_CELoss: 1.9943 stu_CELoss: 2.3201 DMLLoss: 0.4272 
2022-03-18 05:51:15 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 4.5876, tea_CELoss: 1.8933 stu_CELoss: 2.2716 DMLLoss: 0.4227 
2022-03-18 05:52:11 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 4.4645, tea_CELoss: 1.8398 stu_CELoss: 2.2031 DMLLoss: 0.4217 
2022-03-18 05:53:07 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 4.5400, tea_CELoss: 1.9278 stu_CELoss: 2.2186 DMLLoss: 0.3936 
2022-03-18 05:54:04 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 4.8514, tea_CELoss: 2.0728 stu_CELoss: 2.3618 DMLLoss: 0.4168 
2022-03-18 05:55:00 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 4.2307, tea_CELoss: 1.7242 stu_CELoss: 2.0738 DMLLoss: 0.4328 
2022-03-18 05:55:56 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 4.7894, tea_CELoss: 2.0246 stu_CELoss: 2.3522 DMLLoss: 0.4126 
2022-03-18 05:56:53 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 4.8524, tea_CELoss: 2.0572 stu_CELoss: 2.3855 DMLLoss: 0.4097 
2022-03-18 05:57:50 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 4.6340, tea_CELoss: 1.9608 stu_CELoss: 2.2661 DMLLoss: 0.4071 
2022-03-18 05:58:46 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 4.1966, tea_CELoss: 1.6961 stu_CELoss: 2.0614 DMLLoss: 0.4390 
2022-03-18 05:59:42 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 4.8089, tea_CELoss: 2.0707 stu_CELoss: 2.3366 DMLLoss: 0.4016 
2022-03-18 06:00:39 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 4.7220, tea_CELoss: 2.0051 stu_CELoss: 2.3210 DMLLoss: 0.3960 
2022-03-18 06:01:35 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 4.8180, tea_CELoss: 2.1083 stu_CELoss: 2.2894 DMLLoss: 0.4203 
2022-03-18 06:02:31 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 4.7865, tea_CELoss: 2.0541 stu_CELoss: 2.3378 DMLLoss: 0.3946 
2022-03-18 06:03:28 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 4.7384, tea_CELoss: 2.0471 stu_CELoss: 2.2958 DMLLoss: 0.3955 
2022-03-18 06:04:24 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 4.8794, tea_CELoss: 2.1005 stu_CELoss: 2.3692 DMLLoss: 0.4097 
2022-03-18 06:05:21 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 4.4216, tea_CELoss: 1.8517 stu_CELoss: 2.1749 DMLLoss: 0.3950 
2022-03-18 06:06:17 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 5.4624, tea_CELoss: 2.3360 stu_CELoss: 2.6780 DMLLoss: 0.4484 
2022-03-18 06:07:14 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 4.7993, tea_CELoss: 1.9985 stu_CELoss: 2.3687 DMLLoss: 0.4321 
2022-03-18 06:08:10 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 5.0462, tea_CELoss: 2.1545 stu_CELoss: 2.4436 DMLLoss: 0.4481 
2022-03-18 06:09:07 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 4.5774, tea_CELoss: 1.9642 stu_CELoss: 2.2589 DMLLoss: 0.3543 
2022-03-18 06:10:03 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 4.4646, tea_CELoss: 1.9046 stu_CELoss: 2.1541 DMLLoss: 0.4058 
2022-03-18 06:11:00 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 4.7189, tea_CELoss: 1.9897 stu_CELoss: 2.3158 DMLLoss: 0.4134 
2022-03-18 06:11:56 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 4.7111, tea_CELoss: 1.9622 stu_CELoss: 2.3160 DMLLoss: 0.4329 
2022-03-18 06:12:53 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 5.1375, tea_CELoss: 2.1938 stu_CELoss: 2.5013 DMLLoss: 0.4425 
2022-03-18 06:13:49 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 4.8550, tea_CELoss: 2.0711 stu_CELoss: 2.3732 DMLLoss: 0.4107 
2022-03-18 06:14:46 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 5.0360, tea_CELoss: 2.1293 stu_CELoss: 2.4692 DMLLoss: 0.4375 
2022-03-18 06:15:43 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 5.2042, tea_CELoss: 2.2716 stu_CELoss: 2.5369 DMLLoss: 0.3957 
2022-03-18 06:16:39 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 4.8726, tea_CELoss: 2.0669 stu_CELoss: 2.3872 DMLLoss: 0.4185 
2022-03-18 06:17:36 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 5.0737, tea_CELoss: 2.1973 stu_CELoss: 2.4682 DMLLoss: 0.4082 
2022-03-18 06:18:33 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 4.7192, tea_CELoss: 2.0179 stu_CELoss: 2.3221 DMLLoss: 0.3792 
2022-03-18 06:19:29 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 4.9757, tea_CELoss: 2.1535 stu_CELoss: 2.4315 DMLLoss: 0.3907 
2022-03-18 06:20:26 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 4.6631, tea_CELoss: 2.0014 stu_CELoss: 2.2841 DMLLoss: 0.3776 
2022-03-18 06:21:23 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 4.9163, tea_CELoss: 2.0896 stu_CELoss: 2.4184 DMLLoss: 0.4083 
2022-03-18 06:22:20 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 4.4788, tea_CELoss: 1.8919 stu_CELoss: 2.2189 DMLLoss: 0.3680 
2022-03-18 06:23:16 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 4.6048, tea_CELoss: 1.9198 stu_CELoss: 2.3055 DMLLoss: 0.3795 
2022-03-18 06:24:13 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 4.8650, tea_CELoss: 2.1263 stu_CELoss: 2.3811 DMLLoss: 0.3577 
2022-03-18 06:25:09 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 5.2123, tea_CELoss: 2.2682 stu_CELoss: 2.5434 DMLLoss: 0.4008 
2022-03-18 06:26:06 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 4.5433, tea_CELoss: 1.9463 stu_CELoss: 2.2005 DMLLoss: 0.3965 
2022-03-18 06:27:03 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 4.7324, tea_CELoss: 1.9853 stu_CELoss: 2.3341 DMLLoss: 0.4130 
2022-03-18 06:27:59 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 4.2763, tea_CELoss: 1.8002 stu_CELoss: 2.1013 DMLLoss: 0.3748 
2022-03-18 06:28:56 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 4.9416, tea_CELoss: 2.0462 stu_CELoss: 2.4285 DMLLoss: 0.4669 
2022-03-18 06:29:53 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 4.7554, tea_CELoss: 2.0272 stu_CELoss: 2.3183 DMLLoss: 0.4099 
2022-03-18 06:30:49 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 4.7515, tea_CELoss: 2.0197 stu_CELoss: 2.3211 DMLLoss: 0.4107 
2022-03-18 06:31:46 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 4.7063, tea_CELoss: 1.9538 stu_CELoss: 2.3434 DMLLoss: 0.4091 
2022-03-18 06:32:42 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 4.6240, tea_CELoss: 1.9754 stu_CELoss: 2.2916 DMLLoss: 0.3570 
2022-03-18 06:33:39 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 4.6803, tea_CELoss: 2.0290 stu_CELoss: 2.2750 DMLLoss: 0.3763 
2022-03-18 06:34:35 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 4.7332, tea_CELoss: 2.0388 stu_CELoss: 2.3009 DMLLoss: 0.3935 
2022-03-18 06:34:38 - train: epoch 008, train_loss: 4.7841
2022-03-18 06:37:04 - eval: epoch: 008, tea_acc1: 55.472%, tea_acc5: 80.428%, tea_test_loss: 1.8805, stu_acc1: 50.402%, stu_acc5: 76.220%, stu_test_loss: 2.1488
2022-03-18 06:37:06 - until epoch: 008, tea_best_acc1: 57.170%, stu_best_acc1: 51.150%
2022-03-18 06:37:06 - epoch 009 lr: 0.1
2022-03-18 06:38:07 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 4.1668, tea_CELoss: 1.8010 stu_CELoss: 2.0102 DMLLoss: 0.3557 
2022-03-18 06:39:04 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 4.6861, tea_CELoss: 1.9705 stu_CELoss: 2.2946 DMLLoss: 0.4210 
2022-03-18 06:40:00 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 4.4362, tea_CELoss: 1.8494 stu_CELoss: 2.1828 DMLLoss: 0.4040 
2022-03-18 06:40:56 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 5.2614, tea_CELoss: 2.2289 stu_CELoss: 2.5474 DMLLoss: 0.4851 
2022-03-18 06:41:53 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 4.3107, tea_CELoss: 1.8206 stu_CELoss: 2.0888 DMLLoss: 0.4014 
2022-03-18 06:42:49 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 4.4290, tea_CELoss: 1.8534 stu_CELoss: 2.1927 DMLLoss: 0.3828 
2022-03-18 06:43:45 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 4.2281, tea_CELoss: 1.7538 stu_CELoss: 2.0599 DMLLoss: 0.4144 
2022-03-18 06:44:42 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 4.4400, tea_CELoss: 1.8575 stu_CELoss: 2.1888 DMLLoss: 0.3938 
2022-03-18 06:45:38 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 4.3743, tea_CELoss: 1.8263 stu_CELoss: 2.1635 DMLLoss: 0.3846 
2022-03-18 06:46:35 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 4.6284, tea_CELoss: 1.9998 stu_CELoss: 2.2487 DMLLoss: 0.3799 
2022-03-18 06:47:31 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 5.0527, tea_CELoss: 2.1829 stu_CELoss: 2.4291 DMLLoss: 0.4407 
2022-03-18 06:48:28 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 4.6945, tea_CELoss: 2.0282 stu_CELoss: 2.2481 DMLLoss: 0.4182 
2022-03-18 06:49:24 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 5.0670, tea_CELoss: 2.1720 stu_CELoss: 2.4401 DMLLoss: 0.4549 
2022-03-18 06:50:20 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 4.1589, tea_CELoss: 1.7598 stu_CELoss: 2.0460 DMLLoss: 0.3532 
2022-03-18 06:51:17 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 4.4327, tea_CELoss: 1.8979 stu_CELoss: 2.1468 DMLLoss: 0.3881 
2022-03-18 06:52:13 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 4.7230, tea_CELoss: 1.9901 stu_CELoss: 2.2792 DMLLoss: 0.4537 
2022-03-18 06:53:09 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 4.8989, tea_CELoss: 2.0557 stu_CELoss: 2.4404 DMLLoss: 0.4027 
2022-03-18 06:54:06 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 4.6460, tea_CELoss: 1.9925 stu_CELoss: 2.2958 DMLLoss: 0.3577 
2022-03-18 06:55:02 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 4.3432, tea_CELoss: 1.8013 stu_CELoss: 2.1034 DMLLoss: 0.4385 
2022-03-18 06:55:58 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 4.4687, tea_CELoss: 1.8142 stu_CELoss: 2.2426 DMLLoss: 0.4118 
2022-03-18 06:56:55 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 4.8906, tea_CELoss: 2.0812 stu_CELoss: 2.3774 DMLLoss: 0.4320 
2022-03-18 06:57:51 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 4.8827, tea_CELoss: 2.0900 stu_CELoss: 2.3539 DMLLoss: 0.4389 
2022-03-18 06:58:48 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 4.2047, tea_CELoss: 1.7505 stu_CELoss: 2.0478 DMLLoss: 0.4064 
2022-03-18 06:59:44 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 4.7273, tea_CELoss: 2.0366 stu_CELoss: 2.3061 DMLLoss: 0.3846 
2022-03-18 07:00:40 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 4.4803, tea_CELoss: 1.9731 stu_CELoss: 2.1265 DMLLoss: 0.3807 
2022-03-18 07:01:37 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 4.6871, tea_CELoss: 1.9380 stu_CELoss: 2.2987 DMLLoss: 0.4504 
2022-03-18 07:02:33 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 4.7462, tea_CELoss: 2.0363 stu_CELoss: 2.3368 DMLLoss: 0.3730 
2022-03-18 07:03:30 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 4.9820, tea_CELoss: 2.1016 stu_CELoss: 2.4759 DMLLoss: 0.4045 
2022-03-18 07:04:26 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 4.2613, tea_CELoss: 1.8132 stu_CELoss: 2.0281 DMLLoss: 0.4201 
2022-03-18 07:05:23 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 4.1859, tea_CELoss: 1.8473 stu_CELoss: 1.9932 DMLLoss: 0.3454 
2022-03-18 07:06:19 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 4.4966, tea_CELoss: 1.9277 stu_CELoss: 2.1965 DMLLoss: 0.3723 
2022-03-18 07:07:15 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 4.7621, tea_CELoss: 2.0331 stu_CELoss: 2.3428 DMLLoss: 0.3862 
2022-03-18 07:08:12 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 5.1172, tea_CELoss: 2.2182 stu_CELoss: 2.4801 DMLLoss: 0.4189 
2022-03-18 07:09:08 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 4.5605, tea_CELoss: 1.9174 stu_CELoss: 2.2528 DMLLoss: 0.3903 
2022-03-18 07:10:04 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 4.7964, tea_CELoss: 2.0639 stu_CELoss: 2.3837 DMLLoss: 0.3487 
2022-03-18 07:11:01 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 4.4032, tea_CELoss: 1.8706 stu_CELoss: 2.1615 DMLLoss: 0.3711 
2022-03-18 07:11:57 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 4.7518, tea_CELoss: 2.0210 stu_CELoss: 2.3346 DMLLoss: 0.3962 
2022-03-18 07:12:54 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 4.8116, tea_CELoss: 2.0821 stu_CELoss: 2.3258 DMLLoss: 0.4038 
2022-03-18 07:13:50 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 4.3663, tea_CELoss: 1.8966 stu_CELoss: 2.1129 DMLLoss: 0.3569 
2022-03-18 07:14:46 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 5.2390, tea_CELoss: 2.2817 stu_CELoss: 2.5740 DMLLoss: 0.3833 
2022-03-18 07:15:43 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 4.8098, tea_CELoss: 2.0437 stu_CELoss: 2.3423 DMLLoss: 0.4238 
2022-03-18 07:16:39 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 4.5035, tea_CELoss: 1.9874 stu_CELoss: 2.1119 DMLLoss: 0.4041 
2022-03-18 07:17:36 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 4.1529, tea_CELoss: 1.7997 stu_CELoss: 2.0180 DMLLoss: 0.3352 
2022-03-18 07:18:32 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 4.7051, tea_CELoss: 2.0088 stu_CELoss: 2.2878 DMLLoss: 0.4085 
2022-03-18 07:19:28 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 4.9252, tea_CELoss: 2.1363 stu_CELoss: 2.3511 DMLLoss: 0.4378 
2022-03-18 07:20:25 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 5.1165, tea_CELoss: 2.2165 stu_CELoss: 2.4869 DMLLoss: 0.4131 
2022-03-18 07:21:21 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 5.1792, tea_CELoss: 2.2475 stu_CELoss: 2.5312 DMLLoss: 0.4004 
2022-03-18 07:22:18 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 4.7670, tea_CELoss: 2.0277 stu_CELoss: 2.3417 DMLLoss: 0.3976 
2022-03-18 07:23:14 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 4.8697, tea_CELoss: 2.1011 stu_CELoss: 2.3616 DMLLoss: 0.4070 
2022-03-18 07:24:10 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 4.3419, tea_CELoss: 1.8701 stu_CELoss: 2.1026 DMLLoss: 0.3691 
2022-03-18 07:24:13 - train: epoch 009, train_loss: 4.7051
2022-03-18 07:26:39 - eval: epoch: 009, tea_acc1: 56.460%, tea_acc5: 81.364%, tea_test_loss: 1.8253, stu_acc1: 51.174%, stu_acc5: 76.604%, stu_test_loss: 2.1073
2022-03-18 07:26:41 - until epoch: 009, tea_best_acc1: 57.170%, stu_best_acc1: 51.174%
2022-03-18 07:26:41 - epoch 010 lr: 0.1
2022-03-18 07:27:42 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 4.2839, tea_CELoss: 1.8171 stu_CELoss: 2.0876 DMLLoss: 0.3791 
2022-03-18 07:28:39 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 4.4324, tea_CELoss: 1.8827 stu_CELoss: 2.1768 DMLLoss: 0.3729 
2022-03-18 07:29:35 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 4.7757, tea_CELoss: 2.0695 stu_CELoss: 2.3111 DMLLoss: 0.3951 
2022-03-18 07:30:32 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 5.0116, tea_CELoss: 2.1262 stu_CELoss: 2.4279 DMLLoss: 0.4575 
2022-03-18 07:31:28 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 4.5467, tea_CELoss: 1.9753 stu_CELoss: 2.1798 DMLLoss: 0.3916 
2022-03-18 07:32:25 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 4.9938, tea_CELoss: 2.1224 stu_CELoss: 2.4673 DMLLoss: 0.4041 
2022-03-18 07:33:22 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 4.8582, tea_CELoss: 2.1281 stu_CELoss: 2.3362 DMLLoss: 0.3939 
2022-03-18 07:34:18 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 4.9007, tea_CELoss: 2.1095 stu_CELoss: 2.3888 DMLLoss: 0.4024 
2022-03-18 07:35:15 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 4.2209, tea_CELoss: 1.8052 stu_CELoss: 2.0451 DMLLoss: 0.3706 
2022-03-18 07:36:11 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 4.1072, tea_CELoss: 1.7644 stu_CELoss: 1.9732 DMLLoss: 0.3696 
2022-03-18 07:37:08 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 4.4960, tea_CELoss: 1.8089 stu_CELoss: 2.2246 DMLLoss: 0.4625 
2022-03-18 07:38:05 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 4.8092, tea_CELoss: 2.0632 stu_CELoss: 2.3075 DMLLoss: 0.4385 
2022-03-18 07:39:01 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 4.3611, tea_CELoss: 1.8545 stu_CELoss: 2.1249 DMLLoss: 0.3817 
2022-03-18 07:39:58 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 4.9448, tea_CELoss: 2.1072 stu_CELoss: 2.4397 DMLLoss: 0.3979 
2022-03-18 07:40:54 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 4.1219, tea_CELoss: 1.7243 stu_CELoss: 2.0377 DMLLoss: 0.3600 
2022-03-18 07:41:51 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 4.7249, tea_CELoss: 1.9403 stu_CELoss: 2.3238 DMLLoss: 0.4607 
2022-03-18 07:42:47 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 5.0379, tea_CELoss: 2.1538 stu_CELoss: 2.4628 DMLLoss: 0.4213 
2022-03-18 07:43:43 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 4.4766, tea_CELoss: 1.9305 stu_CELoss: 2.1607 DMLLoss: 0.3854 
2022-03-18 07:44:39 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 4.4858, tea_CELoss: 1.9109 stu_CELoss: 2.2015 DMLLoss: 0.3734 
2022-03-18 07:45:35 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 4.7604, tea_CELoss: 2.0778 stu_CELoss: 2.2771 DMLLoss: 0.4055 
2022-03-18 07:46:32 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 4.2651, tea_CELoss: 1.8504 stu_CELoss: 2.0358 DMLLoss: 0.3789 
2022-03-18 07:47:28 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 4.7798, tea_CELoss: 2.0213 stu_CELoss: 2.3618 DMLLoss: 0.3968 
2022-03-18 07:48:24 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 4.9374, tea_CELoss: 2.1550 stu_CELoss: 2.3970 DMLLoss: 0.3854 
2022-03-18 07:49:21 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 5.2780, tea_CELoss: 2.2794 stu_CELoss: 2.6107 DMLLoss: 0.3878 
2022-03-18 07:50:17 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 4.8065, tea_CELoss: 2.0710 stu_CELoss: 2.3413 DMLLoss: 0.3942 
2022-03-18 07:51:13 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 4.7144, tea_CELoss: 1.9995 stu_CELoss: 2.3135 DMLLoss: 0.4013 
2022-03-18 07:52:10 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 4.4231, tea_CELoss: 1.9571 stu_CELoss: 2.0655 DMLLoss: 0.4005 
2022-03-18 07:53:06 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 4.7961, tea_CELoss: 2.0312 stu_CELoss: 2.3531 DMLLoss: 0.4118 
2022-03-18 07:54:02 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 4.8261, tea_CELoss: 2.0637 stu_CELoss: 2.3288 DMLLoss: 0.4337 
2022-03-18 07:54:58 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 4.3062, tea_CELoss: 1.8586 stu_CELoss: 2.0775 DMLLoss: 0.3702 
2022-03-18 07:55:55 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 5.1497, tea_CELoss: 2.2270 stu_CELoss: 2.4820 DMLLoss: 0.4407 
2022-03-18 07:56:51 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 4.6708, tea_CELoss: 1.9715 stu_CELoss: 2.2933 DMLLoss: 0.4060 
2022-03-18 07:57:47 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 4.9520, tea_CELoss: 2.2418 stu_CELoss: 2.3780 DMLLoss: 0.3322 
2022-03-18 07:58:44 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 5.0666, tea_CELoss: 2.2134 stu_CELoss: 2.4222 DMLLoss: 0.4310 
2022-03-18 07:59:40 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 4.9767, tea_CELoss: 2.1349 stu_CELoss: 2.4353 DMLLoss: 0.4065 
2022-03-18 08:00:36 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 4.8826, tea_CELoss: 2.1360 stu_CELoss: 2.3573 DMLLoss: 0.3894 
2022-03-18 08:01:33 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 5.1247, tea_CELoss: 2.1768 stu_CELoss: 2.4800 DMLLoss: 0.4679 
2022-03-18 08:02:29 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 4.7808, tea_CELoss: 2.0598 stu_CELoss: 2.3314 DMLLoss: 0.3896 
2022-03-18 08:03:26 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 4.3795, tea_CELoss: 1.8843 stu_CELoss: 2.1176 DMLLoss: 0.3776 
2022-03-18 08:04:22 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 4.3007, tea_CELoss: 1.8034 stu_CELoss: 2.1230 DMLLoss: 0.3742 
2022-03-18 08:05:18 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 4.2754, tea_CELoss: 1.8553 stu_CELoss: 2.0372 DMLLoss: 0.3828 
2022-03-18 08:06:15 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 4.5941, tea_CELoss: 1.9829 stu_CELoss: 2.2310 DMLLoss: 0.3803 
2022-03-18 08:07:11 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 4.6281, tea_CELoss: 1.9797 stu_CELoss: 2.2530 DMLLoss: 0.3954 
2022-03-18 08:08:08 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 4.4367, tea_CELoss: 1.9416 stu_CELoss: 2.1493 DMLLoss: 0.3459 
2022-03-18 08:09:04 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 4.8493, tea_CELoss: 2.0913 stu_CELoss: 2.3388 DMLLoss: 0.4191 
2022-03-18 08:10:00 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 4.4225, tea_CELoss: 1.8875 stu_CELoss: 2.1680 DMLLoss: 0.3670 
2022-03-18 08:10:57 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 4.5690, tea_CELoss: 1.9957 stu_CELoss: 2.1781 DMLLoss: 0.3952 
2022-03-18 08:11:53 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 4.4594, tea_CELoss: 1.8820 stu_CELoss: 2.1850 DMLLoss: 0.3924 
2022-03-18 08:12:50 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 4.6601, tea_CELoss: 2.0187 stu_CELoss: 2.2572 DMLLoss: 0.3841 
2022-03-18 08:13:46 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 4.2962, tea_CELoss: 1.7929 stu_CELoss: 2.1278 DMLLoss: 0.3755 
2022-03-18 08:13:49 - train: epoch 010, train_loss: 4.6480
2022-03-18 08:16:15 - eval: epoch: 010, tea_acc1: 57.642%, tea_acc5: 82.184%, tea_test_loss: 1.7830, stu_acc1: 50.194%, stu_acc5: 76.082%, stu_test_loss: 2.1521
2022-03-18 08:16:18 - until epoch: 010, tea_best_acc1: 57.642%, stu_best_acc1: 51.174%
2022-03-18 08:16:18 - epoch 011 lr: 0.1
2022-03-18 08:17:19 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 4.5246, tea_CELoss: 1.9526 stu_CELoss: 2.1910 DMLLoss: 0.3809 
2022-03-18 08:18:15 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 5.0327, tea_CELoss: 2.1607 stu_CELoss: 2.4625 DMLLoss: 0.4094 
2022-03-18 08:19:11 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 4.8743, tea_CELoss: 2.0976 stu_CELoss: 2.4061 DMLLoss: 0.3706 
2022-03-18 08:20:07 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 4.9643, tea_CELoss: 2.1741 stu_CELoss: 2.3921 DMLLoss: 0.3981 
2022-03-18 08:21:04 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 4.3786, tea_CELoss: 1.8706 stu_CELoss: 2.1005 DMLLoss: 0.4075 
2022-03-18 08:22:00 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 5.0836, tea_CELoss: 2.1433 stu_CELoss: 2.4806 DMLLoss: 0.4597 
2022-03-18 08:22:56 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 4.6506, tea_CELoss: 2.0260 stu_CELoss: 2.2471 DMLLoss: 0.3775 
2022-03-18 08:23:52 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 4.9353, tea_CELoss: 2.1723 stu_CELoss: 2.3569 DMLLoss: 0.4061 
2022-03-18 08:24:48 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 4.7957, tea_CELoss: 2.0516 stu_CELoss: 2.3503 DMLLoss: 0.3939 
2022-03-18 08:25:45 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 4.3794, tea_CELoss: 1.8608 stu_CELoss: 2.1508 DMLLoss: 0.3679 
2022-03-18 08:26:41 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 4.5312, tea_CELoss: 1.9423 stu_CELoss: 2.2387 DMLLoss: 0.3502 
2022-03-18 08:27:37 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 4.7593, tea_CELoss: 2.0097 stu_CELoss: 2.3770 DMLLoss: 0.3726 
2022-03-18 08:28:33 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 4.5802, tea_CELoss: 1.9895 stu_CELoss: 2.1855 DMLLoss: 0.4052 
2022-03-18 08:29:29 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 4.9120, tea_CELoss: 2.1654 stu_CELoss: 2.3893 DMLLoss: 0.3573 
2022-03-18 08:30:25 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 4.5784, tea_CELoss: 1.9655 stu_CELoss: 2.2242 DMLLoss: 0.3886 
2022-03-18 08:31:22 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 4.8280, tea_CELoss: 2.1437 stu_CELoss: 2.2931 DMLLoss: 0.3912 
2022-03-18 08:32:18 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 4.8726, tea_CELoss: 2.0858 stu_CELoss: 2.3879 DMLLoss: 0.3990 
2022-03-18 08:33:14 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 4.7135, tea_CELoss: 2.0609 stu_CELoss: 2.3190 DMLLoss: 0.3336 
2022-03-18 08:34:10 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 4.5805, tea_CELoss: 1.9184 stu_CELoss: 2.2434 DMLLoss: 0.4186 
2022-03-18 08:35:06 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 4.4097, tea_CELoss: 1.8423 stu_CELoss: 2.1792 DMLLoss: 0.3883 
2022-03-18 08:36:03 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 4.4791, tea_CELoss: 1.9807 stu_CELoss: 2.1439 DMLLoss: 0.3544 
2022-03-18 08:36:59 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 4.2693, tea_CELoss: 1.8356 stu_CELoss: 2.0318 DMLLoss: 0.4019 
2022-03-18 08:37:55 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 4.6992, tea_CELoss: 1.9902 stu_CELoss: 2.3214 DMLLoss: 0.3876 
2022-03-18 08:38:52 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 4.1566, tea_CELoss: 1.8011 stu_CELoss: 2.0124 DMLLoss: 0.3431 
2022-03-18 08:39:48 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 4.6797, tea_CELoss: 2.0397 stu_CELoss: 2.2594 DMLLoss: 0.3806 
2022-03-18 08:40:44 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 4.4267, tea_CELoss: 1.8947 stu_CELoss: 2.1470 DMLLoss: 0.3849 
2022-03-18 08:41:40 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 4.3669, tea_CELoss: 1.8611 stu_CELoss: 2.1338 DMLLoss: 0.3720 
2022-03-18 08:42:36 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 4.1583, tea_CELoss: 1.8145 stu_CELoss: 1.9788 DMLLoss: 0.3649 
2022-03-18 08:43:33 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 4.7361, tea_CELoss: 2.0837 stu_CELoss: 2.3069 DMLLoss: 0.3455 
2022-03-18 08:44:29 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 5.1871, tea_CELoss: 2.2658 stu_CELoss: 2.5257 DMLLoss: 0.3956 
2022-03-18 08:45:25 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 4.4296, tea_CELoss: 1.8993 stu_CELoss: 2.1701 DMLLoss: 0.3602 
2022-03-18 08:46:21 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 4.5333, tea_CELoss: 1.9470 stu_CELoss: 2.2150 DMLLoss: 0.3714 
2022-03-18 08:47:17 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 4.6553, tea_CELoss: 1.9585 stu_CELoss: 2.2940 DMLLoss: 0.4027 
2022-03-18 08:48:13 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 4.4843, tea_CELoss: 1.9289 stu_CELoss: 2.2015 DMLLoss: 0.3540 
2022-03-18 08:49:09 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 4.4144, tea_CELoss: 1.8847 stu_CELoss: 2.1199 DMLLoss: 0.4097 
2022-03-18 08:50:05 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 4.2890, tea_CELoss: 1.8699 stu_CELoss: 2.0238 DMLLoss: 0.3953 
2022-03-18 08:51:02 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 5.0062, tea_CELoss: 2.1428 stu_CELoss: 2.4610 DMLLoss: 0.4024 
2022-03-18 08:51:58 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 4.4956, tea_CELoss: 1.9414 stu_CELoss: 2.1882 DMLLoss: 0.3659 
2022-03-18 08:52:54 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 4.5269, tea_CELoss: 1.9404 stu_CELoss: 2.1966 DMLLoss: 0.3899 
2022-03-18 08:53:50 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 4.0699, tea_CELoss: 1.7125 stu_CELoss: 1.9672 DMLLoss: 0.3901 
2022-03-18 08:54:46 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 4.4600, tea_CELoss: 1.8769 stu_CELoss: 2.1712 DMLLoss: 0.4118 
2022-03-18 08:55:43 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 4.3465, tea_CELoss: 1.8844 stu_CELoss: 2.1055 DMLLoss: 0.3566 
2022-03-18 08:56:39 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 4.3542, tea_CELoss: 1.9173 stu_CELoss: 2.0311 DMLLoss: 0.4058 
2022-03-18 08:57:35 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 5.1149, tea_CELoss: 2.2424 stu_CELoss: 2.4805 DMLLoss: 0.3920 
2022-03-18 08:58:32 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 4.4852, tea_CELoss: 1.9203 stu_CELoss: 2.1328 DMLLoss: 0.4321 
2022-03-18 08:59:28 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 4.7228, tea_CELoss: 2.0512 stu_CELoss: 2.3013 DMLLoss: 0.3703 
2022-03-18 09:00:25 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 4.4242, tea_CELoss: 1.9186 stu_CELoss: 2.1366 DMLLoss: 0.3690 
2022-03-18 09:01:21 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 4.0388, tea_CELoss: 1.6994 stu_CELoss: 1.9550 DMLLoss: 0.3844 
2022-03-18 09:02:17 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 4.2862, tea_CELoss: 1.8582 stu_CELoss: 2.0633 DMLLoss: 0.3648 
2022-03-18 09:03:14 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 4.4308, tea_CELoss: 1.9166 stu_CELoss: 2.1415 DMLLoss: 0.3727 
2022-03-18 09:03:17 - train: epoch 011, train_loss: 4.6033
2022-03-18 09:05:44 - eval: epoch: 011, tea_acc1: 58.572%, tea_acc5: 82.882%, tea_test_loss: 1.7287, stu_acc1: 52.518%, stu_acc5: 78.458%, stu_test_loss: 2.0073
2022-03-18 09:05:47 - until epoch: 011, tea_best_acc1: 58.572%, stu_best_acc1: 52.518%
2022-03-18 09:05:47 - epoch 012 lr: 0.1
2022-03-18 09:06:47 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 4.3203, tea_CELoss: 1.8880 stu_CELoss: 2.0667 DMLLoss: 0.3655 
2022-03-18 09:07:43 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 3.8805, tea_CELoss: 1.6918 stu_CELoss: 1.8253 DMLLoss: 0.3634 
2022-03-18 09:08:40 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 4.2248, tea_CELoss: 1.7812 stu_CELoss: 2.0769 DMLLoss: 0.3666 
2022-03-18 09:09:36 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 4.6427, tea_CELoss: 2.0174 stu_CELoss: 2.2696 DMLLoss: 0.3558 
2022-03-18 09:10:33 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 4.8403, tea_CELoss: 2.0336 stu_CELoss: 2.3573 DMLLoss: 0.4494 
2022-03-18 09:11:30 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.9761, tea_CELoss: 1.7167 stu_CELoss: 1.8898 DMLLoss: 0.3695 
2022-03-18 09:12:26 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 4.1272, tea_CELoss: 1.7709 stu_CELoss: 1.9608 DMLLoss: 0.3955 
2022-03-18 09:13:23 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 5.2011, tea_CELoss: 2.2682 stu_CELoss: 2.5069 DMLLoss: 0.4261 
2022-03-18 09:14:19 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 4.6925, tea_CELoss: 2.0120 stu_CELoss: 2.2735 DMLLoss: 0.4070 
2022-03-18 09:15:16 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 4.2497, tea_CELoss: 1.8480 stu_CELoss: 2.0423 DMLLoss: 0.3594 
2022-03-18 09:16:12 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 5.0395, tea_CELoss: 2.2215 stu_CELoss: 2.4395 DMLLoss: 0.3784 
2022-03-18 09:17:08 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 4.1345, tea_CELoss: 1.7332 stu_CELoss: 1.9625 DMLLoss: 0.4388 
2022-03-18 09:18:05 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 4.1563, tea_CELoss: 1.8313 stu_CELoss: 1.9908 DMLLoss: 0.3341 
2022-03-18 09:19:01 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 4.8598, tea_CELoss: 2.1116 stu_CELoss: 2.3747 DMLLoss: 0.3735 
2022-03-18 09:19:57 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 4.3678, tea_CELoss: 1.8816 stu_CELoss: 2.1166 DMLLoss: 0.3697 
2022-03-18 09:20:54 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 4.3358, tea_CELoss: 1.8492 stu_CELoss: 2.1152 DMLLoss: 0.3714 
2022-03-18 09:21:50 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 4.3908, tea_CELoss: 1.8760 stu_CELoss: 2.1450 DMLLoss: 0.3698 
2022-03-18 09:22:46 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 4.6194, tea_CELoss: 2.0034 stu_CELoss: 2.2450 DMLLoss: 0.3710 
2022-03-18 09:23:43 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 4.9591, tea_CELoss: 2.1967 stu_CELoss: 2.3646 DMLLoss: 0.3978 
2022-03-18 09:24:39 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 4.6071, tea_CELoss: 1.9633 stu_CELoss: 2.2243 DMLLoss: 0.4194 
2022-03-18 09:25:36 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 4.9653, tea_CELoss: 2.1869 stu_CELoss: 2.3783 DMLLoss: 0.4001 
2022-03-18 09:26:33 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 4.8528, tea_CELoss: 2.1411 stu_CELoss: 2.3268 DMLLoss: 0.3848 
2022-03-18 09:27:29 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 4.4975, tea_CELoss: 1.9276 stu_CELoss: 2.1636 DMLLoss: 0.4063 
2022-03-18 09:28:26 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 4.6057, tea_CELoss: 2.0173 stu_CELoss: 2.1922 DMLLoss: 0.3961 
2022-03-18 09:29:23 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 4.2058, tea_CELoss: 1.7899 stu_CELoss: 1.9864 DMLLoss: 0.4295 
2022-03-18 09:30:19 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 4.2861, tea_CELoss: 1.8747 stu_CELoss: 2.0400 DMLLoss: 0.3715 
2022-03-18 09:31:16 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 4.4647, tea_CELoss: 1.8918 stu_CELoss: 2.1506 DMLLoss: 0.4223 
2022-03-18 09:32:13 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 4.7640, tea_CELoss: 2.0339 stu_CELoss: 2.3136 DMLLoss: 0.4164 
2022-03-18 09:33:09 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 4.6512, tea_CELoss: 2.0351 stu_CELoss: 2.2526 DMLLoss: 0.3636 
2022-03-18 09:34:06 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 4.2457, tea_CELoss: 1.8268 stu_CELoss: 2.0442 DMLLoss: 0.3746 
2022-03-18 09:35:02 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 4.5570, tea_CELoss: 1.9809 stu_CELoss: 2.2027 DMLLoss: 0.3733 
2022-03-18 09:35:59 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 4.4212, tea_CELoss: 1.9118 stu_CELoss: 2.1409 DMLLoss: 0.3685 
2022-03-18 09:36:55 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 4.6556, tea_CELoss: 1.9755 stu_CELoss: 2.2798 DMLLoss: 0.4004 
2022-03-18 09:37:52 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 4.5251, tea_CELoss: 1.9671 stu_CELoss: 2.1843 DMLLoss: 0.3738 
2022-03-18 09:38:49 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 4.6371, tea_CELoss: 1.9532 stu_CELoss: 2.2907 DMLLoss: 0.3932 
2022-03-18 09:39:46 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 4.3006, tea_CELoss: 1.8280 stu_CELoss: 2.0732 DMLLoss: 0.3993 
2022-03-18 09:40:42 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 4.7694, tea_CELoss: 2.1177 stu_CELoss: 2.2999 DMLLoss: 0.3518 
2022-03-18 09:41:39 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 4.8354, tea_CELoss: 2.1221 stu_CELoss: 2.3464 DMLLoss: 0.3669 
2022-03-18 09:42:35 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 4.1233, tea_CELoss: 1.7373 stu_CELoss: 1.9870 DMLLoss: 0.3990 
2022-03-18 09:43:32 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 4.7019, tea_CELoss: 2.0761 stu_CELoss: 2.2493 DMLLoss: 0.3765 
2022-03-18 09:44:29 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 4.7822, tea_CELoss: 2.0869 stu_CELoss: 2.2867 DMLLoss: 0.4086 
2022-03-18 09:45:25 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 4.4740, tea_CELoss: 1.9773 stu_CELoss: 2.1318 DMLLoss: 0.3649 
2022-03-18 09:46:22 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 4.9846, tea_CELoss: 2.2056 stu_CELoss: 2.4074 DMLLoss: 0.3715 
2022-03-18 09:47:18 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 4.4080, tea_CELoss: 1.9331 stu_CELoss: 2.1168 DMLLoss: 0.3581 
2022-03-18 09:48:15 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 4.4342, tea_CELoss: 1.9025 stu_CELoss: 2.1484 DMLLoss: 0.3833 
2022-03-18 09:49:12 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 4.6875, tea_CELoss: 2.0159 stu_CELoss: 2.2765 DMLLoss: 0.3951 
2022-03-18 09:50:09 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 4.2465, tea_CELoss: 1.8681 stu_CELoss: 2.0064 DMLLoss: 0.3719 
2022-03-18 09:51:06 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 4.7041, tea_CELoss: 2.0088 stu_CELoss: 2.2897 DMLLoss: 0.4056 
2022-03-18 09:52:03 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 4.1044, tea_CELoss: 1.8147 stu_CELoss: 1.9411 DMLLoss: 0.3486 
2022-03-18 09:53:00 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 4.3844, tea_CELoss: 1.9131 stu_CELoss: 2.1104 DMLLoss: 0.3609 
2022-03-18 09:53:03 - train: epoch 012, train_loss: 4.5589
2022-03-18 09:55:29 - eval: epoch: 012, tea_acc1: 58.360%, tea_acc5: 82.836%, tea_test_loss: 1.7329, stu_acc1: 53.978%, stu_acc5: 79.430%, stu_test_loss: 1.9604
2022-03-18 09:55:32 - until epoch: 012, tea_best_acc1: 58.572%, stu_best_acc1: 53.978%
2022-03-18 09:55:32 - epoch 013 lr: 0.1
2022-03-18 09:56:32 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 4.1216, tea_CELoss: 1.7890 stu_CELoss: 1.9318 DMLLoss: 0.4007 
2022-03-18 09:57:29 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 4.3810, tea_CELoss: 1.8871 stu_CELoss: 2.1109 DMLLoss: 0.3829 
2022-03-18 09:58:25 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 4.6063, tea_CELoss: 1.9832 stu_CELoss: 2.2433 DMLLoss: 0.3798 
2022-03-18 09:59:22 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 4.3465, tea_CELoss: 1.8715 stu_CELoss: 2.0937 DMLLoss: 0.3813 
2022-03-18 10:00:18 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 4.3775, tea_CELoss: 1.9003 stu_CELoss: 2.1006 DMLLoss: 0.3766 
2022-03-18 10:01:15 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 4.5439, tea_CELoss: 1.9850 stu_CELoss: 2.1685 DMLLoss: 0.3904 
2022-03-18 10:02:12 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 4.3176, tea_CELoss: 1.8484 stu_CELoss: 2.0869 DMLLoss: 0.3823 
2022-03-18 10:03:08 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 4.8768, tea_CELoss: 2.1557 stu_CELoss: 2.3541 DMLLoss: 0.3670 
2022-03-18 10:04:05 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 4.6936, tea_CELoss: 2.0721 stu_CELoss: 2.2237 DMLLoss: 0.3977 
2022-03-18 10:05:01 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 4.2735, tea_CELoss: 1.8396 stu_CELoss: 2.0497 DMLLoss: 0.3842 
2022-03-18 10:05:58 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 4.5918, tea_CELoss: 2.0390 stu_CELoss: 2.1430 DMLLoss: 0.4099 
2022-03-18 10:06:55 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 4.9330, tea_CELoss: 2.1352 stu_CELoss: 2.3788 DMLLoss: 0.4190 
2022-03-18 10:07:51 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 4.3955, tea_CELoss: 1.9267 stu_CELoss: 2.0596 DMLLoss: 0.4093 
2022-03-18 10:08:48 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 4.4747, tea_CELoss: 1.9751 stu_CELoss: 2.1049 DMLLoss: 0.3948 
2022-03-18 10:09:44 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 4.8855, tea_CELoss: 2.1187 stu_CELoss: 2.3447 DMLLoss: 0.4221 
2022-03-18 10:10:41 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 4.5266, tea_CELoss: 1.9930 stu_CELoss: 2.1277 DMLLoss: 0.4058 
2022-03-18 10:11:38 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 4.4202, tea_CELoss: 1.8893 stu_CELoss: 2.1106 DMLLoss: 0.4203 
2022-03-18 10:12:34 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 4.6262, tea_CELoss: 1.9226 stu_CELoss: 2.3040 DMLLoss: 0.3996 
2022-03-18 10:13:30 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 4.4087, tea_CELoss: 1.9213 stu_CELoss: 2.1177 DMLLoss: 0.3697 
2022-03-18 10:14:27 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 4.6853, tea_CELoss: 2.0228 stu_CELoss: 2.2765 DMLLoss: 0.3860 
2022-03-18 10:15:23 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 4.9048, tea_CELoss: 2.1006 stu_CELoss: 2.4092 DMLLoss: 0.3951 
2022-03-18 10:16:20 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 4.1069, tea_CELoss: 1.8005 stu_CELoss: 1.9455 DMLLoss: 0.3609 
2022-03-18 10:17:16 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 4.4231, tea_CELoss: 1.8987 stu_CELoss: 2.1380 DMLLoss: 0.3864 
2022-03-18 10:18:13 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 4.8483, tea_CELoss: 2.0229 stu_CELoss: 2.4204 DMLLoss: 0.4050 
2022-03-18 10:19:09 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 4.4953, tea_CELoss: 1.9120 stu_CELoss: 2.2187 DMLLoss: 0.3646 
2022-03-18 10:20:05 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 4.5025, tea_CELoss: 1.9282 stu_CELoss: 2.2036 DMLLoss: 0.3708 
2022-03-18 10:21:02 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 4.5382, tea_CELoss: 1.9201 stu_CELoss: 2.2096 DMLLoss: 0.4085 
2022-03-18 10:21:58 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 4.4942, tea_CELoss: 1.9785 stu_CELoss: 2.1520 DMLLoss: 0.3637 
2022-03-18 10:22:54 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 4.6070, tea_CELoss: 2.0392 stu_CELoss: 2.2267 DMLLoss: 0.3411 
2022-03-18 10:23:50 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 4.2091, tea_CELoss: 1.8396 stu_CELoss: 2.0087 DMLLoss: 0.3609 
2022-03-18 10:24:47 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 4.3014, tea_CELoss: 1.8539 stu_CELoss: 2.0798 DMLLoss: 0.3677 
2022-03-18 10:25:44 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 4.7858, tea_CELoss: 2.0638 stu_CELoss: 2.3136 DMLLoss: 0.4084 
2022-03-18 10:26:40 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 4.2704, tea_CELoss: 1.8433 stu_CELoss: 2.0577 DMLLoss: 0.3694 
2022-03-18 10:27:37 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 4.3571, tea_CELoss: 1.8854 stu_CELoss: 2.0889 DMLLoss: 0.3828 
2022-03-18 10:28:33 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 4.3895, tea_CELoss: 1.9028 stu_CELoss: 2.1212 DMLLoss: 0.3656 
2022-03-18 10:29:30 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 4.7611, tea_CELoss: 2.0881 stu_CELoss: 2.2965 DMLLoss: 0.3765 
2022-03-18 10:30:27 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 4.0292, tea_CELoss: 1.7504 stu_CELoss: 1.9188 DMLLoss: 0.3600 
2022-03-18 10:31:24 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 4.9905, tea_CELoss: 2.2138 stu_CELoss: 2.3935 DMLLoss: 0.3832 
2022-03-18 10:32:20 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 4.8664, tea_CELoss: 2.1056 stu_CELoss: 2.3457 DMLLoss: 0.4151 
2022-03-18 10:33:17 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 4.4636, tea_CELoss: 1.9671 stu_CELoss: 2.1211 DMLLoss: 0.3754 
2022-03-18 10:34:14 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 4.1319, tea_CELoss: 1.7738 stu_CELoss: 1.9797 DMLLoss: 0.3783 
2022-03-18 10:35:10 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 4.7179, tea_CELoss: 2.0226 stu_CELoss: 2.2870 DMLLoss: 0.4083 
2022-03-18 10:36:07 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 4.6351, tea_CELoss: 1.9692 stu_CELoss: 2.2115 DMLLoss: 0.4543 
2022-03-18 10:37:04 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 4.3116, tea_CELoss: 1.8778 stu_CELoss: 2.0616 DMLLoss: 0.3723 
2022-03-18 10:38:01 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 4.2545, tea_CELoss: 1.8432 stu_CELoss: 2.0566 DMLLoss: 0.3546 
2022-03-18 10:38:57 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 4.3833, tea_CELoss: 1.9096 stu_CELoss: 2.0831 DMLLoss: 0.3906 
2022-03-18 10:39:54 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 4.4828, tea_CELoss: 1.9680 stu_CELoss: 2.1740 DMLLoss: 0.3407 
2022-03-18 10:40:51 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 4.6834, tea_CELoss: 1.9990 stu_CELoss: 2.2721 DMLLoss: 0.4124 
2022-03-18 10:41:48 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 4.9171, tea_CELoss: 2.1429 stu_CELoss: 2.3643 DMLLoss: 0.4100 
2022-03-18 10:42:44 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 4.4068, tea_CELoss: 1.8687 stu_CELoss: 2.1330 DMLLoss: 0.4052 
2022-03-18 10:42:47 - train: epoch 013, train_loss: 4.5299
2022-03-18 10:45:14 - eval: epoch: 013, tea_acc1: 59.352%, tea_acc5: 83.248%, tea_test_loss: 1.7051, stu_acc1: 55.694%, stu_acc5: 80.528%, stu_test_loss: 1.8789
2022-03-18 10:45:17 - until epoch: 013, tea_best_acc1: 59.352%, stu_best_acc1: 55.694%
2022-03-18 10:45:17 - epoch 014 lr: 0.1
2022-03-18 10:46:17 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 4.4624, tea_CELoss: 1.9867 stu_CELoss: 2.1274 DMLLoss: 0.3483 
2022-03-18 10:47:13 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 4.8235, tea_CELoss: 2.1059 stu_CELoss: 2.3219 DMLLoss: 0.3956 
2022-03-18 10:48:09 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 4.2727, tea_CELoss: 1.8143 stu_CELoss: 2.0860 DMLLoss: 0.3725 
2022-03-18 10:49:06 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 4.2992, tea_CELoss: 1.8755 stu_CELoss: 2.0375 DMLLoss: 0.3862 
2022-03-18 10:50:02 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 4.2636, tea_CELoss: 1.8968 stu_CELoss: 2.0204 DMLLoss: 0.3464 
2022-03-18 10:50:59 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 4.8641, tea_CELoss: 2.1886 stu_CELoss: 2.2655 DMLLoss: 0.4100 
2022-03-18 10:51:55 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 4.2637, tea_CELoss: 1.8427 stu_CELoss: 2.0835 DMLLoss: 0.3375 
2022-03-18 10:52:52 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 4.2033, tea_CELoss: 1.8389 stu_CELoss: 1.9839 DMLLoss: 0.3806 
2022-03-18 10:53:48 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 4.5014, tea_CELoss: 1.9202 stu_CELoss: 2.2075 DMLLoss: 0.3737 
2022-03-18 10:54:44 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 4.6433, tea_CELoss: 2.0901 stu_CELoss: 2.1697 DMLLoss: 0.3835 
2022-03-18 10:55:40 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 4.4740, tea_CELoss: 1.9277 stu_CELoss: 2.1383 DMLLoss: 0.4081 
2022-03-18 10:56:37 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 4.7881, tea_CELoss: 2.0830 stu_CELoss: 2.3225 DMLLoss: 0.3826 
2022-03-18 10:57:33 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 4.6179, tea_CELoss: 2.0635 stu_CELoss: 2.1756 DMLLoss: 0.3788 
2022-03-18 10:58:30 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 4.7687, tea_CELoss: 2.1345 stu_CELoss: 2.2260 DMLLoss: 0.4082 
2022-03-18 10:59:26 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 4.8577, tea_CELoss: 2.1246 stu_CELoss: 2.3050 DMLLoss: 0.4282 
2022-03-18 11:00:22 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 4.4293, tea_CELoss: 1.9468 stu_CELoss: 2.1112 DMLLoss: 0.3713 
2022-03-18 11:01:19 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 4.7584, tea_CELoss: 2.0337 stu_CELoss: 2.3218 DMLLoss: 0.4029 
2022-03-18 11:02:15 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 4.7043, tea_CELoss: 2.0367 stu_CELoss: 2.2489 DMLLoss: 0.4187 
2022-03-18 11:03:12 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 4.4183, tea_CELoss: 1.9164 stu_CELoss: 2.1078 DMLLoss: 0.3941 
2022-03-18 11:04:08 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 4.2100, tea_CELoss: 1.8120 stu_CELoss: 2.0314 DMLLoss: 0.3666 
2022-03-18 11:05:04 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 4.5666, tea_CELoss: 2.0038 stu_CELoss: 2.1729 DMLLoss: 0.3899 
2022-03-18 11:06:01 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 4.4322, tea_CELoss: 1.8348 stu_CELoss: 2.1842 DMLLoss: 0.4131 
2022-03-18 11:06:57 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 4.2450, tea_CELoss: 1.8500 stu_CELoss: 2.0243 DMLLoss: 0.3707 
2022-03-18 11:07:53 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 4.7269, tea_CELoss: 2.0880 stu_CELoss: 2.2583 DMLLoss: 0.3806 
2022-03-18 11:08:50 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 4.4004, tea_CELoss: 1.9169 stu_CELoss: 2.0994 DMLLoss: 0.3841 
2022-03-18 11:09:46 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 4.1045, tea_CELoss: 1.7373 stu_CELoss: 1.9985 DMLLoss: 0.3687 
2022-03-18 11:10:42 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 4.5294, tea_CELoss: 1.9630 stu_CELoss: 2.1559 DMLLoss: 0.4105 
2022-03-18 11:11:39 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 4.8172, tea_CELoss: 2.1409 stu_CELoss: 2.3193 DMLLoss: 0.3570 
2022-03-18 11:12:35 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 4.6309, tea_CELoss: 2.0253 stu_CELoss: 2.2052 DMLLoss: 0.4004 
2022-03-18 11:13:32 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 4.6780, tea_CELoss: 2.0110 stu_CELoss: 2.2542 DMLLoss: 0.4127 
2022-03-18 11:14:28 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 4.3925, tea_CELoss: 1.8932 stu_CELoss: 2.0730 DMLLoss: 0.4262 
2022-03-18 11:15:24 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 4.5136, tea_CELoss: 1.9668 stu_CELoss: 2.2023 DMLLoss: 0.3445 
2022-03-18 11:16:21 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 4.2445, tea_CELoss: 1.8283 stu_CELoss: 2.0428 DMLLoss: 0.3734 
2022-03-18 11:17:17 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 4.2797, tea_CELoss: 1.9209 stu_CELoss: 2.0161 DMLLoss: 0.3427 
2022-03-18 11:18:14 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 4.3395, tea_CELoss: 1.8950 stu_CELoss: 2.0859 DMLLoss: 0.3586 
2022-03-18 11:19:10 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 4.1467, tea_CELoss: 1.7930 stu_CELoss: 1.9822 DMLLoss: 0.3715 
2022-03-18 11:20:07 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 4.5457, tea_CELoss: 2.0171 stu_CELoss: 2.1545 DMLLoss: 0.3741 
2022-03-18 11:21:03 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 4.8387, tea_CELoss: 2.1077 stu_CELoss: 2.3160 DMLLoss: 0.4150 
2022-03-18 11:22:00 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 4.5551, tea_CELoss: 1.9796 stu_CELoss: 2.2055 DMLLoss: 0.3700 
2022-03-18 11:22:56 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 4.5675, tea_CELoss: 1.9384 stu_CELoss: 2.2112 DMLLoss: 0.4179 
2022-03-18 11:23:53 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 4.5955, tea_CELoss: 2.0321 stu_CELoss: 2.1753 DMLLoss: 0.3881 
2022-03-18 11:24:50 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 4.4098, tea_CELoss: 1.8759 stu_CELoss: 2.1440 DMLLoss: 0.3899 
2022-03-18 11:25:46 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 4.6047, tea_CELoss: 2.0168 stu_CELoss: 2.2252 DMLLoss: 0.3627 
2022-03-18 11:26:43 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 4.3484, tea_CELoss: 1.8479 stu_CELoss: 2.1156 DMLLoss: 0.3850 
2022-03-18 11:27:40 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 4.4712, tea_CELoss: 1.9816 stu_CELoss: 2.1196 DMLLoss: 0.3700 
2022-03-18 11:28:37 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 4.2165, tea_CELoss: 1.8548 stu_CELoss: 1.9743 DMLLoss: 0.3874 
2022-03-18 11:29:33 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 4.2271, tea_CELoss: 1.8816 stu_CELoss: 1.9860 DMLLoss: 0.3595 
2022-03-18 11:30:30 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 4.6609, tea_CELoss: 2.0106 stu_CELoss: 2.2304 DMLLoss: 0.4198 
2022-03-18 11:31:27 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 4.6466, tea_CELoss: 2.0660 stu_CELoss: 2.1973 DMLLoss: 0.3833 
2022-03-18 11:32:24 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 4.5100, tea_CELoss: 2.0086 stu_CELoss: 2.1339 DMLLoss: 0.3676 
2022-03-18 11:32:27 - train: epoch 014, train_loss: 4.5044
2022-03-18 11:34:53 - eval: epoch: 014, tea_acc1: 58.026%, tea_acc5: 82.572%, tea_test_loss: 1.7504, stu_acc1: 54.112%, stu_acc5: 79.608%, stu_test_loss: 1.9433
2022-03-18 11:34:55 - until epoch: 014, tea_best_acc1: 59.352%, stu_best_acc1: 55.694%
2022-03-18 11:34:55 - epoch 015 lr: 0.1
2022-03-18 11:35:56 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 4.2313, tea_CELoss: 1.8695 stu_CELoss: 2.0079 DMLLoss: 0.3540 
2022-03-18 11:36:52 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 4.8578, tea_CELoss: 2.1678 stu_CELoss: 2.2867 DMLLoss: 0.4032 
2022-03-18 11:37:48 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 4.8001, tea_CELoss: 2.0835 stu_CELoss: 2.3116 DMLLoss: 0.4051 
2022-03-18 11:38:45 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 4.4447, tea_CELoss: 1.9411 stu_CELoss: 2.0876 DMLLoss: 0.4160 
2022-03-18 11:39:41 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 4.2162, tea_CELoss: 1.8125 stu_CELoss: 2.0401 DMLLoss: 0.3636 
2022-03-18 11:40:37 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 4.6890, tea_CELoss: 2.0398 stu_CELoss: 2.2904 DMLLoss: 0.3589 
2022-03-18 11:41:33 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 4.5455, tea_CELoss: 1.9446 stu_CELoss: 2.2080 DMLLoss: 0.3929 
2022-03-18 11:42:30 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 4.2174, tea_CELoss: 1.8163 stu_CELoss: 2.0624 DMLLoss: 0.3387 
2022-03-18 11:43:26 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 4.0471, tea_CELoss: 1.7300 stu_CELoss: 1.9670 DMLLoss: 0.3501 
2022-03-18 11:44:22 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 4.5486, tea_CELoss: 1.9911 stu_CELoss: 2.1765 DMLLoss: 0.3811 
2022-03-18 11:45:18 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 4.4143, tea_CELoss: 1.9171 stu_CELoss: 2.1024 DMLLoss: 0.3948 
2022-03-18 11:46:15 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 4.7647, tea_CELoss: 2.1269 stu_CELoss: 2.2548 DMLLoss: 0.3830 
2022-03-18 11:47:11 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 4.7757, tea_CELoss: 2.0875 stu_CELoss: 2.3325 DMLLoss: 0.3558 
2022-03-18 11:48:07 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 4.1644, tea_CELoss: 1.8298 stu_CELoss: 1.9559 DMLLoss: 0.3787 
2022-03-18 11:49:03 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 4.1774, tea_CELoss: 1.8159 stu_CELoss: 2.0468 DMLLoss: 0.3147 
2022-03-18 11:49:59 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 4.7919, tea_CELoss: 2.0293 stu_CELoss: 2.3526 DMLLoss: 0.4099 
2022-03-18 11:50:56 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 4.7320, tea_CELoss: 1.9988 stu_CELoss: 2.2911 DMLLoss: 0.4421 
2022-03-18 11:51:52 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 4.5107, tea_CELoss: 1.9465 stu_CELoss: 2.1904 DMLLoss: 0.3738 
2022-03-18 11:52:48 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 4.0691, tea_CELoss: 1.7708 stu_CELoss: 1.9310 DMLLoss: 0.3673 
2022-03-18 11:53:45 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 4.0797, tea_CELoss: 1.8083 stu_CELoss: 1.8968 DMLLoss: 0.3746 
2022-03-18 11:54:41 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 4.3727, tea_CELoss: 1.9020 stu_CELoss: 2.0663 DMLLoss: 0.4045 
2022-03-18 11:55:37 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 4.6236, tea_CELoss: 2.0312 stu_CELoss: 2.1946 DMLLoss: 0.3979 
2022-03-18 11:56:33 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 4.4792, tea_CELoss: 1.8950 stu_CELoss: 2.1750 DMLLoss: 0.4092 
2022-03-18 11:57:30 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 4.5874, tea_CELoss: 1.9317 stu_CELoss: 2.2435 DMLLoss: 0.4121 
2022-03-18 11:58:26 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 4.7221, tea_CELoss: 2.0738 stu_CELoss: 2.2244 DMLLoss: 0.4239 
2022-03-18 11:59:22 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 4.2867, tea_CELoss: 1.8841 stu_CELoss: 2.0172 DMLLoss: 0.3854 
2022-03-18 12:00:18 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 4.7127, tea_CELoss: 2.1248 stu_CELoss: 2.2288 DMLLoss: 0.3592 
2022-03-18 12:01:14 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 4.4944, tea_CELoss: 1.9377 stu_CELoss: 2.1620 DMLLoss: 0.3947 
2022-03-18 12:02:10 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 4.3150, tea_CELoss: 1.9032 stu_CELoss: 2.0349 DMLLoss: 0.3769 
2022-03-18 12:03:06 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 4.0003, tea_CELoss: 1.7245 stu_CELoss: 1.9326 DMLLoss: 0.3432 
2022-03-18 12:04:02 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 4.4049, tea_CELoss: 1.9500 stu_CELoss: 2.1058 DMLLoss: 0.3491 
2022-03-18 12:04:58 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 4.2305, tea_CELoss: 1.8284 stu_CELoss: 2.0340 DMLLoss: 0.3681 
2022-03-18 12:05:54 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 4.3838, tea_CELoss: 1.9212 stu_CELoss: 2.0820 DMLLoss: 0.3807 
2022-03-18 12:06:50 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 4.2859, tea_CELoss: 1.8387 stu_CELoss: 2.0637 DMLLoss: 0.3835 
2022-03-18 12:07:46 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 4.7936, tea_CELoss: 2.1015 stu_CELoss: 2.2827 DMLLoss: 0.4094 
2022-03-18 12:08:43 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 4.7108, tea_CELoss: 2.0508 stu_CELoss: 2.2558 DMLLoss: 0.4042 
2022-03-18 12:09:39 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 3.7406, tea_CELoss: 1.5687 stu_CELoss: 1.7883 DMLLoss: 0.3836 
2022-03-18 12:10:36 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 4.6142, tea_CELoss: 2.0307 stu_CELoss: 2.2283 DMLLoss: 0.3553 
2022-03-18 12:11:32 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 4.6495, tea_CELoss: 2.0691 stu_CELoss: 2.1853 DMLLoss: 0.3952 
2022-03-18 12:12:28 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 4.5356, tea_CELoss: 1.9891 stu_CELoss: 2.1696 DMLLoss: 0.3769 
2022-03-18 12:13:25 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 4.3545, tea_CELoss: 1.8789 stu_CELoss: 2.0966 DMLLoss: 0.3790 
2022-03-18 12:14:22 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 4.2508, tea_CELoss: 1.8816 stu_CELoss: 2.0166 DMLLoss: 0.3526 
2022-03-18 12:15:18 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 4.2504, tea_CELoss: 1.8472 stu_CELoss: 2.0651 DMLLoss: 0.3381 
2022-03-18 12:16:15 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 4.4788, tea_CELoss: 2.0120 stu_CELoss: 2.1299 DMLLoss: 0.3369 
2022-03-18 12:17:11 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 4.3878, tea_CELoss: 1.9423 stu_CELoss: 2.0637 DMLLoss: 0.3819 
2022-03-18 12:18:08 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 4.5348, tea_CELoss: 2.0047 stu_CELoss: 2.1739 DMLLoss: 0.3562 
2022-03-18 12:19:04 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 4.5521, tea_CELoss: 2.0188 stu_CELoss: 2.1078 DMLLoss: 0.4255 
2022-03-18 12:20:01 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 4.7630, tea_CELoss: 2.1196 stu_CELoss: 2.2534 DMLLoss: 0.3899 
2022-03-18 12:20:58 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 4.4206, tea_CELoss: 1.9077 stu_CELoss: 2.1033 DMLLoss: 0.4096 
2022-03-18 12:21:55 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 4.5996, tea_CELoss: 2.0254 stu_CELoss: 2.2100 DMLLoss: 0.3642 
2022-03-18 12:21:58 - train: epoch 015, train_loss: 4.4823
2022-03-18 12:24:23 - eval: epoch: 015, tea_acc1: 58.870%, tea_acc5: 82.912%, tea_test_loss: 1.7238, stu_acc1: 56.106%, stu_acc5: 80.726%, stu_test_loss: 1.8565
2022-03-18 12:24:25 - until epoch: 015, tea_best_acc1: 59.352%, stu_best_acc1: 56.106%
2022-03-18 12:24:25 - epoch 016 lr: 0.1
2022-03-18 12:25:25 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 4.2392, tea_CELoss: 1.8313 stu_CELoss: 2.0496 DMLLoss: 0.3583 
2022-03-18 12:26:21 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 3.9410, tea_CELoss: 1.7071 stu_CELoss: 1.8674 DMLLoss: 0.3666 
2022-03-18 12:27:17 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 4.6178, tea_CELoss: 1.9753 stu_CELoss: 2.2488 DMLLoss: 0.3937 
2022-03-18 12:28:13 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 4.5193, tea_CELoss: 1.9577 stu_CELoss: 2.2027 DMLLoss: 0.3589 
2022-03-18 12:29:09 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 4.3102, tea_CELoss: 1.9075 stu_CELoss: 2.0536 DMLLoss: 0.3491 
2022-03-18 12:30:05 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 4.1311, tea_CELoss: 1.7698 stu_CELoss: 1.9496 DMLLoss: 0.4117 
2022-03-18 12:31:01 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 4.2378, tea_CELoss: 1.8577 stu_CELoss: 2.0234 DMLLoss: 0.3567 
2022-03-18 12:31:57 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 4.3413, tea_CELoss: 1.8396 stu_CELoss: 2.1173 DMLLoss: 0.3844 
2022-03-18 12:32:53 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 4.4152, tea_CELoss: 1.9269 stu_CELoss: 2.1173 DMLLoss: 0.3710 
2022-03-18 12:33:50 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 4.1794, tea_CELoss: 1.8082 stu_CELoss: 1.9610 DMLLoss: 0.4103 
2022-03-18 12:34:46 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 4.2334, tea_CELoss: 1.8695 stu_CELoss: 2.0026 DMLLoss: 0.3613 
2022-03-18 12:35:42 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 4.2239, tea_CELoss: 1.8613 stu_CELoss: 1.9813 DMLLoss: 0.3813 
2022-03-18 12:36:38 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 4.5987, tea_CELoss: 2.0032 stu_CELoss: 2.1778 DMLLoss: 0.4177 
2022-03-18 12:37:35 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 4.2728, tea_CELoss: 1.9002 stu_CELoss: 2.0339 DMLLoss: 0.3387 
2022-03-18 12:38:31 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 5.0873, tea_CELoss: 2.2416 stu_CELoss: 2.4459 DMLLoss: 0.3997 
2022-03-18 12:39:27 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 4.9984, tea_CELoss: 2.1359 stu_CELoss: 2.4352 DMLLoss: 0.4272 
2022-03-18 12:40:24 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 4.4943, tea_CELoss: 1.9543 stu_CELoss: 2.1424 DMLLoss: 0.3976 
2022-03-18 12:41:20 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 4.7879, tea_CELoss: 2.0869 stu_CELoss: 2.2752 DMLLoss: 0.4258 
2022-03-18 12:42:17 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 4.3222, tea_CELoss: 1.9312 stu_CELoss: 2.0411 DMLLoss: 0.3498 
2022-03-18 12:43:14 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 3.6277, tea_CELoss: 1.5659 stu_CELoss: 1.7019 DMLLoss: 0.3598 
2022-03-18 12:44:10 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 4.4703, tea_CELoss: 1.9672 stu_CELoss: 2.1332 DMLLoss: 0.3699 
2022-03-18 12:45:07 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 4.6534, tea_CELoss: 2.0249 stu_CELoss: 2.2004 DMLLoss: 0.4280 
2022-03-18 12:46:03 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 4.6254, tea_CELoss: 2.0157 stu_CELoss: 2.2196 DMLLoss: 0.3901 
2022-03-18 12:47:00 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 4.2234, tea_CELoss: 1.8478 stu_CELoss: 2.0494 DMLLoss: 0.3262 
2022-03-18 12:47:57 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 4.4868, tea_CELoss: 1.9456 stu_CELoss: 2.1526 DMLLoss: 0.3886 
2022-03-18 12:48:54 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 4.5827, tea_CELoss: 2.0308 stu_CELoss: 2.1647 DMLLoss: 0.3872 
2022-03-18 12:49:51 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 4.3663, tea_CELoss: 1.9202 stu_CELoss: 2.0658 DMLLoss: 0.3804 
2022-03-18 12:50:48 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 4.2920, tea_CELoss: 1.8777 stu_CELoss: 2.0277 DMLLoss: 0.3867 
2022-03-18 12:51:45 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 4.2796, tea_CELoss: 1.9150 stu_CELoss: 2.0075 DMLLoss: 0.3571 
2022-03-18 12:52:43 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 4.7518, tea_CELoss: 2.0797 stu_CELoss: 2.2932 DMLLoss: 0.3790 
2022-03-18 12:53:40 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 4.5508, tea_CELoss: 1.9941 stu_CELoss: 2.2014 DMLLoss: 0.3554 
2022-03-18 12:54:37 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 4.5066, tea_CELoss: 1.9879 stu_CELoss: 2.1519 DMLLoss: 0.3668 
2022-03-18 12:55:34 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 4.5317, tea_CELoss: 1.9830 stu_CELoss: 2.1849 DMLLoss: 0.3638 
2022-03-18 12:56:31 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 4.4237, tea_CELoss: 1.9656 stu_CELoss: 2.0978 DMLLoss: 0.3604 
2022-03-18 12:57:29 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 4.1735, tea_CELoss: 1.8038 stu_CELoss: 2.0043 DMLLoss: 0.3654 
2022-03-18 12:58:26 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 4.1706, tea_CELoss: 1.7773 stu_CELoss: 2.0063 DMLLoss: 0.3870 
2022-03-18 12:59:23 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 4.4194, tea_CELoss: 1.8840 stu_CELoss: 2.1664 DMLLoss: 0.3690 
2022-03-18 13:00:20 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 4.9963, tea_CELoss: 2.2245 stu_CELoss: 2.3842 DMLLoss: 0.3875 
2022-03-18 13:01:17 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 4.6940, tea_CELoss: 2.0404 stu_CELoss: 2.2864 DMLLoss: 0.3673 
2022-03-18 13:02:15 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 4.6596, tea_CELoss: 2.0485 stu_CELoss: 2.1979 DMLLoss: 0.4132 
2022-03-18 13:03:12 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 4.2714, tea_CELoss: 1.8309 stu_CELoss: 2.0501 DMLLoss: 0.3904 
2022-03-18 13:04:09 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 4.1516, tea_CELoss: 1.7753 stu_CELoss: 2.0049 DMLLoss: 0.3714 
2022-03-18 13:05:06 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 4.0322, tea_CELoss: 1.7371 stu_CELoss: 1.9446 DMLLoss: 0.3505 
2022-03-18 13:06:03 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 4.3034, tea_CELoss: 1.8773 stu_CELoss: 2.0613 DMLLoss: 0.3647 
2022-03-18 13:07:01 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 4.7958, tea_CELoss: 2.1181 stu_CELoss: 2.3077 DMLLoss: 0.3700 
2022-03-18 13:07:58 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 4.2040, tea_CELoss: 1.8171 stu_CELoss: 2.0108 DMLLoss: 0.3761 
2022-03-18 13:08:55 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 4.7863, tea_CELoss: 2.1264 stu_CELoss: 2.2988 DMLLoss: 0.3611 
2022-03-18 13:09:52 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 4.3140, tea_CELoss: 1.9226 stu_CELoss: 2.0628 DMLLoss: 0.3286 
2022-03-18 13:10:50 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 4.4445, tea_CELoss: 1.9530 stu_CELoss: 2.1142 DMLLoss: 0.3773 
2022-03-18 13:11:47 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 4.7631, tea_CELoss: 2.0803 stu_CELoss: 2.2760 DMLLoss: 0.4069 
2022-03-18 13:11:50 - train: epoch 016, train_loss: 4.4613
2022-03-18 13:14:12 - eval: epoch: 016, tea_acc1: 59.472%, tea_acc5: 83.052%, tea_test_loss: 1.6995, stu_acc1: 54.626%, stu_acc5: 79.632%, stu_test_loss: 1.9301
2022-03-18 13:14:15 - until epoch: 016, tea_best_acc1: 59.472%, stu_best_acc1: 56.106%
2022-03-18 13:14:15 - epoch 017 lr: 0.1
2022-03-18 13:15:15 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 4.5285, tea_CELoss: 1.9513 stu_CELoss: 2.1971 DMLLoss: 0.3801 
2022-03-18 13:16:11 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 4.7399, tea_CELoss: 2.0668 stu_CELoss: 2.2516 DMLLoss: 0.4214 
2022-03-18 13:17:08 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 4.8616, tea_CELoss: 2.1229 stu_CELoss: 2.3433 DMLLoss: 0.3953 
2022-03-18 13:18:04 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 3.8631, tea_CELoss: 1.6726 stu_CELoss: 1.7866 DMLLoss: 0.4039 
2022-03-18 13:19:01 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 4.5107, tea_CELoss: 2.0063 stu_CELoss: 2.1349 DMLLoss: 0.3695 
2022-03-18 13:19:58 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 4.9402, tea_CELoss: 2.1438 stu_CELoss: 2.3612 DMLLoss: 0.4353 
2022-03-18 13:20:55 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 4.4875, tea_CELoss: 1.9636 stu_CELoss: 2.1245 DMLLoss: 0.3994 
2022-03-18 13:21:52 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 4.3198, tea_CELoss: 1.8977 stu_CELoss: 2.0305 DMLLoss: 0.3916 
2022-03-18 13:22:48 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 4.4010, tea_CELoss: 1.9203 stu_CELoss: 2.1155 DMLLoss: 0.3653 
2022-03-18 13:23:45 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 4.6208, tea_CELoss: 2.0175 stu_CELoss: 2.2176 DMLLoss: 0.3857 
2022-03-18 13:24:42 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 4.9420, tea_CELoss: 2.2055 stu_CELoss: 2.3542 DMLLoss: 0.3823 
2022-03-18 13:25:38 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 4.1714, tea_CELoss: 1.7590 stu_CELoss: 2.0279 DMLLoss: 0.3845 
2022-03-18 13:26:35 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 4.6411, tea_CELoss: 2.0288 stu_CELoss: 2.2272 DMLLoss: 0.3852 
2022-03-18 13:27:32 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 4.5567, tea_CELoss: 1.9541 stu_CELoss: 2.1939 DMLLoss: 0.4087 
2022-03-18 13:28:28 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 4.2625, tea_CELoss: 1.8688 stu_CELoss: 2.0317 DMLLoss: 0.3621 
2022-03-18 13:29:25 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 4.4416, tea_CELoss: 1.9231 stu_CELoss: 2.1298 DMLLoss: 0.3886 
2022-03-18 13:30:22 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 4.1717, tea_CELoss: 1.8299 stu_CELoss: 1.9808 DMLLoss: 0.3610 
2022-03-18 13:31:19 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 4.5151, tea_CELoss: 1.9584 stu_CELoss: 2.1656 DMLLoss: 0.3911 
2022-03-18 13:32:15 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 4.2528, tea_CELoss: 1.8491 stu_CELoss: 2.0093 DMLLoss: 0.3944 
2022-03-18 13:33:12 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 4.4651, tea_CELoss: 1.9813 stu_CELoss: 2.1504 DMLLoss: 0.3333 
2022-03-18 13:34:09 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 4.3134, tea_CELoss: 1.8893 stu_CELoss: 2.0503 DMLLoss: 0.3738 
2022-03-18 13:35:06 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 3.8995, tea_CELoss: 1.6968 stu_CELoss: 1.8788 DMLLoss: 0.3238 
2022-03-18 13:36:03 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 4.5484, tea_CELoss: 1.9535 stu_CELoss: 2.1971 DMLLoss: 0.3978 
2022-03-18 13:37:00 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 4.5328, tea_CELoss: 2.0596 stu_CELoss: 2.1145 DMLLoss: 0.3587 
2022-03-18 13:37:57 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 4.6837, tea_CELoss: 2.0897 stu_CELoss: 2.2151 DMLLoss: 0.3789 
2022-03-18 13:38:54 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 4.3991, tea_CELoss: 1.8873 stu_CELoss: 2.0950 DMLLoss: 0.4168 
2022-03-18 13:39:51 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 4.3098, tea_CELoss: 1.9226 stu_CELoss: 2.0283 DMLLoss: 0.3589 
2022-03-18 13:40:48 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 4.1992, tea_CELoss: 1.8699 stu_CELoss: 1.9636 DMLLoss: 0.3658 
2022-03-18 13:41:45 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 5.1235, tea_CELoss: 2.2434 stu_CELoss: 2.4710 DMLLoss: 0.4091 
2022-03-18 13:42:42 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 4.4526, tea_CELoss: 1.9261 stu_CELoss: 2.1339 DMLLoss: 0.3926 
2022-03-18 13:43:39 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 4.5558, tea_CELoss: 2.0286 stu_CELoss: 2.1690 DMLLoss: 0.3582 
2022-03-18 13:44:36 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 4.1053, tea_CELoss: 1.8055 stu_CELoss: 1.9378 DMLLoss: 0.3620 
2022-03-18 13:45:33 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 4.7767, tea_CELoss: 2.1076 stu_CELoss: 2.2778 DMLLoss: 0.3913 
2022-03-18 13:46:30 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 4.2931, tea_CELoss: 1.9252 stu_CELoss: 2.0195 DMLLoss: 0.3484 
2022-03-18 13:47:27 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 4.4157, tea_CELoss: 1.9486 stu_CELoss: 2.1129 DMLLoss: 0.3543 
2022-03-18 13:48:24 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 4.5672, tea_CELoss: 2.0054 stu_CELoss: 2.1928 DMLLoss: 0.3691 
2022-03-18 13:49:21 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 4.3641, tea_CELoss: 1.9151 stu_CELoss: 2.0640 DMLLoss: 0.3851 
2022-03-18 13:50:18 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 5.0853, tea_CELoss: 2.2327 stu_CELoss: 2.4357 DMLLoss: 0.4169 
2022-03-18 13:51:15 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 4.3125, tea_CELoss: 1.8855 stu_CELoss: 2.0075 DMLLoss: 0.4195 
2022-03-18 13:52:12 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 4.5156, tea_CELoss: 2.0142 stu_CELoss: 2.1211 DMLLoss: 0.3802 
2022-03-18 13:53:09 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 4.6805, tea_CELoss: 2.0276 stu_CELoss: 2.2503 DMLLoss: 0.4026 
2022-03-18 13:54:07 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 4.3351, tea_CELoss: 1.8875 stu_CELoss: 2.0775 DMLLoss: 0.3700 
2022-03-18 13:55:04 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 4.1762, tea_CELoss: 1.8257 stu_CELoss: 1.9533 DMLLoss: 0.3971 
2022-03-18 13:56:01 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 4.7060, tea_CELoss: 2.0386 stu_CELoss: 2.2680 DMLLoss: 0.3994 
2022-03-18 13:56:58 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 4.7398, tea_CELoss: 2.0966 stu_CELoss: 2.2723 DMLLoss: 0.3709 
2022-03-18 13:57:55 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 4.0804, tea_CELoss: 1.8154 stu_CELoss: 1.9326 DMLLoss: 0.3324 
2022-03-18 13:58:52 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 4.8291, tea_CELoss: 2.1029 stu_CELoss: 2.3484 DMLLoss: 0.3777 
2022-03-18 13:59:49 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 4.6917, tea_CELoss: 2.0214 stu_CELoss: 2.2634 DMLLoss: 0.4069 
2022-03-18 14:00:47 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 4.5636, tea_CELoss: 2.0593 stu_CELoss: 2.1259 DMLLoss: 0.3784 
2022-03-18 14:01:44 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 4.2204, tea_CELoss: 1.8382 stu_CELoss: 2.0008 DMLLoss: 0.3814 
2022-03-18 14:01:47 - train: epoch 017, train_loss: 4.4468
2022-03-18 14:04:11 - eval: epoch: 017, tea_acc1: 59.260%, tea_acc5: 83.262%, tea_test_loss: 1.6932, stu_acc1: 56.342%, stu_acc5: 81.034%, stu_test_loss: 1.8481
2022-03-18 14:04:13 - until epoch: 017, tea_best_acc1: 59.472%, stu_best_acc1: 56.342%
2022-03-18 14:04:13 - epoch 018 lr: 0.1
2022-03-18 14:05:15 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 4.1319, tea_CELoss: 1.8402 stu_CELoss: 1.9118 DMLLoss: 0.3798 
2022-03-18 14:06:12 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 4.8891, tea_CELoss: 2.1207 stu_CELoss: 2.4009 DMLLoss: 0.3675 
2022-03-18 14:07:09 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 4.8222, tea_CELoss: 2.0825 stu_CELoss: 2.3218 DMLLoss: 0.4179 
2022-03-18 14:08:05 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 4.6269, tea_CELoss: 2.1020 stu_CELoss: 2.1470 DMLLoss: 0.3779 
2022-03-18 14:09:02 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 4.4549, tea_CELoss: 1.9486 stu_CELoss: 2.1637 DMLLoss: 0.3426 
2022-03-18 14:09:59 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 4.4752, tea_CELoss: 1.8934 stu_CELoss: 2.2086 DMLLoss: 0.3732 
2022-03-18 14:10:56 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 4.0357, tea_CELoss: 1.6519 stu_CELoss: 2.0137 DMLLoss: 0.3701 
2022-03-18 14:11:53 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 4.6537, tea_CELoss: 2.0375 stu_CELoss: 2.2361 DMLLoss: 0.3801 
2022-03-18 14:12:50 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 4.8647, tea_CELoss: 2.1239 stu_CELoss: 2.3290 DMLLoss: 0.4118 
2022-03-18 14:13:46 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 4.1682, tea_CELoss: 1.8030 stu_CELoss: 1.9672 DMLLoss: 0.3980 
2022-03-18 14:14:43 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 4.5781, tea_CELoss: 1.9808 stu_CELoss: 2.1838 DMLLoss: 0.4136 
2022-03-18 14:15:40 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 4.1974, tea_CELoss: 1.8337 stu_CELoss: 1.9679 DMLLoss: 0.3958 
2022-03-18 14:16:37 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 4.8407, tea_CELoss: 2.1689 stu_CELoss: 2.2713 DMLLoss: 0.4005 
2022-03-18 14:17:34 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 4.6622, tea_CELoss: 2.0582 stu_CELoss: 2.2199 DMLLoss: 0.3842 
2022-03-18 14:18:30 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 4.8676, tea_CELoss: 2.1583 stu_CELoss: 2.3378 DMLLoss: 0.3714 
2022-03-18 14:19:27 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 4.5009, tea_CELoss: 1.9873 stu_CELoss: 2.1226 DMLLoss: 0.3909 
2022-03-18 14:20:24 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 4.6704, tea_CELoss: 2.0302 stu_CELoss: 2.2535 DMLLoss: 0.3867 
2022-03-18 14:21:21 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 4.1896, tea_CELoss: 1.8149 stu_CELoss: 2.0059 DMLLoss: 0.3688 
2022-03-18 14:22:18 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 4.5417, tea_CELoss: 2.0415 stu_CELoss: 2.1370 DMLLoss: 0.3633 
2022-03-18 14:23:15 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 4.9986, tea_CELoss: 2.1811 stu_CELoss: 2.4388 DMLLoss: 0.3787 
2022-03-18 14:24:11 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 4.8796, tea_CELoss: 2.1375 stu_CELoss: 2.3250 DMLLoss: 0.4172 
2022-03-18 14:25:08 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 4.4427, tea_CELoss: 1.9527 stu_CELoss: 2.1492 DMLLoss: 0.3408 
2022-03-18 14:26:05 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 4.3195, tea_CELoss: 1.8860 stu_CELoss: 2.0781 DMLLoss: 0.3555 
2022-03-18 14:27:02 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 4.4789, tea_CELoss: 2.0060 stu_CELoss: 2.1187 DMLLoss: 0.3543 
2022-03-18 14:27:59 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 3.8974, tea_CELoss: 1.7034 stu_CELoss: 1.8816 DMLLoss: 0.3125 
2022-03-18 14:28:55 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 4.1622, tea_CELoss: 1.8664 stu_CELoss: 1.9627 DMLLoss: 0.3332 
2022-03-18 14:29:52 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 4.5245, tea_CELoss: 1.9911 stu_CELoss: 2.1729 DMLLoss: 0.3604 
2022-03-18 14:30:49 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 4.1268, tea_CELoss: 1.8348 stu_CELoss: 1.9212 DMLLoss: 0.3708 
2022-03-18 14:31:46 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 4.4000, tea_CELoss: 1.9255 stu_CELoss: 2.1235 DMLLoss: 0.3510 
2022-03-18 14:32:43 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 4.3299, tea_CELoss: 1.8647 stu_CELoss: 2.0783 DMLLoss: 0.3869 
2022-03-18 14:33:40 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 5.2698, tea_CELoss: 2.3225 stu_CELoss: 2.5443 DMLLoss: 0.4030 
2022-03-18 14:34:37 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 4.4073, tea_CELoss: 1.9159 stu_CELoss: 2.1109 DMLLoss: 0.3805 
2022-03-18 14:35:34 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 4.3678, tea_CELoss: 1.8658 stu_CELoss: 2.1004 DMLLoss: 0.4015 
2022-03-18 14:36:31 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 4.6928, tea_CELoss: 2.1124 stu_CELoss: 2.2688 DMLLoss: 0.3115 
2022-03-18 14:37:28 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 4.5067, tea_CELoss: 1.9508 stu_CELoss: 2.1716 DMLLoss: 0.3844 
2022-03-18 14:38:25 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 4.3053, tea_CELoss: 1.8893 stu_CELoss: 2.0530 DMLLoss: 0.3629 
2022-03-18 14:39:22 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 4.7571, tea_CELoss: 2.1400 stu_CELoss: 2.2327 DMLLoss: 0.3843 
2022-03-18 14:40:18 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 4.8373, tea_CELoss: 2.1238 stu_CELoss: 2.3375 DMLLoss: 0.3760 
2022-03-18 14:41:15 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 4.7341, tea_CELoss: 2.1729 stu_CELoss: 2.2327 DMLLoss: 0.3284 
2022-03-18 14:42:12 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 4.7146, tea_CELoss: 2.1403 stu_CELoss: 2.2138 DMLLoss: 0.3605 
2022-03-18 14:43:09 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 4.8615, tea_CELoss: 2.1561 stu_CELoss: 2.3238 DMLLoss: 0.3816 
2022-03-18 14:44:06 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 4.3886, tea_CELoss: 1.9859 stu_CELoss: 2.0715 DMLLoss: 0.3312 
2022-03-18 14:45:03 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 4.6889, tea_CELoss: 2.0590 stu_CELoss: 2.2320 DMLLoss: 0.3980 
2022-03-18 14:46:00 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 4.3211, tea_CELoss: 1.8910 stu_CELoss: 2.0308 DMLLoss: 0.3993 
2022-03-18 14:46:57 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 4.6778, tea_CELoss: 2.0337 stu_CELoss: 2.2367 DMLLoss: 0.4074 
2022-03-18 14:47:54 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 4.3861, tea_CELoss: 1.9127 stu_CELoss: 2.1090 DMLLoss: 0.3644 
2022-03-18 14:48:51 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 4.7114, tea_CELoss: 2.0871 stu_CELoss: 2.2606 DMLLoss: 0.3637 
2022-03-18 14:49:47 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 4.3344, tea_CELoss: 1.9495 stu_CELoss: 2.0429 DMLLoss: 0.3420 
2022-03-18 14:50:44 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 4.2304, tea_CELoss: 1.8547 stu_CELoss: 1.9944 DMLLoss: 0.3813 
2022-03-18 14:51:41 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 4.7022, tea_CELoss: 2.1091 stu_CELoss: 2.1843 DMLLoss: 0.4088 
2022-03-18 14:51:44 - train: epoch 018, train_loss: 4.4313
2022-03-18 14:54:08 - eval: epoch: 018, tea_acc1: 57.626%, tea_acc5: 82.296%, tea_test_loss: 1.7676, stu_acc1: 52.220%, stu_acc5: 77.864%, stu_test_loss: 2.0365
2022-03-18 14:54:10 - until epoch: 018, tea_best_acc1: 59.472%, stu_best_acc1: 56.342%
2022-03-18 14:54:10 - epoch 019 lr: 0.1
2022-03-18 14:55:12 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 4.0403, tea_CELoss: 1.7492 stu_CELoss: 1.9039 DMLLoss: 0.3872 
2022-03-18 14:56:08 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 4.2037, tea_CELoss: 1.8490 stu_CELoss: 1.9781 DMLLoss: 0.3765 
2022-03-18 14:57:05 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 4.6193, tea_CELoss: 2.0525 stu_CELoss: 2.1974 DMLLoss: 0.3694 
2022-03-18 14:58:02 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 4.6549, tea_CELoss: 2.0522 stu_CELoss: 2.2164 DMLLoss: 0.3863 
2022-03-18 14:58:59 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 4.1158, tea_CELoss: 1.8019 stu_CELoss: 1.9599 DMLLoss: 0.3540 
2022-03-18 14:59:55 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 4.5961, tea_CELoss: 2.0572 stu_CELoss: 2.1452 DMLLoss: 0.3937 
2022-03-18 15:00:52 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 4.3061, tea_CELoss: 1.8845 stu_CELoss: 2.0529 DMLLoss: 0.3686 
2022-03-18 15:01:48 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 4.4949, tea_CELoss: 1.9860 stu_CELoss: 2.1454 DMLLoss: 0.3635 
2022-03-18 15:02:45 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 4.3009, tea_CELoss: 1.8315 stu_CELoss: 2.0738 DMLLoss: 0.3956 
2022-03-18 15:03:41 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 4.7720, tea_CELoss: 2.1265 stu_CELoss: 2.2379 DMLLoss: 0.4077 
2022-03-18 15:04:38 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 4.6479, tea_CELoss: 2.0414 stu_CELoss: 2.1681 DMLLoss: 0.4384 
2022-03-18 15:05:34 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 4.3460, tea_CELoss: 1.9221 stu_CELoss: 2.0959 DMLLoss: 0.3280 
2022-03-18 15:06:31 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 4.4918, tea_CELoss: 1.9749 stu_CELoss: 2.1198 DMLLoss: 0.3971 
2022-03-18 15:07:27 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 4.1829, tea_CELoss: 1.7946 stu_CELoss: 2.0294 DMLLoss: 0.3589 
2022-03-18 15:08:24 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 4.7517, tea_CELoss: 2.0833 stu_CELoss: 2.2581 DMLLoss: 0.4103 
2022-03-18 15:09:20 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 4.0283, tea_CELoss: 1.7632 stu_CELoss: 1.8950 DMLLoss: 0.3701 
2022-03-18 15:10:17 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 4.6465, tea_CELoss: 2.0966 stu_CELoss: 2.1956 DMLLoss: 0.3543 
2022-03-18 15:11:14 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 4.3086, tea_CELoss: 1.8859 stu_CELoss: 2.0648 DMLLoss: 0.3579 
2022-03-18 15:12:10 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 4.6531, tea_CELoss: 2.0580 stu_CELoss: 2.2350 DMLLoss: 0.3601 
2022-03-18 15:13:07 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 4.3100, tea_CELoss: 1.8724 stu_CELoss: 2.0565 DMLLoss: 0.3811 
2022-03-18 15:14:03 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 4.5169, tea_CELoss: 1.9407 stu_CELoss: 2.1896 DMLLoss: 0.3866 
2022-03-18 15:15:00 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 4.5000, tea_CELoss: 1.9652 stu_CELoss: 2.1777 DMLLoss: 0.3571 
2022-03-18 15:15:57 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 4.4507, tea_CELoss: 1.9326 stu_CELoss: 2.1237 DMLLoss: 0.3944 
2022-03-18 15:16:53 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 4.6434, tea_CELoss: 2.0700 stu_CELoss: 2.2103 DMLLoss: 0.3631 
2022-03-18 15:17:50 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 4.4916, tea_CELoss: 2.0101 stu_CELoss: 2.1166 DMLLoss: 0.3650 
2022-03-18 15:18:47 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 4.0732, tea_CELoss: 1.7669 stu_CELoss: 1.9388 DMLLoss: 0.3675 
2022-03-18 15:19:43 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 4.5217, tea_CELoss: 1.9709 stu_CELoss: 2.1359 DMLLoss: 0.4149 
2022-03-18 15:20:40 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 4.1837, tea_CELoss: 1.8755 stu_CELoss: 1.9661 DMLLoss: 0.3422 
2022-03-18 15:21:37 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 4.5611, tea_CELoss: 1.9683 stu_CELoss: 2.2202 DMLLoss: 0.3726 
2022-03-18 15:22:34 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 4.8053, tea_CELoss: 2.1432 stu_CELoss: 2.2831 DMLLoss: 0.3791 
2022-03-18 15:23:31 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 4.3635, tea_CELoss: 1.9425 stu_CELoss: 2.0322 DMLLoss: 0.3887 
2022-03-18 15:24:27 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 4.1525, tea_CELoss: 1.8147 stu_CELoss: 1.9862 DMLLoss: 0.3516 
2022-03-18 15:25:24 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 4.2952, tea_CELoss: 1.8579 stu_CELoss: 2.0848 DMLLoss: 0.3524 
2022-03-18 15:26:21 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 4.4112, tea_CELoss: 1.8979 stu_CELoss: 2.1343 DMLLoss: 0.3790 
2022-03-18 15:27:18 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 5.0572, tea_CELoss: 2.2227 stu_CELoss: 2.4046 DMLLoss: 0.4298 
2022-03-18 15:28:15 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 3.8099, tea_CELoss: 1.6389 stu_CELoss: 1.7984 DMLLoss: 0.3725 
2022-03-18 15:29:12 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 4.4354, tea_CELoss: 1.9595 stu_CELoss: 2.1177 DMLLoss: 0.3581 
2022-03-18 15:30:09 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 4.5662, tea_CELoss: 2.0484 stu_CELoss: 2.1518 DMLLoss: 0.3660 
2022-03-18 15:31:06 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 4.3701, tea_CELoss: 1.9626 stu_CELoss: 2.0442 DMLLoss: 0.3633 
2022-03-18 15:32:04 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 4.3584, tea_CELoss: 1.9355 stu_CELoss: 2.0725 DMLLoss: 0.3504 
2022-03-18 15:33:01 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 4.8555, tea_CELoss: 2.1160 stu_CELoss: 2.3486 DMLLoss: 0.3908 
2022-03-18 15:33:58 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 4.2501, tea_CELoss: 1.8641 stu_CELoss: 2.0052 DMLLoss: 0.3807 
2022-03-18 15:34:55 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 4.4320, tea_CELoss: 1.9336 stu_CELoss: 2.1381 DMLLoss: 0.3603 
2022-03-18 15:35:52 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 4.9198, tea_CELoss: 2.1277 stu_CELoss: 2.3760 DMLLoss: 0.4160 
2022-03-18 15:36:49 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 5.0042, tea_CELoss: 2.1889 stu_CELoss: 2.4410 DMLLoss: 0.3744 
2022-03-18 15:37:46 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 4.1144, tea_CELoss: 1.8205 stu_CELoss: 1.9324 DMLLoss: 0.3615 
2022-03-18 15:38:43 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 4.3756, tea_CELoss: 1.8719 stu_CELoss: 2.1000 DMLLoss: 0.4037 
2022-03-18 15:39:40 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 4.4019, tea_CELoss: 1.9723 stu_CELoss: 2.0931 DMLLoss: 0.3365 
2022-03-18 15:40:37 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 4.3067, tea_CELoss: 1.8996 stu_CELoss: 2.0357 DMLLoss: 0.3713 
2022-03-18 15:41:34 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 4.2676, tea_CELoss: 1.8646 stu_CELoss: 2.0318 DMLLoss: 0.3712 
2022-03-18 15:41:37 - train: epoch 019, train_loss: 4.4179
2022-03-18 15:44:00 - eval: epoch: 019, tea_acc1: 57.202%, tea_acc5: 81.528%, tea_test_loss: 1.7969, stu_acc1: 55.530%, stu_acc5: 80.688%, stu_test_loss: 1.8728
2022-03-18 15:44:01 - until epoch: 019, tea_best_acc1: 59.472%, stu_best_acc1: 56.342%
2022-03-18 15:44:01 - epoch 020 lr: 0.1
2022-03-18 15:45:02 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 4.4213, tea_CELoss: 1.9341 stu_CELoss: 2.1373 DMLLoss: 0.3498 
2022-03-18 15:45:59 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 3.8893, tea_CELoss: 1.7103 stu_CELoss: 1.8349 DMLLoss: 0.3440 
2022-03-18 15:46:55 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 4.1516, tea_CELoss: 1.7988 stu_CELoss: 1.9824 DMLLoss: 0.3704 
2022-03-18 15:47:52 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 4.1136, tea_CELoss: 1.7965 stu_CELoss: 1.9527 DMLLoss: 0.3644 
2022-03-18 15:48:49 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 4.1264, tea_CELoss: 1.8259 stu_CELoss: 1.9567 DMLLoss: 0.3438 
2022-03-18 15:49:45 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 4.5279, tea_CELoss: 1.9495 stu_CELoss: 2.1605 DMLLoss: 0.4180 
2022-03-18 15:50:42 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 4.0595, tea_CELoss: 1.7076 stu_CELoss: 1.9532 DMLLoss: 0.3987 
2022-03-18 15:51:39 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 4.6892, tea_CELoss: 2.0642 stu_CELoss: 2.2766 DMLLoss: 0.3484 
2022-03-18 15:52:36 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 4.6184, tea_CELoss: 2.0716 stu_CELoss: 2.2009 DMLLoss: 0.3459 
2022-03-18 15:53:33 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 4.4775, tea_CELoss: 1.9351 stu_CELoss: 2.1685 DMLLoss: 0.3739 
2022-03-18 15:54:30 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 4.2181, tea_CELoss: 1.8010 stu_CELoss: 2.0221 DMLLoss: 0.3950 
2022-03-18 15:55:27 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 4.1583, tea_CELoss: 1.8488 stu_CELoss: 1.9397 DMLLoss: 0.3699 
2022-03-18 15:56:24 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 4.4889, tea_CELoss: 2.0033 stu_CELoss: 2.1358 DMLLoss: 0.3498 
2022-03-18 15:57:21 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 4.3621, tea_CELoss: 1.9389 stu_CELoss: 2.0420 DMLLoss: 0.3812 
2022-03-18 15:58:18 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 4.3502, tea_CELoss: 1.9184 stu_CELoss: 2.0598 DMLLoss: 0.3721 
2022-03-18 15:59:15 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 4.6391, tea_CELoss: 2.1056 stu_CELoss: 2.1648 DMLLoss: 0.3687 
2022-03-18 16:00:12 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 4.3417, tea_CELoss: 1.8981 stu_CELoss: 2.0699 DMLLoss: 0.3737 
2022-03-18 16:01:08 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 4.5187, tea_CELoss: 1.9984 stu_CELoss: 2.1484 DMLLoss: 0.3718 
2022-03-18 16:02:04 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 4.3075, tea_CELoss: 1.9103 stu_CELoss: 2.0230 DMLLoss: 0.3742 
2022-03-18 16:03:01 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 4.2567, tea_CELoss: 1.8331 stu_CELoss: 2.0404 DMLLoss: 0.3832 
2022-03-18 16:03:58 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 4.8917, tea_CELoss: 2.1801 stu_CELoss: 2.3222 DMLLoss: 0.3894 
2022-03-18 16:04:54 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.9175, tea_CELoss: 1.6796 stu_CELoss: 1.8826 DMLLoss: 0.3554 
2022-03-18 16:05:51 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 4.3968, tea_CELoss: 1.9452 stu_CELoss: 2.0716 DMLLoss: 0.3800 
2022-03-18 16:06:48 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 4.7097, tea_CELoss: 2.0794 stu_CELoss: 2.2461 DMLLoss: 0.3843 
2022-03-18 16:07:44 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 4.3981, tea_CELoss: 1.9564 stu_CELoss: 2.0746 DMLLoss: 0.3672 
2022-03-18 16:08:41 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 4.4529, tea_CELoss: 1.9671 stu_CELoss: 2.0623 DMLLoss: 0.4235 
2022-03-18 16:09:38 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 4.2500, tea_CELoss: 1.8707 stu_CELoss: 2.0145 DMLLoss: 0.3647 
2022-03-18 16:10:34 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 4.5534, tea_CELoss: 1.9976 stu_CELoss: 2.1587 DMLLoss: 0.3971 
2022-03-18 16:11:31 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 4.5799, tea_CELoss: 1.9729 stu_CELoss: 2.2049 DMLLoss: 0.4021 
2022-03-18 16:12:27 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 4.6228, tea_CELoss: 2.0142 stu_CELoss: 2.2258 DMLLoss: 0.3828 
2022-03-18 16:13:24 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 4.6080, tea_CELoss: 2.0005 stu_CELoss: 2.2367 DMLLoss: 0.3708 
2022-03-18 16:14:21 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 4.4601, tea_CELoss: 1.9482 stu_CELoss: 2.1553 DMLLoss: 0.3566 
2022-03-18 16:15:17 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 3.9194, tea_CELoss: 1.6820 stu_CELoss: 1.8811 DMLLoss: 0.3563 
2022-03-18 16:16:14 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 4.2690, tea_CELoss: 1.9176 stu_CELoss: 2.0281 DMLLoss: 0.3232 
2022-03-18 16:17:10 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 4.0173, tea_CELoss: 1.7403 stu_CELoss: 1.9232 DMLLoss: 0.3538 
2022-03-18 16:18:07 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 4.5970, tea_CELoss: 2.0425 stu_CELoss: 2.1600 DMLLoss: 0.3944 
2022-03-18 16:19:04 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 4.4346, tea_CELoss: 1.9239 stu_CELoss: 2.1248 DMLLoss: 0.3859 
2022-03-18 16:20:00 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 4.4171, tea_CELoss: 1.9219 stu_CELoss: 2.1384 DMLLoss: 0.3569 
2022-03-18 16:20:57 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 4.6095, tea_CELoss: 2.0080 stu_CELoss: 2.2281 DMLLoss: 0.3734 
2022-03-18 16:21:54 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 4.3256, tea_CELoss: 1.9006 stu_CELoss: 2.0651 DMLLoss: 0.3599 
2022-03-18 16:22:51 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 4.5289, tea_CELoss: 1.9868 stu_CELoss: 2.1497 DMLLoss: 0.3924 
2022-03-18 16:23:48 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 4.4173, tea_CELoss: 1.9238 stu_CELoss: 2.1315 DMLLoss: 0.3620 
2022-03-18 16:24:45 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 4.8016, tea_CELoss: 2.0899 stu_CELoss: 2.3207 DMLLoss: 0.3910 
2022-03-18 16:25:42 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 4.4656, tea_CELoss: 1.9892 stu_CELoss: 2.1066 DMLLoss: 0.3699 
2022-03-18 16:26:38 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 4.5081, tea_CELoss: 2.0552 stu_CELoss: 2.0973 DMLLoss: 0.3556 
2022-03-18 16:27:35 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 4.4454, tea_CELoss: 1.9854 stu_CELoss: 2.1049 DMLLoss: 0.3551 
2022-03-18 16:28:32 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 4.2911, tea_CELoss: 1.8869 stu_CELoss: 2.0570 DMLLoss: 0.3472 
2022-03-18 16:29:29 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 4.2252, tea_CELoss: 1.8947 stu_CELoss: 1.9804 DMLLoss: 0.3500 
2022-03-18 16:30:27 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 4.5817, tea_CELoss: 2.0416 stu_CELoss: 2.1334 DMLLoss: 0.4067 
2022-03-18 16:31:24 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 4.1239, tea_CELoss: 1.8206 stu_CELoss: 1.9617 DMLLoss: 0.3416 
2022-03-18 16:31:26 - train: epoch 020, train_loss: 4.4035
2022-03-18 16:33:49 - eval: epoch: 020, tea_acc1: 58.372%, tea_acc5: 82.590%, tea_test_loss: 1.7481, stu_acc1: 57.018%, stu_acc5: 81.296%, stu_test_loss: 1.8144
2022-03-18 16:33:51 - until epoch: 020, tea_best_acc1: 59.472%, stu_best_acc1: 57.018%
2022-03-18 16:33:51 - epoch 021 lr: 0.1
2022-03-18 16:34:51 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 4.3145, tea_CELoss: 1.9588 stu_CELoss: 2.0277 DMLLoss: 0.3280 
2022-03-18 16:35:48 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 4.4288, tea_CELoss: 1.8832 stu_CELoss: 2.1779 DMLLoss: 0.3676 
2022-03-18 16:36:44 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 3.8854, tea_CELoss: 1.7241 stu_CELoss: 1.7976 DMLLoss: 0.3637 
2022-03-18 16:37:40 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 4.3907, tea_CELoss: 1.9465 stu_CELoss: 2.0936 DMLLoss: 0.3507 
2022-03-18 16:38:37 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 3.7373, tea_CELoss: 1.6512 stu_CELoss: 1.7619 DMLLoss: 0.3242 
2022-03-18 16:39:33 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 4.2044, tea_CELoss: 1.8459 stu_CELoss: 1.9703 DMLLoss: 0.3882 
2022-03-18 16:40:30 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.9591, tea_CELoss: 1.7632 stu_CELoss: 1.8471 DMLLoss: 0.3487 
2022-03-18 16:41:26 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 4.4265, tea_CELoss: 1.9515 stu_CELoss: 2.1191 DMLLoss: 0.3559 
2022-03-18 16:42:23 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 4.1268, tea_CELoss: 1.8059 stu_CELoss: 1.9607 DMLLoss: 0.3601 
2022-03-18 16:43:19 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 4.1472, tea_CELoss: 1.8595 stu_CELoss: 1.9201 DMLLoss: 0.3677 
2022-03-18 16:44:16 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 4.4990, tea_CELoss: 1.9302 stu_CELoss: 2.1745 DMLLoss: 0.3943 
2022-03-18 16:45:12 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 4.2488, tea_CELoss: 1.9039 stu_CELoss: 1.9987 DMLLoss: 0.3462 
2022-03-18 16:46:09 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.9197, tea_CELoss: 1.7325 stu_CELoss: 1.8325 DMLLoss: 0.3547 
2022-03-18 16:47:06 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 3.6482, tea_CELoss: 1.5813 stu_CELoss: 1.7255 DMLLoss: 0.3415 
2022-03-18 16:48:02 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 4.3479, tea_CELoss: 1.8923 stu_CELoss: 2.0638 DMLLoss: 0.3918 
2022-03-18 16:48:59 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 4.2814, tea_CELoss: 1.8403 stu_CELoss: 2.0649 DMLLoss: 0.3763 
2022-03-18 16:49:55 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 4.6888, tea_CELoss: 2.0924 stu_CELoss: 2.2307 DMLLoss: 0.3657 
2022-03-18 16:50:52 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 4.4455, tea_CELoss: 1.9184 stu_CELoss: 2.1374 DMLLoss: 0.3897 
2022-03-18 16:51:48 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 4.7571, tea_CELoss: 2.0826 stu_CELoss: 2.2968 DMLLoss: 0.3777 
2022-03-18 16:52:45 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 4.7394, tea_CELoss: 2.0652 stu_CELoss: 2.2254 DMLLoss: 0.4488 
2022-03-18 16:53:41 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 3.8037, tea_CELoss: 1.6404 stu_CELoss: 1.8256 DMLLoss: 0.3378 
2022-03-18 16:54:38 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 4.4476, tea_CELoss: 1.9924 stu_CELoss: 2.0803 DMLLoss: 0.3748 
2022-03-18 16:55:35 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 4.7205, tea_CELoss: 2.0585 stu_CELoss: 2.2601 DMLLoss: 0.4019 
2022-03-18 16:56:31 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 4.0665, tea_CELoss: 1.7343 stu_CELoss: 1.9695 DMLLoss: 0.3626 
2022-03-18 16:57:28 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 4.4060, tea_CELoss: 1.9405 stu_CELoss: 2.1072 DMLLoss: 0.3583 
2022-03-18 16:58:25 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 4.5263, tea_CELoss: 2.0516 stu_CELoss: 2.1282 DMLLoss: 0.3465 
2022-03-18 16:59:21 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 4.1609, tea_CELoss: 1.8042 stu_CELoss: 1.9498 DMLLoss: 0.4069 
2022-03-18 17:00:18 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 4.3799, tea_CELoss: 1.9604 stu_CELoss: 2.0207 DMLLoss: 0.3988 
2022-03-18 17:01:14 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 4.3112, tea_CELoss: 1.8645 stu_CELoss: 2.0703 DMLLoss: 0.3763 
2022-03-18 17:02:11 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 4.8315, tea_CELoss: 2.1646 stu_CELoss: 2.2828 DMLLoss: 0.3841 
2022-03-18 17:03:07 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 4.6573, tea_CELoss: 1.9896 stu_CELoss: 2.2625 DMLLoss: 0.4052 
2022-03-18 17:04:04 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 4.0757, tea_CELoss: 1.7824 stu_CELoss: 1.9220 DMLLoss: 0.3713 
2022-03-18 17:05:01 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 4.9113, tea_CELoss: 2.2108 stu_CELoss: 2.3364 DMLLoss: 0.3641 
2022-03-18 17:05:57 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 4.7927, tea_CELoss: 2.1386 stu_CELoss: 2.3009 DMLLoss: 0.3532 
2022-03-18 17:06:54 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 4.5289, tea_CELoss: 2.0053 stu_CELoss: 2.1405 DMLLoss: 0.3831 
2022-03-18 17:07:50 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 4.6823, tea_CELoss: 2.0483 stu_CELoss: 2.2556 DMLLoss: 0.3784 
2022-03-18 17:08:47 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 4.1350, tea_CELoss: 1.8110 stu_CELoss: 1.9801 DMLLoss: 0.3439 
2022-03-18 17:09:43 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 4.2708, tea_CELoss: 1.9224 stu_CELoss: 1.9973 DMLLoss: 0.3511 
2022-03-18 17:10:40 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 4.1777, tea_CELoss: 1.8462 stu_CELoss: 1.9821 DMLLoss: 0.3494 
2022-03-18 17:11:36 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 4.6810, tea_CELoss: 2.0964 stu_CELoss: 2.1872 DMLLoss: 0.3974 
2022-03-18 17:12:33 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 4.1547, tea_CELoss: 1.8185 stu_CELoss: 1.9903 DMLLoss: 0.3459 
2022-03-18 17:13:29 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 4.5227, tea_CELoss: 1.9479 stu_CELoss: 2.1899 DMLLoss: 0.3849 
2022-03-18 17:14:26 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 4.7805, tea_CELoss: 2.1473 stu_CELoss: 2.2614 DMLLoss: 0.3718 
2022-03-18 17:15:23 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 4.4932, tea_CELoss: 1.9649 stu_CELoss: 2.1484 DMLLoss: 0.3799 
2022-03-18 17:16:20 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 4.5964, tea_CELoss: 2.0259 stu_CELoss: 2.1910 DMLLoss: 0.3795 
2022-03-18 17:17:16 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 4.3701, tea_CELoss: 1.9313 stu_CELoss: 2.1145 DMLLoss: 0.3243 
2022-03-18 17:18:13 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 4.6002, tea_CELoss: 2.0494 stu_CELoss: 2.1738 DMLLoss: 0.3770 
2022-03-18 17:19:10 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 4.4310, tea_CELoss: 1.9700 stu_CELoss: 2.0977 DMLLoss: 0.3633 
2022-03-18 17:20:07 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 4.0141, tea_CELoss: 1.7029 stu_CELoss: 1.9104 DMLLoss: 0.4008 
2022-03-18 17:21:04 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 4.2466, tea_CELoss: 1.8631 stu_CELoss: 2.0237 DMLLoss: 0.3598 
2022-03-18 17:21:07 - train: epoch 021, train_loss: 4.3959
2022-03-18 17:23:27 - eval: epoch: 021, tea_acc1: 58.688%, tea_acc5: 83.290%, tea_test_loss: 1.7030, stu_acc1: 52.240%, stu_acc5: 77.948%, stu_test_loss: 2.0526
2022-03-18 17:23:29 - until epoch: 021, tea_best_acc1: 59.472%, stu_best_acc1: 57.018%
2022-03-18 17:23:29 - epoch 022 lr: 0.1
2022-03-18 17:24:29 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 3.7780, tea_CELoss: 1.6675 stu_CELoss: 1.7160 DMLLoss: 0.3946 
2022-03-18 17:25:26 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 4.3078, tea_CELoss: 1.8682 stu_CELoss: 2.0564 DMLLoss: 0.3832 
2022-03-18 17:26:22 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 4.1058, tea_CELoss: 1.8088 stu_CELoss: 1.9506 DMLLoss: 0.3464 
2022-03-18 17:27:19 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 4.1204, tea_CELoss: 1.7796 stu_CELoss: 1.9659 DMLLoss: 0.3750 
2022-03-18 17:28:15 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 4.3013, tea_CELoss: 1.9177 stu_CELoss: 2.0245 DMLLoss: 0.3591 
2022-03-18 17:29:12 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 4.3946, tea_CELoss: 1.9596 stu_CELoss: 2.0922 DMLLoss: 0.3428 
2022-03-18 17:30:08 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 4.8130, tea_CELoss: 2.0417 stu_CELoss: 2.3238 DMLLoss: 0.4475 
2022-03-18 17:31:05 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 4.7140, tea_CELoss: 2.1063 stu_CELoss: 2.2454 DMLLoss: 0.3623 
2022-03-18 17:32:01 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 4.6089, tea_CELoss: 2.0521 stu_CELoss: 2.1389 DMLLoss: 0.4179 
2022-03-18 17:32:58 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 4.5988, tea_CELoss: 2.0461 stu_CELoss: 2.1704 DMLLoss: 0.3824 
2022-03-18 17:33:54 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.9027, tea_CELoss: 1.7196 stu_CELoss: 1.8686 DMLLoss: 0.3145 
2022-03-18 17:34:51 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 3.7463, tea_CELoss: 1.6220 stu_CELoss: 1.7532 DMLLoss: 0.3710 
2022-03-18 17:35:47 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 4.6629, tea_CELoss: 2.0966 stu_CELoss: 2.1642 DMLLoss: 0.4021 
2022-03-18 17:36:44 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 4.1861, tea_CELoss: 1.8326 stu_CELoss: 1.9629 DMLLoss: 0.3906 
2022-03-18 17:37:40 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 4.6412, tea_CELoss: 2.0871 stu_CELoss: 2.2101 DMLLoss: 0.3440 
2022-03-18 17:38:37 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 4.1559, tea_CELoss: 1.7885 stu_CELoss: 1.9893 DMLLoss: 0.3780 
2022-03-18 17:39:33 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 4.5500, tea_CELoss: 2.0151 stu_CELoss: 2.1779 DMLLoss: 0.3570 
2022-03-18 17:40:30 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 4.4795, tea_CELoss: 1.9590 stu_CELoss: 2.1320 DMLLoss: 0.3885 
2022-03-18 17:41:27 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 4.3368, tea_CELoss: 1.8994 stu_CELoss: 2.0570 DMLLoss: 0.3805 
2022-03-18 17:42:23 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 4.1661, tea_CELoss: 1.8278 stu_CELoss: 1.9885 DMLLoss: 0.3497 
2022-03-18 17:43:20 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 4.2644, tea_CELoss: 1.8901 stu_CELoss: 1.9984 DMLLoss: 0.3759 
2022-03-18 17:44:17 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 3.8555, tea_CELoss: 1.6718 stu_CELoss: 1.8315 DMLLoss: 0.3522 
2022-03-18 17:45:13 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 4.5355, tea_CELoss: 1.9794 stu_CELoss: 2.1660 DMLLoss: 0.3900 
2022-03-18 17:46:10 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 4.7346, tea_CELoss: 2.0965 stu_CELoss: 2.3055 DMLLoss: 0.3326 
2022-03-18 17:47:07 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 4.4045, tea_CELoss: 1.9277 stu_CELoss: 2.0795 DMLLoss: 0.3973 
2022-03-18 17:48:04 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 4.1732, tea_CELoss: 1.8020 stu_CELoss: 1.9885 DMLLoss: 0.3828 
2022-03-18 17:49:00 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 4.1880, tea_CELoss: 1.8537 stu_CELoss: 1.9954 DMLLoss: 0.3389 
2022-03-18 17:49:57 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 4.7167, tea_CELoss: 2.0925 stu_CELoss: 2.2537 DMLLoss: 0.3705 
2022-03-18 17:50:54 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 4.2689, tea_CELoss: 1.8538 stu_CELoss: 2.0216 DMLLoss: 0.3935 
2022-03-18 17:51:51 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 4.2896, tea_CELoss: 1.8654 stu_CELoss: 2.0550 DMLLoss: 0.3692 
2022-03-18 17:52:48 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 4.5837, tea_CELoss: 2.0648 stu_CELoss: 2.1437 DMLLoss: 0.3753 
2022-03-18 17:53:44 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 4.2775, tea_CELoss: 1.9422 stu_CELoss: 1.9816 DMLLoss: 0.3537 
2022-03-18 17:54:41 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 4.4382, tea_CELoss: 1.9922 stu_CELoss: 2.0929 DMLLoss: 0.3531 
2022-03-18 17:55:38 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 4.1659, tea_CELoss: 1.7965 stu_CELoss: 1.9893 DMLLoss: 0.3801 
2022-03-18 17:56:35 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 4.5329, tea_CELoss: 2.0574 stu_CELoss: 2.1277 DMLLoss: 0.3478 
2022-03-18 17:57:32 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 4.3942, tea_CELoss: 1.9419 stu_CELoss: 2.0963 DMLLoss: 0.3561 
2022-03-18 17:58:29 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 4.3159, tea_CELoss: 1.8468 stu_CELoss: 2.0330 DMLLoss: 0.4361 
2022-03-18 17:59:26 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 4.7160, tea_CELoss: 2.0348 stu_CELoss: 2.2786 DMLLoss: 0.4025 
2022-03-18 18:00:22 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 4.4496, tea_CELoss: 1.9113 stu_CELoss: 2.1597 DMLLoss: 0.3786 
2022-03-18 18:01:19 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 4.4833, tea_CELoss: 1.9968 stu_CELoss: 2.1384 DMLLoss: 0.3480 
2022-03-18 18:02:16 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 4.5049, tea_CELoss: 2.0219 stu_CELoss: 2.1142 DMLLoss: 0.3688 
2022-03-18 18:03:12 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 4.3572, tea_CELoss: 1.9098 stu_CELoss: 2.1031 DMLLoss: 0.3443 
2022-03-18 18:04:09 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 4.3167, tea_CELoss: 1.8622 stu_CELoss: 2.0675 DMLLoss: 0.3870 
2022-03-18 18:05:06 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 4.1548, tea_CELoss: 1.8036 stu_CELoss: 1.9818 DMLLoss: 0.3693 
2022-03-18 18:06:02 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 4.2214, tea_CELoss: 1.8554 stu_CELoss: 1.9954 DMLLoss: 0.3706 
2022-03-18 18:06:59 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 4.6766, tea_CELoss: 2.0902 stu_CELoss: 2.2282 DMLLoss: 0.3582 
2022-03-18 18:07:56 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 4.5385, tea_CELoss: 1.9990 stu_CELoss: 2.1922 DMLLoss: 0.3473 
2022-03-18 18:08:53 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 3.7844, tea_CELoss: 1.6454 stu_CELoss: 1.7611 DMLLoss: 0.3779 
2022-03-18 18:09:49 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 4.3984, tea_CELoss: 1.9561 stu_CELoss: 2.0952 DMLLoss: 0.3471 
2022-03-18 18:10:46 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 4.1484, tea_CELoss: 1.7996 stu_CELoss: 1.9668 DMLLoss: 0.3821 
2022-03-18 18:10:49 - train: epoch 022, train_loss: 4.3810
2022-03-18 18:13:11 - eval: epoch: 022, tea_acc1: 59.848%, tea_acc5: 83.786%, tea_test_loss: 1.6599, stu_acc1: 49.202%, stu_acc5: 75.148%, stu_test_loss: 2.2121
2022-03-18 18:13:15 - until epoch: 022, tea_best_acc1: 59.848%, stu_best_acc1: 57.018%
2022-03-18 18:13:15 - epoch 023 lr: 0.1
2022-03-18 18:14:15 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 4.5669, tea_CELoss: 2.0499 stu_CELoss: 2.0970 DMLLoss: 0.4201 
2022-03-18 18:15:12 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 4.0377, tea_CELoss: 1.7873 stu_CELoss: 1.9132 DMLLoss: 0.3372 
2022-03-18 18:16:09 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 4.1879, tea_CELoss: 1.8094 stu_CELoss: 2.0441 DMLLoss: 0.3343 
2022-03-18 18:17:06 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 4.2759, tea_CELoss: 1.8163 stu_CELoss: 2.0846 DMLLoss: 0.3750 
2022-03-18 18:18:03 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 4.4412, tea_CELoss: 1.9477 stu_CELoss: 2.1247 DMLLoss: 0.3689 
2022-03-18 18:18:59 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 4.5755, tea_CELoss: 1.9590 stu_CELoss: 2.1962 DMLLoss: 0.4202 
2022-03-18 18:19:56 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 4.3225, tea_CELoss: 1.8870 stu_CELoss: 2.0666 DMLLoss: 0.3689 
2022-03-18 18:20:54 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 4.2240, tea_CELoss: 1.8463 stu_CELoss: 1.9799 DMLLoss: 0.3977 
2022-03-18 18:21:50 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 4.4702, tea_CELoss: 1.9556 stu_CELoss: 2.1295 DMLLoss: 0.3852 
2022-03-18 18:22:47 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 4.1627, tea_CELoss: 1.7797 stu_CELoss: 1.9792 DMLLoss: 0.4038 
2022-03-18 18:23:44 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 4.3386, tea_CELoss: 1.9082 stu_CELoss: 2.0722 DMLLoss: 0.3583 
2022-03-18 18:24:41 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 4.1340, tea_CELoss: 1.8004 stu_CELoss: 1.9880 DMLLoss: 0.3457 
2022-03-18 18:25:38 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 4.2085, tea_CELoss: 1.7938 stu_CELoss: 1.9904 DMLLoss: 0.4244 
2022-03-18 18:26:35 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 4.3111, tea_CELoss: 1.9065 stu_CELoss: 2.0519 DMLLoss: 0.3527 
2022-03-18 18:27:32 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 4.0981, tea_CELoss: 1.8093 stu_CELoss: 1.9399 DMLLoss: 0.3489 
2022-03-18 18:28:29 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 4.2863, tea_CELoss: 1.9426 stu_CELoss: 1.9737 DMLLoss: 0.3701 
2022-03-18 18:29:26 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 4.0760, tea_CELoss: 1.7433 stu_CELoss: 1.9584 DMLLoss: 0.3743 
2022-03-18 18:30:23 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 4.6424, tea_CELoss: 2.0819 stu_CELoss: 2.1755 DMLLoss: 0.3850 
2022-03-18 18:31:20 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 4.9015, tea_CELoss: 2.1834 stu_CELoss: 2.2743 DMLLoss: 0.4437 
2022-03-18 18:32:16 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 3.8639, tea_CELoss: 1.6586 stu_CELoss: 1.8190 DMLLoss: 0.3863 
2022-03-18 18:33:13 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 4.1575, tea_CELoss: 1.8648 stu_CELoss: 1.9703 DMLLoss: 0.3224 
2022-03-18 18:34:10 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 3.8725, tea_CELoss: 1.6897 stu_CELoss: 1.8477 DMLLoss: 0.3351 
2022-03-18 18:35:07 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 4.1392, tea_CELoss: 1.8445 stu_CELoss: 1.9590 DMLLoss: 0.3357 
2022-03-18 18:36:03 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 4.3376, tea_CELoss: 1.9194 stu_CELoss: 2.0541 DMLLoss: 0.3641 
2022-03-18 18:37:01 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 4.5148, tea_CELoss: 1.9756 stu_CELoss: 2.1296 DMLLoss: 0.4096 
2022-03-18 18:37:58 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 4.4506, tea_CELoss: 1.9727 stu_CELoss: 2.0931 DMLLoss: 0.3847 
2022-03-18 18:38:55 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 4.6893, tea_CELoss: 2.0647 stu_CELoss: 2.2571 DMLLoss: 0.3674 
2022-03-18 18:39:52 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 4.6827, tea_CELoss: 2.0782 stu_CELoss: 2.2406 DMLLoss: 0.3639 
2022-03-18 18:40:49 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 4.3159, tea_CELoss: 1.8148 stu_CELoss: 2.1040 DMLLoss: 0.3971 
2022-03-18 18:41:46 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 4.5345, tea_CELoss: 2.0273 stu_CELoss: 2.1175 DMLLoss: 0.3897 
2022-03-18 18:42:43 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 4.8566, tea_CELoss: 2.1423 stu_CELoss: 2.2980 DMLLoss: 0.4162 
2022-03-18 18:43:39 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 4.5274, tea_CELoss: 2.0665 stu_CELoss: 2.0813 DMLLoss: 0.3796 
2022-03-18 18:44:36 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 4.1295, tea_CELoss: 1.7905 stu_CELoss: 1.9847 DMLLoss: 0.3542 
2022-03-18 18:45:33 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 4.5314, tea_CELoss: 2.0403 stu_CELoss: 2.1312 DMLLoss: 0.3600 
2022-03-18 18:46:30 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 4.2883, tea_CELoss: 1.9352 stu_CELoss: 2.0174 DMLLoss: 0.3356 
2022-03-18 18:47:27 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 4.1277, tea_CELoss: 1.8534 stu_CELoss: 1.9484 DMLLoss: 0.3259 
2022-03-18 18:48:24 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 4.4088, tea_CELoss: 1.9466 stu_CELoss: 2.0905 DMLLoss: 0.3717 
2022-03-18 18:49:21 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 4.5592, tea_CELoss: 2.0551 stu_CELoss: 2.1390 DMLLoss: 0.3650 
2022-03-18 18:50:17 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 4.5488, tea_CELoss: 2.0565 stu_CELoss: 2.1366 DMLLoss: 0.3558 
2022-03-18 18:51:14 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 4.2409, tea_CELoss: 1.8506 stu_CELoss: 1.9883 DMLLoss: 0.4020 
2022-03-18 18:52:12 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 4.0484, tea_CELoss: 1.7477 stu_CELoss: 1.9178 DMLLoss: 0.3829 
2022-03-18 18:53:09 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 3.9417, tea_CELoss: 1.7434 stu_CELoss: 1.8485 DMLLoss: 0.3499 
2022-03-18 18:54:06 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 4.0784, tea_CELoss: 1.7933 stu_CELoss: 1.9370 DMLLoss: 0.3481 
2022-03-18 18:55:03 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 4.0149, tea_CELoss: 1.7522 stu_CELoss: 1.9001 DMLLoss: 0.3627 
2022-03-18 18:56:00 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 4.2424, tea_CELoss: 1.8568 stu_CELoss: 2.0304 DMLLoss: 0.3552 
2022-03-18 18:56:57 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 4.4307, tea_CELoss: 1.9671 stu_CELoss: 2.0773 DMLLoss: 0.3863 
2022-03-18 18:57:54 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 4.0792, tea_CELoss: 1.7723 stu_CELoss: 1.9209 DMLLoss: 0.3860 
2022-03-18 18:58:51 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 4.3243, tea_CELoss: 1.9338 stu_CELoss: 2.0682 DMLLoss: 0.3223 
2022-03-18 18:59:48 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 4.0457, tea_CELoss: 1.7882 stu_CELoss: 1.9182 DMLLoss: 0.3392 
2022-03-18 19:00:45 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 4.4752, tea_CELoss: 1.9954 stu_CELoss: 2.1001 DMLLoss: 0.3797 
2022-03-18 19:00:48 - train: epoch 023, train_loss: 4.3668
2022-03-18 19:03:09 - eval: epoch: 023, tea_acc1: 60.512%, tea_acc5: 84.140%, tea_test_loss: 1.6383, stu_acc1: 56.226%, stu_acc5: 80.852%, stu_test_loss: 1.8475
2022-03-18 19:03:12 - until epoch: 023, tea_best_acc1: 60.512%, stu_best_acc1: 57.018%
2022-03-18 19:03:12 - epoch 024 lr: 0.1
2022-03-18 19:04:13 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 4.7495, tea_CELoss: 2.1581 stu_CELoss: 2.2078 DMLLoss: 0.3836 
2022-03-18 19:05:09 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 4.0626, tea_CELoss: 1.7935 stu_CELoss: 1.9123 DMLLoss: 0.3567 
2022-03-18 19:06:05 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 4.5326, tea_CELoss: 2.0410 stu_CELoss: 2.1073 DMLLoss: 0.3842 
2022-03-18 19:07:02 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 4.4670, tea_CELoss: 1.9991 stu_CELoss: 2.1198 DMLLoss: 0.3481 
2022-03-18 19:07:58 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 4.3707, tea_CELoss: 1.9141 stu_CELoss: 2.0793 DMLLoss: 0.3774 
2022-03-18 19:08:55 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 4.4442, tea_CELoss: 1.9497 stu_CELoss: 2.1011 DMLLoss: 0.3934 
2022-03-18 19:09:52 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 4.0919, tea_CELoss: 1.7783 stu_CELoss: 1.9488 DMLLoss: 0.3648 
2022-03-18 19:10:48 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 4.0486, tea_CELoss: 1.7228 stu_CELoss: 1.9586 DMLLoss: 0.3672 
2022-03-18 19:11:45 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 4.1648, tea_CELoss: 1.7920 stu_CELoss: 1.9699 DMLLoss: 0.4029 
2022-03-18 19:12:41 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 4.2269, tea_CELoss: 1.8978 stu_CELoss: 1.9519 DMLLoss: 0.3772 
2022-03-18 19:13:38 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 4.0181, tea_CELoss: 1.7463 stu_CELoss: 1.8745 DMLLoss: 0.3974 
2022-03-18 19:14:34 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 4.2734, tea_CELoss: 1.8204 stu_CELoss: 2.0373 DMLLoss: 0.4156 
2022-03-18 19:15:31 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 4.6436, tea_CELoss: 2.0731 stu_CELoss: 2.2134 DMLLoss: 0.3571 
2022-03-18 19:16:27 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 3.8474, tea_CELoss: 1.7148 stu_CELoss: 1.8013 DMLLoss: 0.3313 
2022-03-18 19:17:24 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 4.5647, tea_CELoss: 2.0677 stu_CELoss: 2.1360 DMLLoss: 0.3609 
2022-03-18 19:18:21 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 4.3229, tea_CELoss: 1.9127 stu_CELoss: 2.0755 DMLLoss: 0.3347 
2022-03-18 19:19:18 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 4.2267, tea_CELoss: 1.8742 stu_CELoss: 1.9799 DMLLoss: 0.3726 
2022-03-18 19:20:15 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 4.5280, tea_CELoss: 2.0188 stu_CELoss: 2.1278 DMLLoss: 0.3814 
2022-03-18 19:21:12 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 4.1794, tea_CELoss: 1.8285 stu_CELoss: 2.0029 DMLLoss: 0.3479 
2022-03-18 19:22:09 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 4.4824, tea_CELoss: 1.9225 stu_CELoss: 2.1467 DMLLoss: 0.4132 
2022-03-18 19:23:06 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 4.1268, tea_CELoss: 1.8264 stu_CELoss: 1.9623 DMLLoss: 0.3380 
2022-03-18 19:24:03 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 4.2655, tea_CELoss: 1.8793 stu_CELoss: 2.0453 DMLLoss: 0.3409 
2022-03-18 19:25:00 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 4.3811, tea_CELoss: 1.8927 stu_CELoss: 2.1014 DMLLoss: 0.3870 
2022-03-18 19:25:57 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 4.8391, tea_CELoss: 2.1215 stu_CELoss: 2.3152 DMLLoss: 0.4023 
2022-03-18 19:26:54 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 4.4341, tea_CELoss: 1.9561 stu_CELoss: 2.0964 DMLLoss: 0.3816 
2022-03-18 19:27:51 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 4.9701, tea_CELoss: 2.2480 stu_CELoss: 2.3494 DMLLoss: 0.3727 
2022-03-18 19:28:48 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 4.6368, tea_CELoss: 2.0902 stu_CELoss: 2.1599 DMLLoss: 0.3866 
2022-03-18 19:29:45 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 4.5975, tea_CELoss: 2.0566 stu_CELoss: 2.1701 DMLLoss: 0.3708 
2022-03-18 19:30:42 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 4.0604, tea_CELoss: 1.7764 stu_CELoss: 1.9206 DMLLoss: 0.3634 
2022-03-18 19:31:39 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 4.1354, tea_CELoss: 1.8275 stu_CELoss: 1.9523 DMLLoss: 0.3557 
2022-03-18 19:32:36 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 3.9823, tea_CELoss: 1.6513 stu_CELoss: 1.9199 DMLLoss: 0.4111 
2022-03-18 19:33:33 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 4.7823, tea_CELoss: 2.1319 stu_CELoss: 2.2440 DMLLoss: 0.4065 
2022-03-18 19:34:30 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 4.0025, tea_CELoss: 1.7143 stu_CELoss: 1.9419 DMLLoss: 0.3463 
2022-03-18 19:35:28 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 4.0515, tea_CELoss: 1.7633 stu_CELoss: 1.8833 DMLLoss: 0.4049 
2022-03-18 19:36:25 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 4.4800, tea_CELoss: 1.9577 stu_CELoss: 2.1642 DMLLoss: 0.3581 
2022-03-18 19:37:22 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 4.3230, tea_CELoss: 1.8786 stu_CELoss: 2.0690 DMLLoss: 0.3754 
2022-03-18 19:38:20 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 4.1139, tea_CELoss: 1.8085 stu_CELoss: 1.9522 DMLLoss: 0.3532 
2022-03-18 19:39:17 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 4.6639, tea_CELoss: 2.0706 stu_CELoss: 2.2202 DMLLoss: 0.3732 
2022-03-18 19:40:14 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.9978, tea_CELoss: 1.7943 stu_CELoss: 1.8875 DMLLoss: 0.3160 
2022-03-18 19:41:11 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 4.4791, tea_CELoss: 1.9707 stu_CELoss: 2.1379 DMLLoss: 0.3705 
2022-03-18 19:42:08 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 4.1789, tea_CELoss: 1.8506 stu_CELoss: 2.0110 DMLLoss: 0.3173 
2022-03-18 19:43:05 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 4.2503, tea_CELoss: 1.9089 stu_CELoss: 1.9689 DMLLoss: 0.3726 
2022-03-18 19:44:03 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 4.1421, tea_CELoss: 1.8327 stu_CELoss: 1.9414 DMLLoss: 0.3680 
2022-03-18 19:45:00 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 4.4915, tea_CELoss: 2.0018 stu_CELoss: 2.1467 DMLLoss: 0.3430 
2022-03-18 19:45:57 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 3.8549, tea_CELoss: 1.6656 stu_CELoss: 1.8292 DMLLoss: 0.3602 
2022-03-18 19:46:54 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 4.8255, tea_CELoss: 2.1322 stu_CELoss: 2.2919 DMLLoss: 0.4014 
2022-03-18 19:47:52 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 4.2525, tea_CELoss: 1.8233 stu_CELoss: 2.0469 DMLLoss: 0.3823 
2022-03-18 19:48:49 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 3.8235, tea_CELoss: 1.6490 stu_CELoss: 1.8252 DMLLoss: 0.3493 
2022-03-18 19:49:46 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 4.5712, tea_CELoss: 2.0370 stu_CELoss: 2.1593 DMLLoss: 0.3749 
2022-03-18 19:50:43 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 4.5862, tea_CELoss: 1.9950 stu_CELoss: 2.2196 DMLLoss: 0.3717 
2022-03-18 19:50:46 - train: epoch 024, train_loss: 4.3596
2022-03-18 19:53:08 - eval: epoch: 024, tea_acc1: 59.516%, tea_acc5: 83.342%, tea_test_loss: 1.6896, stu_acc1: 55.802%, stu_acc5: 80.604%, stu_test_loss: 1.8733
2022-03-18 19:53:10 - until epoch: 024, tea_best_acc1: 60.512%, stu_best_acc1: 57.018%
2022-03-18 19:53:10 - epoch 025 lr: 0.1
2022-03-18 19:54:11 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 4.2928, tea_CELoss: 1.9169 stu_CELoss: 2.0245 DMLLoss: 0.3514 
2022-03-18 19:55:07 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 3.6797, tea_CELoss: 1.6142 stu_CELoss: 1.7310 DMLLoss: 0.3346 
2022-03-18 19:56:04 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 4.1967, tea_CELoss: 1.8808 stu_CELoss: 1.9686 DMLLoss: 0.3474 
2022-03-18 19:57:01 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 4.7289, tea_CELoss: 2.1161 stu_CELoss: 2.2252 DMLLoss: 0.3875 
2022-03-18 19:57:57 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 4.1437, tea_CELoss: 1.7557 stu_CELoss: 1.9942 DMLLoss: 0.3938 
2022-03-18 19:58:54 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 4.3660, tea_CELoss: 1.8999 stu_CELoss: 2.0834 DMLLoss: 0.3826 
2022-03-18 19:59:51 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 4.2083, tea_CELoss: 1.8559 stu_CELoss: 1.9808 DMLLoss: 0.3716 
2022-03-18 20:00:47 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 4.0448, tea_CELoss: 1.8301 stu_CELoss: 1.8795 DMLLoss: 0.3352 
2022-03-18 20:01:44 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 4.2582, tea_CELoss: 1.8456 stu_CELoss: 2.0315 DMLLoss: 0.3811 
2022-03-18 20:02:41 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 4.2509, tea_CELoss: 1.8628 stu_CELoss: 2.0041 DMLLoss: 0.3840 
2022-03-18 20:03:37 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 4.2720, tea_CELoss: 1.8604 stu_CELoss: 2.0128 DMLLoss: 0.3989 
2022-03-18 20:04:34 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 4.3900, tea_CELoss: 1.9726 stu_CELoss: 2.0657 DMLLoss: 0.3517 
2022-03-18 20:05:31 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 4.1737, tea_CELoss: 1.8435 stu_CELoss: 1.9624 DMLLoss: 0.3679 
2022-03-18 20:06:28 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 3.7457, tea_CELoss: 1.6618 stu_CELoss: 1.7527 DMLLoss: 0.3312 
2022-03-18 20:07:25 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 4.6344, tea_CELoss: 2.0253 stu_CELoss: 2.2368 DMLLoss: 0.3723 
2022-03-18 20:08:22 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 4.2553, tea_CELoss: 1.9159 stu_CELoss: 1.9660 DMLLoss: 0.3733 
2022-03-18 20:09:19 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 4.5131, tea_CELoss: 1.9885 stu_CELoss: 2.1312 DMLLoss: 0.3934 
2022-03-18 20:10:16 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 4.1211, tea_CELoss: 1.7856 stu_CELoss: 1.9623 DMLLoss: 0.3731 
2022-03-18 20:11:13 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 4.0494, tea_CELoss: 1.7751 stu_CELoss: 1.9374 DMLLoss: 0.3369 
2022-03-18 20:12:10 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 4.2058, tea_CELoss: 1.8631 stu_CELoss: 1.9779 DMLLoss: 0.3648 
2022-03-18 20:13:07 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 3.8153, tea_CELoss: 1.7004 stu_CELoss: 1.7708 DMLLoss: 0.3441 
2022-03-18 20:14:04 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 4.3391, tea_CELoss: 1.9149 stu_CELoss: 2.0764 DMLLoss: 0.3478 
2022-03-18 20:15:01 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 4.1731, tea_CELoss: 1.8884 stu_CELoss: 1.9479 DMLLoss: 0.3367 
2022-03-18 20:15:58 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 4.3404, tea_CELoss: 1.9248 stu_CELoss: 2.0623 DMLLoss: 0.3532 
2022-03-18 20:16:55 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 4.3856, tea_CELoss: 1.9032 stu_CELoss: 2.1274 DMLLoss: 0.3550 
2022-03-18 20:17:51 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 4.1813, tea_CELoss: 1.8836 stu_CELoss: 1.9498 DMLLoss: 0.3479 
2022-03-18 20:18:48 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 4.5617, tea_CELoss: 2.0253 stu_CELoss: 2.1919 DMLLoss: 0.3445 
2022-03-18 20:19:45 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 4.5969, tea_CELoss: 2.0660 stu_CELoss: 2.1627 DMLLoss: 0.3682 
2022-03-18 20:20:42 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 4.7119, tea_CELoss: 2.1171 stu_CELoss: 2.1762 DMLLoss: 0.4186 
2022-03-18 20:21:39 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 4.4492, tea_CELoss: 1.9436 stu_CELoss: 2.1020 DMLLoss: 0.4036 
2022-03-18 20:22:36 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 4.4692, tea_CELoss: 1.9905 stu_CELoss: 2.1369 DMLLoss: 0.3418 
2022-03-18 20:23:33 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 4.4726, tea_CELoss: 1.9660 stu_CELoss: 2.1376 DMLLoss: 0.3690 
2022-03-18 20:24:30 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 4.0942, tea_CELoss: 1.7831 stu_CELoss: 1.9518 DMLLoss: 0.3593 
2022-03-18 20:25:27 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 4.2310, tea_CELoss: 1.8746 stu_CELoss: 1.9959 DMLLoss: 0.3605 
2022-03-18 20:26:24 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 4.4697, tea_CELoss: 1.9597 stu_CELoss: 2.1290 DMLLoss: 0.3809 
2022-03-18 20:27:21 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 4.4467, tea_CELoss: 1.9762 stu_CELoss: 2.0905 DMLLoss: 0.3800 
2022-03-18 20:28:18 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 4.3573, tea_CELoss: 1.8747 stu_CELoss: 2.0995 DMLLoss: 0.3831 
2022-03-18 20:29:15 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 4.3423, tea_CELoss: 1.9313 stu_CELoss: 2.0687 DMLLoss: 0.3423 
2022-03-18 20:30:13 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 4.9911, tea_CELoss: 2.2679 stu_CELoss: 2.3266 DMLLoss: 0.3965 
2022-03-18 20:31:10 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 4.4429, tea_CELoss: 1.9895 stu_CELoss: 2.1195 DMLLoss: 0.3339 
2022-03-18 20:32:07 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 4.8235, tea_CELoss: 2.0655 stu_CELoss: 2.3384 DMLLoss: 0.4196 
2022-03-18 20:33:05 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 4.4689, tea_CELoss: 1.9686 stu_CELoss: 2.1127 DMLLoss: 0.3876 
2022-03-18 20:34:02 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 4.1197, tea_CELoss: 1.8039 stu_CELoss: 1.9730 DMLLoss: 0.3428 
2022-03-18 20:35:00 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 4.1502, tea_CELoss: 1.8133 stu_CELoss: 1.9774 DMLLoss: 0.3595 
2022-03-18 20:35:57 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 4.5133, tea_CELoss: 1.9437 stu_CELoss: 2.1470 DMLLoss: 0.4226 
2022-03-18 20:36:54 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 3.8407, tea_CELoss: 1.7417 stu_CELoss: 1.7775 DMLLoss: 0.3214 
2022-03-18 20:37:52 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 4.2220, tea_CELoss: 1.9011 stu_CELoss: 1.9729 DMLLoss: 0.3481 
2022-03-18 20:38:49 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 4.1486, tea_CELoss: 1.8754 stu_CELoss: 1.9385 DMLLoss: 0.3347 
2022-03-18 20:39:46 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 4.2853, tea_CELoss: 1.9237 stu_CELoss: 1.9974 DMLLoss: 0.3642 
2022-03-18 20:40:44 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 5.0473, tea_CELoss: 2.2589 stu_CELoss: 2.4109 DMLLoss: 0.3775 
2022-03-18 20:40:47 - train: epoch 025, train_loss: 4.3550
2022-03-18 20:43:09 - eval: epoch: 025, tea_acc1: 58.916%, tea_acc5: 82.998%, tea_test_loss: 1.7059, stu_acc1: 56.976%, stu_acc5: 81.592%, stu_test_loss: 1.8070
2022-03-18 20:43:11 - until epoch: 025, tea_best_acc1: 60.512%, stu_best_acc1: 57.018%
2022-03-18 20:43:11 - epoch 026 lr: 0.1
2022-03-18 20:44:13 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 4.3908, tea_CELoss: 1.8522 stu_CELoss: 2.1472 DMLLoss: 0.3913 
2022-03-18 20:45:10 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 4.1802, tea_CELoss: 1.8240 stu_CELoss: 2.0100 DMLLoss: 0.3462 
2022-03-18 20:46:08 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 4.5826, tea_CELoss: 2.0130 stu_CELoss: 2.2037 DMLLoss: 0.3659 
2022-03-18 20:47:06 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 3.9467, tea_CELoss: 1.6783 stu_CELoss: 1.8607 DMLLoss: 0.4077 
2022-03-18 20:48:03 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 4.3939, tea_CELoss: 1.9467 stu_CELoss: 2.1016 DMLLoss: 0.3456 
2022-03-18 20:49:01 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 4.9286, tea_CELoss: 2.1386 stu_CELoss: 2.3851 DMLLoss: 0.4049 
2022-03-18 20:49:59 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 4.2947, tea_CELoss: 1.9170 stu_CELoss: 2.0513 DMLLoss: 0.3264 
2022-03-18 20:50:57 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 3.9205, tea_CELoss: 1.6888 stu_CELoss: 1.8686 DMLLoss: 0.3632 
2022-03-18 20:51:54 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 5.2001, tea_CELoss: 2.3256 stu_CELoss: 2.4540 DMLLoss: 0.4205 
2022-03-18 20:52:52 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 4.2633, tea_CELoss: 1.8264 stu_CELoss: 2.0357 DMLLoss: 0.4012 
2022-03-18 20:53:50 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 4.4445, tea_CELoss: 1.9225 stu_CELoss: 2.1069 DMLLoss: 0.4151 
2022-03-18 20:54:48 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 4.5299, tea_CELoss: 1.9822 stu_CELoss: 2.1748 DMLLoss: 0.3729 
2022-03-18 20:55:45 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 4.2576, tea_CELoss: 1.8775 stu_CELoss: 2.0075 DMLLoss: 0.3727 
2022-03-18 20:56:43 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 4.3324, tea_CELoss: 1.9005 stu_CELoss: 2.0631 DMLLoss: 0.3687 
2022-03-18 20:57:41 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 3.9835, tea_CELoss: 1.7678 stu_CELoss: 1.8159 DMLLoss: 0.3998 
2022-03-18 20:58:38 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 4.3582, tea_CELoss: 1.8698 stu_CELoss: 2.0973 DMLLoss: 0.3911 
2022-03-18 20:59:36 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 4.3938, tea_CELoss: 1.9312 stu_CELoss: 2.0666 DMLLoss: 0.3961 
2022-03-18 21:00:34 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 4.7962, tea_CELoss: 2.1315 stu_CELoss: 2.2830 DMLLoss: 0.3818 
2022-03-18 21:01:32 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 4.5794, tea_CELoss: 2.0705 stu_CELoss: 2.1498 DMLLoss: 0.3591 
2022-03-18 21:02:29 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 4.6099, tea_CELoss: 2.0000 stu_CELoss: 2.2264 DMLLoss: 0.3836 
2022-03-18 21:03:27 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 4.2597, tea_CELoss: 1.8467 stu_CELoss: 2.0331 DMLLoss: 0.3799 
2022-03-18 21:04:25 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 4.2924, tea_CELoss: 1.9082 stu_CELoss: 1.9630 DMLLoss: 0.4212 
2022-03-18 21:05:23 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 4.3080, tea_CELoss: 1.9107 stu_CELoss: 2.0432 DMLLoss: 0.3541 
2022-03-18 21:06:21 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 4.4211, tea_CELoss: 1.9284 stu_CELoss: 2.1113 DMLLoss: 0.3814 
2022-03-18 21:07:18 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 4.6364, tea_CELoss: 2.0987 stu_CELoss: 2.1753 DMLLoss: 0.3624 
2022-03-18 21:08:16 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 4.5416, tea_CELoss: 1.9809 stu_CELoss: 2.1715 DMLLoss: 0.3892 
2022-03-18 21:09:14 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 4.7727, tea_CELoss: 2.1210 stu_CELoss: 2.2799 DMLLoss: 0.3719 
2022-03-18 21:10:11 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 4.4185, tea_CELoss: 1.9056 stu_CELoss: 2.1016 DMLLoss: 0.4113 
2022-03-18 21:11:09 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 4.6549, tea_CELoss: 2.0438 stu_CELoss: 2.2332 DMLLoss: 0.3780 
2022-03-18 21:12:07 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 4.4658, tea_CELoss: 1.9173 stu_CELoss: 2.1678 DMLLoss: 0.3808 
2022-03-18 21:13:04 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 4.5412, tea_CELoss: 2.0540 stu_CELoss: 2.1414 DMLLoss: 0.3457 
2022-03-18 21:14:02 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 4.4149, tea_CELoss: 1.9868 stu_CELoss: 2.0748 DMLLoss: 0.3533 
2022-03-18 21:15:00 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 4.3139, tea_CELoss: 1.8984 stu_CELoss: 2.0887 DMLLoss: 0.3267 
2022-03-18 21:15:58 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 4.5357, tea_CELoss: 1.9893 stu_CELoss: 2.1979 DMLLoss: 0.3486 
2022-03-18 21:16:55 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 4.5539, tea_CELoss: 2.0553 stu_CELoss: 2.1610 DMLLoss: 0.3376 
2022-03-18 21:17:53 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 3.9289, tea_CELoss: 1.7169 stu_CELoss: 1.8639 DMLLoss: 0.3481 
2022-03-18 21:18:51 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 4.2875, tea_CELoss: 1.9091 stu_CELoss: 2.0093 DMLLoss: 0.3692 
2022-03-18 21:19:48 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 4.6094, tea_CELoss: 2.0388 stu_CELoss: 2.2180 DMLLoss: 0.3525 
2022-03-18 21:20:46 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 4.5938, tea_CELoss: 2.0641 stu_CELoss: 2.1528 DMLLoss: 0.3769 
2022-03-18 21:21:43 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 4.3896, tea_CELoss: 1.9440 stu_CELoss: 2.0625 DMLLoss: 0.3832 
2022-03-18 21:22:41 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 4.3201, tea_CELoss: 1.9420 stu_CELoss: 2.0194 DMLLoss: 0.3587 
2022-03-18 21:23:39 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 4.5207, tea_CELoss: 2.0284 stu_CELoss: 2.1417 DMLLoss: 0.3506 
2022-03-18 21:24:36 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 4.5597, tea_CELoss: 1.9800 stu_CELoss: 2.1963 DMLLoss: 0.3834 
2022-03-18 21:25:34 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 4.5194, tea_CELoss: 2.0644 stu_CELoss: 2.1138 DMLLoss: 0.3411 
2022-03-18 21:26:32 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 5.0771, tea_CELoss: 2.2590 stu_CELoss: 2.4424 DMLLoss: 0.3757 
2022-03-18 21:27:29 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 4.4257, tea_CELoss: 1.9478 stu_CELoss: 2.0747 DMLLoss: 0.4032 
2022-03-18 21:28:27 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 4.0366, tea_CELoss: 1.7924 stu_CELoss: 1.8631 DMLLoss: 0.3812 
2022-03-18 21:29:25 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 4.1426, tea_CELoss: 1.8534 stu_CELoss: 1.9063 DMLLoss: 0.3829 
2022-03-18 21:30:23 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 4.4639, tea_CELoss: 1.9226 stu_CELoss: 2.1566 DMLLoss: 0.3847 
2022-03-18 21:31:21 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 4.4956, tea_CELoss: 1.9550 stu_CELoss: 2.1604 DMLLoss: 0.3801 
2022-03-18 21:31:24 - train: epoch 026, train_loss: 4.3487
2022-03-18 21:33:47 - eval: epoch: 026, tea_acc1: 60.454%, tea_acc5: 84.288%, tea_test_loss: 1.6229, stu_acc1: 56.486%, stu_acc5: 81.492%, stu_test_loss: 1.8270
2022-03-18 21:33:50 - until epoch: 026, tea_best_acc1: 60.512%, stu_best_acc1: 57.018%
2022-03-18 21:33:50 - epoch 027 lr: 0.1
2022-03-18 21:34:51 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 4.3591, tea_CELoss: 1.9467 stu_CELoss: 2.0435 DMLLoss: 0.3689 
2022-03-18 21:35:49 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 4.2782, tea_CELoss: 1.8976 stu_CELoss: 2.0314 DMLLoss: 0.3492 
2022-03-18 21:36:46 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 4.5200, tea_CELoss: 2.0115 stu_CELoss: 2.1466 DMLLoss: 0.3619 
2022-03-18 21:37:43 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 4.7045, tea_CELoss: 2.0875 stu_CELoss: 2.2418 DMLLoss: 0.3753 
2022-03-18 21:38:40 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 4.2604, tea_CELoss: 1.8049 stu_CELoss: 2.0629 DMLLoss: 0.3926 
2022-03-18 21:39:38 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 4.4167, tea_CELoss: 1.9420 stu_CELoss: 2.0928 DMLLoss: 0.3819 
2022-03-18 21:40:35 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 4.1847, tea_CELoss: 1.8476 stu_CELoss: 2.0111 DMLLoss: 0.3260 
2022-03-18 21:41:32 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 4.3408, tea_CELoss: 1.9067 stu_CELoss: 2.0832 DMLLoss: 0.3510 
2022-03-18 21:42:29 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 4.2101, tea_CELoss: 1.8376 stu_CELoss: 2.0193 DMLLoss: 0.3531 
2022-03-18 21:43:27 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 4.1789, tea_CELoss: 1.8303 stu_CELoss: 1.9964 DMLLoss: 0.3522 
2022-03-18 21:44:24 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 4.1797, tea_CELoss: 1.8528 stu_CELoss: 1.9449 DMLLoss: 0.3820 
2022-03-18 21:45:21 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 4.6064, tea_CELoss: 1.9776 stu_CELoss: 2.2426 DMLLoss: 0.3861 
2022-03-18 21:46:18 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 4.2135, tea_CELoss: 1.8543 stu_CELoss: 1.9938 DMLLoss: 0.3653 
2022-03-18 21:47:16 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 4.6935, tea_CELoss: 2.0771 stu_CELoss: 2.2528 DMLLoss: 0.3637 
2022-03-18 21:48:13 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 4.6809, tea_CELoss: 2.1279 stu_CELoss: 2.1508 DMLLoss: 0.4023 
2022-03-18 21:49:10 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 4.3845, tea_CELoss: 1.9871 stu_CELoss: 2.0608 DMLLoss: 0.3366 
2022-03-18 21:50:07 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 4.2226, tea_CELoss: 1.8751 stu_CELoss: 2.0034 DMLLoss: 0.3441 
2022-03-18 21:51:04 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 4.1641, tea_CELoss: 1.7595 stu_CELoss: 1.9775 DMLLoss: 0.4271 
2022-03-18 21:52:01 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 4.5959, tea_CELoss: 1.9926 stu_CELoss: 2.2110 DMLLoss: 0.3923 
2022-03-18 21:52:59 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 4.2401, tea_CELoss: 1.8879 stu_CELoss: 2.0101 DMLLoss: 0.3421 
2022-03-18 21:53:56 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 4.5605, tea_CELoss: 2.0330 stu_CELoss: 2.1576 DMLLoss: 0.3698 
2022-03-18 21:54:53 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 4.8192, tea_CELoss: 2.1201 stu_CELoss: 2.2893 DMLLoss: 0.4097 
2022-03-18 21:55:50 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 4.7632, tea_CELoss: 2.1210 stu_CELoss: 2.2376 DMLLoss: 0.4046 
2022-03-18 21:56:48 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 4.2312, tea_CELoss: 1.9028 stu_CELoss: 1.9642 DMLLoss: 0.3641 
2022-03-18 21:57:45 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 4.2352, tea_CELoss: 1.8363 stu_CELoss: 1.9975 DMLLoss: 0.4013 
2022-03-18 21:58:42 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 4.4346, tea_CELoss: 1.9719 stu_CELoss: 2.1108 DMLLoss: 0.3519 
2022-03-18 21:59:40 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 4.2753, tea_CELoss: 1.8728 stu_CELoss: 2.0307 DMLLoss: 0.3718 
2022-03-18 22:00:37 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 4.4776, tea_CELoss: 1.9641 stu_CELoss: 2.1157 DMLLoss: 0.3978 
2022-03-18 22:01:35 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 4.4507, tea_CELoss: 1.9975 stu_CELoss: 2.1134 DMLLoss: 0.3399 
2022-03-18 22:02:32 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 4.3853, tea_CELoss: 1.9758 stu_CELoss: 2.0328 DMLLoss: 0.3768 
2022-03-18 22:03:29 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 3.8462, tea_CELoss: 1.6589 stu_CELoss: 1.8365 DMLLoss: 0.3508 
2022-03-18 22:04:27 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 3.9936, tea_CELoss: 1.7347 stu_CELoss: 1.8725 DMLLoss: 0.3864 
2022-03-18 22:05:24 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 4.4240, tea_CELoss: 1.9604 stu_CELoss: 2.0961 DMLLoss: 0.3676 
2022-03-18 22:06:22 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 4.0659, tea_CELoss: 1.7734 stu_CELoss: 1.9095 DMLLoss: 0.3830 
2022-03-18 22:07:20 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 4.7782, tea_CELoss: 2.0974 stu_CELoss: 2.2794 DMLLoss: 0.4014 
2022-03-18 22:08:17 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 4.4650, tea_CELoss: 1.9813 stu_CELoss: 2.1437 DMLLoss: 0.3401 
2022-03-18 22:09:15 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 4.4004, tea_CELoss: 1.9579 stu_CELoss: 2.0526 DMLLoss: 0.3899 
2022-03-18 22:10:12 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 3.8259, tea_CELoss: 1.7064 stu_CELoss: 1.7965 DMLLoss: 0.3230 
2022-03-18 22:11:10 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 4.3299, tea_CELoss: 1.8837 stu_CELoss: 2.0486 DMLLoss: 0.3976 
2022-03-18 22:12:08 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 4.6415, tea_CELoss: 2.0498 stu_CELoss: 2.1961 DMLLoss: 0.3956 
2022-03-18 22:13:05 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 4.4309, tea_CELoss: 1.9547 stu_CELoss: 2.0987 DMLLoss: 0.3774 
2022-03-18 22:14:03 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 4.3082, tea_CELoss: 1.8868 stu_CELoss: 2.0419 DMLLoss: 0.3795 
2022-03-18 22:15:00 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 4.1030, tea_CELoss: 1.7698 stu_CELoss: 1.9916 DMLLoss: 0.3416 
2022-03-18 22:15:58 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 4.5482, tea_CELoss: 1.9931 stu_CELoss: 2.1881 DMLLoss: 0.3669 
2022-03-18 22:16:56 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 4.3503, tea_CELoss: 1.9237 stu_CELoss: 2.0346 DMLLoss: 0.3919 
2022-03-18 22:17:53 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.9982, tea_CELoss: 1.7648 stu_CELoss: 1.8981 DMLLoss: 0.3352 
2022-03-18 22:18:50 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 4.8776, tea_CELoss: 2.1732 stu_CELoss: 2.2968 DMLLoss: 0.4077 
2022-03-18 22:19:48 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 4.6635, tea_CELoss: 2.0284 stu_CELoss: 2.2630 DMLLoss: 0.3721 
2022-03-18 22:20:46 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 4.1793, tea_CELoss: 1.8734 stu_CELoss: 1.9482 DMLLoss: 0.3576 
2022-03-18 22:21:43 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 4.1426, tea_CELoss: 1.8264 stu_CELoss: 1.9551 DMLLoss: 0.3611 
2022-03-18 22:21:46 - train: epoch 027, train_loss: 4.3384
2022-03-18 22:24:08 - eval: epoch: 027, tea_acc1: 58.830%, tea_acc5: 82.594%, tea_test_loss: 1.7394, stu_acc1: 57.252%, stu_acc5: 81.716%, stu_test_loss: 1.7953
2022-03-18 22:24:11 - until epoch: 027, tea_best_acc1: 60.512%, stu_best_acc1: 57.252%
2022-03-18 22:24:11 - epoch 028 lr: 0.1
2022-03-18 22:25:13 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 4.0411, tea_CELoss: 1.7306 stu_CELoss: 1.8933 DMLLoss: 0.4171 
2022-03-18 22:26:10 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 4.3183, tea_CELoss: 1.9340 stu_CELoss: 2.0169 DMLLoss: 0.3674 
2022-03-18 22:27:08 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 4.4032, tea_CELoss: 1.9453 stu_CELoss: 2.0941 DMLLoss: 0.3638 
2022-03-18 22:28:05 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 4.4053, tea_CELoss: 1.9501 stu_CELoss: 2.0486 DMLLoss: 0.4065 
2022-03-18 22:29:02 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 4.0885, tea_CELoss: 1.7750 stu_CELoss: 1.9226 DMLLoss: 0.3908 
2022-03-18 22:30:00 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 4.6567, tea_CELoss: 2.0579 stu_CELoss: 2.2348 DMLLoss: 0.3639 
2022-03-18 22:30:57 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 4.7178, tea_CELoss: 2.1114 stu_CELoss: 2.2391 DMLLoss: 0.3673 
2022-03-18 22:31:55 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 3.9596, tea_CELoss: 1.7181 stu_CELoss: 1.8966 DMLLoss: 0.3449 
2022-03-18 22:32:52 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 4.0489, tea_CELoss: 1.7938 stu_CELoss: 1.9135 DMLLoss: 0.3416 
2022-03-18 22:33:49 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 4.2431, tea_CELoss: 1.8691 stu_CELoss: 1.9587 DMLLoss: 0.4154 
2022-03-18 22:34:47 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.8378, tea_CELoss: 1.6568 stu_CELoss: 1.8141 DMLLoss: 0.3668 
2022-03-18 22:35:44 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 4.2425, tea_CELoss: 1.8633 stu_CELoss: 2.0182 DMLLoss: 0.3610 
2022-03-18 22:36:42 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 4.3882, tea_CELoss: 1.9620 stu_CELoss: 2.0735 DMLLoss: 0.3527 
2022-03-18 22:37:39 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 4.6873, tea_CELoss: 2.0930 stu_CELoss: 2.2082 DMLLoss: 0.3861 
2022-03-18 22:38:37 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 4.3985, tea_CELoss: 1.9411 stu_CELoss: 2.1004 DMLLoss: 0.3570 
2022-03-18 22:39:35 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 4.2936, tea_CELoss: 1.9111 stu_CELoss: 2.0045 DMLLoss: 0.3780 
2022-03-18 22:40:32 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 4.5410, tea_CELoss: 2.0259 stu_CELoss: 2.1415 DMLLoss: 0.3736 
2022-03-18 22:41:30 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 4.1846, tea_CELoss: 1.8639 stu_CELoss: 1.9454 DMLLoss: 0.3753 
2022-03-18 22:42:27 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 4.2878, tea_CELoss: 1.9541 stu_CELoss: 1.9892 DMLLoss: 0.3444 
2022-03-18 22:43:24 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 4.3864, tea_CELoss: 1.9386 stu_CELoss: 2.0819 DMLLoss: 0.3660 
2022-03-18 22:44:22 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 4.9246, tea_CELoss: 2.1552 stu_CELoss: 2.3294 DMLLoss: 0.4401 
2022-03-18 22:45:20 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 4.4600, tea_CELoss: 1.9832 stu_CELoss: 2.1046 DMLLoss: 0.3723 
2022-03-18 22:46:17 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 4.3030, tea_CELoss: 1.9118 stu_CELoss: 2.0204 DMLLoss: 0.3708 
2022-03-18 22:47:14 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 4.5088, tea_CELoss: 1.9546 stu_CELoss: 2.1610 DMLLoss: 0.3933 
2022-03-18 22:48:12 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 4.5669, tea_CELoss: 1.9879 stu_CELoss: 2.1873 DMLLoss: 0.3917 
2022-03-18 22:49:09 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 4.2674, tea_CELoss: 1.9156 stu_CELoss: 1.9876 DMLLoss: 0.3642 
2022-03-18 22:50:07 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 4.1613, tea_CELoss: 1.8390 stu_CELoss: 1.9200 DMLLoss: 0.4022 
2022-03-18 22:51:04 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 4.5594, tea_CELoss: 2.0328 stu_CELoss: 2.1464 DMLLoss: 0.3802 
2022-03-18 22:52:02 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 4.7120, tea_CELoss: 2.1402 stu_CELoss: 2.1795 DMLLoss: 0.3923 
2022-03-18 22:52:59 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 4.2423, tea_CELoss: 1.8883 stu_CELoss: 1.9859 DMLLoss: 0.3681 
2022-03-18 22:53:56 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 4.3569, tea_CELoss: 1.9137 stu_CELoss: 2.0789 DMLLoss: 0.3643 
2022-03-18 22:54:54 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 3.9746, tea_CELoss: 1.7265 stu_CELoss: 1.8777 DMLLoss: 0.3703 
2022-03-18 22:55:51 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 4.5726, tea_CELoss: 2.0184 stu_CELoss: 2.1659 DMLLoss: 0.3883 
2022-03-18 22:56:49 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 4.7773, tea_CELoss: 2.1657 stu_CELoss: 2.2372 DMLLoss: 0.3744 
2022-03-18 22:57:46 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 4.3005, tea_CELoss: 1.9051 stu_CELoss: 2.0635 DMLLoss: 0.3319 
2022-03-18 22:58:44 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 4.1845, tea_CELoss: 1.8041 stu_CELoss: 2.0221 DMLLoss: 0.3583 
2022-03-18 22:59:41 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 4.3579, tea_CELoss: 1.9325 stu_CELoss: 2.0329 DMLLoss: 0.3925 
2022-03-18 23:00:39 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 3.7840, tea_CELoss: 1.6198 stu_CELoss: 1.8071 DMLLoss: 0.3570 
2022-03-18 23:01:36 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 4.6739, tea_CELoss: 2.0634 stu_CELoss: 2.2334 DMLLoss: 0.3772 
2022-03-18 23:02:34 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 4.4814, tea_CELoss: 1.9750 stu_CELoss: 2.1571 DMLLoss: 0.3493 
2022-03-18 23:03:31 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 4.4682, tea_CELoss: 2.0183 stu_CELoss: 2.0649 DMLLoss: 0.3850 
2022-03-18 23:04:29 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 4.5168, tea_CELoss: 1.9869 stu_CELoss: 2.1501 DMLLoss: 0.3799 
2022-03-18 23:05:27 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 4.2031, tea_CELoss: 1.8531 stu_CELoss: 1.9702 DMLLoss: 0.3797 
2022-03-18 23:06:25 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 4.1376, tea_CELoss: 1.8712 stu_CELoss: 1.9323 DMLLoss: 0.3341 
2022-03-18 23:07:23 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 4.5444, tea_CELoss: 2.0130 stu_CELoss: 2.1442 DMLLoss: 0.3872 
2022-03-18 23:08:20 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 4.3130, tea_CELoss: 1.9169 stu_CELoss: 2.0394 DMLLoss: 0.3567 
2022-03-18 23:09:18 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 4.7147, tea_CELoss: 2.1626 stu_CELoss: 2.1772 DMLLoss: 0.3748 
2022-03-18 23:10:15 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 4.0722, tea_CELoss: 1.7753 stu_CELoss: 1.9355 DMLLoss: 0.3614 
2022-03-18 23:11:13 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 4.5473, tea_CELoss: 1.9214 stu_CELoss: 2.2320 DMLLoss: 0.3939 
2022-03-18 23:12:10 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 4.1045, tea_CELoss: 1.7777 stu_CELoss: 1.9501 DMLLoss: 0.3766 
2022-03-18 23:12:13 - train: epoch 028, train_loss: 4.3336
2022-03-18 23:14:36 - eval: epoch: 028, tea_acc1: 59.808%, tea_acc5: 83.428%, tea_test_loss: 1.6772, stu_acc1: 57.172%, stu_acc5: 81.812%, stu_test_loss: 1.7943
2022-03-18 23:14:38 - until epoch: 028, tea_best_acc1: 60.512%, stu_best_acc1: 57.252%
2022-03-18 23:14:38 - epoch 029 lr: 0.1
2022-03-18 23:15:40 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 4.4032, tea_CELoss: 1.9485 stu_CELoss: 2.0825 DMLLoss: 0.3722 
2022-03-18 23:16:37 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 4.1788, tea_CELoss: 1.8782 stu_CELoss: 1.9206 DMLLoss: 0.3801 
2022-03-18 23:17:35 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 4.7216, tea_CELoss: 2.1045 stu_CELoss: 2.2537 DMLLoss: 0.3634 
2022-03-18 23:18:32 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 4.7216, tea_CELoss: 2.1315 stu_CELoss: 2.2029 DMLLoss: 0.3873 
2022-03-18 23:19:29 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 4.4482, tea_CELoss: 1.9803 stu_CELoss: 2.0905 DMLLoss: 0.3773 
2022-03-18 23:20:27 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 4.5676, tea_CELoss: 1.9850 stu_CELoss: 2.1422 DMLLoss: 0.4404 
2022-03-18 23:21:25 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 3.5516, tea_CELoss: 1.5374 stu_CELoss: 1.6361 DMLLoss: 0.3781 
2022-03-18 23:22:22 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 4.3842, tea_CELoss: 1.9093 stu_CELoss: 2.0773 DMLLoss: 0.3976 
2022-03-18 23:23:20 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 4.4779, tea_CELoss: 1.9276 stu_CELoss: 2.1843 DMLLoss: 0.3659 
2022-03-18 23:24:17 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 4.3814, tea_CELoss: 1.8895 stu_CELoss: 2.0794 DMLLoss: 0.4125 
2022-03-18 23:25:15 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 4.3742, tea_CELoss: 1.9520 stu_CELoss: 2.0483 DMLLoss: 0.3740 
2022-03-18 23:26:13 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 4.4966, tea_CELoss: 1.9613 stu_CELoss: 2.1323 DMLLoss: 0.4029 
2022-03-18 23:27:10 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 4.2320, tea_CELoss: 1.8873 stu_CELoss: 1.9797 DMLLoss: 0.3650 
2022-03-18 23:28:08 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 4.3961, tea_CELoss: 1.9783 stu_CELoss: 2.0424 DMLLoss: 0.3754 
2022-03-18 23:29:05 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 4.2116, tea_CELoss: 1.7779 stu_CELoss: 2.0708 DMLLoss: 0.3629 
2022-03-18 23:30:03 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 4.4219, tea_CELoss: 1.8911 stu_CELoss: 2.1770 DMLLoss: 0.3538 
2022-03-18 23:31:00 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 3.9440, tea_CELoss: 1.7632 stu_CELoss: 1.8351 DMLLoss: 0.3457 
2022-03-18 23:31:57 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 4.5620, tea_CELoss: 2.0541 stu_CELoss: 2.1545 DMLLoss: 0.3533 
2022-03-18 23:32:54 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 4.2183, tea_CELoss: 1.8296 stu_CELoss: 2.0317 DMLLoss: 0.3570 
2022-03-18 23:33:52 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 4.2190, tea_CELoss: 1.8225 stu_CELoss: 2.0142 DMLLoss: 0.3824 
2022-03-18 23:34:49 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 4.3843, tea_CELoss: 1.9347 stu_CELoss: 2.0966 DMLLoss: 0.3529 
2022-03-18 23:35:46 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 4.5030, tea_CELoss: 1.9629 stu_CELoss: 2.1584 DMLLoss: 0.3817 
2022-03-18 23:36:43 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 4.3246, tea_CELoss: 1.8874 stu_CELoss: 2.0638 DMLLoss: 0.3733 
2022-03-18 23:37:40 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 4.0031, tea_CELoss: 1.7958 stu_CELoss: 1.8719 DMLLoss: 0.3354 
2022-03-18 23:38:37 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 4.4713, tea_CELoss: 2.0110 stu_CELoss: 2.0899 DMLLoss: 0.3704 
2022-03-18 23:39:34 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 4.5156, tea_CELoss: 2.0026 stu_CELoss: 2.1229 DMLLoss: 0.3902 
2022-03-18 23:40:31 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 4.3170, tea_CELoss: 1.8885 stu_CELoss: 2.0644 DMLLoss: 0.3640 
2022-03-18 23:41:28 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 4.3195, tea_CELoss: 1.9162 stu_CELoss: 2.0169 DMLLoss: 0.3864 
2022-03-18 23:42:25 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 4.3295, tea_CELoss: 1.8934 stu_CELoss: 2.0661 DMLLoss: 0.3699 
2022-03-18 23:43:22 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 4.4352, tea_CELoss: 1.9966 stu_CELoss: 2.0755 DMLLoss: 0.3630 
2022-03-18 23:44:19 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 4.3540, tea_CELoss: 1.9443 stu_CELoss: 2.0742 DMLLoss: 0.3355 
2022-03-18 23:45:16 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 4.3961, tea_CELoss: 1.9461 stu_CELoss: 2.0705 DMLLoss: 0.3795 
2022-03-18 23:46:13 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 3.9640, tea_CELoss: 1.6922 stu_CELoss: 1.9067 DMLLoss: 0.3651 
2022-03-18 23:47:10 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 4.0620, tea_CELoss: 1.7443 stu_CELoss: 1.9438 DMLLoss: 0.3739 
2022-03-18 23:48:07 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 4.4276, tea_CELoss: 1.9727 stu_CELoss: 2.0627 DMLLoss: 0.3921 
2022-03-18 23:49:04 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 4.2157, tea_CELoss: 1.8414 stu_CELoss: 1.9861 DMLLoss: 0.3883 
2022-03-18 23:50:01 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 4.0377, tea_CELoss: 1.7768 stu_CELoss: 1.9127 DMLLoss: 0.3482 
2022-03-18 23:50:58 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 4.2123, tea_CELoss: 1.8574 stu_CELoss: 1.9645 DMLLoss: 0.3904 
2022-03-18 23:51:55 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 4.1697, tea_CELoss: 1.8324 stu_CELoss: 1.9791 DMLLoss: 0.3582 
2022-03-18 23:52:52 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 4.3368, tea_CELoss: 1.9279 stu_CELoss: 2.0359 DMLLoss: 0.3730 
2022-03-18 23:53:49 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 4.2925, tea_CELoss: 1.9664 stu_CELoss: 1.9722 DMLLoss: 0.3539 
2022-03-18 23:54:46 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 4.1778, tea_CELoss: 1.8405 stu_CELoss: 1.9786 DMLLoss: 0.3587 
2022-03-18 23:55:43 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 4.2804, tea_CELoss: 1.9395 stu_CELoss: 2.0130 DMLLoss: 0.3279 
2022-03-18 23:56:40 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 4.4330, tea_CELoss: 1.9894 stu_CELoss: 2.0843 DMLLoss: 0.3593 
2022-03-18 23:57:37 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 4.8078, tea_CELoss: 2.1451 stu_CELoss: 2.2780 DMLLoss: 0.3847 
2022-03-18 23:58:33 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 4.5056, tea_CELoss: 2.0058 stu_CELoss: 2.1529 DMLLoss: 0.3468 
2022-03-18 23:59:31 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 3.8286, tea_CELoss: 1.6265 stu_CELoss: 1.8427 DMLLoss: 0.3595 
