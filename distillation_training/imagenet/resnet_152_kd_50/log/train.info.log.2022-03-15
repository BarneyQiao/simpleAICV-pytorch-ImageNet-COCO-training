2022-03-15 21:25:56 - teacher: resnet152
2022-03-15 21:25:56 - student: resnet50
2022-03-15 21:25:56 - num_classes: 1000
2022-03-15 21:25:56 - input_image_size: 224
2022-03-15 21:25:56 - scale: 1.1428571428571428
2022-03-15 21:25:56 - teacher_pretrained_model_path: /root/code/simpleAICV-pytorch-ImageNet-COCO-training/pretrained_models/resnet/resnet152-acc78.372.pth
2022-03-15 21:25:56 - student_pretrained_model_path: 
2022-03-15 21:25:56 - freeze_teacher: True
2022-03-15 21:25:56 - loss_list: ['CELoss', 'KDLoss']
2022-03-15 21:25:56 - T: 1
2022-03-15 21:25:56 - train_criterion: {'CELoss': CELoss(
  (loss): CrossEntropyLoss()
), 'KDLoss': KDLoss()}
2022-03-15 21:25:56 - loss_name: KDLoss
2022-03-15 21:25:56 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-15 21:25:56 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7ff4bbcfbc10>
2022-03-15 21:25:56 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7ff4bbcfb940>
2022-03-15 21:25:56 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7ff4bbcfb910>
2022-03-15 21:25:56 - seed: 0
2022-03-15 21:25:56 - batch_size: 256
2022-03-15 21:25:56 - num_workers: 16
2022-03-15 21:25:56 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-15 21:25:56 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-15 21:25:56 - epochs: 100
2022-03-15 21:25:56 - print_interval: 100
2022-03-15 21:25:56 - distributed: True
2022-03-15 21:25:56 - sync_bn: False
2022-03-15 21:25:56 - apex: True
2022-03-15 21:25:56 - gpus_type: NVIDIA GeForce RTX 3090
2022-03-15 21:25:56 - gpus_num: 2
2022-03-15 21:25:56 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7ff4b16a7db0>
2022-03-15 21:26:01 - --------------------parameters--------------------
2022-03-15 21:26:01 - name: teacher.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.0.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.1.weight, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.1.bias, grad: False
2022-03-15 21:26:01 - name: teacher.fc.weight, grad: False
2022-03-15 21:26:01 - name: teacher.fc.bias, grad: False
2022-03-15 21:26:01 - name: student.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.0.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.1.weight, grad: True
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.1.bias, grad: True
2022-03-15 21:26:01 - name: student.fc.weight, grad: True
2022-03-15 21:26:01 - name: student.fc.bias, grad: True
2022-03-15 21:26:01 - --------------------buffers--------------------
2022-03-15 21:26:01 - name: teacher.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.6.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer2.7.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.23.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.24.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.25.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.26.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.27.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.28.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.29.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.30.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.31.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.32.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.33.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.34.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer3.35.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: teacher.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.1.running_var, grad: False
2022-03-15 21:26:01 - name: student.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-15 21:26:01 - epoch 001 lr: 0.1
2022-03-15 21:26:40 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 13.1415, stu_CELoss: 6.8988 KDLoss: 6.2428 
2022-03-15 21:27:13 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 13.2013, stu_CELoss: 6.8861 KDLoss: 6.3151 
2022-03-15 21:27:46 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 12.9578, stu_CELoss: 6.8866 KDLoss: 6.0712 
2022-03-15 21:28:19 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 12.9402, stu_CELoss: 6.7961 KDLoss: 6.1441 
2022-03-15 21:28:52 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 12.9000, stu_CELoss: 6.8174 KDLoss: 6.0827 
2022-03-15 21:29:25 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 12.6832, stu_CELoss: 6.6496 KDLoss: 6.0335 
2022-03-15 21:29:58 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 12.5065, stu_CELoss: 6.6250 KDLoss: 5.8815 
2022-03-15 21:30:31 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 12.5351, stu_CELoss: 6.6308 KDLoss: 5.9043 
2022-03-15 21:31:04 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 12.1113, stu_CELoss: 6.4474 KDLoss: 5.6639 
2022-03-15 21:31:37 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 12.3537, stu_CELoss: 6.5109 KDLoss: 5.8429 
2022-03-15 21:32:10 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 11.7892, stu_CELoss: 6.3377 KDLoss: 5.4515 
2022-03-15 21:32:42 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 11.8481, stu_CELoss: 6.2276 KDLoss: 5.6205 
2022-03-15 21:33:15 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 11.9127, stu_CELoss: 6.2873 KDLoss: 5.6254 
2022-03-15 21:33:47 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 11.5643, stu_CELoss: 6.1637 KDLoss: 5.4006 
2022-03-15 21:34:20 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 11.5206, stu_CELoss: 6.0641 KDLoss: 5.4566 
2022-03-15 21:34:52 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 11.3751, stu_CELoss: 6.0741 KDLoss: 5.3010 
2022-03-15 21:35:24 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 11.1549, stu_CELoss: 5.8939 KDLoss: 5.2610 
2022-03-15 21:35:57 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 11.0503, stu_CELoss: 5.9024 KDLoss: 5.1479 
2022-03-15 21:36:29 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 10.9418, stu_CELoss: 5.7835 KDLoss: 5.1583 
2022-03-15 21:37:01 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 10.6374, stu_CELoss: 5.6554 KDLoss: 4.9820 
2022-03-15 21:37:34 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 10.5746, stu_CELoss: 5.6316 KDLoss: 4.9429 
2022-03-15 21:38:06 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 10.4242, stu_CELoss: 5.5256 KDLoss: 4.8987 
2022-03-15 21:38:39 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 10.3431, stu_CELoss: 5.5290 KDLoss: 4.8140 
2022-03-15 21:39:12 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 10.1605, stu_CELoss: 5.4620 KDLoss: 4.6986 
2022-03-15 21:39:45 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 9.9471, stu_CELoss: 5.3455 KDLoss: 4.6017 
2022-03-15 21:40:17 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 10.2992, stu_CELoss: 5.5400 KDLoss: 4.7592 
2022-03-15 21:40:50 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 10.0338, stu_CELoss: 5.4481 KDLoss: 4.5857 
2022-03-15 21:41:22 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 9.7799, stu_CELoss: 5.2329 KDLoss: 4.5470 
2022-03-15 21:41:54 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 9.5029, stu_CELoss: 5.1338 KDLoss: 4.3691 
2022-03-15 21:42:26 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 9.9400, stu_CELoss: 5.3125 KDLoss: 4.6275 
2022-03-15 21:42:59 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 9.9087, stu_CELoss: 5.3045 KDLoss: 4.6042 
2022-03-15 21:43:31 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 9.6781, stu_CELoss: 5.1943 KDLoss: 4.4837 
2022-03-15 21:44:04 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 9.2280, stu_CELoss: 4.9848 KDLoss: 4.2432 
2022-03-15 21:44:36 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 9.2637, stu_CELoss: 4.9611 KDLoss: 4.3026 
2022-03-15 21:45:08 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 9.3430, stu_CELoss: 5.0210 KDLoss: 4.3220 
2022-03-15 21:45:41 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 9.1932, stu_CELoss: 4.9474 KDLoss: 4.2458 
2022-03-15 21:46:13 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 9.3119, stu_CELoss: 5.0567 KDLoss: 4.2551 
2022-03-15 21:46:46 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 8.8713, stu_CELoss: 4.7856 KDLoss: 4.0857 
2022-03-15 21:47:18 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 8.9527, stu_CELoss: 4.8818 KDLoss: 4.0708 
2022-03-15 21:47:50 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 8.8980, stu_CELoss: 4.8194 KDLoss: 4.0786 
2022-03-15 21:48:23 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 9.0058, stu_CELoss: 4.8742 KDLoss: 4.1316 
2022-03-15 21:48:55 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 8.6157, stu_CELoss: 4.6596 KDLoss: 3.9561 
2022-03-15 21:49:28 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 8.5703, stu_CELoss: 4.6169 KDLoss: 3.9534 
2022-03-15 21:50:00 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 8.2865, stu_CELoss: 4.4765 KDLoss: 3.8100 
2022-03-15 21:50:33 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 8.7603, stu_CELoss: 4.7470 KDLoss: 4.0133 
2022-03-15 21:51:05 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 8.7860, stu_CELoss: 4.7677 KDLoss: 4.0182 
2022-03-15 21:51:38 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 8.1096, stu_CELoss: 4.4536 KDLoss: 3.6560 
2022-03-15 21:52:10 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 8.5925, stu_CELoss: 4.7726 KDLoss: 3.8199 
2022-03-15 21:52:42 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 8.2550, stu_CELoss: 4.4868 KDLoss: 3.7681 
2022-03-15 21:53:15 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 8.0089, stu_CELoss: 4.3485 KDLoss: 3.6604 
2022-03-15 21:53:16 - train: epoch 001, train_loss: 10.3948
2022-03-15 21:55:46 - eval: epoch: 001, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 13.856%, stu_acc5: 31.986%, stu_test_loss: 4.6314
2022-03-15 21:55:47 - until epoch: 001, tea_best_acc1: 78.334%, stu_best_acc1: 13.856%
2022-03-15 21:55:47 - epoch 002 lr: 0.1
2022-03-15 21:56:26 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 7.7412, stu_CELoss: 4.2521 KDLoss: 3.4891 
2022-03-15 21:56:59 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 7.5562, stu_CELoss: 4.1458 KDLoss: 3.4104 
2022-03-15 21:57:31 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 8.0227, stu_CELoss: 4.4131 KDLoss: 3.6097 
2022-03-15 21:58:04 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 7.9689, stu_CELoss: 4.3770 KDLoss: 3.5918 
2022-03-15 21:58:37 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 7.5857, stu_CELoss: 4.1813 KDLoss: 3.4043 
2022-03-15 21:59:09 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 7.3311, stu_CELoss: 4.0223 KDLoss: 3.3087 
2022-03-15 21:59:42 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 8.0602, stu_CELoss: 4.3513 KDLoss: 3.7089 
2022-03-15 22:00:15 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 7.4074, stu_CELoss: 4.0848 KDLoss: 3.3226 
2022-03-15 22:00:48 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 7.2964, stu_CELoss: 3.9877 KDLoss: 3.3087 
2022-03-15 22:01:20 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 7.5554, stu_CELoss: 4.1674 KDLoss: 3.3880 
2022-03-15 22:01:52 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 7.7894, stu_CELoss: 4.3420 KDLoss: 3.4474 
2022-03-15 22:02:24 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 7.0941, stu_CELoss: 3.8880 KDLoss: 3.2060 
2022-03-15 22:02:56 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 7.4058, stu_CELoss: 4.0554 KDLoss: 3.3504 
2022-03-15 22:03:29 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 7.8894, stu_CELoss: 4.2991 KDLoss: 3.5903 
2022-03-15 22:04:01 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 7.3456, stu_CELoss: 4.0453 KDLoss: 3.3003 
2022-03-15 22:04:34 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 7.1140, stu_CELoss: 3.9592 KDLoss: 3.1548 
2022-03-15 22:05:06 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 6.9191, stu_CELoss: 3.8450 KDLoss: 3.0740 
2022-03-15 22:05:38 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 6.9392, stu_CELoss: 3.8669 KDLoss: 3.0724 
2022-03-15 22:06:11 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 6.6059, stu_CELoss: 3.6403 KDLoss: 2.9656 
2022-03-15 22:06:44 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 6.7188, stu_CELoss: 3.6438 KDLoss: 3.0750 
2022-03-15 22:07:16 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 7.1005, stu_CELoss: 3.9044 KDLoss: 3.1961 
2022-03-15 22:07:49 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 6.8917, stu_CELoss: 3.8150 KDLoss: 3.0766 
2022-03-15 22:08:22 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 6.9034, stu_CELoss: 3.7959 KDLoss: 3.1075 
2022-03-15 22:08:55 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 6.8135, stu_CELoss: 3.7195 KDLoss: 3.0940 
2022-03-15 22:09:27 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 6.5988, stu_CELoss: 3.6029 KDLoss: 2.9959 
2022-03-15 22:10:00 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 6.7371, stu_CELoss: 3.6793 KDLoss: 3.0578 
2022-03-15 22:10:33 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 7.0217, stu_CELoss: 3.8869 KDLoss: 3.1348 
2022-03-15 22:11:05 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 6.9755, stu_CELoss: 3.8524 KDLoss: 3.1231 
2022-03-15 22:11:38 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 6.6594, stu_CELoss: 3.6430 KDLoss: 3.0164 
2022-03-15 22:12:10 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 6.3027, stu_CELoss: 3.4901 KDLoss: 2.8126 
2022-03-15 22:12:43 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 6.8855, stu_CELoss: 3.7545 KDLoss: 3.1309 
2022-03-15 22:13:16 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 6.5306, stu_CELoss: 3.6853 KDLoss: 2.8453 
2022-03-15 22:13:49 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 6.5117, stu_CELoss: 3.7116 KDLoss: 2.8001 
2022-03-15 22:14:21 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 6.5075, stu_CELoss: 3.5966 KDLoss: 2.9109 
2022-03-15 22:14:54 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 6.2567, stu_CELoss: 3.4924 KDLoss: 2.7643 
2022-03-15 22:15:27 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 6.3967, stu_CELoss: 3.5769 KDLoss: 2.8198 
2022-03-15 22:15:59 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 6.7805, stu_CELoss: 3.8277 KDLoss: 2.9527 
2022-03-15 22:16:32 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 6.0023, stu_CELoss: 3.4202 KDLoss: 2.5821 
2022-03-15 22:17:05 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 6.4846, stu_CELoss: 3.5982 KDLoss: 2.8864 
2022-03-15 22:17:38 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 6.2523, stu_CELoss: 3.4443 KDLoss: 2.8080 
2022-03-15 22:18:11 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 6.3460, stu_CELoss: 3.4840 KDLoss: 2.8619 
2022-03-15 22:18:43 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 6.2347, stu_CELoss: 3.4671 KDLoss: 2.7676 
2022-03-15 22:19:16 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 6.0439, stu_CELoss: 3.4555 KDLoss: 2.5884 
2022-03-15 22:19:49 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 5.8841, stu_CELoss: 3.3131 KDLoss: 2.5711 
2022-03-15 22:20:22 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 5.7438, stu_CELoss: 3.1939 KDLoss: 2.5499 
2022-03-15 22:20:55 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 6.1088, stu_CELoss: 3.3666 KDLoss: 2.7422 
2022-03-15 22:21:27 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 5.8640, stu_CELoss: 3.2285 KDLoss: 2.6355 
2022-03-15 22:22:00 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 6.1593, stu_CELoss: 3.4997 KDLoss: 2.6596 
2022-03-15 22:22:33 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 5.9813, stu_CELoss: 3.3220 KDLoss: 2.6593 
2022-03-15 22:23:05 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 6.1235, stu_CELoss: 3.4104 KDLoss: 2.7131 
2022-03-15 22:23:07 - train: epoch 002, train_loss: 6.8577
2022-03-15 22:25:35 - eval: epoch: 002, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 29.328%, stu_acc5: 54.628%, stu_test_loss: 3.3764
2022-03-15 22:25:37 - until epoch: 002, tea_best_acc1: 78.334%, stu_best_acc1: 29.328%
2022-03-15 22:25:37 - epoch 003 lr: 0.1
2022-03-15 22:26:14 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 5.7891, stu_CELoss: 3.3060 KDLoss: 2.4832 
2022-03-15 22:26:47 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 6.0220, stu_CELoss: 3.3376 KDLoss: 2.6844 
2022-03-15 22:27:20 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 5.9840, stu_CELoss: 3.3023 KDLoss: 2.6817 
2022-03-15 22:27:53 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 5.9605, stu_CELoss: 3.3535 KDLoss: 2.6070 
2022-03-15 22:28:25 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 5.9192, stu_CELoss: 3.3295 KDLoss: 2.5898 
2022-03-15 22:28:58 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 5.4688, stu_CELoss: 3.0451 KDLoss: 2.4237 
2022-03-15 22:29:30 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 6.0193, stu_CELoss: 3.4642 KDLoss: 2.5551 
2022-03-15 22:30:03 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 6.1428, stu_CELoss: 3.4874 KDLoss: 2.6554 
2022-03-15 22:30:36 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 5.5959, stu_CELoss: 3.1575 KDLoss: 2.4384 
2022-03-15 22:31:09 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 5.7029, stu_CELoss: 3.2886 KDLoss: 2.4143 
2022-03-15 22:31:41 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 5.6049, stu_CELoss: 3.1389 KDLoss: 2.4660 
2022-03-15 22:32:14 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 5.7116, stu_CELoss: 3.1997 KDLoss: 2.5119 
2022-03-15 22:32:47 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 5.3447, stu_CELoss: 3.0327 KDLoss: 2.3120 
2022-03-15 22:33:20 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 5.5011, stu_CELoss: 3.0755 KDLoss: 2.4256 
2022-03-15 22:33:54 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 5.9322, stu_CELoss: 3.4036 KDLoss: 2.5285 
2022-03-15 22:34:27 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 5.5358, stu_CELoss: 3.0496 KDLoss: 2.4862 
2022-03-15 22:35:00 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 5.5189, stu_CELoss: 3.1289 KDLoss: 2.3900 
2022-03-15 22:35:33 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 5.2582, stu_CELoss: 3.0117 KDLoss: 2.2465 
2022-03-15 22:36:06 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 5.7594, stu_CELoss: 3.2022 KDLoss: 2.5572 
2022-03-15 22:36:39 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 5.8059, stu_CELoss: 3.3368 KDLoss: 2.4691 
2022-03-15 22:37:12 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 5.6553, stu_CELoss: 3.2350 KDLoss: 2.4204 
2022-03-15 22:37:45 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 6.1068, stu_CELoss: 3.5054 KDLoss: 2.6014 
2022-03-15 22:38:18 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 5.2167, stu_CELoss: 3.0520 KDLoss: 2.1647 
2022-03-15 22:38:52 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 5.3094, stu_CELoss: 2.9573 KDLoss: 2.3521 
2022-03-15 22:39:25 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 5.6124, stu_CELoss: 3.2066 KDLoss: 2.4058 
2022-03-15 22:39:58 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 5.5700, stu_CELoss: 3.1918 KDLoss: 2.3782 
2022-03-15 22:40:31 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 5.6168, stu_CELoss: 3.2891 KDLoss: 2.3277 
2022-03-15 22:41:05 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 5.4124, stu_CELoss: 3.0470 KDLoss: 2.3654 
2022-03-15 22:41:38 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 5.3309, stu_CELoss: 3.0649 KDLoss: 2.2659 
2022-03-15 22:42:11 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 5.2850, stu_CELoss: 2.9706 KDLoss: 2.3144 
2022-03-15 22:42:44 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 5.8345, stu_CELoss: 3.3481 KDLoss: 2.4865 
2022-03-15 22:43:18 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 5.5054, stu_CELoss: 3.1581 KDLoss: 2.3473 
2022-03-15 22:43:51 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 5.2652, stu_CELoss: 2.9916 KDLoss: 2.2736 
2022-03-15 22:44:24 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 5.7319, stu_CELoss: 3.2925 KDLoss: 2.4394 
2022-03-15 22:44:57 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 5.3273, stu_CELoss: 3.0025 KDLoss: 2.3249 
2022-03-15 22:45:30 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 5.0013, stu_CELoss: 2.8067 KDLoss: 2.1946 
2022-03-15 22:46:04 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 5.4868, stu_CELoss: 3.0759 KDLoss: 2.4109 
2022-03-15 22:46:37 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 5.6887, stu_CELoss: 3.2297 KDLoss: 2.4590 
2022-03-15 22:47:10 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 5.8536, stu_CELoss: 3.3822 KDLoss: 2.4714 
2022-03-15 22:47:43 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 4.8126, stu_CELoss: 2.6924 KDLoss: 2.1202 
2022-03-15 22:48:16 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 5.1908, stu_CELoss: 3.0445 KDLoss: 2.1463 
2022-03-15 22:48:49 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 5.1119, stu_CELoss: 2.9662 KDLoss: 2.1457 
2022-03-15 22:49:22 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 4.9492, stu_CELoss: 2.7441 KDLoss: 2.2051 
2022-03-15 22:49:55 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 5.0333, stu_CELoss: 2.9431 KDLoss: 2.0902 
2022-03-15 22:50:27 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 5.2272, stu_CELoss: 2.9842 KDLoss: 2.2429 
2022-03-15 22:51:00 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 4.8221, stu_CELoss: 2.7519 KDLoss: 2.0703 
2022-03-15 22:51:32 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 4.9850, stu_CELoss: 2.9014 KDLoss: 2.0836 
2022-03-15 22:52:05 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 5.6238, stu_CELoss: 3.2376 KDLoss: 2.3861 
2022-03-15 22:52:38 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 5.1572, stu_CELoss: 2.9728 KDLoss: 2.1843 
2022-03-15 22:53:10 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 5.3396, stu_CELoss: 3.0738 KDLoss: 2.2658 
2022-03-15 22:53:12 - train: epoch 003, train_loss: 5.4897
2022-03-15 22:55:42 - eval: epoch: 003, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 34.624%, stu_acc5: 59.668%, stu_test_loss: 3.1187
2022-03-15 22:55:44 - until epoch: 003, tea_best_acc1: 78.334%, stu_best_acc1: 34.624%
2022-03-15 22:55:44 - epoch 004 lr: 0.1
2022-03-15 22:56:21 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 5.3736, stu_CELoss: 3.1856 KDLoss: 2.1880 
2022-03-15 22:56:54 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 4.8110, stu_CELoss: 2.7291 KDLoss: 2.0819 
2022-03-15 22:57:26 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 5.0438, stu_CELoss: 2.8846 KDLoss: 2.1592 
2022-03-15 22:57:59 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 4.7435, stu_CELoss: 2.7747 KDLoss: 1.9688 
2022-03-15 22:58:32 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 4.8687, stu_CELoss: 2.7238 KDLoss: 2.1449 
2022-03-15 22:59:06 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 5.5106, stu_CELoss: 3.2103 KDLoss: 2.3004 
2022-03-15 22:59:38 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 5.0071, stu_CELoss: 2.9909 KDLoss: 2.0162 
2022-03-15 23:00:11 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 5.1150, stu_CELoss: 2.9385 KDLoss: 2.1765 
2022-03-15 23:00:45 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 4.6406, stu_CELoss: 2.5883 KDLoss: 2.0523 
2022-03-15 23:01:18 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 5.0283, stu_CELoss: 2.8719 KDLoss: 2.1564 
2022-03-15 23:01:51 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 5.2827, stu_CELoss: 3.0885 KDLoss: 2.1942 
2022-03-15 23:02:24 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 4.8340, stu_CELoss: 2.7245 KDLoss: 2.1095 
2022-03-15 23:02:57 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 4.8700, stu_CELoss: 2.7730 KDLoss: 2.0970 
2022-03-15 23:03:30 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 5.0123, stu_CELoss: 2.9470 KDLoss: 2.0654 
2022-03-15 23:04:03 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 4.9264, stu_CELoss: 2.7769 KDLoss: 2.1495 
2022-03-15 23:04:37 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 5.2047, stu_CELoss: 3.0112 KDLoss: 2.1935 
2022-03-15 23:05:09 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 4.7129, stu_CELoss: 2.6875 KDLoss: 2.0253 
2022-03-15 23:05:43 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 5.2104, stu_CELoss: 3.0273 KDLoss: 2.1831 
2022-03-15 23:06:16 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 5.0130, stu_CELoss: 2.8666 KDLoss: 2.1464 
2022-03-15 23:06:49 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 4.9465, stu_CELoss: 2.8990 KDLoss: 2.0474 
2022-03-15 23:07:22 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 4.8823, stu_CELoss: 2.7813 KDLoss: 2.1010 
2022-03-15 23:07:55 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 5.1501, stu_CELoss: 2.9854 KDLoss: 2.1647 
2022-03-15 23:08:28 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 4.3099, stu_CELoss: 2.5002 KDLoss: 1.8097 
2022-03-15 23:09:01 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 4.4059, stu_CELoss: 2.5386 KDLoss: 1.8673 
2022-03-15 23:09:34 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 4.3829, stu_CELoss: 2.5578 KDLoss: 1.8251 
2022-03-15 23:10:07 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 4.8098, stu_CELoss: 2.7892 KDLoss: 2.0206 
2022-03-15 23:10:39 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 4.5185, stu_CELoss: 2.6204 KDLoss: 1.8981 
2022-03-15 23:11:12 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 4.6183, stu_CELoss: 2.6676 KDLoss: 1.9508 
2022-03-15 23:11:45 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 4.9038, stu_CELoss: 2.7554 KDLoss: 2.1484 
2022-03-15 23:12:18 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 5.1338, stu_CELoss: 2.9301 KDLoss: 2.2037 
2022-03-15 23:12:51 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 4.8664, stu_CELoss: 2.8546 KDLoss: 2.0118 
2022-03-15 23:13:23 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 5.0298, stu_CELoss: 2.8930 KDLoss: 2.1368 
2022-03-15 23:13:56 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 4.8241, stu_CELoss: 2.8224 KDLoss: 2.0016 
2022-03-15 23:14:28 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 4.8661, stu_CELoss: 2.7716 KDLoss: 2.0944 
2022-03-15 23:15:01 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 4.7812, stu_CELoss: 2.8721 KDLoss: 1.9091 
2022-03-15 23:15:34 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 4.7537, stu_CELoss: 2.7804 KDLoss: 1.9733 
2022-03-15 23:16:07 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 4.8186, stu_CELoss: 2.7726 KDLoss: 2.0459 
2022-03-15 23:16:39 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 4.5607, stu_CELoss: 2.5863 KDLoss: 1.9745 
2022-03-15 23:17:12 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 4.5932, stu_CELoss: 2.6361 KDLoss: 1.9571 
2022-03-15 23:17:45 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 4.4955, stu_CELoss: 2.6290 KDLoss: 1.8665 
2022-03-15 23:18:18 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 4.6474, stu_CELoss: 2.6728 KDLoss: 1.9746 
2022-03-15 23:18:50 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 4.6388, stu_CELoss: 2.6863 KDLoss: 1.9524 
2022-03-15 23:19:23 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 4.6272, stu_CELoss: 2.6615 KDLoss: 1.9657 
2022-03-15 23:19:56 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 4.8541, stu_CELoss: 2.7322 KDLoss: 2.1218 
2022-03-15 23:20:28 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 4.1334, stu_CELoss: 2.3706 KDLoss: 1.7628 
2022-03-15 23:21:01 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 4.4023, stu_CELoss: 2.5321 KDLoss: 1.8702 
2022-03-15 23:21:34 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 4.5141, stu_CELoss: 2.5901 KDLoss: 1.9240 
2022-03-15 23:22:06 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 4.1382, stu_CELoss: 2.3730 KDLoss: 1.7651 
2022-03-15 23:22:39 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 4.8795, stu_CELoss: 2.8674 KDLoss: 2.0122 
2022-03-15 23:23:11 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 4.6829, stu_CELoss: 2.6615 KDLoss: 2.0214 
2022-03-15 23:23:13 - train: epoch 004, train_loss: 4.8691
2022-03-15 23:25:43 - eval: epoch: 004, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 42.944%, stu_acc5: 69.242%, stu_test_loss: 2.5790
2022-03-15 23:25:45 - until epoch: 004, tea_best_acc1: 78.334%, stu_best_acc1: 42.944%
2022-03-15 23:25:45 - epoch 005 lr: 0.1
2022-03-15 23:26:23 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 4.6933, stu_CELoss: 2.7232 KDLoss: 1.9701 
2022-03-15 23:26:55 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 4.6626, stu_CELoss: 2.6949 KDLoss: 1.9677 
2022-03-15 23:27:28 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 4.7956, stu_CELoss: 2.7945 KDLoss: 2.0011 
2022-03-15 23:28:01 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 4.3768, stu_CELoss: 2.5669 KDLoss: 1.8100 
2022-03-15 23:28:34 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 4.4320, stu_CELoss: 2.5327 KDLoss: 1.8993 
2022-03-15 23:29:06 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 4.7076, stu_CELoss: 2.7542 KDLoss: 1.9534 
2022-03-15 23:29:39 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 4.5423, stu_CELoss: 2.6838 KDLoss: 1.8585 
2022-03-15 23:30:12 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 5.1932, stu_CELoss: 3.0616 KDLoss: 2.1316 
2022-03-15 23:30:45 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 4.4743, stu_CELoss: 2.5928 KDLoss: 1.8816 
2022-03-15 23:31:18 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 4.7213, stu_CELoss: 2.7055 KDLoss: 2.0158 
2022-03-15 23:31:51 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 4.5317, stu_CELoss: 2.6042 KDLoss: 1.9275 
2022-03-15 23:32:24 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 4.7750, stu_CELoss: 2.7599 KDLoss: 2.0151 
2022-03-15 23:32:56 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 4.2117, stu_CELoss: 2.4882 KDLoss: 1.7235 
2022-03-15 23:33:29 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 4.4516, stu_CELoss: 2.6600 KDLoss: 1.7916 
2022-03-15 23:34:02 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 4.3943, stu_CELoss: 2.5435 KDLoss: 1.8507 
2022-03-15 23:34:34 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.9275, stu_CELoss: 2.2964 KDLoss: 1.6311 
2022-03-15 23:35:07 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 4.8189, stu_CELoss: 2.8253 KDLoss: 1.9936 
2022-03-15 23:35:40 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 4.2863, stu_CELoss: 2.4674 KDLoss: 1.8189 
2022-03-15 23:36:13 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 4.5043, stu_CELoss: 2.5886 KDLoss: 1.9157 
2022-03-15 23:36:46 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 4.5163, stu_CELoss: 2.6349 KDLoss: 1.8813 
2022-03-15 23:37:19 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 4.3791, stu_CELoss: 2.5674 KDLoss: 1.8117 
2022-03-15 23:37:52 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 4.6000, stu_CELoss: 2.5640 KDLoss: 2.0360 
2022-03-15 23:38:25 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.9539, stu_CELoss: 2.2688 KDLoss: 1.6850 
2022-03-15 23:38:58 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 4.3478, stu_CELoss: 2.5466 KDLoss: 1.8013 
2022-03-15 23:39:30 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 4.8263, stu_CELoss: 2.7901 KDLoss: 2.0361 
2022-03-15 23:40:03 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 4.5088, stu_CELoss: 2.5766 KDLoss: 1.9322 
2022-03-15 23:40:36 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 4.5800, stu_CELoss: 2.6682 KDLoss: 1.9118 
2022-03-15 23:41:09 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 4.2814, stu_CELoss: 2.4585 KDLoss: 1.8229 
2022-03-15 23:41:42 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 4.5148, stu_CELoss: 2.6691 KDLoss: 1.8457 
2022-03-15 23:42:15 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 4.5804, stu_CELoss: 2.6771 KDLoss: 1.9033 
2022-03-15 23:42:48 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 4.6780, stu_CELoss: 2.7054 KDLoss: 1.9726 
2022-03-15 23:43:21 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 4.5011, stu_CELoss: 2.6334 KDLoss: 1.8677 
2022-03-15 23:43:54 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 4.2147, stu_CELoss: 2.4620 KDLoss: 1.7527 
2022-03-15 23:44:27 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 4.4241, stu_CELoss: 2.6361 KDLoss: 1.7880 
2022-03-15 23:45:00 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 4.5948, stu_CELoss: 2.7271 KDLoss: 1.8677 
2022-03-15 23:45:33 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 4.4318, stu_CELoss: 2.6299 KDLoss: 1.8018 
2022-03-15 23:46:05 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 4.2744, stu_CELoss: 2.4733 KDLoss: 1.8011 
2022-03-15 23:46:38 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 4.5601, stu_CELoss: 2.6481 KDLoss: 1.9120 
2022-03-15 23:47:11 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 4.5664, stu_CELoss: 2.6614 KDLoss: 1.9050 
2022-03-15 23:47:44 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 4.3229, stu_CELoss: 2.5776 KDLoss: 1.7453 
2022-03-15 23:48:17 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 4.5587, stu_CELoss: 2.6406 KDLoss: 1.9181 
2022-03-15 23:48:50 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 4.4557, stu_CELoss: 2.6696 KDLoss: 1.7861 
2022-03-15 23:49:23 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 4.4516, stu_CELoss: 2.6298 KDLoss: 1.8218 
2022-03-15 23:49:56 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 4.5529, stu_CELoss: 2.7256 KDLoss: 1.8274 
2022-03-15 23:50:29 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 4.5337, stu_CELoss: 2.6264 KDLoss: 1.9073 
2022-03-15 23:51:02 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 4.2569, stu_CELoss: 2.4999 KDLoss: 1.7570 
2022-03-15 23:51:34 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 4.0284, stu_CELoss: 2.3615 KDLoss: 1.6669 
2022-03-15 23:52:07 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 4.0802, stu_CELoss: 2.4568 KDLoss: 1.6234 
2022-03-15 23:52:40 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 4.8564, stu_CELoss: 2.8536 KDLoss: 2.0028 
2022-03-15 23:53:12 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 4.2711, stu_CELoss: 2.5528 KDLoss: 1.7182 
2022-03-15 23:53:14 - train: epoch 005, train_loss: 4.4961
2022-03-15 23:55:42 - eval: epoch: 005, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 39.922%, stu_acc5: 65.616%, stu_test_loss: 2.7611
2022-03-15 23:55:44 - until epoch: 005, tea_best_acc1: 78.334%, stu_best_acc1: 42.944%
2022-03-15 23:55:44 - epoch 006 lr: 0.1
2022-03-15 23:56:22 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 4.1021, stu_CELoss: 2.4265 KDLoss: 1.6756 
2022-03-15 23:56:54 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 4.3194, stu_CELoss: 2.5683 KDLoss: 1.7511 
2022-03-15 23:57:26 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 4.3183, stu_CELoss: 2.4540 KDLoss: 1.8643 
2022-03-15 23:57:59 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 4.1470, stu_CELoss: 2.4616 KDLoss: 1.6854 
2022-03-15 23:58:32 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 4.0990, stu_CELoss: 2.3856 KDLoss: 1.7134 
2022-03-15 23:59:04 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 4.0624, stu_CELoss: 2.3411 KDLoss: 1.7213 
2022-03-15 23:59:37 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 4.8822, stu_CELoss: 2.8592 KDLoss: 2.0230 
2022-03-16 00:00:10 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 4.0694, stu_CELoss: 2.3669 KDLoss: 1.7025 
2022-03-16 00:00:43 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.9243, stu_CELoss: 2.3122 KDLoss: 1.6121 
2022-03-16 00:01:15 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 4.3846, stu_CELoss: 2.5905 KDLoss: 1.7941 
2022-03-16 00:01:48 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 4.6596, stu_CELoss: 2.7045 KDLoss: 1.9551 
2022-03-16 00:02:21 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 4.4602, stu_CELoss: 2.6378 KDLoss: 1.8224 
2022-03-16 00:02:53 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 4.2780, stu_CELoss: 2.5187 KDLoss: 1.7592 
2022-03-16 00:03:26 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 4.2828, stu_CELoss: 2.5727 KDLoss: 1.7101 
2022-03-16 00:03:58 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 4.7522, stu_CELoss: 2.7506 KDLoss: 2.0016 
2022-03-16 00:04:31 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 4.0690, stu_CELoss: 2.3614 KDLoss: 1.7076 
2022-03-16 00:05:03 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 4.4375, stu_CELoss: 2.6905 KDLoss: 1.7470 
2022-03-16 00:05:36 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 4.3416, stu_CELoss: 2.5269 KDLoss: 1.8147 
2022-03-16 00:06:09 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 4.3281, stu_CELoss: 2.5338 KDLoss: 1.7943 
2022-03-16 00:06:41 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 4.5169, stu_CELoss: 2.6819 KDLoss: 1.8350 
2022-03-16 00:07:14 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 4.0515, stu_CELoss: 2.3993 KDLoss: 1.6522 
2022-03-16 00:07:47 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.9215, stu_CELoss: 2.2812 KDLoss: 1.6403 
2022-03-16 00:08:20 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 4.1084, stu_CELoss: 2.4810 KDLoss: 1.6274 
2022-03-16 00:08:52 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 4.3813, stu_CELoss: 2.5300 KDLoss: 1.8513 
2022-03-16 00:09:25 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 4.1660, stu_CELoss: 2.4118 KDLoss: 1.7542 
2022-03-16 00:09:57 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 4.0795, stu_CELoss: 2.4407 KDLoss: 1.6388 
2022-03-16 00:10:30 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 4.2678, stu_CELoss: 2.4624 KDLoss: 1.8054 
2022-03-16 00:11:02 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.9194, stu_CELoss: 2.2837 KDLoss: 1.6357 
2022-03-16 00:11:35 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 4.2202, stu_CELoss: 2.4865 KDLoss: 1.7337 
2022-03-16 00:12:07 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 4.2079, stu_CELoss: 2.4564 KDLoss: 1.7515 
2022-03-16 00:12:39 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.9401, stu_CELoss: 2.3148 KDLoss: 1.6253 
2022-03-16 00:13:11 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 4.0390, stu_CELoss: 2.3534 KDLoss: 1.6856 
2022-03-16 00:13:44 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 4.3628, stu_CELoss: 2.4947 KDLoss: 1.8681 
2022-03-16 00:14:16 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 4.2633, stu_CELoss: 2.4509 KDLoss: 1.8124 
2022-03-16 00:14:48 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 4.2258, stu_CELoss: 2.5098 KDLoss: 1.7160 
2022-03-16 00:15:20 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 4.1084, stu_CELoss: 2.4513 KDLoss: 1.6570 
2022-03-16 00:15:52 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.9732, stu_CELoss: 2.3479 KDLoss: 1.6253 
2022-03-16 00:16:24 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.9943, stu_CELoss: 2.3680 KDLoss: 1.6263 
2022-03-16 00:16:56 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 4.0830, stu_CELoss: 2.4104 KDLoss: 1.6726 
2022-03-16 00:17:28 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.9949, stu_CELoss: 2.3638 KDLoss: 1.6311 
2022-03-16 00:18:00 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 4.0511, stu_CELoss: 2.4187 KDLoss: 1.6325 
2022-03-16 00:18:33 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.9798, stu_CELoss: 2.3178 KDLoss: 1.6619 
2022-03-16 00:19:05 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 4.5939, stu_CELoss: 2.6432 KDLoss: 1.9507 
2022-03-16 00:19:37 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 4.2828, stu_CELoss: 2.4919 KDLoss: 1.7909 
2022-03-16 00:20:09 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 4.2918, stu_CELoss: 2.5678 KDLoss: 1.7240 
2022-03-16 00:20:41 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 4.0966, stu_CELoss: 2.3875 KDLoss: 1.7091 
2022-03-16 00:21:15 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 4.0375, stu_CELoss: 2.4285 KDLoss: 1.6090 
2022-03-16 00:21:49 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 4.1953, stu_CELoss: 2.4617 KDLoss: 1.7336 
2022-03-16 00:22:22 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 4.1319, stu_CELoss: 2.5027 KDLoss: 1.6292 
2022-03-16 00:22:53 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 4.0737, stu_CELoss: 2.3450 KDLoss: 1.7287 
2022-03-16 00:22:55 - train: epoch 006, train_loss: 4.2409
2022-03-16 00:25:25 - eval: epoch: 006, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 49.362%, stu_acc5: 75.112%, stu_test_loss: 2.2084
2022-03-16 00:25:27 - until epoch: 006, tea_best_acc1: 78.334%, stu_best_acc1: 49.362%
2022-03-16 00:25:27 - epoch 007 lr: 0.1
2022-03-16 00:26:05 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 3.7482, stu_CELoss: 2.2018 KDLoss: 1.5463 
2022-03-16 00:26:37 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 4.2033, stu_CELoss: 2.4276 KDLoss: 1.7757 
2022-03-16 00:27:10 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 4.3035, stu_CELoss: 2.5426 KDLoss: 1.7609 
2022-03-16 00:27:43 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.9132, stu_CELoss: 2.3736 KDLoss: 1.5397 
2022-03-16 00:28:15 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 3.7780, stu_CELoss: 2.2158 KDLoss: 1.5622 
2022-03-16 00:28:48 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 4.4058, stu_CELoss: 2.6460 KDLoss: 1.7598 
2022-03-16 00:29:21 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 4.1646, stu_CELoss: 2.4378 KDLoss: 1.7268 
2022-03-16 00:29:53 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 4.0883, stu_CELoss: 2.4395 KDLoss: 1.6488 
2022-03-16 00:30:27 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 4.1704, stu_CELoss: 2.4732 KDLoss: 1.6972 
2022-03-16 00:31:00 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.8597, stu_CELoss: 2.3156 KDLoss: 1.5441 
2022-03-16 00:31:33 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 4.2280, stu_CELoss: 2.4954 KDLoss: 1.7325 
2022-03-16 00:32:05 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 3.9983, stu_CELoss: 2.3847 KDLoss: 1.6135 
2022-03-16 00:32:38 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 3.7872, stu_CELoss: 2.2049 KDLoss: 1.5823 
2022-03-16 00:33:11 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 4.4188, stu_CELoss: 2.5846 KDLoss: 1.8342 
2022-03-16 00:33:43 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 4.2519, stu_CELoss: 2.4929 KDLoss: 1.7589 
2022-03-16 00:34:16 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 4.1851, stu_CELoss: 2.4611 KDLoss: 1.7240 
2022-03-16 00:34:49 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.9246, stu_CELoss: 2.3026 KDLoss: 1.6220 
2022-03-16 00:35:22 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 4.1065, stu_CELoss: 2.5026 KDLoss: 1.6039 
2022-03-16 00:35:55 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 4.0390, stu_CELoss: 2.3668 KDLoss: 1.6722 
2022-03-16 00:36:33 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.5360, stu_CELoss: 2.0215 KDLoss: 1.5144 
2022-03-16 00:37:12 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 4.2457, stu_CELoss: 2.5622 KDLoss: 1.6835 
2022-03-16 00:37:50 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 3.7155, stu_CELoss: 2.1945 KDLoss: 1.5211 
2022-03-16 00:38:28 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 4.2977, stu_CELoss: 2.5617 KDLoss: 1.7360 
2022-03-16 00:39:07 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 4.5530, stu_CELoss: 2.6532 KDLoss: 1.8998 
2022-03-16 00:39:45 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 4.0494, stu_CELoss: 2.3431 KDLoss: 1.7063 
2022-03-16 00:40:23 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 3.9108, stu_CELoss: 2.2587 KDLoss: 1.6521 
2022-03-16 00:41:02 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 3.9096, stu_CELoss: 2.3095 KDLoss: 1.6001 
2022-03-16 00:41:40 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 3.8726, stu_CELoss: 2.3166 KDLoss: 1.5560 
2022-03-16 00:42:18 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 3.8430, stu_CELoss: 2.2530 KDLoss: 1.5901 
2022-03-16 00:42:56 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 4.2725, stu_CELoss: 2.5669 KDLoss: 1.7056 
2022-03-16 00:43:34 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 3.7092, stu_CELoss: 2.2309 KDLoss: 1.4783 
2022-03-16 00:44:13 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 3.7526, stu_CELoss: 2.1455 KDLoss: 1.6072 
2022-03-16 00:44:51 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 4.4605, stu_CELoss: 2.6494 KDLoss: 1.8110 
2022-03-16 00:45:29 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 4.3048, stu_CELoss: 2.5678 KDLoss: 1.7370 
2022-03-16 00:46:07 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 4.3586, stu_CELoss: 2.5684 KDLoss: 1.7902 
2022-03-16 00:46:45 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 3.8715, stu_CELoss: 2.3042 KDLoss: 1.5673 
2022-03-16 00:47:23 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.9252, stu_CELoss: 2.2683 KDLoss: 1.6569 
2022-03-16 00:48:01 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 4.5024, stu_CELoss: 2.6923 KDLoss: 1.8101 
2022-03-16 00:48:39 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 4.0348, stu_CELoss: 2.4015 KDLoss: 1.6332 
2022-03-16 00:49:17 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 4.1593, stu_CELoss: 2.5187 KDLoss: 1.6406 
2022-03-16 00:49:56 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 3.9639, stu_CELoss: 2.3193 KDLoss: 1.6446 
2022-03-16 00:50:33 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 3.7842, stu_CELoss: 2.2046 KDLoss: 1.5797 
2022-03-16 00:51:12 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 4.0649, stu_CELoss: 2.4401 KDLoss: 1.6248 
2022-03-16 00:51:50 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 3.8203, stu_CELoss: 2.2145 KDLoss: 1.6058 
2022-03-16 00:52:28 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 4.0847, stu_CELoss: 2.4045 KDLoss: 1.6802 
2022-03-16 00:53:06 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 4.2438, stu_CELoss: 2.4209 KDLoss: 1.8229 
2022-03-16 00:53:44 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.7484, stu_CELoss: 2.1957 KDLoss: 1.5527 
2022-03-16 00:54:23 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 4.3601, stu_CELoss: 2.6140 KDLoss: 1.7461 
2022-03-16 00:55:01 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 4.0186, stu_CELoss: 2.3927 KDLoss: 1.6259 
2022-03-16 00:55:36 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 3.9510, stu_CELoss: 2.3378 KDLoss: 1.6132 
2022-03-16 00:55:37 - train: epoch 007, train_loss: 4.0619
2022-03-16 00:58:27 - eval: epoch: 007, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 50.556%, stu_acc5: 76.638%, stu_test_loss: 2.1249
2022-03-16 00:58:29 - until epoch: 007, tea_best_acc1: 78.334%, stu_best_acc1: 50.556%
2022-03-16 00:58:29 - epoch 008 lr: 0.1
2022-03-16 00:59:12 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 3.8478, stu_CELoss: 2.2770 KDLoss: 1.5708 
2022-03-16 00:59:50 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 4.0759, stu_CELoss: 2.4312 KDLoss: 1.6447 
2022-03-16 01:00:28 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 3.9933, stu_CELoss: 2.3210 KDLoss: 1.6724 
2022-03-16 01:01:05 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 3.8071, stu_CELoss: 2.2224 KDLoss: 1.5847 
2022-03-16 01:01:43 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 3.5754, stu_CELoss: 2.0837 KDLoss: 1.4917 
2022-03-16 01:02:21 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 3.7736, stu_CELoss: 2.2443 KDLoss: 1.5293 
2022-03-16 01:02:58 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 4.0760, stu_CELoss: 2.3663 KDLoss: 1.7097 
2022-03-16 01:03:36 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 3.4924, stu_CELoss: 2.0249 KDLoss: 1.4675 
2022-03-16 01:04:14 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 3.8444, stu_CELoss: 2.2978 KDLoss: 1.5466 
2022-03-16 01:04:52 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 4.0917, stu_CELoss: 2.3977 KDLoss: 1.6940 
2022-03-16 01:05:30 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 3.8374, stu_CELoss: 2.2754 KDLoss: 1.5620 
2022-03-16 01:06:08 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 3.5676, stu_CELoss: 2.0198 KDLoss: 1.5478 
2022-03-16 01:06:46 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 4.2079, stu_CELoss: 2.4406 KDLoss: 1.7674 
2022-03-16 01:07:23 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 3.8213, stu_CELoss: 2.2718 KDLoss: 1.5495 
2022-03-16 01:08:01 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 4.0812, stu_CELoss: 2.4189 KDLoss: 1.6623 
2022-03-16 01:08:39 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 4.0061, stu_CELoss: 2.3888 KDLoss: 1.6173 
2022-03-16 01:09:17 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 4.0333, stu_CELoss: 2.4122 KDLoss: 1.6210 
2022-03-16 01:09:55 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.9806, stu_CELoss: 2.3929 KDLoss: 1.5876 
2022-03-16 01:10:33 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 3.6429, stu_CELoss: 2.1256 KDLoss: 1.5173 
2022-03-16 01:11:11 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 4.2100, stu_CELoss: 2.6024 KDLoss: 1.6076 
2022-03-16 01:11:49 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 4.0902, stu_CELoss: 2.3777 KDLoss: 1.7125 
2022-03-16 01:12:27 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 4.0975, stu_CELoss: 2.4738 KDLoss: 1.6237 
2022-03-16 01:13:05 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 3.8273, stu_CELoss: 2.3332 KDLoss: 1.4941 
2022-03-16 01:13:42 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 3.6857, stu_CELoss: 2.2238 KDLoss: 1.4619 
2022-03-16 01:14:20 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 3.8966, stu_CELoss: 2.2968 KDLoss: 1.5998 
2022-03-16 01:14:57 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 3.9748, stu_CELoss: 2.2914 KDLoss: 1.6834 
2022-03-16 01:15:36 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 4.3247, stu_CELoss: 2.5630 KDLoss: 1.7618 
2022-03-16 01:16:13 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.8618, stu_CELoss: 2.3416 KDLoss: 1.5203 
2022-03-16 01:16:51 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 4.0390, stu_CELoss: 2.4376 KDLoss: 1.6015 
2022-03-16 01:17:29 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 4.4266, stu_CELoss: 2.6524 KDLoss: 1.7742 
2022-03-16 01:18:07 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.9561, stu_CELoss: 2.3388 KDLoss: 1.6173 
2022-03-16 01:18:45 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 4.0528, stu_CELoss: 2.4483 KDLoss: 1.6045 
2022-03-16 01:19:23 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 4.0073, stu_CELoss: 2.3682 KDLoss: 1.6391 
2022-03-16 01:20:00 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 4.1050, stu_CELoss: 2.4837 KDLoss: 1.6213 
2022-03-16 01:20:38 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 4.0341, stu_CELoss: 2.3641 KDLoss: 1.6701 
2022-03-16 01:21:16 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 4.1609, stu_CELoss: 2.4763 KDLoss: 1.6845 
2022-03-16 01:21:54 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 3.5931, stu_CELoss: 2.1666 KDLoss: 1.4265 
2022-03-16 01:22:32 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 3.9125, stu_CELoss: 2.2881 KDLoss: 1.6244 
2022-03-16 01:23:09 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 4.1891, stu_CELoss: 2.4871 KDLoss: 1.7019 
2022-03-16 01:23:47 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 4.3399, stu_CELoss: 2.5798 KDLoss: 1.7601 
2022-03-16 01:24:25 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 3.8768, stu_CELoss: 2.2532 KDLoss: 1.6236 
2022-03-16 01:25:03 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 3.7006, stu_CELoss: 2.2031 KDLoss: 1.4975 
2022-03-16 01:25:41 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 3.4389, stu_CELoss: 2.0006 KDLoss: 1.4382 
2022-03-16 01:26:18 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 3.9188, stu_CELoss: 2.3501 KDLoss: 1.5687 
2022-03-16 01:26:56 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.8796, stu_CELoss: 2.2693 KDLoss: 1.6103 
2022-03-16 01:27:34 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.9155, stu_CELoss: 2.2764 KDLoss: 1.6391 
2022-03-16 01:28:12 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 3.8110, stu_CELoss: 2.2967 KDLoss: 1.5143 
2022-03-16 01:28:49 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 3.7722, stu_CELoss: 2.2508 KDLoss: 1.5214 
2022-03-16 01:29:27 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 3.6704, stu_CELoss: 2.2259 KDLoss: 1.4446 
2022-03-16 01:30:00 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 3.9109, stu_CELoss: 2.3662 KDLoss: 1.5447 
2022-03-16 01:30:02 - train: epoch 008, train_loss: 3.9383
2022-03-16 01:32:31 - eval: epoch: 008, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 48.538%, stu_acc5: 74.774%, stu_test_loss: 2.2397
2022-03-16 01:32:32 - until epoch: 008, tea_best_acc1: 78.334%, stu_best_acc1: 50.556%
2022-03-16 01:32:32 - epoch 009 lr: 0.1
2022-03-16 01:33:10 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 3.4228, stu_CELoss: 2.0166 KDLoss: 1.4062 
2022-03-16 01:33:42 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 3.6849, stu_CELoss: 2.2291 KDLoss: 1.4558 
2022-03-16 01:34:15 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 3.5205, stu_CELoss: 2.1439 KDLoss: 1.3766 
2022-03-16 01:34:48 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 4.1063, stu_CELoss: 2.4177 KDLoss: 1.6886 
2022-03-16 01:35:20 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.4660, stu_CELoss: 2.0691 KDLoss: 1.3969 
2022-03-16 01:35:53 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 3.6172, stu_CELoss: 2.1817 KDLoss: 1.4354 
2022-03-16 01:36:25 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 3.4936, stu_CELoss: 2.0458 KDLoss: 1.4478 
2022-03-16 01:36:58 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 3.6527, stu_CELoss: 2.1835 KDLoss: 1.4692 
2022-03-16 01:37:30 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 3.5795, stu_CELoss: 2.0734 KDLoss: 1.5061 
2022-03-16 01:38:03 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 3.9083, stu_CELoss: 2.3263 KDLoss: 1.5819 
2022-03-16 01:38:36 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 4.0195, stu_CELoss: 2.4523 KDLoss: 1.5672 
2022-03-16 01:39:09 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.8616, stu_CELoss: 2.2761 KDLoss: 1.5855 
2022-03-16 01:39:42 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.9763, stu_CELoss: 2.4399 KDLoss: 1.5364 
2022-03-16 01:40:14 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 3.5038, stu_CELoss: 2.0414 KDLoss: 1.4624 
2022-03-16 01:40:47 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 3.5808, stu_CELoss: 2.1448 KDLoss: 1.4360 
2022-03-16 01:41:20 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 3.6525, stu_CELoss: 2.1716 KDLoss: 1.4809 
2022-03-16 01:41:53 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 4.1260, stu_CELoss: 2.4520 KDLoss: 1.6739 
2022-03-16 01:42:25 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 3.9538, stu_CELoss: 2.3454 KDLoss: 1.6083 
2022-03-16 01:42:58 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 3.3496, stu_CELoss: 1.9716 KDLoss: 1.3780 
2022-03-16 01:43:31 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 3.8947, stu_CELoss: 2.2325 KDLoss: 1.6622 
2022-03-16 01:44:03 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 3.9454, stu_CELoss: 2.3560 KDLoss: 1.5894 
2022-03-16 01:44:36 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 4.0630, stu_CELoss: 2.4064 KDLoss: 1.6565 
2022-03-16 01:45:09 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 3.2751, stu_CELoss: 1.9738 KDLoss: 1.3013 
2022-03-16 01:45:41 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 3.8386, stu_CELoss: 2.2799 KDLoss: 1.5588 
2022-03-16 01:46:14 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 3.7158, stu_CELoss: 2.2240 KDLoss: 1.4917 
2022-03-16 01:46:47 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 3.8064, stu_CELoss: 2.2796 KDLoss: 1.5268 
2022-03-16 01:47:19 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 4.0933, stu_CELoss: 2.4418 KDLoss: 1.6515 
2022-03-16 01:47:52 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.9384, stu_CELoss: 2.3952 KDLoss: 1.5432 
2022-03-16 01:48:24 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 3.3906, stu_CELoss: 2.0130 KDLoss: 1.3776 
2022-03-16 01:48:57 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 3.5942, stu_CELoss: 2.1048 KDLoss: 1.4894 
2022-03-16 01:49:29 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 3.6028, stu_CELoss: 2.1625 KDLoss: 1.4403 
2022-03-16 01:50:02 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 3.8743, stu_CELoss: 2.3437 KDLoss: 1.5306 
2022-03-16 01:50:35 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 4.3197, stu_CELoss: 2.5709 KDLoss: 1.7489 
2022-03-16 01:51:07 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.8237, stu_CELoss: 2.2437 KDLoss: 1.5800 
2022-03-16 01:51:40 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.9929, stu_CELoss: 2.4117 KDLoss: 1.5811 
2022-03-16 01:52:12 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 3.4646, stu_CELoss: 2.1075 KDLoss: 1.3571 
2022-03-16 01:52:45 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 4.0342, stu_CELoss: 2.3730 KDLoss: 1.6612 
2022-03-16 01:53:17 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 4.0333, stu_CELoss: 2.4059 KDLoss: 1.6275 
2022-03-16 01:53:50 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 3.6586, stu_CELoss: 2.1952 KDLoss: 1.4634 
2022-03-16 01:54:23 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 4.2030, stu_CELoss: 2.5847 KDLoss: 1.6183 
2022-03-16 01:54:55 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 4.1058, stu_CELoss: 2.4182 KDLoss: 1.6876 
2022-03-16 01:55:28 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 3.6705, stu_CELoss: 2.2496 KDLoss: 1.4209 
2022-03-16 01:56:01 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 3.3782, stu_CELoss: 1.9876 KDLoss: 1.3906 
2022-03-16 01:56:34 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 3.7275, stu_CELoss: 2.2498 KDLoss: 1.4777 
2022-03-16 01:57:07 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 3.9964, stu_CELoss: 2.3457 KDLoss: 1.6507 
2022-03-16 01:57:39 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 4.1113, stu_CELoss: 2.4786 KDLoss: 1.6327 
2022-03-16 01:58:12 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 4.2236, stu_CELoss: 2.5485 KDLoss: 1.6751 
2022-03-16 01:58:45 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.8638, stu_CELoss: 2.3112 KDLoss: 1.5526 
2022-03-16 01:59:18 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 3.8638, stu_CELoss: 2.2898 KDLoss: 1.5740 
2022-03-16 01:59:50 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 3.5655, stu_CELoss: 2.0649 KDLoss: 1.5006 
2022-03-16 01:59:52 - train: epoch 009, train_loss: 3.8379
2022-03-16 02:02:21 - eval: epoch: 009, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 50.014%, stu_acc5: 75.532%, stu_test_loss: 2.1730
2022-03-16 02:02:23 - until epoch: 009, tea_best_acc1: 78.334%, stu_best_acc1: 50.556%
2022-03-16 02:02:23 - epoch 010 lr: 0.1
2022-03-16 02:03:01 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 3.4497, stu_CELoss: 2.0544 KDLoss: 1.3954 
2022-03-16 02:03:34 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.7267, stu_CELoss: 2.2149 KDLoss: 1.5118 
2022-03-16 02:04:07 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.7505, stu_CELoss: 2.2806 KDLoss: 1.4699 
2022-03-16 02:04:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 3.8378, stu_CELoss: 2.2574 KDLoss: 1.5804 
2022-03-16 02:05:13 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 3.7591, stu_CELoss: 2.2833 KDLoss: 1.4758 
2022-03-16 02:05:45 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 4.0172, stu_CELoss: 2.4611 KDLoss: 1.5560 
2022-03-16 02:06:18 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 3.8536, stu_CELoss: 2.3238 KDLoss: 1.5298 
2022-03-16 02:06:51 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 3.8173, stu_CELoss: 2.3438 KDLoss: 1.4735 
2022-03-16 02:07:23 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 3.3253, stu_CELoss: 1.9824 KDLoss: 1.3429 
2022-03-16 02:07:56 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 3.4698, stu_CELoss: 2.0358 KDLoss: 1.4340 
2022-03-16 02:08:29 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 3.6681, stu_CELoss: 2.1889 KDLoss: 1.4791 
2022-03-16 02:09:02 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 3.8804, stu_CELoss: 2.3495 KDLoss: 1.5309 
2022-03-16 02:09:35 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 3.4013, stu_CELoss: 2.1011 KDLoss: 1.3002 
2022-03-16 02:10:07 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 4.1483, stu_CELoss: 2.4556 KDLoss: 1.6928 
2022-03-16 02:10:40 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 3.3769, stu_CELoss: 1.9660 KDLoss: 1.4109 
2022-03-16 02:11:13 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 3.7451, stu_CELoss: 2.2315 KDLoss: 1.5137 
2022-03-16 02:11:46 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 4.1197, stu_CELoss: 2.4647 KDLoss: 1.6549 
2022-03-16 02:12:18 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 3.6658, stu_CELoss: 2.1836 KDLoss: 1.4822 
2022-03-16 02:12:51 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 3.5028, stu_CELoss: 2.1274 KDLoss: 1.3753 
2022-03-16 02:13:24 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 3.8961, stu_CELoss: 2.2834 KDLoss: 1.6126 
2022-03-16 02:13:57 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 3.5657, stu_CELoss: 2.1215 KDLoss: 1.4442 
2022-03-16 02:14:29 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.7242, stu_CELoss: 2.2251 KDLoss: 1.4991 
2022-03-16 02:15:02 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 3.8920, stu_CELoss: 2.3984 KDLoss: 1.4936 
2022-03-16 02:15:35 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 4.2541, stu_CELoss: 2.5578 KDLoss: 1.6963 
2022-03-16 02:16:08 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 3.7488, stu_CELoss: 2.3083 KDLoss: 1.4405 
2022-03-16 02:16:41 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 3.7707, stu_CELoss: 2.2609 KDLoss: 1.5098 
2022-03-16 02:17:14 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 3.5626, stu_CELoss: 2.1557 KDLoss: 1.4069 
2022-03-16 02:17:47 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 4.0674, stu_CELoss: 2.3597 KDLoss: 1.7077 
2022-03-16 02:18:20 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 3.9840, stu_CELoss: 2.3958 KDLoss: 1.5882 
2022-03-16 02:18:53 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 3.7725, stu_CELoss: 2.2312 KDLoss: 1.5412 
2022-03-16 02:19:26 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 4.0614, stu_CELoss: 2.4526 KDLoss: 1.6088 
2022-03-16 02:19:59 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 3.8476, stu_CELoss: 2.3438 KDLoss: 1.5038 
2022-03-16 02:20:32 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 3.9849, stu_CELoss: 2.3784 KDLoss: 1.6065 
2022-03-16 02:21:05 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 4.0751, stu_CELoss: 2.4510 KDLoss: 1.6241 
2022-03-16 02:21:37 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 3.9814, stu_CELoss: 2.3859 KDLoss: 1.5955 
2022-03-16 02:22:10 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 4.0241, stu_CELoss: 2.3950 KDLoss: 1.6291 
2022-03-16 02:22:43 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 4.0855, stu_CELoss: 2.4526 KDLoss: 1.6329 
2022-03-16 02:23:16 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 3.7127, stu_CELoss: 2.2158 KDLoss: 1.4970 
2022-03-16 02:23:49 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 3.6358, stu_CELoss: 2.1546 KDLoss: 1.4813 
2022-03-16 02:24:22 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 3.5794, stu_CELoss: 2.1235 KDLoss: 1.4560 
2022-03-16 02:24:55 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 3.3837, stu_CELoss: 2.0383 KDLoss: 1.3454 
2022-03-16 02:25:28 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 3.7780, stu_CELoss: 2.2405 KDLoss: 1.5375 
2022-03-16 02:26:01 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 3.6690, stu_CELoss: 2.1806 KDLoss: 1.4884 
2022-03-16 02:26:34 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 3.6453, stu_CELoss: 2.2019 KDLoss: 1.4433 
2022-03-16 02:27:07 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 3.8344, stu_CELoss: 2.3333 KDLoss: 1.5010 
2022-03-16 02:27:40 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 3.8352, stu_CELoss: 2.2158 KDLoss: 1.6195 
2022-03-16 02:28:13 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 3.7464, stu_CELoss: 2.2710 KDLoss: 1.4754 
2022-03-16 02:28:46 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 3.6742, stu_CELoss: 2.1634 KDLoss: 1.5108 
2022-03-16 02:29:19 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 3.7812, stu_CELoss: 2.2835 KDLoss: 1.4977 
2022-03-16 02:29:51 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 3.5810, stu_CELoss: 2.1060 KDLoss: 1.4750 
2022-03-16 02:29:53 - train: epoch 010, train_loss: 3.7616
2022-03-16 02:32:23 - eval: epoch: 010, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 51.460%, stu_acc5: 77.010%, stu_test_loss: 2.0981
2022-03-16 02:32:24 - until epoch: 010, tea_best_acc1: 78.334%, stu_best_acc1: 51.460%
2022-03-16 02:32:24 - epoch 011 lr: 0.1
2022-03-16 02:33:02 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 3.6519, stu_CELoss: 2.2257 KDLoss: 1.4262 
2022-03-16 02:33:35 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 3.9368, stu_CELoss: 2.4049 KDLoss: 1.5319 
2022-03-16 02:34:07 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 3.6764, stu_CELoss: 2.2936 KDLoss: 1.3828 
2022-03-16 02:34:40 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 4.2633, stu_CELoss: 2.5225 KDLoss: 1.7408 
2022-03-16 02:35:13 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 3.3416, stu_CELoss: 2.0379 KDLoss: 1.3038 
2022-03-16 02:35:45 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 3.8851, stu_CELoss: 2.3929 KDLoss: 1.4922 
2022-03-16 02:36:18 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 3.9744, stu_CELoss: 2.3225 KDLoss: 1.6519 
2022-03-16 02:36:51 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 4.0857, stu_CELoss: 2.4228 KDLoss: 1.6629 
2022-03-16 02:37:24 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 3.8620, stu_CELoss: 2.3371 KDLoss: 1.5249 
2022-03-16 02:37:57 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 3.4893, stu_CELoss: 2.1367 KDLoss: 1.3527 
2022-03-16 02:38:30 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 3.4830, stu_CELoss: 2.1142 KDLoss: 1.3689 
2022-03-16 02:39:02 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 4.0580, stu_CELoss: 2.4405 KDLoss: 1.6175 
2022-03-16 02:39:36 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 3.6525, stu_CELoss: 2.1636 KDLoss: 1.4890 
2022-03-16 02:40:09 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 4.0829, stu_CELoss: 2.4741 KDLoss: 1.6088 
2022-03-16 02:40:42 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 3.7580, stu_CELoss: 2.2838 KDLoss: 1.4742 
2022-03-16 02:41:15 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 3.8207, stu_CELoss: 2.3352 KDLoss: 1.4855 
2022-03-16 02:41:48 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 4.1131, stu_CELoss: 2.4614 KDLoss: 1.6517 
2022-03-16 02:42:21 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 3.8633, stu_CELoss: 2.3444 KDLoss: 1.5188 
2022-03-16 02:42:54 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 3.7031, stu_CELoss: 2.1692 KDLoss: 1.5338 
2022-03-16 02:43:27 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 3.6766, stu_CELoss: 2.1461 KDLoss: 1.5305 
2022-03-16 02:44:00 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 3.5099, stu_CELoss: 2.1483 KDLoss: 1.3616 
2022-03-16 02:44:33 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 3.6151, stu_CELoss: 2.1014 KDLoss: 1.5137 
2022-03-16 02:45:05 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 3.7916, stu_CELoss: 2.2478 KDLoss: 1.5438 
2022-03-16 02:45:38 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 3.4955, stu_CELoss: 2.0803 KDLoss: 1.4152 
2022-03-16 02:46:11 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.7956, stu_CELoss: 2.3291 KDLoss: 1.4665 
2022-03-16 02:46:44 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 3.5287, stu_CELoss: 2.0859 KDLoss: 1.4428 
2022-03-16 02:47:17 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 3.6686, stu_CELoss: 2.1893 KDLoss: 1.4794 
2022-03-16 02:47:50 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 3.3847, stu_CELoss: 2.0115 KDLoss: 1.3732 
2022-03-16 02:48:23 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 3.8585, stu_CELoss: 2.3241 KDLoss: 1.5343 
2022-03-16 02:48:56 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.8954, stu_CELoss: 2.3970 KDLoss: 1.4984 
2022-03-16 02:49:29 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 3.5494, stu_CELoss: 2.2052 KDLoss: 1.3442 
2022-03-16 02:50:02 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 3.7247, stu_CELoss: 2.2488 KDLoss: 1.4758 
2022-03-16 02:50:34 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 3.6581, stu_CELoss: 2.2088 KDLoss: 1.4493 
2022-03-16 02:51:07 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 3.4096, stu_CELoss: 2.1199 KDLoss: 1.2897 
2022-03-16 02:51:40 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 3.5653, stu_CELoss: 2.0850 KDLoss: 1.4802 
2022-03-16 02:52:13 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.3968, stu_CELoss: 2.0435 KDLoss: 1.3533 
2022-03-16 02:52:45 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 4.0055, stu_CELoss: 2.4708 KDLoss: 1.5348 
2022-03-16 02:53:18 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 3.6359, stu_CELoss: 2.2020 KDLoss: 1.4339 
2022-03-16 02:53:51 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 3.6062, stu_CELoss: 2.1233 KDLoss: 1.4829 
2022-03-16 02:54:24 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 3.4662, stu_CELoss: 2.0089 KDLoss: 1.4573 
2022-03-16 02:54:57 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 3.7251, stu_CELoss: 2.2149 KDLoss: 1.5102 
2022-03-16 02:55:30 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 3.4293, stu_CELoss: 2.0704 KDLoss: 1.3589 
2022-03-16 02:56:03 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 3.5206, stu_CELoss: 2.1049 KDLoss: 1.4157 
2022-03-16 02:56:35 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 4.0739, stu_CELoss: 2.5303 KDLoss: 1.5436 
2022-03-16 02:57:09 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 3.6308, stu_CELoss: 2.1741 KDLoss: 1.4567 
2022-03-16 02:57:41 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 3.7284, stu_CELoss: 2.2687 KDLoss: 1.4598 
2022-03-16 02:58:14 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 3.4344, stu_CELoss: 2.0628 KDLoss: 1.3717 
2022-03-16 02:58:47 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 3.3693, stu_CELoss: 1.9755 KDLoss: 1.3938 
2022-03-16 02:59:20 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 3.4480, stu_CELoss: 2.0344 KDLoss: 1.4136 
2022-03-16 02:59:52 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 3.5582, stu_CELoss: 2.1497 KDLoss: 1.4085 
2022-03-16 02:59:54 - train: epoch 011, train_loss: 3.7020
2022-03-16 03:02:23 - eval: epoch: 011, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 51.008%, stu_acc5: 76.832%, stu_test_loss: 2.1239
2022-03-16 03:02:25 - until epoch: 011, tea_best_acc1: 78.334%, stu_best_acc1: 51.460%
2022-03-16 03:02:25 - epoch 012 lr: 0.1
2022-03-16 03:03:02 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 3.2785, stu_CELoss: 2.0006 KDLoss: 1.2779 
2022-03-16 03:03:35 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 3.0914, stu_CELoss: 1.8139 KDLoss: 1.2775 
2022-03-16 03:04:07 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 3.4952, stu_CELoss: 2.0865 KDLoss: 1.4087 
2022-03-16 03:04:40 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 3.6808, stu_CELoss: 2.2305 KDLoss: 1.4504 
2022-03-16 03:05:13 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.7375, stu_CELoss: 2.2459 KDLoss: 1.4917 
2022-03-16 03:05:46 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.1786, stu_CELoss: 1.8365 KDLoss: 1.3421 
2022-03-16 03:06:19 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 3.2654, stu_CELoss: 1.9279 KDLoss: 1.3375 
2022-03-16 03:06:52 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 4.1133, stu_CELoss: 2.5672 KDLoss: 1.5460 
2022-03-16 03:07:25 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 3.6076, stu_CELoss: 2.2106 KDLoss: 1.3970 
2022-03-16 03:07:58 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 3.3584, stu_CELoss: 2.0543 KDLoss: 1.3041 
2022-03-16 03:08:31 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 4.1733, stu_CELoss: 2.5479 KDLoss: 1.6254 
2022-03-16 03:09:04 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 3.1533, stu_CELoss: 1.8787 KDLoss: 1.2745 
2022-03-16 03:09:37 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 3.2690, stu_CELoss: 1.9847 KDLoss: 1.2843 
2022-03-16 03:10:10 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 4.1482, stu_CELoss: 2.4755 KDLoss: 1.6727 
2022-03-16 03:10:43 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 3.4459, stu_CELoss: 2.0665 KDLoss: 1.3794 
2022-03-16 03:11:16 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 3.6389, stu_CELoss: 2.1669 KDLoss: 1.4720 
2022-03-16 03:11:49 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 3.3790, stu_CELoss: 2.0183 KDLoss: 1.3607 
2022-03-16 03:12:22 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 3.8411, stu_CELoss: 2.3432 KDLoss: 1.4979 
2022-03-16 03:12:55 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 3.9054, stu_CELoss: 2.4019 KDLoss: 1.5035 
2022-03-16 03:13:28 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.6390, stu_CELoss: 2.2101 KDLoss: 1.4289 
2022-03-16 03:14:01 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 3.8943, stu_CELoss: 2.3931 KDLoss: 1.5012 
2022-03-16 03:14:34 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 3.9844, stu_CELoss: 2.4639 KDLoss: 1.5205 
2022-03-16 03:15:07 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 3.6155, stu_CELoss: 2.1564 KDLoss: 1.4591 
2022-03-16 03:15:40 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 3.9194, stu_CELoss: 2.3025 KDLoss: 1.6169 
2022-03-16 03:16:12 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 3.2591, stu_CELoss: 1.9368 KDLoss: 1.3223 
2022-03-16 03:16:46 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 3.3429, stu_CELoss: 2.0684 KDLoss: 1.2745 
2022-03-16 03:17:19 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 3.6055, stu_CELoss: 2.2131 KDLoss: 1.3924 
2022-03-16 03:17:51 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 3.6640, stu_CELoss: 2.2819 KDLoss: 1.3822 
2022-03-16 03:18:25 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 3.7225, stu_CELoss: 2.3164 KDLoss: 1.4061 
2022-03-16 03:18:57 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 3.3766, stu_CELoss: 2.0477 KDLoss: 1.3289 
2022-03-16 03:19:30 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 3.7971, stu_CELoss: 2.2816 KDLoss: 1.5155 
2022-03-16 03:20:03 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 3.6431, stu_CELoss: 2.1502 KDLoss: 1.4929 
2022-03-16 03:20:36 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 3.7282, stu_CELoss: 2.2456 KDLoss: 1.4826 
2022-03-16 03:21:09 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 3.5756, stu_CELoss: 2.1774 KDLoss: 1.3982 
2022-03-16 03:21:43 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 3.8191, stu_CELoss: 2.2681 KDLoss: 1.5510 
2022-03-16 03:22:16 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 3.6105, stu_CELoss: 2.1147 KDLoss: 1.4958 
2022-03-16 03:22:49 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 3.8630, stu_CELoss: 2.3764 KDLoss: 1.4866 
2022-03-16 03:23:21 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 3.8384, stu_CELoss: 2.3505 KDLoss: 1.4878 
2022-03-16 03:23:54 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 3.3342, stu_CELoss: 1.9606 KDLoss: 1.3736 
2022-03-16 03:24:27 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 3.5825, stu_CELoss: 2.1943 KDLoss: 1.3883 
2022-03-16 03:25:00 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 3.7812, stu_CELoss: 2.2593 KDLoss: 1.5219 
2022-03-16 03:25:33 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 3.6022, stu_CELoss: 2.1527 KDLoss: 1.4495 
2022-03-16 03:26:06 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 3.9375, stu_CELoss: 2.4001 KDLoss: 1.5374 
2022-03-16 03:26:39 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 3.3160, stu_CELoss: 2.0646 KDLoss: 1.2514 
2022-03-16 03:27:12 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 3.5908, stu_CELoss: 2.1104 KDLoss: 1.4803 
2022-03-16 03:27:45 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.7172, stu_CELoss: 2.2431 KDLoss: 1.4741 
2022-03-16 03:28:18 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 3.3050, stu_CELoss: 1.9760 KDLoss: 1.3290 
2022-03-16 03:28:51 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 3.7895, stu_CELoss: 2.2393 KDLoss: 1.5501 
2022-03-16 03:29:23 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 3.2824, stu_CELoss: 2.0063 KDLoss: 1.2761 
2022-03-16 03:29:56 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 3.5865, stu_CELoss: 2.1685 KDLoss: 1.4181 
2022-03-16 03:29:57 - train: epoch 012, train_loss: 3.6467
2022-03-16 03:32:27 - eval: epoch: 012, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 52.328%, stu_acc5: 77.940%, stu_test_loss: 2.0466
2022-03-16 03:32:28 - until epoch: 012, tea_best_acc1: 78.334%, stu_best_acc1: 52.328%
2022-03-16 03:32:28 - epoch 013 lr: 0.1
2022-03-16 03:33:06 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 3.3025, stu_CELoss: 1.9454 KDLoss: 1.3571 
2022-03-16 03:33:39 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 3.5210, stu_CELoss: 2.1579 KDLoss: 1.3631 
2022-03-16 03:34:11 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 3.5573, stu_CELoss: 2.2250 KDLoss: 1.3323 
2022-03-16 03:34:44 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 3.4913, stu_CELoss: 2.0857 KDLoss: 1.4056 
2022-03-16 03:35:16 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 3.7644, stu_CELoss: 2.2428 KDLoss: 1.5215 
2022-03-16 03:35:48 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 3.5180, stu_CELoss: 2.1601 KDLoss: 1.3579 
2022-03-16 03:36:21 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 3.4177, stu_CELoss: 2.0112 KDLoss: 1.4065 
2022-03-16 03:36:54 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 3.8389, stu_CELoss: 2.3471 KDLoss: 1.4918 
2022-03-16 03:37:26 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 3.4862, stu_CELoss: 2.1866 KDLoss: 1.2996 
2022-03-16 03:37:59 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 3.2871, stu_CELoss: 2.0136 KDLoss: 1.2735 
2022-03-16 03:38:32 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 3.6829, stu_CELoss: 2.1905 KDLoss: 1.4924 
2022-03-16 03:39:05 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 3.9356, stu_CELoss: 2.3762 KDLoss: 1.5594 
2022-03-16 03:39:38 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 3.3913, stu_CELoss: 2.0374 KDLoss: 1.3538 
2022-03-16 03:40:11 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 3.4746, stu_CELoss: 2.0992 KDLoss: 1.3754 
2022-03-16 03:40:44 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.8015, stu_CELoss: 2.2958 KDLoss: 1.5057 
2022-03-16 03:41:16 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 3.6989, stu_CELoss: 2.2224 KDLoss: 1.4765 
2022-03-16 03:41:49 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 3.6663, stu_CELoss: 2.1986 KDLoss: 1.4677 
2022-03-16 03:42:22 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 3.6532, stu_CELoss: 2.1994 KDLoss: 1.4538 
2022-03-16 03:42:55 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 3.5317, stu_CELoss: 2.1868 KDLoss: 1.3450 
2022-03-16 03:43:28 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.7267, stu_CELoss: 2.2557 KDLoss: 1.4710 
2022-03-16 03:44:00 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.7455, stu_CELoss: 2.2651 KDLoss: 1.4804 
2022-03-16 03:44:33 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 3.4672, stu_CELoss: 2.0693 KDLoss: 1.3979 
2022-03-16 03:45:06 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 3.5397, stu_CELoss: 2.0774 KDLoss: 1.4623 
2022-03-16 03:45:38 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 3.8363, stu_CELoss: 2.2923 KDLoss: 1.5439 
2022-03-16 03:46:11 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 3.5364, stu_CELoss: 2.1706 KDLoss: 1.3658 
2022-03-16 03:46:43 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 3.7033, stu_CELoss: 2.2641 KDLoss: 1.4392 
2022-03-16 03:47:16 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 3.5241, stu_CELoss: 2.1572 KDLoss: 1.3668 
2022-03-16 03:47:49 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 3.7416, stu_CELoss: 2.2150 KDLoss: 1.5266 
2022-03-16 03:48:22 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 3.7108, stu_CELoss: 2.2348 KDLoss: 1.4760 
2022-03-16 03:48:54 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 3.4078, stu_CELoss: 2.0196 KDLoss: 1.3882 
2022-03-16 03:49:27 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 3.5639, stu_CELoss: 2.1195 KDLoss: 1.4444 
2022-03-16 03:50:00 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.7330, stu_CELoss: 2.2645 KDLoss: 1.4685 
2022-03-16 03:50:33 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 3.3358, stu_CELoss: 1.9767 KDLoss: 1.3592 
2022-03-16 03:51:05 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 3.6175, stu_CELoss: 2.1299 KDLoss: 1.4876 
2022-03-16 03:51:38 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 3.5430, stu_CELoss: 2.1324 KDLoss: 1.4106 
2022-03-16 03:52:10 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.8127, stu_CELoss: 2.2812 KDLoss: 1.5316 
2022-03-16 03:52:43 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 3.2187, stu_CELoss: 1.8806 KDLoss: 1.3381 
2022-03-16 03:53:16 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 3.8024, stu_CELoss: 2.3590 KDLoss: 1.4434 
2022-03-16 03:53:49 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 3.8317, stu_CELoss: 2.2882 KDLoss: 1.5435 
2022-03-16 03:54:21 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 3.5428, stu_CELoss: 2.1593 KDLoss: 1.3835 
2022-03-16 03:54:54 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 3.2502, stu_CELoss: 1.9344 KDLoss: 1.3158 
2022-03-16 03:55:27 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 3.9245, stu_CELoss: 2.2955 KDLoss: 1.6290 
2022-03-16 03:56:00 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.6858, stu_CELoss: 2.1724 KDLoss: 1.5134 
2022-03-16 03:56:33 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 3.4881, stu_CELoss: 2.0342 KDLoss: 1.4539 
2022-03-16 03:57:06 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 3.3738, stu_CELoss: 2.0424 KDLoss: 1.3314 
2022-03-16 03:57:39 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 3.8245, stu_CELoss: 2.1880 KDLoss: 1.6364 
2022-03-16 03:58:11 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 3.6238, stu_CELoss: 2.2304 KDLoss: 1.3933 
2022-03-16 03:58:44 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 3.6934, stu_CELoss: 2.2809 KDLoss: 1.4126 
2022-03-16 03:59:17 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 3.7983, stu_CELoss: 2.2619 KDLoss: 1.5364 
2022-03-16 03:59:49 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 3.4331, stu_CELoss: 2.0632 KDLoss: 1.3699 
2022-03-16 03:59:51 - train: epoch 013, train_loss: 3.6012
2022-03-16 04:02:21 - eval: epoch: 013, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 50.552%, stu_acc5: 75.766%, stu_test_loss: 2.1623
2022-03-16 04:02:22 - until epoch: 013, tea_best_acc1: 78.334%, stu_best_acc1: 52.328%
2022-03-16 04:02:22 - epoch 014 lr: 0.1
2022-03-16 04:03:00 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 3.5449, stu_CELoss: 2.1922 KDLoss: 1.3527 
2022-03-16 04:03:33 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 3.9251, stu_CELoss: 2.3777 KDLoss: 1.5474 
2022-03-16 04:04:05 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 3.2563, stu_CELoss: 2.0003 KDLoss: 1.2560 
2022-03-16 04:04:38 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 3.2610, stu_CELoss: 1.9969 KDLoss: 1.2640 
2022-03-16 04:05:10 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 3.2624, stu_CELoss: 2.0199 KDLoss: 1.2425 
2022-03-16 04:05:43 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 3.4509, stu_CELoss: 2.1763 KDLoss: 1.2746 
2022-03-16 04:06:16 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 3.3263, stu_CELoss: 2.0429 KDLoss: 1.2833 
2022-03-16 04:06:48 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 3.3791, stu_CELoss: 2.0568 KDLoss: 1.3223 
2022-03-16 04:07:21 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 3.4843, stu_CELoss: 2.1199 KDLoss: 1.3644 
2022-03-16 04:07:53 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 3.9235, stu_CELoss: 2.3544 KDLoss: 1.5691 
2022-03-16 04:08:26 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 3.6593, stu_CELoss: 2.1551 KDLoss: 1.5042 
2022-03-16 04:08:59 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 3.7981, stu_CELoss: 2.2996 KDLoss: 1.4985 
2022-03-16 04:09:32 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 3.7628, stu_CELoss: 2.2719 KDLoss: 1.4909 
2022-03-16 04:10:05 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 3.7519, stu_CELoss: 2.2933 KDLoss: 1.4586 
2022-03-16 04:10:38 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 3.7870, stu_CELoss: 2.2988 KDLoss: 1.4882 
2022-03-16 04:11:11 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 3.7651, stu_CELoss: 2.2692 KDLoss: 1.4958 
2022-03-16 04:11:44 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 3.6708, stu_CELoss: 2.2301 KDLoss: 1.4406 
2022-03-16 04:12:17 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.7929, stu_CELoss: 2.2662 KDLoss: 1.5267 
2022-03-16 04:12:50 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 3.5954, stu_CELoss: 2.2134 KDLoss: 1.3819 
2022-03-16 04:13:23 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 3.4059, stu_CELoss: 2.0460 KDLoss: 1.3599 
2022-03-16 04:13:56 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.4796, stu_CELoss: 2.1080 KDLoss: 1.3716 
2022-03-16 04:14:28 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 3.3888, stu_CELoss: 1.9834 KDLoss: 1.4054 
2022-03-16 04:15:01 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 3.3502, stu_CELoss: 1.9991 KDLoss: 1.3511 
2022-03-16 04:15:34 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 3.7831, stu_CELoss: 2.3196 KDLoss: 1.4635 
2022-03-16 04:16:07 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 3.4053, stu_CELoss: 2.0474 KDLoss: 1.3579 
2022-03-16 04:16:40 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 3.2797, stu_CELoss: 1.9383 KDLoss: 1.3414 
2022-03-16 04:17:13 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 3.5661, stu_CELoss: 2.1552 KDLoss: 1.4108 
2022-03-16 04:17:46 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 4.0806, stu_CELoss: 2.4425 KDLoss: 1.6380 
2022-03-16 04:18:19 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 3.6680, stu_CELoss: 2.2086 KDLoss: 1.4594 
2022-03-16 04:18:52 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 3.6893, stu_CELoss: 2.2018 KDLoss: 1.4876 
2022-03-16 04:19:25 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 3.3841, stu_CELoss: 2.0438 KDLoss: 1.3402 
2022-03-16 04:19:57 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 3.7374, stu_CELoss: 2.2354 KDLoss: 1.5020 
2022-03-16 04:20:30 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 3.2106, stu_CELoss: 1.9663 KDLoss: 1.2444 
2022-03-16 04:21:03 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 3.3019, stu_CELoss: 2.0696 KDLoss: 1.2322 
2022-03-16 04:21:36 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 3.4225, stu_CELoss: 2.1169 KDLoss: 1.3056 
2022-03-16 04:22:09 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 3.2378, stu_CELoss: 1.9622 KDLoss: 1.2756 
2022-03-16 04:22:41 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 3.7415, stu_CELoss: 2.2250 KDLoss: 1.5165 
2022-03-16 04:23:14 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 3.7126, stu_CELoss: 2.2743 KDLoss: 1.4383 
2022-03-16 04:23:47 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 3.6614, stu_CELoss: 2.2214 KDLoss: 1.4400 
2022-03-16 04:24:20 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 3.6849, stu_CELoss: 2.1778 KDLoss: 1.5071 
2022-03-16 04:24:52 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 3.8105, stu_CELoss: 2.2071 KDLoss: 1.6034 
2022-03-16 04:25:26 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 3.4791, stu_CELoss: 2.0405 KDLoss: 1.4386 
2022-03-16 04:25:58 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 3.7865, stu_CELoss: 2.2887 KDLoss: 1.4978 
2022-03-16 04:26:31 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 3.2471, stu_CELoss: 1.9465 KDLoss: 1.3005 
2022-03-16 04:27:04 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 3.5040, stu_CELoss: 2.1447 KDLoss: 1.3592 
2022-03-16 04:27:37 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 3.3599, stu_CELoss: 2.0053 KDLoss: 1.3546 
2022-03-16 04:28:10 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 3.4148, stu_CELoss: 2.0587 KDLoss: 1.3560 
2022-03-16 04:28:43 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 3.7872, stu_CELoss: 2.3028 KDLoss: 1.4844 
2022-03-16 04:29:15 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 3.6430, stu_CELoss: 2.2426 KDLoss: 1.4004 
2022-03-16 04:29:48 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 3.6686, stu_CELoss: 2.2489 KDLoss: 1.4197 
2022-03-16 04:29:49 - train: epoch 014, train_loss: 3.5653
2022-03-16 04:32:19 - eval: epoch: 014, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 52.254%, stu_acc5: 77.796%, stu_test_loss: 2.0643
2022-03-16 04:32:20 - until epoch: 014, tea_best_acc1: 78.334%, stu_best_acc1: 52.328%
2022-03-16 04:32:20 - epoch 015 lr: 0.1
2022-03-16 04:32:58 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 3.2901, stu_CELoss: 2.0122 KDLoss: 1.2779 
2022-03-16 04:33:31 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 3.7480, stu_CELoss: 2.3143 KDLoss: 1.4337 
2022-03-16 04:34:03 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 3.7410, stu_CELoss: 2.2970 KDLoss: 1.4440 
2022-03-16 04:34:36 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 3.4442, stu_CELoss: 2.0450 KDLoss: 1.3992 
2022-03-16 04:35:09 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 3.4228, stu_CELoss: 2.0369 KDLoss: 1.3860 
2022-03-16 04:35:42 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.6191, stu_CELoss: 2.2605 KDLoss: 1.3586 
2022-03-16 04:36:15 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 3.6141, stu_CELoss: 2.1743 KDLoss: 1.4398 
2022-03-16 04:36:48 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 3.3470, stu_CELoss: 2.0711 KDLoss: 1.2760 
2022-03-16 04:37:21 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 3.2392, stu_CELoss: 1.9887 KDLoss: 1.2505 
2022-03-16 04:37:54 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 3.5772, stu_CELoss: 2.1991 KDLoss: 1.3781 
2022-03-16 04:38:26 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 3.2898, stu_CELoss: 2.0525 KDLoss: 1.2373 
2022-03-16 04:38:59 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 3.4179, stu_CELoss: 2.1418 KDLoss: 1.2762 
2022-03-16 04:39:32 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 3.8419, stu_CELoss: 2.3662 KDLoss: 1.4757 
2022-03-16 04:40:05 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 3.4658, stu_CELoss: 2.1184 KDLoss: 1.3474 
2022-03-16 04:40:38 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 3.4274, stu_CELoss: 2.0658 KDLoss: 1.3616 
2022-03-16 04:41:11 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 3.6910, stu_CELoss: 2.2123 KDLoss: 1.4787 
2022-03-16 04:41:44 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 3.7966, stu_CELoss: 2.2653 KDLoss: 1.5313 
2022-03-16 04:42:17 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 3.5324, stu_CELoss: 2.1480 KDLoss: 1.3844 
2022-03-16 04:42:49 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 3.3626, stu_CELoss: 1.9775 KDLoss: 1.3851 
2022-03-16 04:43:22 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 3.0606, stu_CELoss: 1.8712 KDLoss: 1.1894 
2022-03-16 04:43:55 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 3.4774, stu_CELoss: 2.1265 KDLoss: 1.3509 
2022-03-16 04:44:28 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 3.7067, stu_CELoss: 2.2489 KDLoss: 1.4578 
2022-03-16 04:45:01 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 3.5997, stu_CELoss: 2.1384 KDLoss: 1.4612 
2022-03-16 04:45:34 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 3.4928, stu_CELoss: 2.1010 KDLoss: 1.3919 
2022-03-16 04:46:07 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.4733, stu_CELoss: 2.1428 KDLoss: 1.3305 
2022-03-16 04:46:40 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 3.3929, stu_CELoss: 1.9895 KDLoss: 1.4034 
2022-03-16 04:47:12 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.6277, stu_CELoss: 2.2591 KDLoss: 1.3686 
2022-03-16 04:47:46 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 3.4517, stu_CELoss: 2.0943 KDLoss: 1.3574 
2022-03-16 04:48:18 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 3.5690, stu_CELoss: 2.1544 KDLoss: 1.4147 
2022-03-16 04:48:51 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 3.2436, stu_CELoss: 1.9467 KDLoss: 1.2969 
2022-03-16 04:49:24 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 3.3255, stu_CELoss: 2.0573 KDLoss: 1.2682 
2022-03-16 04:49:57 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 3.5779, stu_CELoss: 2.1249 KDLoss: 1.4531 
2022-03-16 04:50:29 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 3.4267, stu_CELoss: 2.0672 KDLoss: 1.3595 
2022-03-16 04:51:02 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 3.2383, stu_CELoss: 1.9572 KDLoss: 1.2811 
2022-03-16 04:51:35 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 3.7205, stu_CELoss: 2.2723 KDLoss: 1.4482 
2022-03-16 04:52:08 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 3.8059, stu_CELoss: 2.2946 KDLoss: 1.5112 
2022-03-16 04:52:41 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.9042, stu_CELoss: 1.6962 KDLoss: 1.2080 
2022-03-16 04:53:14 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 3.7457, stu_CELoss: 2.2825 KDLoss: 1.4632 
2022-03-16 04:53:47 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 3.9757, stu_CELoss: 2.3794 KDLoss: 1.5963 
2022-03-16 04:54:19 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 3.5624, stu_CELoss: 2.1554 KDLoss: 1.4070 
2022-03-16 04:54:52 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 3.5311, stu_CELoss: 2.1043 KDLoss: 1.4268 
2022-03-16 04:55:25 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 3.3255, stu_CELoss: 2.0090 KDLoss: 1.3164 
2022-03-16 04:55:58 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 3.4367, stu_CELoss: 2.0074 KDLoss: 1.4293 
2022-03-16 04:56:31 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 3.5723, stu_CELoss: 2.1693 KDLoss: 1.4030 
2022-03-16 04:57:03 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 3.5211, stu_CELoss: 2.0990 KDLoss: 1.4220 
2022-03-16 04:57:36 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 3.5667, stu_CELoss: 2.1573 KDLoss: 1.4094 
2022-03-16 04:58:09 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 3.4605, stu_CELoss: 2.1038 KDLoss: 1.3567 
2022-03-16 04:58:42 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 3.5642, stu_CELoss: 2.1839 KDLoss: 1.3803 
2022-03-16 04:59:14 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 3.4965, stu_CELoss: 2.0869 KDLoss: 1.4097 
2022-03-16 04:59:47 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 3.6060, stu_CELoss: 2.2163 KDLoss: 1.3897 
2022-03-16 04:59:48 - train: epoch 015, train_loss: 3.5328
2022-03-16 05:02:18 - eval: epoch: 015, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 52.580%, stu_acc5: 78.064%, stu_test_loss: 2.0358
2022-03-16 05:02:19 - until epoch: 015, tea_best_acc1: 78.334%, stu_best_acc1: 52.580%
2022-03-16 05:02:19 - epoch 016 lr: 0.1
2022-03-16 05:02:57 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 3.2573, stu_CELoss: 2.0056 KDLoss: 1.2518 
2022-03-16 05:03:30 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 3.2462, stu_CELoss: 1.8668 KDLoss: 1.3794 
2022-03-16 05:04:03 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 3.5993, stu_CELoss: 2.1932 KDLoss: 1.4060 
2022-03-16 05:04:36 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.5004, stu_CELoss: 2.1121 KDLoss: 1.3883 
2022-03-16 05:05:09 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 3.5338, stu_CELoss: 2.1690 KDLoss: 1.3647 
2022-03-16 05:05:42 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 3.2884, stu_CELoss: 1.9852 KDLoss: 1.3032 
2022-03-16 05:06:15 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 3.2507, stu_CELoss: 1.9985 KDLoss: 1.2522 
2022-03-16 05:06:48 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 3.5814, stu_CELoss: 2.1504 KDLoss: 1.4310 
2022-03-16 05:07:21 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.6239, stu_CELoss: 2.2095 KDLoss: 1.4145 
2022-03-16 05:07:53 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 3.3482, stu_CELoss: 1.9929 KDLoss: 1.3553 
2022-03-16 05:08:26 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 3.4480, stu_CELoss: 2.0765 KDLoss: 1.3715 
2022-03-16 05:08:59 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 3.5400, stu_CELoss: 2.0826 KDLoss: 1.4574 
2022-03-16 05:09:32 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 3.5147, stu_CELoss: 2.1530 KDLoss: 1.3616 
2022-03-16 05:10:04 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 3.2160, stu_CELoss: 2.0216 KDLoss: 1.1944 
2022-03-16 05:10:37 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 3.8248, stu_CELoss: 2.3406 KDLoss: 1.4843 
2022-03-16 05:11:10 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 3.8125, stu_CELoss: 2.3770 KDLoss: 1.4355 
2022-03-16 05:11:43 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 3.5810, stu_CELoss: 2.1480 KDLoss: 1.4330 
2022-03-16 05:12:16 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 3.8158, stu_CELoss: 2.3277 KDLoss: 1.4881 
2022-03-16 05:12:49 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 3.4273, stu_CELoss: 2.1539 KDLoss: 1.2734 
2022-03-16 05:13:22 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.9506, stu_CELoss: 1.7231 KDLoss: 1.2275 
2022-03-16 05:13:55 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 3.5151, stu_CELoss: 2.1374 KDLoss: 1.3777 
2022-03-16 05:14:28 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 3.6687, stu_CELoss: 2.2245 KDLoss: 1.4442 
2022-03-16 05:15:00 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 3.6821, stu_CELoss: 2.2086 KDLoss: 1.4735 
2022-03-16 05:15:33 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 3.6370, stu_CELoss: 2.1663 KDLoss: 1.4707 
2022-03-16 05:16:06 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 3.5772, stu_CELoss: 2.1539 KDLoss: 1.4233 
2022-03-16 05:16:39 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 3.6849, stu_CELoss: 2.1608 KDLoss: 1.5241 
2022-03-16 05:17:11 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 3.3697, stu_CELoss: 2.0710 KDLoss: 1.2987 
2022-03-16 05:17:44 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 3.2090, stu_CELoss: 1.9248 KDLoss: 1.2842 
2022-03-16 05:18:17 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 3.5348, stu_CELoss: 2.1202 KDLoss: 1.4147 
2022-03-16 05:18:50 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 3.7001, stu_CELoss: 2.2380 KDLoss: 1.4621 
2022-03-16 05:19:23 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 3.7168, stu_CELoss: 2.2185 KDLoss: 1.4983 
2022-03-16 05:19:56 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 3.7454, stu_CELoss: 2.2688 KDLoss: 1.4766 
2022-03-16 05:20:29 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 3.7128, stu_CELoss: 2.2722 KDLoss: 1.4406 
2022-03-16 05:21:02 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 3.2908, stu_CELoss: 2.0593 KDLoss: 1.2315 
2022-03-16 05:21:34 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 3.4108, stu_CELoss: 2.0324 KDLoss: 1.3784 
2022-03-16 05:22:07 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 3.1406, stu_CELoss: 1.9083 KDLoss: 1.2324 
2022-03-16 05:22:39 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.5562, stu_CELoss: 2.1246 KDLoss: 1.4316 
2022-03-16 05:23:12 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.8907, stu_CELoss: 2.3952 KDLoss: 1.4955 
2022-03-16 05:23:45 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 3.8898, stu_CELoss: 2.3136 KDLoss: 1.5762 
2022-03-16 05:24:17 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 3.5533, stu_CELoss: 2.1714 KDLoss: 1.3819 
2022-03-16 05:24:50 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 3.5119, stu_CELoss: 2.1054 KDLoss: 1.4064 
2022-03-16 05:25:22 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 3.3945, stu_CELoss: 1.9983 KDLoss: 1.3962 
2022-03-16 05:25:55 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 3.1785, stu_CELoss: 1.9199 KDLoss: 1.2586 
2022-03-16 05:26:28 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 3.4620, stu_CELoss: 2.1128 KDLoss: 1.3492 
2022-03-16 05:27:01 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 3.7131, stu_CELoss: 2.2394 KDLoss: 1.4737 
2022-03-16 05:27:33 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 3.2445, stu_CELoss: 1.9657 KDLoss: 1.2787 
2022-03-16 05:28:06 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.6452, stu_CELoss: 2.2964 KDLoss: 1.3489 
2022-03-16 05:28:39 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 3.5100, stu_CELoss: 2.1201 KDLoss: 1.3899 
2022-03-16 05:29:11 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 3.6255, stu_CELoss: 2.1714 KDLoss: 1.4541 
2022-03-16 05:29:43 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 3.8291, stu_CELoss: 2.2532 KDLoss: 1.5759 
2022-03-16 05:29:45 - train: epoch 016, train_loss: 3.5056
2022-03-16 05:32:16 - eval: epoch: 016, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 54.298%, stu_acc5: 79.370%, stu_test_loss: 1.9506
2022-03-16 05:32:18 - until epoch: 016, tea_best_acc1: 78.334%, stu_best_acc1: 54.298%
2022-03-16 05:32:18 - epoch 017 lr: 0.1
2022-03-16 05:32:56 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 3.5605, stu_CELoss: 2.1751 KDLoss: 1.3855 
2022-03-16 05:33:28 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 3.6411, stu_CELoss: 2.1978 KDLoss: 1.4433 
2022-03-16 05:34:00 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 3.7919, stu_CELoss: 2.3245 KDLoss: 1.4674 
2022-03-16 05:34:32 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 3.1423, stu_CELoss: 1.8414 KDLoss: 1.3008 
2022-03-16 05:35:05 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 3.4193, stu_CELoss: 2.1050 KDLoss: 1.3143 
2022-03-16 05:35:38 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.8253, stu_CELoss: 2.2652 KDLoss: 1.5601 
2022-03-16 05:36:10 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 3.4503, stu_CELoss: 2.0995 KDLoss: 1.3508 
2022-03-16 05:36:43 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 3.1883, stu_CELoss: 1.9865 KDLoss: 1.2018 
2022-03-16 05:37:15 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 3.4780, stu_CELoss: 2.0829 KDLoss: 1.3951 
2022-03-16 05:37:48 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 3.5658, stu_CELoss: 2.1711 KDLoss: 1.3946 
2022-03-16 05:38:21 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.9440, stu_CELoss: 2.4219 KDLoss: 1.5220 
2022-03-16 05:38:54 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 3.0585, stu_CELoss: 1.8623 KDLoss: 1.1961 
2022-03-16 05:39:27 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 3.6220, stu_CELoss: 2.2329 KDLoss: 1.3892 
2022-03-16 05:40:00 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 3.5310, stu_CELoss: 2.1333 KDLoss: 1.3977 
2022-03-16 05:40:33 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 3.3034, stu_CELoss: 1.9893 KDLoss: 1.3141 
2022-03-16 05:41:06 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 3.4881, stu_CELoss: 2.1380 KDLoss: 1.3501 
2022-03-16 05:41:39 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 3.4290, stu_CELoss: 2.0131 KDLoss: 1.4160 
2022-03-16 05:42:12 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 3.5161, stu_CELoss: 2.1659 KDLoss: 1.3502 
2022-03-16 05:42:45 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 3.3150, stu_CELoss: 2.0100 KDLoss: 1.3050 
2022-03-16 05:43:18 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 3.4336, stu_CELoss: 2.0812 KDLoss: 1.3523 
2022-03-16 05:43:51 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 3.4083, stu_CELoss: 2.0541 KDLoss: 1.3542 
2022-03-16 05:44:23 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 3.1242, stu_CELoss: 1.8613 KDLoss: 1.2630 
2022-03-16 05:44:56 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 3.5786, stu_CELoss: 2.2186 KDLoss: 1.3600 
2022-03-16 05:45:29 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 3.5048, stu_CELoss: 2.1680 KDLoss: 1.3368 
2022-03-16 05:46:02 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 3.7856, stu_CELoss: 2.2930 KDLoss: 1.4926 
2022-03-16 05:46:34 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 3.4225, stu_CELoss: 2.0396 KDLoss: 1.3829 
2022-03-16 05:47:07 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 3.4573, stu_CELoss: 2.0948 KDLoss: 1.3626 
2022-03-16 05:47:40 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.4933, stu_CELoss: 2.0854 KDLoss: 1.4079 
2022-03-16 05:48:12 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 3.9322, stu_CELoss: 2.4319 KDLoss: 1.5003 
2022-03-16 05:48:45 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 3.5440, stu_CELoss: 2.1390 KDLoss: 1.4051 
2022-03-16 05:49:18 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.5455, stu_CELoss: 2.1758 KDLoss: 1.3697 
2022-03-16 05:49:51 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 3.1532, stu_CELoss: 1.9015 KDLoss: 1.2517 
2022-03-16 05:50:23 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 3.4741, stu_CELoss: 2.2164 KDLoss: 1.2577 
2022-03-16 05:50:56 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 3.5128, stu_CELoss: 2.0848 KDLoss: 1.4280 
2022-03-16 05:51:29 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 3.5423, stu_CELoss: 2.1249 KDLoss: 1.4174 
2022-03-16 05:52:01 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 3.6436, stu_CELoss: 2.1890 KDLoss: 1.4545 
2022-03-16 05:52:34 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 3.1682, stu_CELoss: 1.9441 KDLoss: 1.2241 
2022-03-16 05:53:07 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 3.9165, stu_CELoss: 2.4015 KDLoss: 1.5150 
2022-03-16 05:53:39 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 3.2476, stu_CELoss: 1.9549 KDLoss: 1.2927 
2022-03-16 05:54:12 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 3.4966, stu_CELoss: 2.1978 KDLoss: 1.2989 
2022-03-16 05:54:44 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 3.7695, stu_CELoss: 2.2529 KDLoss: 1.5166 
2022-03-16 05:55:17 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 3.4492, stu_CELoss: 2.0842 KDLoss: 1.3650 
2022-03-16 05:55:50 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 3.1403, stu_CELoss: 1.8706 KDLoss: 1.2697 
2022-03-16 05:56:22 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 3.4145, stu_CELoss: 2.1083 KDLoss: 1.3062 
2022-03-16 05:56:55 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 3.6864, stu_CELoss: 2.2565 KDLoss: 1.4299 
2022-03-16 05:57:28 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 3.1858, stu_CELoss: 1.9163 KDLoss: 1.2696 
2022-03-16 05:58:00 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 3.7156, stu_CELoss: 2.2497 KDLoss: 1.4659 
2022-03-16 05:58:33 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 3.4644, stu_CELoss: 2.1699 KDLoss: 1.2945 
2022-03-16 05:59:05 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 3.4343, stu_CELoss: 2.1196 KDLoss: 1.3147 
2022-03-16 05:59:37 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 3.3140, stu_CELoss: 2.0577 KDLoss: 1.2562 
2022-03-16 05:59:39 - train: epoch 017, train_loss: 3.4821
2022-03-16 06:02:08 - eval: epoch: 017, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 51.194%, stu_acc5: 76.678%, stu_test_loss: 2.1152
2022-03-16 06:02:09 - until epoch: 017, tea_best_acc1: 78.334%, stu_best_acc1: 54.298%
2022-03-16 06:02:09 - epoch 018 lr: 0.1
2022-03-16 06:02:47 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 3.1589, stu_CELoss: 1.8654 KDLoss: 1.2935 
2022-03-16 06:03:20 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 3.6454, stu_CELoss: 2.3087 KDLoss: 1.3367 
2022-03-16 06:03:53 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 3.8893, stu_CELoss: 2.2949 KDLoss: 1.5944 
2022-03-16 06:04:25 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 3.5444, stu_CELoss: 2.1459 KDLoss: 1.3985 
2022-03-16 06:04:58 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 3.6303, stu_CELoss: 2.1656 KDLoss: 1.4647 
2022-03-16 06:05:31 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 3.4238, stu_CELoss: 2.1127 KDLoss: 1.3111 
2022-03-16 06:06:03 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 3.2338, stu_CELoss: 1.9162 KDLoss: 1.3176 
2022-03-16 06:06:36 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 3.7410, stu_CELoss: 2.2213 KDLoss: 1.5197 
2022-03-16 06:07:08 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 3.9109, stu_CELoss: 2.4534 KDLoss: 1.4575 
2022-03-16 06:07:41 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 3.2873, stu_CELoss: 1.9773 KDLoss: 1.3100 
2022-03-16 06:08:14 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.6229, stu_CELoss: 2.1700 KDLoss: 1.4529 
2022-03-16 06:08:47 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 3.0206, stu_CELoss: 1.8541 KDLoss: 1.1665 
2022-03-16 06:09:20 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 3.6787, stu_CELoss: 2.2502 KDLoss: 1.4286 
2022-03-16 06:09:53 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 3.5553, stu_CELoss: 2.1853 KDLoss: 1.3700 
2022-03-16 06:10:25 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 3.9805, stu_CELoss: 2.4483 KDLoss: 1.5321 
2022-03-16 06:10:58 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 3.6909, stu_CELoss: 2.1910 KDLoss: 1.4998 
2022-03-16 06:11:31 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 3.4821, stu_CELoss: 2.1353 KDLoss: 1.3469 
2022-03-16 06:12:04 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 3.3886, stu_CELoss: 2.0540 KDLoss: 1.3346 
2022-03-16 06:12:36 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 3.6193, stu_CELoss: 2.1491 KDLoss: 1.4702 
2022-03-16 06:13:09 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 4.0394, stu_CELoss: 2.5308 KDLoss: 1.5086 
2022-03-16 06:13:42 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.9047, stu_CELoss: 2.3670 KDLoss: 1.5377 
2022-03-16 06:14:15 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 3.3441, stu_CELoss: 2.0323 KDLoss: 1.3118 
2022-03-16 06:14:48 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 3.4806, stu_CELoss: 2.1272 KDLoss: 1.3534 
2022-03-16 06:15:21 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 3.3002, stu_CELoss: 2.0541 KDLoss: 1.2462 
2022-03-16 06:15:53 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 3.1257, stu_CELoss: 1.8508 KDLoss: 1.2750 
2022-03-16 06:16:26 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 3.3047, stu_CELoss: 1.9855 KDLoss: 1.3191 
2022-03-16 06:16:59 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 3.3858, stu_CELoss: 2.0941 KDLoss: 1.2917 
2022-03-16 06:17:32 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 3.2881, stu_CELoss: 1.9929 KDLoss: 1.2952 
2022-03-16 06:18:05 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 3.4348, stu_CELoss: 2.1395 KDLoss: 1.2953 
2022-03-16 06:18:38 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 3.5214, stu_CELoss: 2.1388 KDLoss: 1.3826 
2022-03-16 06:19:11 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 4.0569, stu_CELoss: 2.5481 KDLoss: 1.5089 
2022-03-16 06:19:44 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 3.6042, stu_CELoss: 2.1888 KDLoss: 1.4153 
2022-03-16 06:20:17 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 3.4965, stu_CELoss: 2.0921 KDLoss: 1.4044 
2022-03-16 06:20:50 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 3.6139, stu_CELoss: 2.2652 KDLoss: 1.3487 
2022-03-16 06:21:22 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 3.5177, stu_CELoss: 2.1283 KDLoss: 1.3894 
2022-03-16 06:21:55 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 3.5684, stu_CELoss: 2.1237 KDLoss: 1.4447 
2022-03-16 06:22:28 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.6629, stu_CELoss: 2.2156 KDLoss: 1.4473 
2022-03-16 06:23:01 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 3.6908, stu_CELoss: 2.2625 KDLoss: 1.4284 
2022-03-16 06:23:34 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 3.6596, stu_CELoss: 2.3152 KDLoss: 1.3444 
2022-03-16 06:24:07 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 3.5827, stu_CELoss: 2.2497 KDLoss: 1.3330 
2022-03-16 06:24:39 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 3.5064, stu_CELoss: 2.2329 KDLoss: 1.2736 
2022-03-16 06:25:12 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 3.4519, stu_CELoss: 2.0941 KDLoss: 1.3578 
2022-03-16 06:25:45 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 3.5442, stu_CELoss: 2.2294 KDLoss: 1.3147 
2022-03-16 06:26:17 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.3666, stu_CELoss: 2.0301 KDLoss: 1.3365 
2022-03-16 06:26:50 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 3.5517, stu_CELoss: 2.1854 KDLoss: 1.3663 
2022-03-16 06:27:23 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 3.5041, stu_CELoss: 2.0859 KDLoss: 1.4182 
2022-03-16 06:27:56 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 3.5163, stu_CELoss: 2.1490 KDLoss: 1.3674 
2022-03-16 06:28:28 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 3.3517, stu_CELoss: 2.0427 KDLoss: 1.3090 
2022-03-16 06:29:01 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 3.3485, stu_CELoss: 2.0133 KDLoss: 1.3352 
2022-03-16 06:29:33 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 3.6580, stu_CELoss: 2.2647 KDLoss: 1.3933 
2022-03-16 06:29:35 - train: epoch 018, train_loss: 3.4572
2022-03-16 06:32:05 - eval: epoch: 018, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 53.594%, stu_acc5: 78.894%, stu_test_loss: 1.9871
2022-03-16 06:32:06 - until epoch: 018, tea_best_acc1: 78.334%, stu_best_acc1: 54.298%
2022-03-16 06:32:06 - epoch 019 lr: 0.1
2022-03-16 06:32:45 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 3.0224, stu_CELoss: 1.8261 KDLoss: 1.1964 
2022-03-16 06:33:18 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 3.3716, stu_CELoss: 2.0228 KDLoss: 1.3488 
2022-03-16 06:33:50 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 3.7116, stu_CELoss: 2.2828 KDLoss: 1.4288 
2022-03-16 06:34:23 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 3.5395, stu_CELoss: 2.1859 KDLoss: 1.3536 
2022-03-16 06:34:56 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 3.2542, stu_CELoss: 1.9882 KDLoss: 1.2660 
2022-03-16 06:35:29 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 3.4822, stu_CELoss: 2.1614 KDLoss: 1.3208 
2022-03-16 06:36:02 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 3.2940, stu_CELoss: 2.0653 KDLoss: 1.2287 
2022-03-16 06:36:35 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 3.4667, stu_CELoss: 2.0924 KDLoss: 1.3744 
2022-03-16 06:37:08 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 3.4099, stu_CELoss: 2.0290 KDLoss: 1.3808 
2022-03-16 06:37:41 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 3.6917, stu_CELoss: 2.2814 KDLoss: 1.4103 
2022-03-16 06:38:14 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 3.4511, stu_CELoss: 2.1001 KDLoss: 1.3510 
2022-03-16 06:38:47 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 3.4983, stu_CELoss: 2.1152 KDLoss: 1.3831 
2022-03-16 06:39:20 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 3.2786, stu_CELoss: 2.0272 KDLoss: 1.2514 
2022-03-16 06:39:53 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 3.2570, stu_CELoss: 1.9793 KDLoss: 1.2777 
2022-03-16 06:40:26 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.7693, stu_CELoss: 2.2735 KDLoss: 1.4958 
2022-03-16 06:40:59 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 3.1839, stu_CELoss: 1.8808 KDLoss: 1.3031 
2022-03-16 06:41:32 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.6751, stu_CELoss: 2.2596 KDLoss: 1.4155 
2022-03-16 06:42:05 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 3.1863, stu_CELoss: 1.9491 KDLoss: 1.2372 
2022-03-16 06:42:39 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 3.7533, stu_CELoss: 2.2765 KDLoss: 1.4768 
2022-03-16 06:43:12 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 3.3845, stu_CELoss: 2.0601 KDLoss: 1.3245 
2022-03-16 06:43:45 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 3.4442, stu_CELoss: 2.0726 KDLoss: 1.3717 
2022-03-16 06:44:18 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 3.5540, stu_CELoss: 2.1549 KDLoss: 1.3991 
2022-03-16 06:44:50 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 3.5536, stu_CELoss: 2.1023 KDLoss: 1.4514 
2022-03-16 06:45:23 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 3.5842, stu_CELoss: 2.1871 KDLoss: 1.3971 
2022-03-16 06:45:56 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 3.5069, stu_CELoss: 2.1703 KDLoss: 1.3366 
2022-03-16 06:46:29 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 3.2189, stu_CELoss: 1.9340 KDLoss: 1.2850 
2022-03-16 06:47:01 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 3.3686, stu_CELoss: 2.0401 KDLoss: 1.3285 
2022-03-16 06:47:34 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 3.2088, stu_CELoss: 1.9519 KDLoss: 1.2569 
2022-03-16 06:48:06 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 3.6510, stu_CELoss: 2.1888 KDLoss: 1.4621 
2022-03-16 06:48:39 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 3.7883, stu_CELoss: 2.3039 KDLoss: 1.4844 
2022-03-16 06:49:11 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 3.3426, stu_CELoss: 2.0120 KDLoss: 1.3305 
2022-03-16 06:49:44 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 3.0897, stu_CELoss: 1.8562 KDLoss: 1.2336 
2022-03-16 06:50:17 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 3.3355, stu_CELoss: 2.0338 KDLoss: 1.3017 
2022-03-16 06:50:49 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 3.5726, stu_CELoss: 2.1159 KDLoss: 1.4567 
2022-03-16 06:51:22 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 3.8046, stu_CELoss: 2.3097 KDLoss: 1.4949 
2022-03-16 06:51:54 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 3.0471, stu_CELoss: 1.8050 KDLoss: 1.2421 
2022-03-16 06:52:27 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 3.4591, stu_CELoss: 2.1549 KDLoss: 1.3042 
2022-03-16 06:53:00 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.7273, stu_CELoss: 2.2043 KDLoss: 1.5230 
2022-03-16 06:53:32 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 3.3348, stu_CELoss: 2.0342 KDLoss: 1.3005 
2022-03-16 06:54:05 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 3.4510, stu_CELoss: 2.1461 KDLoss: 1.3049 
2022-03-16 06:54:38 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 3.4425, stu_CELoss: 2.1269 KDLoss: 1.3156 
2022-03-16 06:55:11 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 3.4279, stu_CELoss: 2.0916 KDLoss: 1.3363 
2022-03-16 06:55:43 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 3.4406, stu_CELoss: 2.0827 KDLoss: 1.3580 
2022-03-16 06:56:16 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 3.8304, stu_CELoss: 2.3182 KDLoss: 1.5121 
2022-03-16 06:56:49 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.6768, stu_CELoss: 2.3071 KDLoss: 1.3697 
2022-03-16 06:57:21 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 3.3075, stu_CELoss: 1.9896 KDLoss: 1.3180 
2022-03-16 06:57:54 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 3.3409, stu_CELoss: 2.0258 KDLoss: 1.3150 
2022-03-16 06:58:27 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 3.2553, stu_CELoss: 2.0637 KDLoss: 1.1916 
2022-03-16 06:59:00 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 3.2708, stu_CELoss: 2.0236 KDLoss: 1.2472 
2022-03-16 06:59:32 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 3.2934, stu_CELoss: 1.9757 KDLoss: 1.3177 
2022-03-16 06:59:33 - train: epoch 019, train_loss: 3.4376
2022-03-16 07:02:04 - eval: epoch: 019, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 53.320%, stu_acc5: 78.136%, stu_test_loss: 1.9963
2022-03-16 07:02:05 - until epoch: 019, tea_best_acc1: 78.334%, stu_best_acc1: 54.298%
2022-03-16 07:02:05 - epoch 020 lr: 0.1
2022-03-16 07:02:43 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 3.4107, stu_CELoss: 2.0858 KDLoss: 1.3249 
2022-03-16 07:03:16 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 3.1734, stu_CELoss: 1.8971 KDLoss: 1.2763 
2022-03-16 07:03:49 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 3.2781, stu_CELoss: 1.9522 KDLoss: 1.3259 
2022-03-16 07:04:21 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 3.1918, stu_CELoss: 1.8723 KDLoss: 1.3195 
2022-03-16 07:04:54 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 3.2920, stu_CELoss: 2.0022 KDLoss: 1.2898 
2022-03-16 07:05:27 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 3.5092, stu_CELoss: 2.1631 KDLoss: 1.3461 
2022-03-16 07:06:00 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 3.1849, stu_CELoss: 1.8871 KDLoss: 1.2979 
2022-03-16 07:06:32 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 3.4731, stu_CELoss: 2.1039 KDLoss: 1.3693 
2022-03-16 07:07:05 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.7839, stu_CELoss: 2.2371 KDLoss: 1.5468 
2022-03-16 07:07:38 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 3.5456, stu_CELoss: 2.1860 KDLoss: 1.3597 
2022-03-16 07:08:11 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 3.3435, stu_CELoss: 2.0099 KDLoss: 1.3336 
2022-03-16 07:08:44 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 3.1722, stu_CELoss: 1.8955 KDLoss: 1.2767 
2022-03-16 07:09:17 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 3.2394, stu_CELoss: 1.9981 KDLoss: 1.2413 
2022-03-16 07:09:50 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 3.2633, stu_CELoss: 1.9980 KDLoss: 1.2653 
2022-03-16 07:10:22 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 3.2708, stu_CELoss: 1.9497 KDLoss: 1.3211 
2022-03-16 07:10:55 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 3.5840, stu_CELoss: 2.1686 KDLoss: 1.4154 
2022-03-16 07:11:28 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 3.3572, stu_CELoss: 2.0758 KDLoss: 1.2814 
2022-03-16 07:12:01 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 3.5252, stu_CELoss: 2.1508 KDLoss: 1.3744 
2022-03-16 07:12:33 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 3.4138, stu_CELoss: 2.0481 KDLoss: 1.3657 
2022-03-16 07:13:06 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 3.4695, stu_CELoss: 2.0938 KDLoss: 1.3757 
2022-03-16 07:13:39 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 3.8950, stu_CELoss: 2.3898 KDLoss: 1.5052 
2022-03-16 07:14:11 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.0993, stu_CELoss: 1.8923 KDLoss: 1.2070 
2022-03-16 07:14:44 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 3.3547, stu_CELoss: 2.0797 KDLoss: 1.2750 
2022-03-16 07:15:17 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 3.6594, stu_CELoss: 2.2517 KDLoss: 1.4076 
2022-03-16 07:15:50 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 3.2167, stu_CELoss: 1.9883 KDLoss: 1.2284 
2022-03-16 07:16:22 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 3.2392, stu_CELoss: 2.0080 KDLoss: 1.2312 
2022-03-16 07:16:55 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 3.4478, stu_CELoss: 2.0674 KDLoss: 1.3804 
2022-03-16 07:17:28 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 3.7460, stu_CELoss: 2.2488 KDLoss: 1.4972 
2022-03-16 07:18:00 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 3.4861, stu_CELoss: 2.1429 KDLoss: 1.3431 
2022-03-16 07:18:33 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 3.6303, stu_CELoss: 2.2227 KDLoss: 1.4076 
2022-03-16 07:19:05 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.5994, stu_CELoss: 2.1943 KDLoss: 1.4050 
2022-03-16 07:19:38 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 3.6555, stu_CELoss: 2.1732 KDLoss: 1.4823 
2022-03-16 07:20:10 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.9775, stu_CELoss: 1.7712 KDLoss: 1.2063 
2022-03-16 07:20:43 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 3.4972, stu_CELoss: 2.0974 KDLoss: 1.3998 
2022-03-16 07:21:16 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 3.1324, stu_CELoss: 1.8873 KDLoss: 1.2451 
2022-03-16 07:21:48 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 3.5318, stu_CELoss: 2.1618 KDLoss: 1.3700 
2022-03-16 07:22:21 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 3.4639, stu_CELoss: 2.0823 KDLoss: 1.3816 
2022-03-16 07:22:53 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 3.4123, stu_CELoss: 2.0372 KDLoss: 1.3752 
2022-03-16 07:23:26 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 3.5646, stu_CELoss: 2.1764 KDLoss: 1.3882 
2022-03-16 07:23:58 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 3.2507, stu_CELoss: 1.9601 KDLoss: 1.2907 
2022-03-16 07:24:31 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 3.4597, stu_CELoss: 2.1350 KDLoss: 1.3247 
2022-03-16 07:25:04 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 3.4004, stu_CELoss: 2.1084 KDLoss: 1.2920 
2022-03-16 07:25:36 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 3.6023, stu_CELoss: 2.2855 KDLoss: 1.3167 
2022-03-16 07:26:08 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 3.4528, stu_CELoss: 2.1159 KDLoss: 1.3369 
2022-03-16 07:26:41 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 3.6969, stu_CELoss: 2.2364 KDLoss: 1.4605 
2022-03-16 07:27:14 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 3.2846, stu_CELoss: 2.0501 KDLoss: 1.2345 
2022-03-16 07:27:46 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 3.3285, stu_CELoss: 2.0286 KDLoss: 1.2998 
2022-03-16 07:28:18 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 3.2804, stu_CELoss: 2.0255 KDLoss: 1.2549 
2022-03-16 07:28:51 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 3.6545, stu_CELoss: 2.1801 KDLoss: 1.4743 
2022-03-16 07:29:23 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 3.2293, stu_CELoss: 2.0049 KDLoss: 1.2243 
2022-03-16 07:29:24 - train: epoch 020, train_loss: 3.4222
2022-03-16 07:31:54 - eval: epoch: 020, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 54.550%, stu_acc5: 79.302%, stu_test_loss: 1.9490
2022-03-16 07:31:56 - until epoch: 020, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 07:31:56 - epoch 021 lr: 0.1
2022-03-16 07:32:33 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 3.3558, stu_CELoss: 2.0600 KDLoss: 1.2958 
2022-03-16 07:33:05 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 3.4189, stu_CELoss: 2.0322 KDLoss: 1.3867 
2022-03-16 07:33:38 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 3.1234, stu_CELoss: 1.8802 KDLoss: 1.2431 
2022-03-16 07:34:11 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 3.5785, stu_CELoss: 2.2310 KDLoss: 1.3474 
2022-03-16 07:34:43 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.8944, stu_CELoss: 1.7319 KDLoss: 1.1625 
2022-03-16 07:35:16 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 3.3144, stu_CELoss: 2.0400 KDLoss: 1.2744 
2022-03-16 07:35:49 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.0313, stu_CELoss: 1.8148 KDLoss: 1.2165 
2022-03-16 07:36:22 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 3.3675, stu_CELoss: 2.0529 KDLoss: 1.3147 
2022-03-16 07:36:55 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 3.0898, stu_CELoss: 1.8864 KDLoss: 1.2034 
2022-03-16 07:37:27 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 3.2612, stu_CELoss: 2.0024 KDLoss: 1.2589 
2022-03-16 07:38:00 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 3.3357, stu_CELoss: 2.0857 KDLoss: 1.2500 
2022-03-16 07:38:33 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 3.1941, stu_CELoss: 1.9848 KDLoss: 1.2093 
2022-03-16 07:39:05 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.0305, stu_CELoss: 1.8409 KDLoss: 1.1895 
2022-03-16 07:39:38 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.9574, stu_CELoss: 1.7011 KDLoss: 1.2563 
2022-03-16 07:40:11 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 3.3052, stu_CELoss: 2.0181 KDLoss: 1.2871 
2022-03-16 07:40:43 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 3.2455, stu_CELoss: 1.9946 KDLoss: 1.2510 
2022-03-16 07:41:16 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 3.6349, stu_CELoss: 2.1939 KDLoss: 1.4410 
2022-03-16 07:41:49 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 3.3343, stu_CELoss: 2.0497 KDLoss: 1.2846 
2022-03-16 07:42:22 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 3.6563, stu_CELoss: 2.2929 KDLoss: 1.3634 
2022-03-16 07:42:54 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 3.6108, stu_CELoss: 2.2138 KDLoss: 1.3970 
2022-03-16 07:43:27 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.9467, stu_CELoss: 1.8143 KDLoss: 1.1324 
2022-03-16 07:44:00 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.4883, stu_CELoss: 2.1087 KDLoss: 1.3796 
2022-03-16 07:44:33 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 3.6410, stu_CELoss: 2.2402 KDLoss: 1.4008 
2022-03-16 07:45:06 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 3.1530, stu_CELoss: 1.8733 KDLoss: 1.2797 
2022-03-16 07:45:38 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 3.3671, stu_CELoss: 2.0536 KDLoss: 1.3135 
2022-03-16 07:46:11 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 3.3620, stu_CELoss: 2.0641 KDLoss: 1.2979 
2022-03-16 07:46:44 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 3.2153, stu_CELoss: 1.8604 KDLoss: 1.3549 
2022-03-16 07:47:17 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 3.3039, stu_CELoss: 2.0371 KDLoss: 1.2668 
2022-03-16 07:47:50 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 3.3415, stu_CELoss: 2.0114 KDLoss: 1.3301 
2022-03-16 07:48:23 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 3.8947, stu_CELoss: 2.3566 KDLoss: 1.5381 
2022-03-16 07:48:56 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 3.6322, stu_CELoss: 2.2003 KDLoss: 1.4319 
2022-03-16 07:49:29 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 3.1183, stu_CELoss: 1.8825 KDLoss: 1.2358 
2022-03-16 07:50:01 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 3.7693, stu_CELoss: 2.3402 KDLoss: 1.4291 
2022-03-16 07:50:34 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 3.6191, stu_CELoss: 2.2782 KDLoss: 1.3409 
2022-03-16 07:51:07 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 3.4105, stu_CELoss: 2.0747 KDLoss: 1.3358 
2022-03-16 07:51:40 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 3.5594, stu_CELoss: 2.1286 KDLoss: 1.4307 
2022-03-16 07:52:13 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 3.3959, stu_CELoss: 2.0411 KDLoss: 1.3548 
2022-03-16 07:52:47 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 3.2750, stu_CELoss: 2.0192 KDLoss: 1.2558 
2022-03-16 07:53:19 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 3.2649, stu_CELoss: 1.9724 KDLoss: 1.2925 
2022-03-16 07:53:52 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 3.6701, stu_CELoss: 2.2857 KDLoss: 1.3844 
2022-03-16 07:54:25 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 3.2225, stu_CELoss: 1.9344 KDLoss: 1.2881 
2022-03-16 07:54:58 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 3.5250, stu_CELoss: 2.1588 KDLoss: 1.3662 
2022-03-16 07:55:31 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 3.7453, stu_CELoss: 2.3001 KDLoss: 1.4452 
2022-03-16 07:56:04 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 3.5173, stu_CELoss: 2.1075 KDLoss: 1.4099 
2022-03-16 07:56:37 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 3.6470, stu_CELoss: 2.2168 KDLoss: 1.4302 
2022-03-16 07:57:10 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 3.5361, stu_CELoss: 2.1716 KDLoss: 1.3645 
2022-03-16 07:57:43 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 3.4469, stu_CELoss: 2.2246 KDLoss: 1.2223 
2022-03-16 07:58:16 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 3.4453, stu_CELoss: 2.1126 KDLoss: 1.3327 
2022-03-16 07:58:50 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 3.0450, stu_CELoss: 1.8181 KDLoss: 1.2269 
2022-03-16 07:59:22 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 3.2408, stu_CELoss: 1.9360 KDLoss: 1.3048 
2022-03-16 07:59:23 - train: epoch 021, train_loss: 3.4062
2022-03-16 08:01:53 - eval: epoch: 021, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 49.220%, stu_acc5: 74.376%, stu_test_loss: 2.4572
2022-03-16 08:01:55 - until epoch: 021, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 08:01:55 - epoch 022 lr: 0.1
2022-03-16 08:02:32 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.7785, stu_CELoss: 1.6831 KDLoss: 1.0953 
2022-03-16 08:03:05 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 3.3143, stu_CELoss: 2.0245 KDLoss: 1.2898 
2022-03-16 08:03:38 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 3.2091, stu_CELoss: 1.9611 KDLoss: 1.2480 
2022-03-16 08:04:11 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 3.0787, stu_CELoss: 1.8893 KDLoss: 1.1894 
2022-03-16 08:04:43 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 3.3120, stu_CELoss: 2.0173 KDLoss: 1.2946 
2022-03-16 08:05:16 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 3.4930, stu_CELoss: 2.1494 KDLoss: 1.3436 
2022-03-16 08:05:49 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 3.6930, stu_CELoss: 2.2247 KDLoss: 1.4683 
2022-03-16 08:06:22 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 3.6788, stu_CELoss: 2.2943 KDLoss: 1.3845 
2022-03-16 08:06:55 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 3.5599, stu_CELoss: 2.1579 KDLoss: 1.4020 
2022-03-16 08:07:28 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 3.7397, stu_CELoss: 2.2651 KDLoss: 1.4746 
2022-03-16 08:08:01 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.1911, stu_CELoss: 1.9286 KDLoss: 1.2624 
2022-03-16 08:08:34 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.8917, stu_CELoss: 1.6995 KDLoss: 1.1921 
2022-03-16 08:09:07 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 3.7429, stu_CELoss: 2.2333 KDLoss: 1.5096 
2022-03-16 08:09:40 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 3.2826, stu_CELoss: 1.9422 KDLoss: 1.3404 
2022-03-16 08:10:13 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 3.6783, stu_CELoss: 2.2322 KDLoss: 1.4462 
2022-03-16 08:10:46 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 3.1830, stu_CELoss: 1.9141 KDLoss: 1.2689 
2022-03-16 08:11:19 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 3.3396, stu_CELoss: 2.1242 KDLoss: 1.2153 
2022-03-16 08:11:51 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.4865, stu_CELoss: 2.1364 KDLoss: 1.3501 
2022-03-16 08:12:24 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 3.5514, stu_CELoss: 2.0934 KDLoss: 1.4580 
2022-03-16 08:12:57 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 3.1685, stu_CELoss: 1.9171 KDLoss: 1.2514 
2022-03-16 08:13:30 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 3.0404, stu_CELoss: 1.8953 KDLoss: 1.1451 
2022-03-16 08:14:03 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 3.0782, stu_CELoss: 1.8667 KDLoss: 1.2115 
2022-03-16 08:14:35 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 3.7104, stu_CELoss: 2.2327 KDLoss: 1.4777 
2022-03-16 08:15:08 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 3.9499, stu_CELoss: 2.3581 KDLoss: 1.5918 
2022-03-16 08:15:41 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 3.3092, stu_CELoss: 2.0466 KDLoss: 1.2626 
2022-03-16 08:16:14 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 3.2059, stu_CELoss: 1.9302 KDLoss: 1.2757 
2022-03-16 08:16:47 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 3.2607, stu_CELoss: 2.0180 KDLoss: 1.2427 
2022-03-16 08:17:20 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.8888, stu_CELoss: 2.2942 KDLoss: 1.5947 
2022-03-16 08:17:53 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 3.3236, stu_CELoss: 1.9344 KDLoss: 1.3891 
2022-03-16 08:18:26 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.3191, stu_CELoss: 2.0117 KDLoss: 1.3074 
2022-03-16 08:18:59 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.3940, stu_CELoss: 2.0857 KDLoss: 1.3083 
2022-03-16 08:19:32 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 3.3848, stu_CELoss: 2.0495 KDLoss: 1.3353 
2022-03-16 08:20:04 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 3.4248, stu_CELoss: 2.0826 KDLoss: 1.3422 
2022-03-16 08:20:37 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 3.1597, stu_CELoss: 1.9162 KDLoss: 1.2435 
2022-03-16 08:21:10 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.5704, stu_CELoss: 2.1743 KDLoss: 1.3961 
2022-03-16 08:21:43 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 3.5683, stu_CELoss: 2.1355 KDLoss: 1.4328 
2022-03-16 08:22:16 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 3.4340, stu_CELoss: 2.0531 KDLoss: 1.3809 
2022-03-16 08:22:48 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 3.5382, stu_CELoss: 2.1838 KDLoss: 1.3544 
2022-03-16 08:23:21 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 3.3353, stu_CELoss: 2.0099 KDLoss: 1.3254 
2022-03-16 08:23:54 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 3.2813, stu_CELoss: 2.1144 KDLoss: 1.1669 
2022-03-16 08:24:27 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 3.4285, stu_CELoss: 2.0825 KDLoss: 1.3460 
2022-03-16 08:25:00 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 3.5535, stu_CELoss: 2.1765 KDLoss: 1.3770 
2022-03-16 08:25:32 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 3.3807, stu_CELoss: 2.0591 KDLoss: 1.3216 
2022-03-16 08:26:05 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 3.3048, stu_CELoss: 2.0287 KDLoss: 1.2761 
2022-03-16 08:26:37 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 3.3016, stu_CELoss: 1.9899 KDLoss: 1.3117 
2022-03-16 08:27:10 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 3.6961, stu_CELoss: 2.3224 KDLoss: 1.3736 
2022-03-16 08:27:43 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 3.4062, stu_CELoss: 2.1394 KDLoss: 1.2668 
2022-03-16 08:28:15 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.8866, stu_CELoss: 1.7113 KDLoss: 1.1754 
2022-03-16 08:28:48 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 3.4457, stu_CELoss: 2.1249 KDLoss: 1.3208 
2022-03-16 08:29:20 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 3.2519, stu_CELoss: 1.9748 KDLoss: 1.2771 
2022-03-16 08:29:22 - train: epoch 022, train_loss: 3.3933
2022-03-16 08:31:51 - eval: epoch: 022, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 51.296%, stu_acc5: 76.730%, stu_test_loss: 2.1067
2022-03-16 08:31:52 - until epoch: 022, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 08:31:52 - epoch 023 lr: 0.1
2022-03-16 08:32:31 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 3.5182, stu_CELoss: 2.1031 KDLoss: 1.4151 
2022-03-16 08:33:03 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.9802, stu_CELoss: 1.8451 KDLoss: 1.1351 
2022-03-16 08:33:36 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 3.2482, stu_CELoss: 1.9919 KDLoss: 1.2562 
2022-03-16 08:34:08 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 3.1707, stu_CELoss: 1.9416 KDLoss: 1.2291 
2022-03-16 08:34:40 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 3.4950, stu_CELoss: 2.1100 KDLoss: 1.3850 
2022-03-16 08:35:13 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 3.6967, stu_CELoss: 2.2125 KDLoss: 1.4843 
2022-03-16 08:35:45 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 3.3608, stu_CELoss: 2.0413 KDLoss: 1.3195 
2022-03-16 08:36:18 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 3.3996, stu_CELoss: 2.0828 KDLoss: 1.3168 
2022-03-16 08:36:51 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 3.5534, stu_CELoss: 2.2230 KDLoss: 1.3304 
2022-03-16 08:37:24 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 3.2072, stu_CELoss: 1.9100 KDLoss: 1.2972 
2022-03-16 08:37:57 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 3.5029, stu_CELoss: 2.0819 KDLoss: 1.4210 
2022-03-16 08:38:30 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 3.1982, stu_CELoss: 1.9576 KDLoss: 1.2406 
2022-03-16 08:39:03 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 3.1540, stu_CELoss: 1.9096 KDLoss: 1.2443 
2022-03-16 08:39:36 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 3.3279, stu_CELoss: 2.0440 KDLoss: 1.2839 
2022-03-16 08:40:09 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 3.1033, stu_CELoss: 1.8866 KDLoss: 1.2167 
2022-03-16 08:40:42 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 3.3058, stu_CELoss: 2.0313 KDLoss: 1.2745 
2022-03-16 08:41:15 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 3.2377, stu_CELoss: 1.8880 KDLoss: 1.3497 
2022-03-16 08:41:47 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 3.5874, stu_CELoss: 2.2577 KDLoss: 1.3297 
2022-03-16 08:42:20 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 3.6121, stu_CELoss: 2.2548 KDLoss: 1.3573 
2022-03-16 08:42:53 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.9472, stu_CELoss: 1.7812 KDLoss: 1.1660 
2022-03-16 08:43:26 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 3.2656, stu_CELoss: 1.9711 KDLoss: 1.2945 
2022-03-16 08:43:59 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.9311, stu_CELoss: 1.8331 KDLoss: 1.0980 
2022-03-16 08:44:32 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 3.1704, stu_CELoss: 1.9174 KDLoss: 1.2529 
2022-03-16 08:45:05 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 3.4399, stu_CELoss: 2.0587 KDLoss: 1.3811 
2022-03-16 08:45:38 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 3.5212, stu_CELoss: 2.0962 KDLoss: 1.4250 
2022-03-16 08:46:11 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 3.6290, stu_CELoss: 2.1608 KDLoss: 1.4682 
2022-03-16 08:46:43 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 3.4788, stu_CELoss: 2.1945 KDLoss: 1.2842 
2022-03-16 08:47:16 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 3.8120, stu_CELoss: 2.3110 KDLoss: 1.5009 
2022-03-16 08:47:49 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 3.3074, stu_CELoss: 1.9463 KDLoss: 1.3612 
2022-03-16 08:48:22 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 3.4379, stu_CELoss: 2.1256 KDLoss: 1.3123 
2022-03-16 08:48:54 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 3.7737, stu_CELoss: 2.2922 KDLoss: 1.4815 
2022-03-16 08:49:27 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 3.4515, stu_CELoss: 2.1307 KDLoss: 1.3209 
2022-03-16 08:49:59 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 3.1495, stu_CELoss: 1.9333 KDLoss: 1.2161 
2022-03-16 08:50:32 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 3.6740, stu_CELoss: 2.2084 KDLoss: 1.4656 
2022-03-16 08:51:05 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 3.2532, stu_CELoss: 2.0232 KDLoss: 1.2300 
2022-03-16 08:51:37 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 3.3843, stu_CELoss: 2.0788 KDLoss: 1.3055 
2022-03-16 08:52:10 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 3.4719, stu_CELoss: 2.1002 KDLoss: 1.3717 
2022-03-16 08:52:43 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 3.4123, stu_CELoss: 2.1141 KDLoss: 1.2982 
2022-03-16 08:53:16 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 3.5327, stu_CELoss: 2.1956 KDLoss: 1.3371 
2022-03-16 08:53:48 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 3.2679, stu_CELoss: 2.0056 KDLoss: 1.2623 
2022-03-16 08:54:21 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 3.2996, stu_CELoss: 1.9947 KDLoss: 1.3049 
2022-03-16 08:54:53 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 3.0466, stu_CELoss: 1.7842 KDLoss: 1.2624 
2022-03-16 08:55:26 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 3.2806, stu_CELoss: 2.0034 KDLoss: 1.2773 
2022-03-16 08:55:59 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 3.0566, stu_CELoss: 1.8601 KDLoss: 1.1964 
2022-03-16 08:56:31 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 3.2235, stu_CELoss: 1.9700 KDLoss: 1.2534 
2022-03-16 08:57:04 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 3.3667, stu_CELoss: 2.1177 KDLoss: 1.2489 
2022-03-16 08:57:36 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 3.1237, stu_CELoss: 1.9092 KDLoss: 1.2145 
2022-03-16 08:58:09 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 3.6700, stu_CELoss: 2.1893 KDLoss: 1.4807 
2022-03-16 08:58:42 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 3.0477, stu_CELoss: 1.8840 KDLoss: 1.1638 
2022-03-16 08:59:14 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 3.5480, stu_CELoss: 2.1226 KDLoss: 1.4254 
2022-03-16 08:59:15 - train: epoch 023, train_loss: 3.3769
2022-03-16 09:01:46 - eval: epoch: 023, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 52.138%, stu_acc5: 77.234%, stu_test_loss: 2.0718
2022-03-16 09:01:48 - until epoch: 023, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 09:01:48 - epoch 024 lr: 0.1
2022-03-16 09:02:26 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 3.7215, stu_CELoss: 2.2524 KDLoss: 1.4690 
2022-03-16 09:02:59 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 3.1293, stu_CELoss: 1.9256 KDLoss: 1.2037 
2022-03-16 09:03:31 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 3.4107, stu_CELoss: 2.1133 KDLoss: 1.2974 
2022-03-16 09:04:04 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 3.3961, stu_CELoss: 2.0543 KDLoss: 1.3418 
2022-03-16 09:04:37 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 3.6119, stu_CELoss: 2.1649 KDLoss: 1.4470 
2022-03-16 09:05:10 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 3.5887, stu_CELoss: 2.1675 KDLoss: 1.4212 
2022-03-16 09:05:43 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 3.2721, stu_CELoss: 1.9660 KDLoss: 1.3062 
2022-03-16 09:06:16 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 3.3564, stu_CELoss: 1.9467 KDLoss: 1.4097 
2022-03-16 09:06:49 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 3.1097, stu_CELoss: 1.8632 KDLoss: 1.2465 
2022-03-16 09:07:21 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 3.3154, stu_CELoss: 2.0418 KDLoss: 1.2736 
2022-03-16 09:07:54 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 3.1908, stu_CELoss: 1.9095 KDLoss: 1.2813 
2022-03-16 09:08:27 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 3.2320, stu_CELoss: 1.9810 KDLoss: 1.2510 
2022-03-16 09:09:00 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.5627, stu_CELoss: 2.2068 KDLoss: 1.3559 
2022-03-16 09:09:33 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.9251, stu_CELoss: 1.7975 KDLoss: 1.1276 
2022-03-16 09:10:06 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 3.5665, stu_CELoss: 2.1674 KDLoss: 1.3991 
2022-03-16 09:10:38 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 3.3792, stu_CELoss: 2.0563 KDLoss: 1.3229 
2022-03-16 09:11:12 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 3.3703, stu_CELoss: 2.0326 KDLoss: 1.3378 
2022-03-16 09:11:44 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 3.5187, stu_CELoss: 2.1114 KDLoss: 1.4073 
2022-03-16 09:12:17 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 3.1693, stu_CELoss: 1.9538 KDLoss: 1.2155 
2022-03-16 09:12:50 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 3.5072, stu_CELoss: 2.0868 KDLoss: 1.4204 
2022-03-16 09:13:23 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.9908, stu_CELoss: 1.9079 KDLoss: 1.0830 
2022-03-16 09:13:56 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.9555, stu_CELoss: 1.8727 KDLoss: 1.0828 
2022-03-16 09:14:29 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 3.5111, stu_CELoss: 2.0793 KDLoss: 1.4318 
2022-03-16 09:15:02 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 3.8006, stu_CELoss: 2.3233 KDLoss: 1.4773 
2022-03-16 09:15:35 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 3.2613, stu_CELoss: 1.9825 KDLoss: 1.2789 
2022-03-16 09:16:07 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 3.7756, stu_CELoss: 2.3936 KDLoss: 1.3821 
2022-03-16 09:16:40 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 3.6334, stu_CELoss: 2.2209 KDLoss: 1.4126 
2022-03-16 09:17:13 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 3.5120, stu_CELoss: 2.1859 KDLoss: 1.3261 
2022-03-16 09:17:46 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 3.1882, stu_CELoss: 1.8954 KDLoss: 1.2928 
2022-03-16 09:18:19 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 3.0561, stu_CELoss: 1.8875 KDLoss: 1.1687 
2022-03-16 09:18:52 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.9297, stu_CELoss: 1.7811 KDLoss: 1.1487 
2022-03-16 09:19:25 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 3.7687, stu_CELoss: 2.2383 KDLoss: 1.5303 
2022-03-16 09:19:57 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 3.2085, stu_CELoss: 1.9202 KDLoss: 1.2883 
2022-03-16 09:20:30 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 3.0422, stu_CELoss: 1.8606 KDLoss: 1.1816 
2022-03-16 09:21:03 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 3.4521, stu_CELoss: 2.1147 KDLoss: 1.3374 
2022-03-16 09:21:36 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 3.3472, stu_CELoss: 2.0372 KDLoss: 1.3099 
2022-03-16 09:22:09 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 3.1488, stu_CELoss: 1.9767 KDLoss: 1.1722 
2022-03-16 09:22:42 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 3.6003, stu_CELoss: 2.2152 KDLoss: 1.3850 
2022-03-16 09:23:15 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.1498, stu_CELoss: 1.8830 KDLoss: 1.2669 
2022-03-16 09:23:48 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 3.5854, stu_CELoss: 2.1676 KDLoss: 1.4178 
2022-03-16 09:24:21 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 3.4163, stu_CELoss: 2.0490 KDLoss: 1.3673 
2022-03-16 09:24:54 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 3.3700, stu_CELoss: 2.0460 KDLoss: 1.3240 
2022-03-16 09:25:26 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 3.1298, stu_CELoss: 1.9176 KDLoss: 1.2122 
2022-03-16 09:25:59 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 3.5400, stu_CELoss: 2.1948 KDLoss: 1.3452 
2022-03-16 09:26:32 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.9672, stu_CELoss: 1.7825 KDLoss: 1.1847 
2022-03-16 09:27:04 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 3.6843, stu_CELoss: 2.2759 KDLoss: 1.4083 
2022-03-16 09:27:37 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 3.2984, stu_CELoss: 1.9981 KDLoss: 1.3004 
2022-03-16 09:28:09 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.9938, stu_CELoss: 1.7715 KDLoss: 1.2223 
2022-03-16 09:28:42 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 3.5133, stu_CELoss: 2.2002 KDLoss: 1.3131 
2022-03-16 09:29:14 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 3.6810, stu_CELoss: 2.2503 KDLoss: 1.4307 
2022-03-16 09:29:15 - train: epoch 024, train_loss: 3.3636
2022-03-16 09:31:45 - eval: epoch: 024, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 53.732%, stu_acc5: 78.948%, stu_test_loss: 1.9870
2022-03-16 09:31:46 - until epoch: 024, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 09:31:46 - epoch 025 lr: 0.1
2022-03-16 09:32:25 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 3.2505, stu_CELoss: 1.9781 KDLoss: 1.2724 
2022-03-16 09:32:57 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.9192, stu_CELoss: 1.7687 KDLoss: 1.1505 
2022-03-16 09:33:30 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 3.3540, stu_CELoss: 1.9979 KDLoss: 1.3561 
2022-03-16 09:34:03 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 3.4857, stu_CELoss: 2.1655 KDLoss: 1.3201 
2022-03-16 09:34:35 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 3.2128, stu_CELoss: 1.9903 KDLoss: 1.2225 
2022-03-16 09:35:08 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 3.2516, stu_CELoss: 2.0475 KDLoss: 1.2040 
2022-03-16 09:35:40 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 3.2636, stu_CELoss: 1.9471 KDLoss: 1.3164 
2022-03-16 09:36:13 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 3.2877, stu_CELoss: 1.9696 KDLoss: 1.3182 
2022-03-16 09:36:46 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 3.3175, stu_CELoss: 2.0310 KDLoss: 1.2865 
2022-03-16 09:37:19 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 3.3023, stu_CELoss: 2.0373 KDLoss: 1.2650 
2022-03-16 09:37:51 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 3.3330, stu_CELoss: 2.0616 KDLoss: 1.2715 
2022-03-16 09:38:24 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 3.4383, stu_CELoss: 2.1075 KDLoss: 1.3308 
2022-03-16 09:38:57 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 3.3262, stu_CELoss: 2.0347 KDLoss: 1.2914 
2022-03-16 09:39:30 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 3.1132, stu_CELoss: 1.8291 KDLoss: 1.2842 
2022-03-16 09:40:03 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 3.4137, stu_CELoss: 2.1899 KDLoss: 1.2238 
2022-03-16 09:40:36 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 3.2817, stu_CELoss: 1.9794 KDLoss: 1.3023 
2022-03-16 09:41:08 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 3.4272, stu_CELoss: 2.1161 KDLoss: 1.3111 
2022-03-16 09:41:41 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 3.1621, stu_CELoss: 1.8843 KDLoss: 1.2779 
2022-03-16 09:42:14 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 3.1368, stu_CELoss: 1.9509 KDLoss: 1.1860 
2022-03-16 09:42:47 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 3.4854, stu_CELoss: 2.1371 KDLoss: 1.3483 
2022-03-16 09:43:19 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.9007, stu_CELoss: 1.7049 KDLoss: 1.1958 
2022-03-16 09:43:52 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 3.2959, stu_CELoss: 2.0132 KDLoss: 1.2827 
2022-03-16 09:44:25 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 3.2601, stu_CELoss: 1.9755 KDLoss: 1.2846 
2022-03-16 09:44:58 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 3.4155, stu_CELoss: 2.0425 KDLoss: 1.3730 
2022-03-16 09:45:30 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 3.3902, stu_CELoss: 2.0690 KDLoss: 1.3212 
2022-03-16 09:46:03 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 3.1144, stu_CELoss: 1.9095 KDLoss: 1.2049 
2022-03-16 09:46:36 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 3.5500, stu_CELoss: 2.2420 KDLoss: 1.3080 
2022-03-16 09:47:08 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 3.6140, stu_CELoss: 2.2320 KDLoss: 1.3820 
2022-03-16 09:47:41 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 3.5122, stu_CELoss: 2.1954 KDLoss: 1.3167 
2022-03-16 09:48:13 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 3.3433, stu_CELoss: 1.9907 KDLoss: 1.3526 
2022-03-16 09:48:46 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 3.3451, stu_CELoss: 2.0352 KDLoss: 1.3099 
2022-03-16 09:49:19 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 3.4088, stu_CELoss: 2.0764 KDLoss: 1.3324 
2022-03-16 09:49:51 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 3.1407, stu_CELoss: 1.8758 KDLoss: 1.2649 
2022-03-16 09:50:24 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 3.4303, stu_CELoss: 2.0647 KDLoss: 1.3656 
2022-03-16 09:50:57 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 3.4927, stu_CELoss: 2.1020 KDLoss: 1.3908 
2022-03-16 09:51:29 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 3.5263, stu_CELoss: 2.1304 KDLoss: 1.3959 
2022-03-16 09:52:02 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 3.3639, stu_CELoss: 2.0784 KDLoss: 1.2856 
2022-03-16 09:52:34 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 3.3376, stu_CELoss: 2.0581 KDLoss: 1.2795 
2022-03-16 09:53:07 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 3.6522, stu_CELoss: 2.2636 KDLoss: 1.3886 
2022-03-16 09:53:40 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 3.3864, stu_CELoss: 2.0753 KDLoss: 1.3111 
2022-03-16 09:54:13 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 3.6228, stu_CELoss: 2.2147 KDLoss: 1.4081 
2022-03-16 09:54:46 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 3.4439, stu_CELoss: 2.1063 KDLoss: 1.3376 
2022-03-16 09:55:18 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 3.1170, stu_CELoss: 1.8619 KDLoss: 1.2551 
2022-03-16 09:55:51 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 3.1411, stu_CELoss: 1.9197 KDLoss: 1.2214 
2022-03-16 09:56:24 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 3.3385, stu_CELoss: 2.0424 KDLoss: 1.2961 
2022-03-16 09:56:57 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.8965, stu_CELoss: 1.7837 KDLoss: 1.1128 
2022-03-16 09:57:29 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 3.2984, stu_CELoss: 2.0002 KDLoss: 1.2981 
2022-03-16 09:58:02 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 3.0252, stu_CELoss: 1.8999 KDLoss: 1.1254 
2022-03-16 09:58:34 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 3.1893, stu_CELoss: 1.9858 KDLoss: 1.2035 
2022-03-16 09:59:07 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 3.7739, stu_CELoss: 2.3820 KDLoss: 1.3919 
2022-03-16 09:59:08 - train: epoch 025, train_loss: 3.3559
2022-03-16 10:01:37 - eval: epoch: 025, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 53.058%, stu_acc5: 78.566%, stu_test_loss: 2.0055
2022-03-16 10:01:39 - until epoch: 025, tea_best_acc1: 78.334%, stu_best_acc1: 54.550%
2022-03-16 10:01:39 - epoch 026 lr: 0.1
2022-03-16 10:02:17 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 3.4018, stu_CELoss: 1.9876 KDLoss: 1.4141 
2022-03-16 10:02:50 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 3.1293, stu_CELoss: 1.9062 KDLoss: 1.2231 
2022-03-16 10:03:23 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 3.4387, stu_CELoss: 2.1451 KDLoss: 1.2935 
2022-03-16 10:03:55 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.9928, stu_CELoss: 1.8111 KDLoss: 1.1817 
2022-03-16 10:04:28 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 3.2988, stu_CELoss: 2.0309 KDLoss: 1.2679 
2022-03-16 10:05:01 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 3.8858, stu_CELoss: 2.4242 KDLoss: 1.4615 
2022-03-16 10:05:34 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 3.3081, stu_CELoss: 2.0544 KDLoss: 1.2537 
2022-03-16 10:06:07 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.9300, stu_CELoss: 1.7984 KDLoss: 1.1315 
2022-03-16 10:06:40 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 3.8165, stu_CELoss: 2.4600 KDLoss: 1.3565 
2022-03-16 10:07:13 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 3.4179, stu_CELoss: 2.0572 KDLoss: 1.3607 
2022-03-16 10:07:46 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 3.5071, stu_CELoss: 2.0770 KDLoss: 1.4301 
2022-03-16 10:08:19 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 3.6135, stu_CELoss: 2.1951 KDLoss: 1.4184 
2022-03-16 10:08:52 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 3.1580, stu_CELoss: 1.9672 KDLoss: 1.1909 
2022-03-16 10:09:24 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 3.4769, stu_CELoss: 2.1218 KDLoss: 1.3550 
2022-03-16 10:09:57 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 3.0298, stu_CELoss: 1.8706 KDLoss: 1.1592 
2022-03-16 10:10:30 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 3.1371, stu_CELoss: 1.9893 KDLoss: 1.1478 
2022-03-16 10:11:03 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 3.2329, stu_CELoss: 1.9613 KDLoss: 1.2716 
2022-03-16 10:11:36 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 3.7611, stu_CELoss: 2.3820 KDLoss: 1.3790 
2022-03-16 10:12:09 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.5524, stu_CELoss: 2.2105 KDLoss: 1.3420 
2022-03-16 10:12:42 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 3.4876, stu_CELoss: 2.1177 KDLoss: 1.3699 
2022-03-16 10:13:14 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 3.2962, stu_CELoss: 1.9914 KDLoss: 1.3048 
2022-03-16 10:13:48 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 3.2922, stu_CELoss: 1.9978 KDLoss: 1.2944 
2022-03-16 10:14:20 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 3.4053, stu_CELoss: 2.0313 KDLoss: 1.3740 
2022-03-16 10:14:54 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 3.4683, stu_CELoss: 2.0920 KDLoss: 1.3763 
2022-03-16 10:15:27 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 3.3001, stu_CELoss: 2.0615 KDLoss: 1.2386 
2022-03-16 10:15:59 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 3.5709, stu_CELoss: 2.1753 KDLoss: 1.3956 
2022-03-16 10:16:32 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 3.6015, stu_CELoss: 2.2696 KDLoss: 1.3319 
2022-03-16 10:17:05 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 3.3854, stu_CELoss: 2.0745 KDLoss: 1.3109 
2022-03-16 10:17:38 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 3.7829, stu_CELoss: 2.2843 KDLoss: 1.4986 
2022-03-16 10:18:11 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 3.5500, stu_CELoss: 2.1546 KDLoss: 1.3954 
2022-03-16 10:18:43 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 3.4841, stu_CELoss: 2.1266 KDLoss: 1.3575 
2022-03-16 10:19:16 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 3.4348, stu_CELoss: 2.1256 KDLoss: 1.3092 
2022-03-16 10:19:49 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 3.4264, stu_CELoss: 2.0953 KDLoss: 1.3311 
2022-03-16 10:20:22 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 3.4832, stu_CELoss: 2.0728 KDLoss: 1.4105 
2022-03-16 10:20:54 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 3.4691, stu_CELoss: 2.2009 KDLoss: 1.2682 
2022-03-16 10:21:27 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.8735, stu_CELoss: 1.7575 KDLoss: 1.1160 
2022-03-16 10:22:00 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 3.3224, stu_CELoss: 2.0158 KDLoss: 1.3065 
2022-03-16 10:22:33 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 3.7626, stu_CELoss: 2.2997 KDLoss: 1.4629 
2022-03-16 10:23:05 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 3.4854, stu_CELoss: 2.1369 KDLoss: 1.3485 
2022-03-16 10:23:38 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 3.2353, stu_CELoss: 1.9984 KDLoss: 1.2370 
2022-03-16 10:24:11 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 3.3013, stu_CELoss: 2.0172 KDLoss: 1.2842 
2022-03-16 10:24:44 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 3.3992, stu_CELoss: 2.0991 KDLoss: 1.3002 
2022-03-16 10:25:16 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 3.7129, stu_CELoss: 2.2830 KDLoss: 1.4299 
2022-03-16 10:25:49 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 3.3563, stu_CELoss: 2.1570 KDLoss: 1.1993 
2022-03-16 10:26:22 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 3.9426, stu_CELoss: 2.5155 KDLoss: 1.4272 
2022-03-16 10:26:55 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 3.2363, stu_CELoss: 2.0249 KDLoss: 1.2114 
2022-03-16 10:27:27 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 3.0883, stu_CELoss: 1.8615 KDLoss: 1.2268 
2022-03-16 10:28:00 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 3.1850, stu_CELoss: 2.0020 KDLoss: 1.1830 
2022-03-16 10:28:32 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 3.2928, stu_CELoss: 1.9935 KDLoss: 1.2994 
2022-03-16 10:29:05 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 3.3077, stu_CELoss: 2.0675 KDLoss: 1.2402 
2022-03-16 10:29:06 - train: epoch 026, train_loss: 3.3485
2022-03-16 10:31:35 - eval: epoch: 026, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 55.780%, stu_acc5: 80.996%, stu_test_loss: 1.8506
2022-03-16 10:31:36 - until epoch: 026, tea_best_acc1: 78.334%, stu_best_acc1: 55.780%
2022-03-16 10:31:36 - epoch 027 lr: 0.1
2022-03-16 10:32:15 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 3.3227, stu_CELoss: 1.9813 KDLoss: 1.3413 
2022-03-16 10:32:47 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 3.2936, stu_CELoss: 2.0204 KDLoss: 1.2732 
2022-03-16 10:33:20 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 3.5544, stu_CELoss: 2.1865 KDLoss: 1.3679 
2022-03-16 10:33:52 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 3.7094, stu_CELoss: 2.2550 KDLoss: 1.4545 
2022-03-16 10:34:25 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 3.4230, stu_CELoss: 2.0148 KDLoss: 1.4082 
2022-03-16 10:34:58 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 3.3317, stu_CELoss: 2.0588 KDLoss: 1.2729 
2022-03-16 10:35:30 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 3.1822, stu_CELoss: 1.9878 KDLoss: 1.1944 
2022-03-16 10:36:03 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 3.3842, stu_CELoss: 2.0365 KDLoss: 1.3477 
2022-03-16 10:36:36 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 3.3623, stu_CELoss: 2.0177 KDLoss: 1.3446 
2022-03-16 10:37:09 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 3.2264, stu_CELoss: 1.9398 KDLoss: 1.2866 
2022-03-16 10:37:41 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 3.2890, stu_CELoss: 2.0129 KDLoss: 1.2761 
2022-03-16 10:38:14 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.7931, stu_CELoss: 2.2748 KDLoss: 1.5183 
2022-03-16 10:38:47 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 3.3887, stu_CELoss: 1.9812 KDLoss: 1.4074 
2022-03-16 10:39:20 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 3.6801, stu_CELoss: 2.2035 KDLoss: 1.4766 
2022-03-16 10:39:53 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 3.5114, stu_CELoss: 2.1337 KDLoss: 1.3777 
2022-03-16 10:40:26 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 3.4920, stu_CELoss: 2.1185 KDLoss: 1.3735 
2022-03-16 10:40:59 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 3.3061, stu_CELoss: 1.9870 KDLoss: 1.3190 
2022-03-16 10:41:31 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 3.2324, stu_CELoss: 1.8883 KDLoss: 1.3441 
2022-03-16 10:42:04 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 3.4451, stu_CELoss: 2.1328 KDLoss: 1.3123 
2022-03-16 10:42:37 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 3.0827, stu_CELoss: 1.9107 KDLoss: 1.1721 
2022-03-16 10:43:10 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 3.6203, stu_CELoss: 2.2280 KDLoss: 1.3923 
2022-03-16 10:43:43 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 3.5604, stu_CELoss: 2.1852 KDLoss: 1.3751 
2022-03-16 10:44:15 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 3.7032, stu_CELoss: 2.2856 KDLoss: 1.4176 
2022-03-16 10:44:48 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 3.2379, stu_CELoss: 2.0049 KDLoss: 1.2330 
2022-03-16 10:45:21 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 3.1598, stu_CELoss: 1.9186 KDLoss: 1.2412 
2022-03-16 10:45:54 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 3.4819, stu_CELoss: 2.1733 KDLoss: 1.3086 
2022-03-16 10:46:26 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 3.2841, stu_CELoss: 2.0085 KDLoss: 1.2755 
2022-03-16 10:46:59 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 3.5487, stu_CELoss: 2.1478 KDLoss: 1.4008 
2022-03-16 10:47:32 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 3.4942, stu_CELoss: 2.1065 KDLoss: 1.3877 
2022-03-16 10:48:05 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 3.2477, stu_CELoss: 2.0335 KDLoss: 1.2143 
2022-03-16 10:48:38 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.9787, stu_CELoss: 1.7890 KDLoss: 1.1897 
2022-03-16 10:49:10 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 3.2407, stu_CELoss: 1.8980 KDLoss: 1.3427 
2022-03-16 10:49:43 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 3.2088, stu_CELoss: 2.0016 KDLoss: 1.2072 
2022-03-16 10:50:16 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 3.2203, stu_CELoss: 1.9153 KDLoss: 1.3050 
2022-03-16 10:50:49 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 3.5222, stu_CELoss: 2.2021 KDLoss: 1.3201 
2022-03-16 10:51:21 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 3.2705, stu_CELoss: 2.0674 KDLoss: 1.2031 
2022-03-16 10:51:54 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 3.3799, stu_CELoss: 2.0805 KDLoss: 1.2994 
2022-03-16 10:52:27 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.9075, stu_CELoss: 1.7485 KDLoss: 1.1590 
2022-03-16 10:52:59 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 3.2813, stu_CELoss: 2.0202 KDLoss: 1.2611 
2022-03-16 10:53:32 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 3.7130, stu_CELoss: 2.2317 KDLoss: 1.4813 
2022-03-16 10:54:05 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 3.3521, stu_CELoss: 2.0964 KDLoss: 1.2558 
2022-03-16 10:54:38 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 3.4026, stu_CELoss: 2.0418 KDLoss: 1.3608 
2022-03-16 10:55:10 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 3.0834, stu_CELoss: 1.9301 KDLoss: 1.1533 
2022-03-16 10:55:43 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 3.5521, stu_CELoss: 2.1741 KDLoss: 1.3780 
2022-03-16 10:56:16 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 3.0833, stu_CELoss: 1.9214 KDLoss: 1.1620 
2022-03-16 10:56:49 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.1734, stu_CELoss: 1.9113 KDLoss: 1.2621 
2022-03-16 10:57:21 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 3.7974, stu_CELoss: 2.3575 KDLoss: 1.4398 
2022-03-16 10:57:54 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 3.5531, stu_CELoss: 2.1888 KDLoss: 1.3643 
2022-03-16 10:58:27 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 3.0853, stu_CELoss: 1.8979 KDLoss: 1.1874 
2022-03-16 10:58:59 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 3.1202, stu_CELoss: 1.9076 KDLoss: 1.2125 
2022-03-16 10:59:01 - train: epoch 027, train_loss: 3.3378
2022-03-16 11:01:30 - eval: epoch: 027, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 55.538%, stu_acc5: 80.296%, stu_test_loss: 1.8877
2022-03-16 11:01:31 - until epoch: 027, tea_best_acc1: 78.334%, stu_best_acc1: 55.780%
2022-03-16 11:01:31 - epoch 028 lr: 0.1
2022-03-16 11:02:10 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.9113, stu_CELoss: 1.7707 KDLoss: 1.1406 
2022-03-16 11:02:43 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 3.3704, stu_CELoss: 2.0525 KDLoss: 1.3179 
2022-03-16 11:03:15 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 3.3762, stu_CELoss: 2.1027 KDLoss: 1.2735 
2022-03-16 11:03:48 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 3.2639, stu_CELoss: 2.0008 KDLoss: 1.2631 
2022-03-16 11:04:21 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 3.0253, stu_CELoss: 1.8364 KDLoss: 1.1889 
2022-03-16 11:04:53 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 3.5829, stu_CELoss: 2.2707 KDLoss: 1.3122 
2022-03-16 11:05:25 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.4146, stu_CELoss: 2.1118 KDLoss: 1.3029 
2022-03-16 11:05:58 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 3.0597, stu_CELoss: 1.8695 KDLoss: 1.1902 
2022-03-16 11:06:31 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 3.0664, stu_CELoss: 1.8963 KDLoss: 1.1701 
2022-03-16 11:07:04 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 3.3574, stu_CELoss: 2.0155 KDLoss: 1.3419 
2022-03-16 11:07:36 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.2499, stu_CELoss: 1.9377 KDLoss: 1.3122 
2022-03-16 11:08:09 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 3.1750, stu_CELoss: 1.9588 KDLoss: 1.2162 
2022-03-16 11:08:42 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 3.3568, stu_CELoss: 2.0710 KDLoss: 1.2858 
2022-03-16 11:09:15 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.5738, stu_CELoss: 2.1974 KDLoss: 1.3764 
2022-03-16 11:09:48 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 3.3612, stu_CELoss: 2.0630 KDLoss: 1.2982 
2022-03-16 11:10:21 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 3.2307, stu_CELoss: 1.9208 KDLoss: 1.3099 
2022-03-16 11:10:53 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 3.3595, stu_CELoss: 2.0944 KDLoss: 1.2652 
2022-03-16 11:11:26 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 3.1846, stu_CELoss: 1.9803 KDLoss: 1.2042 
2022-03-16 11:11:59 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 3.2390, stu_CELoss: 1.9923 KDLoss: 1.2467 
2022-03-16 11:12:31 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 3.3131, stu_CELoss: 2.0235 KDLoss: 1.2896 
2022-03-16 11:13:04 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 3.6545, stu_CELoss: 2.2824 KDLoss: 1.3720 
2022-03-16 11:13:37 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 3.4746, stu_CELoss: 2.1052 KDLoss: 1.3694 
2022-03-16 11:14:09 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 3.2818, stu_CELoss: 1.9799 KDLoss: 1.3020 
2022-03-16 11:14:43 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 3.6043, stu_CELoss: 2.1325 KDLoss: 1.4718 
2022-03-16 11:15:15 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 3.5364, stu_CELoss: 2.1580 KDLoss: 1.3783 
2022-03-16 11:15:48 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 3.2955, stu_CELoss: 1.9825 KDLoss: 1.3129 
2022-03-16 11:16:21 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 3.0530, stu_CELoss: 1.8760 KDLoss: 1.1770 
2022-03-16 11:16:54 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 3.4407, stu_CELoss: 2.1383 KDLoss: 1.3024 
2022-03-16 11:17:27 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 3.5093, stu_CELoss: 2.2327 KDLoss: 1.2766 
2022-03-16 11:17:59 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 3.1425, stu_CELoss: 1.9629 KDLoss: 1.1796 
2022-03-16 11:18:32 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 3.2042, stu_CELoss: 1.9966 KDLoss: 1.2076 
2022-03-16 11:19:05 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.9539, stu_CELoss: 1.8315 KDLoss: 1.1224 
2022-03-16 11:19:38 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 3.3924, stu_CELoss: 2.0898 KDLoss: 1.3026 
2022-03-16 11:20:11 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 3.4497, stu_CELoss: 2.1660 KDLoss: 1.2838 
2022-03-16 11:20:43 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 3.5009, stu_CELoss: 2.0727 KDLoss: 1.4282 
2022-03-16 11:21:16 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 3.0331, stu_CELoss: 1.9160 KDLoss: 1.1171 
2022-03-16 11:21:48 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 3.1538, stu_CELoss: 1.9282 KDLoss: 1.2256 
2022-03-16 11:22:21 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 3.0889, stu_CELoss: 1.8296 KDLoss: 1.2593 
2022-03-16 11:22:53 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 3.6612, stu_CELoss: 2.2494 KDLoss: 1.4118 
2022-03-16 11:23:26 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 3.4902, stu_CELoss: 2.1650 KDLoss: 1.3252 
2022-03-16 11:23:59 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 3.4342, stu_CELoss: 2.0915 KDLoss: 1.3427 
2022-03-16 11:24:31 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 3.3921, stu_CELoss: 2.0784 KDLoss: 1.3136 
2022-03-16 11:25:03 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 3.0858, stu_CELoss: 1.9115 KDLoss: 1.1744 
2022-03-16 11:25:36 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 3.1772, stu_CELoss: 1.9751 KDLoss: 1.2021 
2022-03-16 11:26:08 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 3.4537, stu_CELoss: 2.1296 KDLoss: 1.3241 
2022-03-16 11:26:41 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 3.3107, stu_CELoss: 2.0785 KDLoss: 1.2322 
2022-03-16 11:27:14 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 3.4721, stu_CELoss: 2.2027 KDLoss: 1.2694 
2022-03-16 11:27:46 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 3.2326, stu_CELoss: 2.0243 KDLoss: 1.2083 
2022-03-16 11:28:19 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 3.5709, stu_CELoss: 2.1435 KDLoss: 1.4274 
2022-03-16 11:28:51 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 3.1254, stu_CELoss: 1.9541 KDLoss: 1.1713 
2022-03-16 11:28:53 - train: epoch 028, train_loss: 3.3291
2022-03-16 11:31:22 - eval: epoch: 028, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 56.694%, stu_acc5: 81.326%, stu_test_loss: 1.8216
2022-03-16 11:31:24 - until epoch: 028, tea_best_acc1: 78.334%, stu_best_acc1: 56.694%
2022-03-16 11:31:24 - epoch 029 lr: 0.1
2022-03-16 11:32:02 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 3.4233, stu_CELoss: 2.0661 KDLoss: 1.3572 
2022-03-16 11:32:34 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 3.1429, stu_CELoss: 1.8971 KDLoss: 1.2458 
2022-03-16 11:33:07 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 3.6773, stu_CELoss: 2.3539 KDLoss: 1.3235 
2022-03-16 11:33:40 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 3.7621, stu_CELoss: 2.2628 KDLoss: 1.4992 
2022-03-16 11:34:13 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 3.4519, stu_CELoss: 2.1634 KDLoss: 1.2885 
2022-03-16 11:34:45 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 3.3698, stu_CELoss: 2.0778 KDLoss: 1.2919 
2022-03-16 11:35:18 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.7959, stu_CELoss: 1.6585 KDLoss: 1.1374 
2022-03-16 11:35:51 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 3.2701, stu_CELoss: 2.0344 KDLoss: 1.2357 
2022-03-16 11:36:24 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 3.2381, stu_CELoss: 2.0122 KDLoss: 1.2259 
2022-03-16 11:36:57 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 3.0768, stu_CELoss: 1.8984 KDLoss: 1.1784 
2022-03-16 11:37:30 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 3.2383, stu_CELoss: 1.9610 KDLoss: 1.2772 
2022-03-16 11:38:03 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 3.4129, stu_CELoss: 2.0821 KDLoss: 1.3307 
2022-03-16 11:38:36 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 3.3551, stu_CELoss: 2.0762 KDLoss: 1.2788 
2022-03-16 11:39:08 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 3.2366, stu_CELoss: 1.9801 KDLoss: 1.2564 
2022-03-16 11:39:41 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 3.1764, stu_CELoss: 1.9615 KDLoss: 1.2149 
2022-03-16 11:40:13 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 3.2359, stu_CELoss: 1.9951 KDLoss: 1.2408 
2022-03-16 11:40:46 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 3.0346, stu_CELoss: 1.8691 KDLoss: 1.1655 
2022-03-16 11:41:19 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 3.3238, stu_CELoss: 2.0619 KDLoss: 1.2618 
2022-03-16 11:41:51 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 3.1165, stu_CELoss: 1.9533 KDLoss: 1.1632 
2022-03-16 11:42:24 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 3.2252, stu_CELoss: 1.9438 KDLoss: 1.2814 
2022-03-16 11:42:57 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 3.4780, stu_CELoss: 2.1354 KDLoss: 1.3427 
2022-03-16 11:43:30 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 3.4341, stu_CELoss: 2.1169 KDLoss: 1.3172 
2022-03-16 11:44:03 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 3.2462, stu_CELoss: 2.0323 KDLoss: 1.2139 
2022-03-16 11:44:36 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 3.2109, stu_CELoss: 1.9151 KDLoss: 1.2958 
2022-03-16 11:45:08 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 3.3347, stu_CELoss: 2.0753 KDLoss: 1.2593 
2022-03-16 11:45:41 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 3.5297, stu_CELoss: 2.1778 KDLoss: 1.3518 
2022-03-16 11:46:14 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 3.1996, stu_CELoss: 1.9796 KDLoss: 1.2201 
2022-03-16 11:46:46 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 3.2557, stu_CELoss: 1.9868 KDLoss: 1.2689 
2022-03-16 11:47:19 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 3.4243, stu_CELoss: 2.1119 KDLoss: 1.3124 
2022-03-16 11:47:52 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 3.5116, stu_CELoss: 2.1277 KDLoss: 1.3840 
2022-03-16 11:48:24 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 3.5302, stu_CELoss: 2.1862 KDLoss: 1.3440 
2022-03-16 11:48:57 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 3.4790, stu_CELoss: 2.1047 KDLoss: 1.3743 
2022-03-16 11:49:30 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.9622, stu_CELoss: 1.8604 KDLoss: 1.1018 
2022-03-16 11:50:03 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 3.3023, stu_CELoss: 1.9822 KDLoss: 1.3201 
2022-03-16 11:50:35 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.3287, stu_CELoss: 2.0871 KDLoss: 1.2417 
2022-03-16 11:51:08 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 3.2703, stu_CELoss: 1.9818 KDLoss: 1.2885 
2022-03-16 11:51:41 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 3.1887, stu_CELoss: 1.9219 KDLoss: 1.2668 
2022-03-16 11:52:14 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 3.2148, stu_CELoss: 1.9910 KDLoss: 1.2239 
2022-03-16 11:52:47 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 3.1745, stu_CELoss: 1.9694 KDLoss: 1.2051 
2022-03-16 11:53:20 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 3.2361, stu_CELoss: 1.9682 KDLoss: 1.2679 
2022-03-16 11:53:53 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 3.3987, stu_CELoss: 2.0715 KDLoss: 1.3272 
2022-03-16 11:54:26 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 3.4612, stu_CELoss: 2.0863 KDLoss: 1.3748 
2022-03-16 11:54:58 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 3.3666, stu_CELoss: 2.0322 KDLoss: 1.3344 
2022-03-16 11:55:31 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 3.4407, stu_CELoss: 2.1108 KDLoss: 1.3299 
2022-03-16 11:56:04 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 3.6902, stu_CELoss: 2.2824 KDLoss: 1.4078 
2022-03-16 11:56:37 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 3.4747, stu_CELoss: 2.0778 KDLoss: 1.3969 
2022-03-16 11:57:09 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.7814, stu_CELoss: 1.6968 KDLoss: 1.0846 
2022-03-16 11:57:42 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 3.4496, stu_CELoss: 2.0610 KDLoss: 1.3886 
2022-03-16 11:58:15 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 3.6315, stu_CELoss: 2.2131 KDLoss: 1.4184 
2022-03-16 11:58:47 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.9207, stu_CELoss: 1.8122 KDLoss: 1.1085 
2022-03-16 11:58:49 - train: epoch 029, train_loss: 3.3226
2022-03-16 12:01:19 - eval: epoch: 029, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 53.662%, stu_acc5: 78.846%, stu_test_loss: 1.9871
2022-03-16 12:01:20 - until epoch: 029, tea_best_acc1: 78.334%, stu_best_acc1: 56.694%
2022-03-16 12:01:20 - epoch 030 lr: 0.1
2022-03-16 12:01:58 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 3.2950, stu_CELoss: 2.0554 KDLoss: 1.2396 
2022-03-16 12:02:31 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 3.1637, stu_CELoss: 1.8734 KDLoss: 1.2903 
2022-03-16 12:03:03 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 3.0615, stu_CELoss: 1.8441 KDLoss: 1.2174 
2022-03-16 12:03:36 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 3.1959, stu_CELoss: 1.9656 KDLoss: 1.2303 
2022-03-16 12:04:09 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.4671, stu_CELoss: 2.1086 KDLoss: 1.3585 
2022-03-16 12:04:42 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 3.1964, stu_CELoss: 1.9064 KDLoss: 1.2899 
2022-03-16 12:05:14 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 3.0397, stu_CELoss: 1.8635 KDLoss: 1.1762 
2022-03-16 12:05:47 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 3.4896, stu_CELoss: 2.1576 KDLoss: 1.3320 
2022-03-16 12:06:20 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 3.4280, stu_CELoss: 2.1114 KDLoss: 1.3166 
2022-03-16 12:06:53 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 3.0173, stu_CELoss: 1.8373 KDLoss: 1.1800 
2022-03-16 12:07:26 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 3.0515, stu_CELoss: 1.9059 KDLoss: 1.1457 
2022-03-16 12:07:59 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 3.4724, stu_CELoss: 2.1164 KDLoss: 1.3560 
2022-03-16 12:08:32 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 3.0746, stu_CELoss: 1.8859 KDLoss: 1.1887 
2022-03-16 12:09:04 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 3.1650, stu_CELoss: 1.8936 KDLoss: 1.2714 
2022-03-16 12:09:37 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 3.2510, stu_CELoss: 1.9599 KDLoss: 1.2911 
2022-03-16 12:10:10 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 3.3837, stu_CELoss: 2.0171 KDLoss: 1.3665 
2022-03-16 12:10:43 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 3.4483, stu_CELoss: 2.1492 KDLoss: 1.2990 
2022-03-16 12:11:16 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 3.5084, stu_CELoss: 2.1203 KDLoss: 1.3881 
2022-03-16 12:11:49 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 3.3775, stu_CELoss: 2.0919 KDLoss: 1.2856 
2022-03-16 12:12:22 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 3.2317, stu_CELoss: 2.0127 KDLoss: 1.2191 
2022-03-16 12:12:55 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 3.5615, stu_CELoss: 2.1647 KDLoss: 1.3968 
2022-03-16 12:13:28 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 3.3475, stu_CELoss: 2.0745 KDLoss: 1.2731 
2022-03-16 12:14:01 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 3.3474, stu_CELoss: 2.0192 KDLoss: 1.3282 
2022-03-16 12:14:34 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 3.5433, stu_CELoss: 2.1971 KDLoss: 1.3462 
2022-03-16 12:15:07 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 3.4864, stu_CELoss: 2.2024 KDLoss: 1.2839 
2022-03-16 12:15:40 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 3.5133, stu_CELoss: 2.1676 KDLoss: 1.3457 
2022-03-16 12:16:13 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 3.1400, stu_CELoss: 1.9615 KDLoss: 1.1785 
2022-03-16 12:16:46 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 3.2344, stu_CELoss: 2.0330 KDLoss: 1.2014 
2022-03-16 12:17:19 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 3.2392, stu_CELoss: 1.9632 KDLoss: 1.2759 
2022-03-16 12:17:52 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 3.6182, stu_CELoss: 2.2558 KDLoss: 1.3623 
2022-03-16 12:18:25 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 3.4343, stu_CELoss: 2.0685 KDLoss: 1.3658 
2022-03-16 12:18:57 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 3.1748, stu_CELoss: 1.9156 KDLoss: 1.2592 
2022-03-16 12:19:30 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.6550, stu_CELoss: 2.2222 KDLoss: 1.4328 
2022-03-16 12:20:03 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 3.3183, stu_CELoss: 1.9674 KDLoss: 1.3508 
2022-03-16 12:20:36 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 3.2040, stu_CELoss: 2.0519 KDLoss: 1.1521 
2022-03-16 12:21:09 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 3.3756, stu_CELoss: 2.0399 KDLoss: 1.3357 
2022-03-16 12:21:42 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 3.2330, stu_CELoss: 1.9826 KDLoss: 1.2504 
2022-03-16 12:22:14 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 3.2990, stu_CELoss: 1.9910 KDLoss: 1.3080 
2022-03-16 12:22:47 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 3.2183, stu_CELoss: 1.9434 KDLoss: 1.2749 
2022-03-16 12:23:20 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 3.2144, stu_CELoss: 2.0359 KDLoss: 1.1785 
2022-03-16 12:23:53 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 3.3619, stu_CELoss: 2.0610 KDLoss: 1.3009 
2022-03-16 12:24:25 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.4576, stu_CELoss: 2.1306 KDLoss: 1.3270 
2022-03-16 12:24:58 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 3.0779, stu_CELoss: 1.9092 KDLoss: 1.1686 
2022-03-16 12:25:31 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 3.2137, stu_CELoss: 1.9630 KDLoss: 1.2507 
2022-03-16 12:26:03 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 3.5767, stu_CELoss: 2.1520 KDLoss: 1.4247 
2022-03-16 12:26:36 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 3.0963, stu_CELoss: 1.8880 KDLoss: 1.2082 
2022-03-16 12:27:09 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 3.0967, stu_CELoss: 1.8878 KDLoss: 1.2088 
2022-03-16 12:27:41 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 3.2995, stu_CELoss: 2.0255 KDLoss: 1.2739 
2022-03-16 12:28:14 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 3.5699, stu_CELoss: 2.2275 KDLoss: 1.3425 
2022-03-16 12:28:46 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 3.3308, stu_CELoss: 2.0194 KDLoss: 1.3114 
2022-03-16 12:28:48 - train: epoch 030, train_loss: 3.3148
2022-03-16 12:31:17 - eval: epoch: 030, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 55.698%, stu_acc5: 80.428%, stu_test_loss: 1.8746
2022-03-16 12:31:19 - until epoch: 030, tea_best_acc1: 78.334%, stu_best_acc1: 56.694%
2022-03-16 12:31:19 - epoch 031 lr: 0.010000000000000002
2022-03-16 12:31:57 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.9233, stu_CELoss: 1.8424 KDLoss: 1.0809 
2022-03-16 12:32:30 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.5448, stu_CELoss: 1.6431 KDLoss: 0.9017 
2022-03-16 12:33:02 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.3625, stu_CELoss: 1.5063 KDLoss: 0.8561 
2022-03-16 12:33:35 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.4276, stu_CELoss: 1.5811 KDLoss: 0.8465 
2022-03-16 12:34:07 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.6207, stu_CELoss: 1.7051 KDLoss: 0.9156 
2022-03-16 12:34:40 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.4586, stu_CELoss: 1.6146 KDLoss: 0.8439 
2022-03-16 12:35:13 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.4793, stu_CELoss: 1.6264 KDLoss: 0.8529 
2022-03-16 12:35:46 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.3706, stu_CELoss: 1.5536 KDLoss: 0.8170 
2022-03-16 12:36:19 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.3642, stu_CELoss: 1.5683 KDLoss: 0.7959 
2022-03-16 12:36:52 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.6762, stu_CELoss: 1.7656 KDLoss: 0.9107 
2022-03-16 12:37:25 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.5763, stu_CELoss: 1.6559 KDLoss: 0.9203 
2022-03-16 12:37:58 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.4434, stu_CELoss: 1.5247 KDLoss: 0.9187 
2022-03-16 12:38:31 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.9018, stu_CELoss: 1.2326 KDLoss: 0.6692 
2022-03-16 12:39:04 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.3711, stu_CELoss: 1.5227 KDLoss: 0.8484 
2022-03-16 12:39:36 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.5743, stu_CELoss: 1.6452 KDLoss: 0.9291 
2022-03-16 12:40:09 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.2697, stu_CELoss: 1.5270 KDLoss: 0.7427 
2022-03-16 12:40:42 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.3181, stu_CELoss: 1.5224 KDLoss: 0.7957 
2022-03-16 12:41:15 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.0289, stu_CELoss: 1.2512 KDLoss: 0.7776 
2022-03-16 12:41:48 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.3133, stu_CELoss: 1.4706 KDLoss: 0.8427 
2022-03-16 12:42:21 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.3649, stu_CELoss: 1.5383 KDLoss: 0.8265 
2022-03-16 12:42:54 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.0513, stu_CELoss: 1.3486 KDLoss: 0.7027 
2022-03-16 12:43:27 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.2174, stu_CELoss: 1.4395 KDLoss: 0.7779 
2022-03-16 12:44:00 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.2153, stu_CELoss: 1.5044 KDLoss: 0.7109 
2022-03-16 12:44:33 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.1239, stu_CELoss: 1.4041 KDLoss: 0.7198 
2022-03-16 12:45:06 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.4553, stu_CELoss: 1.7003 KDLoss: 0.7549 
2022-03-16 12:45:39 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.3483, stu_CELoss: 1.5412 KDLoss: 0.8071 
2022-03-16 12:46:12 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.4623, stu_CELoss: 1.6125 KDLoss: 0.8497 
2022-03-16 12:46:44 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.4814, stu_CELoss: 1.6244 KDLoss: 0.8570 
2022-03-16 12:47:17 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.4375, stu_CELoss: 1.6703 KDLoss: 0.7672 
2022-03-16 12:47:50 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.9628, stu_CELoss: 1.9467 KDLoss: 1.0161 
2022-03-16 12:48:22 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.2936, stu_CELoss: 1.5343 KDLoss: 0.7593 
2022-03-16 12:48:55 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.3750, stu_CELoss: 1.5414 KDLoss: 0.8336 
2022-03-16 12:49:28 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.1039, stu_CELoss: 1.4514 KDLoss: 0.6525 
2022-03-16 12:50:01 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.4672, stu_CELoss: 1.7358 KDLoss: 0.7314 
2022-03-16 12:50:33 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.4645, stu_CELoss: 1.6099 KDLoss: 0.8546 
2022-03-16 12:51:06 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.2387, stu_CELoss: 1.4883 KDLoss: 0.7503 
2022-03-16 12:51:39 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.1657, stu_CELoss: 1.4368 KDLoss: 0.7289 
2022-03-16 12:52:12 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.0442, stu_CELoss: 1.3576 KDLoss: 0.6866 
2022-03-16 12:52:45 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.1680, stu_CELoss: 1.3944 KDLoss: 0.7736 
2022-03-16 12:53:18 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.1886, stu_CELoss: 1.4312 KDLoss: 0.7574 
2022-03-16 12:53:50 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.1594, stu_CELoss: 1.5221 KDLoss: 0.6372 
2022-03-16 12:54:23 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.2828, stu_CELoss: 1.5161 KDLoss: 0.7667 
2022-03-16 12:54:56 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.0950, stu_CELoss: 1.4447 KDLoss: 0.6503 
2022-03-16 12:55:29 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.5122, stu_CELoss: 1.7192 KDLoss: 0.7929 
2022-03-16 12:56:02 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.1027, stu_CELoss: 1.4045 KDLoss: 0.6982 
2022-03-16 12:56:35 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.1555, stu_CELoss: 1.4277 KDLoss: 0.7278 
2022-03-16 12:57:08 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.9146, stu_CELoss: 1.2760 KDLoss: 0.6387 
2022-03-16 12:57:41 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.0500, stu_CELoss: 1.4099 KDLoss: 0.6402 
2022-03-16 12:58:13 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.2361, stu_CELoss: 1.5014 KDLoss: 0.7347 
2022-03-16 12:58:45 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.0028, stu_CELoss: 1.3832 KDLoss: 0.6196 
2022-03-16 12:58:47 - train: epoch 031, train_loss: 2.3047
2022-03-16 13:01:17 - eval: epoch: 031, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 69.958%, stu_acc5: 89.554%, stu_test_loss: 1.2088
2022-03-16 13:01:19 - until epoch: 031, tea_best_acc1: 78.334%, stu_best_acc1: 69.958%
2022-03-16 13:01:19 - epoch 032 lr: 0.010000000000000002
2022-03-16 13:01:56 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.8592, stu_CELoss: 1.2356 KDLoss: 0.6235 
2022-03-16 13:02:28 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.9273, stu_CELoss: 1.2714 KDLoss: 0.6560 
2022-03-16 13:03:01 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.9546, stu_CELoss: 1.3010 KDLoss: 0.6536 
2022-03-16 13:03:33 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.3092, stu_CELoss: 1.5584 KDLoss: 0.7508 
2022-03-16 13:04:06 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.0492, stu_CELoss: 1.3539 KDLoss: 0.6953 
2022-03-16 13:04:38 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.2375, stu_CELoss: 1.4885 KDLoss: 0.7491 
2022-03-16 13:05:11 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.9453, stu_CELoss: 1.3081 KDLoss: 0.6372 
2022-03-16 13:05:43 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.2658, stu_CELoss: 1.5541 KDLoss: 0.7117 
2022-03-16 13:06:16 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.1154, stu_CELoss: 1.3373 KDLoss: 0.7780 
2022-03-16 13:06:49 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.4187, stu_CELoss: 1.6169 KDLoss: 0.8018 
2022-03-16 13:07:22 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.0714, stu_CELoss: 1.3907 KDLoss: 0.6807 
2022-03-16 13:07:55 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.1242, stu_CELoss: 1.4174 KDLoss: 0.7068 
2022-03-16 13:08:27 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.1573, stu_CELoss: 1.4265 KDLoss: 0.7308 
2022-03-16 13:09:00 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.1489, stu_CELoss: 1.4615 KDLoss: 0.6874 
2022-03-16 13:09:33 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.1444, stu_CELoss: 1.4868 KDLoss: 0.6576 
2022-03-16 13:10:06 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.3594, stu_CELoss: 1.6223 KDLoss: 0.7371 
2022-03-16 13:10:39 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.2965, stu_CELoss: 1.5371 KDLoss: 0.7593 
2022-03-16 13:11:12 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.6419, stu_CELoss: 1.8260 KDLoss: 0.8159 
2022-03-16 13:11:45 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.7671, stu_CELoss: 1.1484 KDLoss: 0.6187 
2022-03-16 13:12:18 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.0002, stu_CELoss: 1.2867 KDLoss: 0.7135 
2022-03-16 13:12:51 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.9737, stu_CELoss: 1.2890 KDLoss: 0.6847 
2022-03-16 13:13:24 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.1293, stu_CELoss: 1.4391 KDLoss: 0.6902 
2022-03-16 13:13:57 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.0368, stu_CELoss: 1.4236 KDLoss: 0.6132 
2022-03-16 13:14:30 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.9426, stu_CELoss: 1.2585 KDLoss: 0.6841 
2022-03-16 13:15:03 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.0150, stu_CELoss: 1.3833 KDLoss: 0.6317 
2022-03-16 13:15:36 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.7712, stu_CELoss: 1.2106 KDLoss: 0.5606 
2022-03-16 13:16:09 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.9268, stu_CELoss: 1.2716 KDLoss: 0.6552 
2022-03-16 13:16:42 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.0905, stu_CELoss: 1.4590 KDLoss: 0.6314 
2022-03-16 13:17:15 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.9187, stu_CELoss: 1.2821 KDLoss: 0.6366 
2022-03-16 13:17:48 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.8010, stu_CELoss: 1.2086 KDLoss: 0.5923 
2022-03-16 13:18:20 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.2681, stu_CELoss: 1.5360 KDLoss: 0.7321 
2022-03-16 13:18:53 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.1570, stu_CELoss: 1.4834 KDLoss: 0.6736 
2022-03-16 13:19:26 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.1867, stu_CELoss: 1.4698 KDLoss: 0.7169 
2022-03-16 13:19:59 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.9182, stu_CELoss: 1.2673 KDLoss: 0.6509 
2022-03-16 13:20:32 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.9556, stu_CELoss: 1.3347 KDLoss: 0.6209 
2022-03-16 13:21:05 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.2189, stu_CELoss: 1.5478 KDLoss: 0.6710 
2022-03-16 13:21:38 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.1066, stu_CELoss: 1.4196 KDLoss: 0.6870 
2022-03-16 13:22:11 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.8628, stu_CELoss: 1.2342 KDLoss: 0.6287 
2022-03-16 13:22:44 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.7408, stu_CELoss: 1.1570 KDLoss: 0.5838 
2022-03-16 13:23:17 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.0629, stu_CELoss: 1.4572 KDLoss: 0.6057 
2022-03-16 13:23:50 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.8019, stu_CELoss: 1.2326 KDLoss: 0.5693 
2022-03-16 13:24:23 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.9138, stu_CELoss: 1.2785 KDLoss: 0.6353 
2022-03-16 13:24:56 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.2430, stu_CELoss: 1.5032 KDLoss: 0.7397 
2022-03-16 13:25:29 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.9020, stu_CELoss: 1.2324 KDLoss: 0.6696 
2022-03-16 13:26:02 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.2329, stu_CELoss: 1.5426 KDLoss: 0.6902 
2022-03-16 13:26:35 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.0427, stu_CELoss: 1.3879 KDLoss: 0.6548 
2022-03-16 13:27:08 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.9859, stu_CELoss: 1.4301 KDLoss: 0.5557 
2022-03-16 13:27:40 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.7939, stu_CELoss: 1.2109 KDLoss: 0.5831 
2022-03-16 13:28:13 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.0228, stu_CELoss: 1.3070 KDLoss: 0.7158 
2022-03-16 13:28:46 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.0737, stu_CELoss: 1.3652 KDLoss: 0.7085 
2022-03-16 13:28:47 - train: epoch 032, train_loss: 2.0838
2022-03-16 13:31:17 - eval: epoch: 032, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.816%, stu_acc5: 90.164%, stu_test_loss: 1.1699
2022-03-16 13:31:18 - until epoch: 032, tea_best_acc1: 78.334%, stu_best_acc1: 70.816%
2022-03-16 13:31:18 - epoch 033 lr: 0.010000000000000002
2022-03-16 13:31:56 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.7218, stu_CELoss: 1.1499 KDLoss: 0.5719 
2022-03-16 13:32:29 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.0474, stu_CELoss: 1.3823 KDLoss: 0.6652 
2022-03-16 13:33:01 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.0426, stu_CELoss: 1.3896 KDLoss: 0.6530 
2022-03-16 13:33:34 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.7702, stu_CELoss: 1.2462 KDLoss: 0.5240 
2022-03-16 13:34:07 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.1687, stu_CELoss: 1.4649 KDLoss: 0.7038 
2022-03-16 13:34:40 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 2.0429, stu_CELoss: 1.3719 KDLoss: 0.6710 
2022-03-16 13:35:13 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.1677, stu_CELoss: 1.4846 KDLoss: 0.6831 
2022-03-16 13:35:46 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.2059, stu_CELoss: 1.4838 KDLoss: 0.7221 
2022-03-16 13:36:19 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.9204, stu_CELoss: 1.3331 KDLoss: 0.5873 
2022-03-16 13:36:52 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.0531, stu_CELoss: 1.4112 KDLoss: 0.6419 
2022-03-16 13:37:25 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.6763, stu_CELoss: 1.1263 KDLoss: 0.5500 
2022-03-16 13:37:57 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.3700, stu_CELoss: 1.6361 KDLoss: 0.7339 
2022-03-16 13:38:30 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.0051, stu_CELoss: 1.3678 KDLoss: 0.6373 
2022-03-16 13:39:03 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.9538, stu_CELoss: 1.2546 KDLoss: 0.6992 
2022-03-16 13:39:36 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.2870, stu_CELoss: 1.6296 KDLoss: 0.6574 
2022-03-16 13:40:08 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.2542, stu_CELoss: 1.4973 KDLoss: 0.7569 
2022-03-16 13:40:41 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 2.0565, stu_CELoss: 1.4069 KDLoss: 0.6495 
2022-03-16 13:41:14 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.9612, stu_CELoss: 1.3096 KDLoss: 0.6516 
2022-03-16 13:41:46 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 2.0367, stu_CELoss: 1.4124 KDLoss: 0.6243 
2022-03-16 13:42:19 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.8284, stu_CELoss: 1.2091 KDLoss: 0.6193 
2022-03-16 13:42:52 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.1522, stu_CELoss: 1.4421 KDLoss: 0.7101 
2022-03-16 13:43:25 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0761, stu_CELoss: 1.4672 KDLoss: 0.6090 
2022-03-16 13:43:57 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.9803, stu_CELoss: 1.3368 KDLoss: 0.6435 
2022-03-16 13:44:30 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.2632, stu_CELoss: 1.6119 KDLoss: 0.6513 
2022-03-16 13:45:03 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.8648, stu_CELoss: 1.2676 KDLoss: 0.5973 
2022-03-16 13:45:35 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.7133, stu_CELoss: 1.1764 KDLoss: 0.5369 
2022-03-16 13:46:08 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.9788, stu_CELoss: 1.3461 KDLoss: 0.6327 
2022-03-16 13:46:41 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.9802, stu_CELoss: 1.3178 KDLoss: 0.6624 
2022-03-16 13:47:14 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.8742, stu_CELoss: 1.2797 KDLoss: 0.5945 
2022-03-16 13:47:46 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.9814, stu_CELoss: 1.3270 KDLoss: 0.6544 
2022-03-16 13:48:19 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.0372, stu_CELoss: 1.3512 KDLoss: 0.6860 
2022-03-16 13:48:52 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.1728, stu_CELoss: 1.4067 KDLoss: 0.7661 
2022-03-16 13:49:25 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.9686, stu_CELoss: 1.3667 KDLoss: 0.6019 
2022-03-16 13:49:58 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.8166, stu_CELoss: 1.2071 KDLoss: 0.6094 
2022-03-16 13:50:30 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.1307, stu_CELoss: 1.3772 KDLoss: 0.7536 
2022-03-16 13:51:03 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.2633, stu_CELoss: 1.4909 KDLoss: 0.7724 
2022-03-16 13:51:35 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.7019, stu_CELoss: 1.1402 KDLoss: 0.5616 
2022-03-16 13:52:08 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.9680, stu_CELoss: 1.3540 KDLoss: 0.6140 
2022-03-16 13:52:41 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.2572, stu_CELoss: 1.5923 KDLoss: 0.6649 
2022-03-16 13:53:14 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.1909, stu_CELoss: 1.5324 KDLoss: 0.6585 
2022-03-16 13:53:47 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.0408, stu_CELoss: 1.3967 KDLoss: 0.6441 
2022-03-16 13:54:19 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.6821, stu_CELoss: 1.1006 KDLoss: 0.5815 
2022-03-16 13:54:52 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.0839, stu_CELoss: 1.4544 KDLoss: 0.6295 
2022-03-16 13:55:24 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0902, stu_CELoss: 1.4121 KDLoss: 0.6782 
2022-03-16 13:55:57 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.2049, stu_CELoss: 1.5071 KDLoss: 0.6977 
2022-03-16 13:56:30 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.0287, stu_CELoss: 1.4056 KDLoss: 0.6231 
2022-03-16 13:57:02 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.9998, stu_CELoss: 1.3905 KDLoss: 0.6094 
2022-03-16 13:57:35 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.3167, stu_CELoss: 1.6095 KDLoss: 0.7072 
2022-03-16 13:58:08 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.9162, stu_CELoss: 1.3488 KDLoss: 0.5674 
2022-03-16 13:58:40 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.2441, stu_CELoss: 1.5658 KDLoss: 0.6783 
2022-03-16 13:58:42 - train: epoch 033, train_loss: 1.9964
2022-03-16 14:01:12 - eval: epoch: 033, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.124%, stu_acc5: 90.310%, stu_test_loss: 1.1529
2022-03-16 14:01:13 - until epoch: 033, tea_best_acc1: 78.334%, stu_best_acc1: 71.124%
2022-03-16 14:01:13 - epoch 034 lr: 0.010000000000000002
2022-03-16 14:01:51 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.9001, stu_CELoss: 1.2982 KDLoss: 0.6019 
2022-03-16 14:02:23 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.0235, stu_CELoss: 1.3925 KDLoss: 0.6310 
2022-03-16 14:02:56 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.9495, stu_CELoss: 1.3457 KDLoss: 0.6038 
2022-03-16 14:03:29 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.6705, stu_CELoss: 1.1685 KDLoss: 0.5020 
2022-03-16 14:04:01 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.8396, stu_CELoss: 1.2698 KDLoss: 0.5698 
2022-03-16 14:04:34 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.0715, stu_CELoss: 1.4101 KDLoss: 0.6614 
2022-03-16 14:05:06 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.8277, stu_CELoss: 1.2211 KDLoss: 0.6066 
2022-03-16 14:05:39 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.0764, stu_CELoss: 1.4301 KDLoss: 0.6463 
2022-03-16 14:06:11 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.7894, stu_CELoss: 1.2005 KDLoss: 0.5890 
2022-03-16 14:06:44 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.8537, stu_CELoss: 1.3093 KDLoss: 0.5445 
2022-03-16 14:07:16 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.9698, stu_CELoss: 1.2840 KDLoss: 0.6857 
2022-03-16 14:07:49 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.0292, stu_CELoss: 1.4005 KDLoss: 0.6287 
2022-03-16 14:08:21 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.8035, stu_CELoss: 1.1745 KDLoss: 0.6290 
2022-03-16 14:08:54 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.8370, stu_CELoss: 1.2192 KDLoss: 0.6177 
2022-03-16 14:09:26 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.8879, stu_CELoss: 1.3073 KDLoss: 0.5806 
2022-03-16 14:09:59 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.0151, stu_CELoss: 1.3472 KDLoss: 0.6680 
2022-03-16 14:10:32 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.0661, stu_CELoss: 1.3931 KDLoss: 0.6730 
2022-03-16 14:11:04 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1025, stu_CELoss: 1.4469 KDLoss: 0.6556 
2022-03-16 14:11:37 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.0258, stu_CELoss: 1.4078 KDLoss: 0.6180 
2022-03-16 14:12:10 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.9319, stu_CELoss: 1.2903 KDLoss: 0.6417 
2022-03-16 14:12:43 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.9922, stu_CELoss: 1.3380 KDLoss: 0.6543 
2022-03-16 14:13:16 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7607, stu_CELoss: 1.2405 KDLoss: 0.5202 
2022-03-16 14:13:48 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.9924, stu_CELoss: 1.4368 KDLoss: 0.5556 
2022-03-16 14:14:21 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.7257, stu_CELoss: 1.1859 KDLoss: 0.5398 
2022-03-16 14:14:54 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.1138, stu_CELoss: 1.4324 KDLoss: 0.6815 
2022-03-16 14:15:27 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.2480, stu_CELoss: 1.5430 KDLoss: 0.7051 
2022-03-16 14:16:00 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.1191, stu_CELoss: 1.4848 KDLoss: 0.6343 
2022-03-16 14:16:32 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.6443, stu_CELoss: 1.1353 KDLoss: 0.5090 
2022-03-16 14:17:05 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.7626, stu_CELoss: 1.1945 KDLoss: 0.5681 
2022-03-16 14:17:38 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.6744, stu_CELoss: 1.1246 KDLoss: 0.5498 
2022-03-16 14:18:11 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.8124, stu_CELoss: 1.2050 KDLoss: 0.6074 
2022-03-16 14:18:44 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.0373, stu_CELoss: 1.3732 KDLoss: 0.6640 
2022-03-16 14:19:16 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.8335, stu_CELoss: 1.2489 KDLoss: 0.5847 
2022-03-16 14:19:49 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.1696, stu_CELoss: 1.5510 KDLoss: 0.6186 
2022-03-16 14:20:22 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.8963, stu_CELoss: 1.3039 KDLoss: 0.5924 
2022-03-16 14:20:55 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.6391, stu_CELoss: 1.1191 KDLoss: 0.5200 
2022-03-16 14:21:28 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.9510, stu_CELoss: 1.3596 KDLoss: 0.5913 
2022-03-16 14:22:00 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.7838, stu_CELoss: 1.2196 KDLoss: 0.5642 
2022-03-16 14:22:33 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.3838, stu_CELoss: 1.7010 KDLoss: 0.6828 
2022-03-16 14:23:05 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.9334, stu_CELoss: 1.3222 KDLoss: 0.6112 
2022-03-16 14:23:38 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.1379, stu_CELoss: 1.5010 KDLoss: 0.6369 
2022-03-16 14:24:11 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.8768, stu_CELoss: 1.2354 KDLoss: 0.6414 
2022-03-16 14:24:43 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.7883, stu_CELoss: 1.1757 KDLoss: 0.6127 
2022-03-16 14:25:16 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6889, stu_CELoss: 1.1313 KDLoss: 0.5576 
2022-03-16 14:25:49 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.0126, stu_CELoss: 1.4259 KDLoss: 0.5867 
2022-03-16 14:26:21 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.1680, stu_CELoss: 1.5162 KDLoss: 0.6518 
2022-03-16 14:26:54 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.0207, stu_CELoss: 1.3790 KDLoss: 0.6417 
2022-03-16 14:27:26 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.1258, stu_CELoss: 1.3999 KDLoss: 0.7260 
2022-03-16 14:27:59 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.0457, stu_CELoss: 1.4427 KDLoss: 0.6030 
2022-03-16 14:28:31 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.8002, stu_CELoss: 1.2002 KDLoss: 0.5999 
2022-03-16 14:28:33 - train: epoch 034, train_loss: 1.9424
2022-03-16 14:31:03 - eval: epoch: 034, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.560%, stu_acc5: 89.866%, stu_test_loss: 1.1861
2022-03-16 14:31:04 - until epoch: 034, tea_best_acc1: 78.334%, stu_best_acc1: 71.124%
2022-03-16 14:31:04 - epoch 035 lr: 0.010000000000000002
2022-03-16 14:31:43 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.7446, stu_CELoss: 1.2264 KDLoss: 0.5182 
2022-03-16 14:32:15 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.6746, stu_CELoss: 1.1246 KDLoss: 0.5500 
2022-03-16 14:32:48 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.8527, stu_CELoss: 1.2858 KDLoss: 0.5669 
2022-03-16 14:33:20 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.8332, stu_CELoss: 1.2119 KDLoss: 0.6212 
2022-03-16 14:33:53 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.9755, stu_CELoss: 1.3380 KDLoss: 0.6375 
2022-03-16 14:34:26 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.9123, stu_CELoss: 1.2749 KDLoss: 0.6374 
2022-03-16 14:34:59 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.6876, stu_CELoss: 1.1631 KDLoss: 0.5245 
2022-03-16 14:35:31 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.8926, stu_CELoss: 1.2619 KDLoss: 0.6307 
2022-03-16 14:36:04 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.0958, stu_CELoss: 1.4297 KDLoss: 0.6661 
2022-03-16 14:36:37 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.5944, stu_CELoss: 1.0975 KDLoss: 0.4968 
2022-03-16 14:37:10 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.0390, stu_CELoss: 1.3955 KDLoss: 0.6436 
2022-03-16 14:37:42 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.6237, stu_CELoss: 1.0941 KDLoss: 0.5296 
2022-03-16 14:38:14 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9356, stu_CELoss: 1.3380 KDLoss: 0.5976 
2022-03-16 14:38:47 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.0370, stu_CELoss: 1.3953 KDLoss: 0.6418 
2022-03-16 14:39:20 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.9682, stu_CELoss: 1.3732 KDLoss: 0.5950 
2022-03-16 14:39:53 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.7389, stu_CELoss: 1.1862 KDLoss: 0.5527 
2022-03-16 14:40:26 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.6680, stu_CELoss: 1.1282 KDLoss: 0.5398 
2022-03-16 14:40:59 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.1012, stu_CELoss: 1.4257 KDLoss: 0.6756 
2022-03-16 14:41:32 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.9165, stu_CELoss: 1.3114 KDLoss: 0.6050 
2022-03-16 14:42:05 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.0616, stu_CELoss: 1.4262 KDLoss: 0.6355 
2022-03-16 14:42:37 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.0612, stu_CELoss: 1.4305 KDLoss: 0.6307 
2022-03-16 14:43:10 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.8697, stu_CELoss: 1.2739 KDLoss: 0.5959 
2022-03-16 14:43:42 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.8523, stu_CELoss: 1.2982 KDLoss: 0.5541 
2022-03-16 14:44:15 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.0078, stu_CELoss: 1.3554 KDLoss: 0.6524 
2022-03-16 14:44:48 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.9309, stu_CELoss: 1.3125 KDLoss: 0.6184 
2022-03-16 14:45:20 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.8794, stu_CELoss: 1.2695 KDLoss: 0.6099 
2022-03-16 14:45:53 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.1067, stu_CELoss: 1.4252 KDLoss: 0.6815 
2022-03-16 14:46:25 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.1014, stu_CELoss: 1.4325 KDLoss: 0.6689 
2022-03-16 14:46:57 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.7355, stu_CELoss: 1.2015 KDLoss: 0.5340 
2022-03-16 14:47:29 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7512, stu_CELoss: 1.1852 KDLoss: 0.5660 
2022-03-16 14:48:01 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.0171, stu_CELoss: 1.4103 KDLoss: 0.6067 
2022-03-16 14:48:33 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.9074, stu_CELoss: 1.2982 KDLoss: 0.6092 
2022-03-16 14:49:06 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.9134, stu_CELoss: 1.3340 KDLoss: 0.5794 
2022-03-16 14:49:38 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.3197, stu_CELoss: 1.6188 KDLoss: 0.7009 
2022-03-16 14:50:11 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.4764, stu_CELoss: 1.0494 KDLoss: 0.4271 
2022-03-16 14:50:43 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.0206, stu_CELoss: 1.3435 KDLoss: 0.6771 
2022-03-16 14:51:15 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.7831, stu_CELoss: 1.2014 KDLoss: 0.5817 
2022-03-16 14:51:48 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5357, stu_CELoss: 1.0512 KDLoss: 0.4845 
2022-03-16 14:52:20 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.0724, stu_CELoss: 1.3844 KDLoss: 0.6880 
2022-03-16 14:52:52 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.7509, stu_CELoss: 1.2143 KDLoss: 0.5367 
2022-03-16 14:53:25 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.0367, stu_CELoss: 1.3518 KDLoss: 0.6850 
2022-03-16 14:53:57 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.8812, stu_CELoss: 1.3009 KDLoss: 0.5803 
2022-03-16 14:54:29 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.0219, stu_CELoss: 1.3664 KDLoss: 0.6556 
2022-03-16 14:55:02 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.0719, stu_CELoss: 1.4105 KDLoss: 0.6614 
2022-03-16 14:55:34 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.9989, stu_CELoss: 1.3519 KDLoss: 0.6470 
2022-03-16 14:56:07 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.0501, stu_CELoss: 1.3569 KDLoss: 0.6933 
2022-03-16 14:56:39 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.0467, stu_CELoss: 1.4040 KDLoss: 0.6427 
2022-03-16 14:57:12 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.0520, stu_CELoss: 1.3799 KDLoss: 0.6721 
2022-03-16 14:57:44 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.9407, stu_CELoss: 1.3180 KDLoss: 0.6227 
2022-03-16 14:58:16 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.8328, stu_CELoss: 1.3139 KDLoss: 0.5189 
2022-03-16 14:58:17 - train: epoch 035, train_loss: 1.9061
2022-03-16 15:00:46 - eval: epoch: 035, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.836%, stu_acc5: 90.650%, stu_test_loss: 1.1252
2022-03-16 15:00:47 - until epoch: 035, tea_best_acc1: 78.334%, stu_best_acc1: 71.836%
2022-03-16 15:00:47 - epoch 036 lr: 0.010000000000000002
2022-03-16 15:01:26 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.0074, stu_CELoss: 1.4164 KDLoss: 0.5910 
2022-03-16 15:01:58 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.5686, stu_CELoss: 1.0037 KDLoss: 0.5649 
2022-03-16 15:02:31 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.5168, stu_CELoss: 1.0622 KDLoss: 0.4546 
2022-03-16 15:03:03 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.9476, stu_CELoss: 1.3290 KDLoss: 0.6186 
2022-03-16 15:03:36 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.8588, stu_CELoss: 1.2790 KDLoss: 0.5798 
2022-03-16 15:04:08 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.8329, stu_CELoss: 1.2517 KDLoss: 0.5812 
2022-03-16 15:04:41 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.7778, stu_CELoss: 1.1856 KDLoss: 0.5922 
2022-03-16 15:05:14 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.6588, stu_CELoss: 1.1068 KDLoss: 0.5521 
2022-03-16 15:05:46 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.8289, stu_CELoss: 1.2755 KDLoss: 0.5534 
2022-03-16 15:06:19 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.7207, stu_CELoss: 1.1822 KDLoss: 0.5386 
2022-03-16 15:06:51 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7652, stu_CELoss: 1.2306 KDLoss: 0.5345 
2022-03-16 15:07:24 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.8548, stu_CELoss: 1.2853 KDLoss: 0.5695 
2022-03-16 15:07:56 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.9066, stu_CELoss: 1.3136 KDLoss: 0.5930 
2022-03-16 15:08:29 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8847, stu_CELoss: 1.2967 KDLoss: 0.5881 
2022-03-16 15:09:02 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.0031, stu_CELoss: 1.3403 KDLoss: 0.6628 
2022-03-16 15:09:34 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.0506, stu_CELoss: 1.4349 KDLoss: 0.6157 
2022-03-16 15:10:07 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.8260, stu_CELoss: 1.2630 KDLoss: 0.5630 
2022-03-16 15:10:39 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.6840, stu_CELoss: 1.1742 KDLoss: 0.5098 
2022-03-16 15:11:12 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.2343, stu_CELoss: 1.5109 KDLoss: 0.7235 
2022-03-16 15:11:45 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.8255, stu_CELoss: 1.2703 KDLoss: 0.5552 
2022-03-16 15:12:17 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.8711, stu_CELoss: 1.2994 KDLoss: 0.5718 
2022-03-16 15:12:50 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.9318, stu_CELoss: 1.3179 KDLoss: 0.6140 
2022-03-16 15:13:22 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.1035, stu_CELoss: 1.4628 KDLoss: 0.6407 
2022-03-16 15:13:55 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.0814, stu_CELoss: 1.4714 KDLoss: 0.6100 
2022-03-16 15:14:28 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.7340, stu_CELoss: 1.1647 KDLoss: 0.5693 
2022-03-16 15:15:01 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.9639, stu_CELoss: 1.3694 KDLoss: 0.5945 
2022-03-16 15:15:33 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.7037, stu_CELoss: 1.2179 KDLoss: 0.4858 
2022-03-16 15:16:05 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.9285, stu_CELoss: 1.2923 KDLoss: 0.6362 
2022-03-16 15:16:38 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.7416, stu_CELoss: 1.1896 KDLoss: 0.5520 
2022-03-16 15:17:10 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.8480, stu_CELoss: 1.2366 KDLoss: 0.6114 
2022-03-16 15:17:43 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.9276, stu_CELoss: 1.3148 KDLoss: 0.6128 
2022-03-16 15:18:15 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.0331, stu_CELoss: 1.4341 KDLoss: 0.5990 
2022-03-16 15:18:47 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.7996, stu_CELoss: 1.2723 KDLoss: 0.5273 
2022-03-16 15:19:20 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.7461, stu_CELoss: 1.2366 KDLoss: 0.5095 
2022-03-16 15:19:52 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.1662, stu_CELoss: 1.5116 KDLoss: 0.6546 
2022-03-16 15:20:25 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.9555, stu_CELoss: 1.3344 KDLoss: 0.6211 
2022-03-16 15:20:57 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.8177, stu_CELoss: 1.2434 KDLoss: 0.5743 
2022-03-16 15:21:30 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.9691, stu_CELoss: 1.4057 KDLoss: 0.5634 
2022-03-16 15:22:02 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.0025, stu_CELoss: 1.3599 KDLoss: 0.6426 
2022-03-16 15:22:35 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.7822, stu_CELoss: 1.2429 KDLoss: 0.5393 
2022-03-16 15:23:08 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.9160, stu_CELoss: 1.2781 KDLoss: 0.6379 
2022-03-16 15:23:40 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.0725, stu_CELoss: 1.4066 KDLoss: 0.6659 
2022-03-16 15:24:13 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.6941, stu_CELoss: 1.1799 KDLoss: 0.5142 
2022-03-16 15:24:45 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.0970, stu_CELoss: 1.4374 KDLoss: 0.6597 
2022-03-16 15:25:18 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.9039, stu_CELoss: 1.2652 KDLoss: 0.6387 
2022-03-16 15:25:50 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.6698, stu_CELoss: 1.1229 KDLoss: 0.5469 
2022-03-16 15:26:23 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.7976, stu_CELoss: 1.1945 KDLoss: 0.6031 
2022-03-16 15:26:55 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.8208, stu_CELoss: 1.2281 KDLoss: 0.5927 
2022-03-16 15:27:28 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.9898, stu_CELoss: 1.2845 KDLoss: 0.7053 
2022-03-16 15:27:59 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.7823, stu_CELoss: 1.1966 KDLoss: 0.5857 
2022-03-16 15:28:01 - train: epoch 036, train_loss: 1.8786
2022-03-16 15:30:29 - eval: epoch: 036, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.814%, stu_acc5: 90.702%, stu_test_loss: 1.1258
2022-03-16 15:30:30 - until epoch: 036, tea_best_acc1: 78.334%, stu_best_acc1: 71.836%
2022-03-16 15:30:30 - epoch 037 lr: 0.010000000000000002
2022-03-16 15:31:08 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.6872, stu_CELoss: 1.1524 KDLoss: 0.5348 
2022-03-16 15:31:41 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.7336, stu_CELoss: 1.2250 KDLoss: 0.5086 
2022-03-16 15:32:14 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.7964, stu_CELoss: 1.2234 KDLoss: 0.5730 
2022-03-16 15:32:46 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5915, stu_CELoss: 1.0671 KDLoss: 0.5244 
2022-03-16 15:33:19 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.0146, stu_CELoss: 1.4243 KDLoss: 0.5903 
2022-03-16 15:33:51 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.2975, stu_CELoss: 1.6110 KDLoss: 0.6866 
2022-03-16 15:34:24 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.0568, stu_CELoss: 1.3959 KDLoss: 0.6609 
2022-03-16 15:34:57 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5653, stu_CELoss: 1.0743 KDLoss: 0.4910 
2022-03-16 15:35:29 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.0729, stu_CELoss: 1.4259 KDLoss: 0.6470 
2022-03-16 15:36:02 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.7795, stu_CELoss: 1.2615 KDLoss: 0.5180 
2022-03-16 15:36:35 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.0108, stu_CELoss: 1.4248 KDLoss: 0.5860 
2022-03-16 15:37:07 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.9351, stu_CELoss: 1.3108 KDLoss: 0.6242 
2022-03-16 15:37:40 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.2548, stu_CELoss: 1.5607 KDLoss: 0.6941 
2022-03-16 15:38:13 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.1666, stu_CELoss: 1.4746 KDLoss: 0.6920 
2022-03-16 15:38:45 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.8121, stu_CELoss: 1.2037 KDLoss: 0.6085 
2022-03-16 15:39:17 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.7300, stu_CELoss: 1.2833 KDLoss: 0.4467 
2022-03-16 15:39:50 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.7285, stu_CELoss: 1.1935 KDLoss: 0.5350 
2022-03-16 15:40:22 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.0320, stu_CELoss: 1.4017 KDLoss: 0.6303 
2022-03-16 15:40:55 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.7873, stu_CELoss: 1.2077 KDLoss: 0.5796 
2022-03-16 15:41:27 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.9354, stu_CELoss: 1.2900 KDLoss: 0.6454 
2022-03-16 15:42:00 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.8506, stu_CELoss: 1.2205 KDLoss: 0.6301 
2022-03-16 15:42:33 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.8222, stu_CELoss: 1.2447 KDLoss: 0.5775 
2022-03-16 15:43:05 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.6405, stu_CELoss: 1.1256 KDLoss: 0.5149 
2022-03-16 15:43:38 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.0695, stu_CELoss: 1.3918 KDLoss: 0.6776 
2022-03-16 15:44:10 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.8931, stu_CELoss: 1.3269 KDLoss: 0.5662 
2022-03-16 15:44:43 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.2425, stu_CELoss: 1.6432 KDLoss: 0.5993 
2022-03-16 15:45:16 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.1217, stu_CELoss: 1.4810 KDLoss: 0.6406 
2022-03-16 15:45:48 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.8522, stu_CELoss: 1.3060 KDLoss: 0.5462 
2022-03-16 15:46:21 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.8833, stu_CELoss: 1.2998 KDLoss: 0.5834 
2022-03-16 15:46:53 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.1893, stu_CELoss: 1.5327 KDLoss: 0.6565 
2022-03-16 15:47:26 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.8059, stu_CELoss: 1.2535 KDLoss: 0.5524 
2022-03-16 15:47:58 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 2.1434, stu_CELoss: 1.4786 KDLoss: 0.6648 
2022-03-16 15:48:31 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.7946, stu_CELoss: 1.2858 KDLoss: 0.5089 
2022-03-16 15:49:04 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.6358, stu_CELoss: 1.1012 KDLoss: 0.5346 
2022-03-16 15:49:36 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.8782, stu_CELoss: 1.3197 KDLoss: 0.5585 
2022-03-16 15:50:09 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.8341, stu_CELoss: 1.3188 KDLoss: 0.5153 
2022-03-16 15:50:42 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.8793, stu_CELoss: 1.2517 KDLoss: 0.6275 
2022-03-16 15:51:14 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.1227, stu_CELoss: 1.3914 KDLoss: 0.7313 
2022-03-16 15:51:47 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.1265, stu_CELoss: 1.5233 KDLoss: 0.6031 
2022-03-16 15:52:19 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.9240, stu_CELoss: 1.3208 KDLoss: 0.6031 
2022-03-16 15:52:51 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.9715, stu_CELoss: 1.3515 KDLoss: 0.6200 
2022-03-16 15:53:24 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.9846, stu_CELoss: 1.3875 KDLoss: 0.5970 
2022-03-16 15:53:56 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.8949, stu_CELoss: 1.2838 KDLoss: 0.6111 
2022-03-16 15:54:28 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.7910, stu_CELoss: 1.2900 KDLoss: 0.5010 
2022-03-16 15:55:00 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.9288, stu_CELoss: 1.2932 KDLoss: 0.6356 
2022-03-16 15:55:33 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.8016, stu_CELoss: 1.2318 KDLoss: 0.5698 
2022-03-16 15:56:05 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.7404, stu_CELoss: 1.1908 KDLoss: 0.5497 
2022-03-16 15:56:37 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.7702, stu_CELoss: 1.2085 KDLoss: 0.5617 
2022-03-16 15:57:10 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.9662, stu_CELoss: 1.3608 KDLoss: 0.6054 
2022-03-16 15:57:42 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.7574, stu_CELoss: 1.2271 KDLoss: 0.5303 
2022-03-16 15:57:44 - train: epoch 037, train_loss: 1.8651
2022-03-16 16:00:12 - eval: epoch: 037, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.736%, stu_acc5: 90.370%, stu_test_loss: 1.1447
2022-03-16 16:00:13 - until epoch: 037, tea_best_acc1: 78.334%, stu_best_acc1: 71.836%
2022-03-16 16:00:13 - epoch 038 lr: 0.010000000000000002
2022-03-16 16:00:51 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.6187, stu_CELoss: 1.1091 KDLoss: 0.5096 
2022-03-16 16:01:24 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.9216, stu_CELoss: 1.3610 KDLoss: 0.5605 
2022-03-16 16:01:56 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.6563, stu_CELoss: 1.1182 KDLoss: 0.5381 
2022-03-16 16:02:29 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.6642, stu_CELoss: 1.1248 KDLoss: 0.5394 
2022-03-16 16:03:02 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.5267, stu_CELoss: 1.0337 KDLoss: 0.4930 
2022-03-16 16:03:35 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.9445, stu_CELoss: 1.3881 KDLoss: 0.5564 
2022-03-16 16:04:08 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.7623, stu_CELoss: 1.1717 KDLoss: 0.5906 
2022-03-16 16:04:41 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.6907, stu_CELoss: 1.1579 KDLoss: 0.5328 
2022-03-16 16:05:14 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.9559, stu_CELoss: 1.4095 KDLoss: 0.5464 
2022-03-16 16:05:47 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.1421, stu_CELoss: 1.4465 KDLoss: 0.6956 
2022-03-16 16:06:20 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.9457, stu_CELoss: 1.3368 KDLoss: 0.6089 
2022-03-16 16:06:52 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.9278, stu_CELoss: 1.3329 KDLoss: 0.5949 
2022-03-16 16:07:25 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.7564, stu_CELoss: 1.1649 KDLoss: 0.5915 
2022-03-16 16:07:58 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7012, stu_CELoss: 1.1926 KDLoss: 0.5086 
2022-03-16 16:08:30 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.7067, stu_CELoss: 1.2397 KDLoss: 0.4670 
2022-03-16 16:09:03 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.7500, stu_CELoss: 1.2271 KDLoss: 0.5229 
2022-03-16 16:09:36 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.8283, stu_CELoss: 1.2290 KDLoss: 0.5993 
2022-03-16 16:10:09 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.0173, stu_CELoss: 1.4140 KDLoss: 0.6032 
2022-03-16 16:10:41 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.0674, stu_CELoss: 1.4223 KDLoss: 0.6451 
2022-03-16 16:11:14 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.9310, stu_CELoss: 1.3572 KDLoss: 0.5738 
2022-03-16 16:11:47 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.8140, stu_CELoss: 1.2466 KDLoss: 0.5674 
2022-03-16 16:12:19 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.6888, stu_CELoss: 1.1976 KDLoss: 0.4911 
2022-03-16 16:12:52 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.7028, stu_CELoss: 1.1599 KDLoss: 0.5429 
2022-03-16 16:13:25 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.2499, stu_CELoss: 1.5605 KDLoss: 0.6894 
2022-03-16 16:13:57 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.8201, stu_CELoss: 1.2576 KDLoss: 0.5625 
2022-03-16 16:14:30 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 2.0091, stu_CELoss: 1.3883 KDLoss: 0.6208 
2022-03-16 16:15:03 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.8286, stu_CELoss: 1.2991 KDLoss: 0.5295 
2022-03-16 16:15:35 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.0191, stu_CELoss: 1.4641 KDLoss: 0.5549 
2022-03-16 16:16:08 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.8619, stu_CELoss: 1.2573 KDLoss: 0.6046 
2022-03-16 16:16:41 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.7304, stu_CELoss: 1.2090 KDLoss: 0.5214 
2022-03-16 16:17:13 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.0231, stu_CELoss: 1.4218 KDLoss: 0.6013 
2022-03-16 16:17:46 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.5989, stu_CELoss: 1.1067 KDLoss: 0.4923 
2022-03-16 16:18:19 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.5069, stu_CELoss: 1.0286 KDLoss: 0.4782 
2022-03-16 16:18:51 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.8161, stu_CELoss: 1.2327 KDLoss: 0.5834 
2022-03-16 16:19:24 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.9341, stu_CELoss: 1.3277 KDLoss: 0.6063 
2022-03-16 16:19:56 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.7660, stu_CELoss: 1.1876 KDLoss: 0.5784 
2022-03-16 16:20:29 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.8017, stu_CELoss: 1.2192 KDLoss: 0.5826 
2022-03-16 16:21:02 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.1354, stu_CELoss: 1.4661 KDLoss: 0.6693 
2022-03-16 16:21:34 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.7958, stu_CELoss: 1.1949 KDLoss: 0.6009 
2022-03-16 16:22:07 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.7579, stu_CELoss: 1.2568 KDLoss: 0.5011 
2022-03-16 16:22:40 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6262, stu_CELoss: 1.0920 KDLoss: 0.5342 
2022-03-16 16:23:12 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.7368, stu_CELoss: 1.1965 KDLoss: 0.5403 
2022-03-16 16:23:45 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.9316, stu_CELoss: 1.3520 KDLoss: 0.5796 
2022-03-16 16:24:18 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.9382, stu_CELoss: 1.3298 KDLoss: 0.6084 
2022-03-16 16:24:50 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.0000, stu_CELoss: 1.3837 KDLoss: 0.6163 
2022-03-16 16:25:23 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.9020, stu_CELoss: 1.3135 KDLoss: 0.5885 
2022-03-16 16:25:56 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.8691, stu_CELoss: 1.3482 KDLoss: 0.5208 
2022-03-16 16:26:28 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.7763, stu_CELoss: 1.2356 KDLoss: 0.5407 
2022-03-16 16:27:01 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.8418, stu_CELoss: 1.2153 KDLoss: 0.6265 
2022-03-16 16:27:33 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7303, stu_CELoss: 1.1646 KDLoss: 0.5657 
2022-03-16 16:27:34 - train: epoch 038, train_loss: 1.8539
2022-03-16 16:30:02 - eval: epoch: 038, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.032%, stu_acc5: 90.342%, stu_test_loss: 1.1603
2022-03-16 16:30:04 - until epoch: 038, tea_best_acc1: 78.334%, stu_best_acc1: 71.836%
2022-03-16 16:30:04 - epoch 039 lr: 0.010000000000000002
2022-03-16 16:30:43 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.7825, stu_CELoss: 1.2320 KDLoss: 0.5505 
2022-03-16 16:31:15 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.0714, stu_CELoss: 1.4482 KDLoss: 0.6232 
2022-03-16 16:31:48 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.7112, stu_CELoss: 1.1892 KDLoss: 0.5220 
2022-03-16 16:32:20 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.8032, stu_CELoss: 1.2643 KDLoss: 0.5389 
2022-03-16 16:32:52 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.8889, stu_CELoss: 1.2990 KDLoss: 0.5899 
2022-03-16 16:33:25 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.6159, stu_CELoss: 1.1183 KDLoss: 0.4976 
2022-03-16 16:33:58 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.2715, stu_CELoss: 1.5750 KDLoss: 0.6965 
2022-03-16 16:34:30 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.7870, stu_CELoss: 1.2420 KDLoss: 0.5450 
2022-03-16 16:35:03 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.0992, stu_CELoss: 1.4080 KDLoss: 0.6912 
2022-03-16 16:35:35 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.8250, stu_CELoss: 1.2425 KDLoss: 0.5824 
2022-03-16 16:36:08 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.9682, stu_CELoss: 1.4056 KDLoss: 0.5626 
2022-03-16 16:36:41 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.7545, stu_CELoss: 1.2186 KDLoss: 0.5359 
2022-03-16 16:37:14 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.9651, stu_CELoss: 1.4179 KDLoss: 0.5471 
2022-03-16 16:37:46 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.7725, stu_CELoss: 1.2302 KDLoss: 0.5423 
2022-03-16 16:38:19 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.5351, stu_CELoss: 1.0356 KDLoss: 0.4995 
2022-03-16 16:38:51 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.8843, stu_CELoss: 1.2462 KDLoss: 0.6380 
2022-03-16 16:39:24 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.7355, stu_CELoss: 1.2262 KDLoss: 0.5094 
2022-03-16 16:39:56 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.9559, stu_CELoss: 1.3466 KDLoss: 0.6094 
2022-03-16 16:40:29 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.5976, stu_CELoss: 1.0879 KDLoss: 0.5097 
2022-03-16 16:41:01 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.9193, stu_CELoss: 1.2624 KDLoss: 0.6569 
2022-03-16 16:41:33 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.9438, stu_CELoss: 1.3783 KDLoss: 0.5655 
2022-03-16 16:42:07 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.7219, stu_CELoss: 1.1852 KDLoss: 0.5367 
2022-03-16 16:42:38 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.3276, stu_CELoss: 1.6157 KDLoss: 0.7119 
2022-03-16 16:43:10 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.0567, stu_CELoss: 1.4305 KDLoss: 0.6262 
2022-03-16 16:43:41 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.9570, stu_CELoss: 1.3419 KDLoss: 0.6151 
2022-03-16 16:44:13 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.8135, stu_CELoss: 1.2276 KDLoss: 0.5859 
2022-03-16 16:44:45 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.0841, stu_CELoss: 1.4091 KDLoss: 0.6751 
2022-03-16 16:45:17 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.8038, stu_CELoss: 1.2244 KDLoss: 0.5795 
2022-03-16 16:45:49 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.6264, stu_CELoss: 1.1324 KDLoss: 0.4940 
2022-03-16 16:46:21 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.9109, stu_CELoss: 1.3446 KDLoss: 0.5662 
2022-03-16 16:46:53 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.7006, stu_CELoss: 1.1595 KDLoss: 0.5410 
2022-03-16 16:47:24 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.8937, stu_CELoss: 1.2312 KDLoss: 0.6625 
2022-03-16 16:47:57 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.8819, stu_CELoss: 1.2594 KDLoss: 0.6225 
2022-03-16 16:48:28 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.9668, stu_CELoss: 1.3714 KDLoss: 0.5953 
2022-03-16 16:49:00 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.0740, stu_CELoss: 1.4504 KDLoss: 0.6236 
2022-03-16 16:49:31 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.0436, stu_CELoss: 1.4407 KDLoss: 0.6029 
2022-03-16 16:50:03 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.7395, stu_CELoss: 1.1632 KDLoss: 0.5762 
2022-03-16 16:50:35 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.5142, stu_CELoss: 1.0411 KDLoss: 0.4730 
2022-03-16 16:51:07 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.9055, stu_CELoss: 1.3003 KDLoss: 0.6052 
2022-03-16 16:51:38 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.9094, stu_CELoss: 1.2507 KDLoss: 0.6588 
2022-03-16 16:52:10 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.7523, stu_CELoss: 1.2156 KDLoss: 0.5367 
2022-03-16 16:52:42 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.9633, stu_CELoss: 1.3856 KDLoss: 0.5776 
2022-03-16 16:53:13 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.8985, stu_CELoss: 1.2823 KDLoss: 0.6162 
2022-03-16 16:53:45 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.7528, stu_CELoss: 1.2099 KDLoss: 0.5429 
2022-03-16 16:54:17 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.7732, stu_CELoss: 1.2565 KDLoss: 0.5167 
2022-03-16 16:54:49 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.8718, stu_CELoss: 1.3127 KDLoss: 0.5591 
2022-03-16 16:55:21 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7705, stu_CELoss: 1.2084 KDLoss: 0.5621 
2022-03-16 16:55:52 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.5697, stu_CELoss: 1.0873 KDLoss: 0.4825 
2022-03-16 16:56:24 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.7232, stu_CELoss: 1.1392 KDLoss: 0.5841 
2022-03-16 16:56:55 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.9716, stu_CELoss: 1.3361 KDLoss: 0.6355 
2022-03-16 16:56:57 - train: epoch 039, train_loss: 1.8500
2022-03-16 16:59:24 - eval: epoch: 039, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.836%, stu_acc5: 90.132%, stu_test_loss: 1.1711
2022-03-16 16:59:25 - until epoch: 039, tea_best_acc1: 78.334%, stu_best_acc1: 71.836%
2022-03-16 16:59:25 - epoch 040 lr: 0.010000000000000002
2022-03-16 17:00:03 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.0311, stu_CELoss: 1.4939 KDLoss: 0.5372 
2022-03-16 17:00:35 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.9673, stu_CELoss: 1.3685 KDLoss: 0.5988 
2022-03-16 17:01:06 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.9542, stu_CELoss: 1.3909 KDLoss: 0.5633 
2022-03-16 17:01:38 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.5845, stu_CELoss: 1.0664 KDLoss: 0.5181 
2022-03-16 17:02:10 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.7070, stu_CELoss: 1.2178 KDLoss: 0.4892 
2022-03-16 17:02:42 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.5591, stu_CELoss: 1.0797 KDLoss: 0.4794 
2022-03-16 17:03:14 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.7774, stu_CELoss: 1.2401 KDLoss: 0.5373 
2022-03-16 17:03:46 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.8433, stu_CELoss: 1.2621 KDLoss: 0.5812 
2022-03-16 17:04:18 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.6923, stu_CELoss: 1.1376 KDLoss: 0.5546 
2022-03-16 17:04:50 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.6601, stu_CELoss: 1.1367 KDLoss: 0.5235 
2022-03-16 17:05:22 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.5566, stu_CELoss: 1.0559 KDLoss: 0.5007 
2022-03-16 17:05:54 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.8102, stu_CELoss: 1.2301 KDLoss: 0.5800 
2022-03-16 17:06:26 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.5827, stu_CELoss: 1.0763 KDLoss: 0.5064 
2022-03-16 17:06:58 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.9953, stu_CELoss: 1.4386 KDLoss: 0.5567 
2022-03-16 17:07:31 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.0385, stu_CELoss: 1.4964 KDLoss: 0.5420 
2022-03-16 17:08:02 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.7639, stu_CELoss: 1.2228 KDLoss: 0.5411 
2022-03-16 17:08:35 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.9879, stu_CELoss: 1.3400 KDLoss: 0.6478 
2022-03-16 17:09:07 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.7260, stu_CELoss: 1.2044 KDLoss: 0.5216 
2022-03-16 17:09:39 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.8037, stu_CELoss: 1.2355 KDLoss: 0.5681 
2022-03-16 17:10:11 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.8113, stu_CELoss: 1.2430 KDLoss: 0.5683 
2022-03-16 17:10:43 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.6116, stu_CELoss: 1.0850 KDLoss: 0.5266 
2022-03-16 17:11:15 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.9391, stu_CELoss: 1.3492 KDLoss: 0.5900 
2022-03-16 17:11:47 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.8627, stu_CELoss: 1.2805 KDLoss: 0.5822 
2022-03-16 17:12:19 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.8767, stu_CELoss: 1.2502 KDLoss: 0.6265 
2022-03-16 17:12:51 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.0004, stu_CELoss: 1.4167 KDLoss: 0.5837 
2022-03-16 17:13:23 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.8505, stu_CELoss: 1.2929 KDLoss: 0.5576 
2022-03-16 17:13:55 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.9677, stu_CELoss: 1.3883 KDLoss: 0.5794 
2022-03-16 17:14:27 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.8581, stu_CELoss: 1.2663 KDLoss: 0.5918 
2022-03-16 17:14:59 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.0449, stu_CELoss: 1.4322 KDLoss: 0.6127 
2022-03-16 17:15:31 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.0290, stu_CELoss: 1.4127 KDLoss: 0.6163 
2022-03-16 17:16:03 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.9178, stu_CELoss: 1.3017 KDLoss: 0.6161 
2022-03-16 17:16:34 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.0195, stu_CELoss: 1.3965 KDLoss: 0.6230 
2022-03-16 17:17:06 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.9166, stu_CELoss: 1.3408 KDLoss: 0.5758 
2022-03-16 17:17:38 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.7848, stu_CELoss: 1.2074 KDLoss: 0.5775 
2022-03-16 17:18:09 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.9326, stu_CELoss: 1.3613 KDLoss: 0.5713 
2022-03-16 17:18:41 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.9795, stu_CELoss: 1.3928 KDLoss: 0.5867 
2022-03-16 17:19:12 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.8144, stu_CELoss: 1.1948 KDLoss: 0.6196 
2022-03-16 17:19:44 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.6983, stu_CELoss: 1.1665 KDLoss: 0.5318 
2022-03-16 17:20:16 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.9072, stu_CELoss: 1.3140 KDLoss: 0.5932 
2022-03-16 17:20:48 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.6656, stu_CELoss: 1.1182 KDLoss: 0.5475 
2022-03-16 17:21:20 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.8484, stu_CELoss: 1.2202 KDLoss: 0.6282 
2022-03-16 17:21:51 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.8855, stu_CELoss: 1.2857 KDLoss: 0.5998 
2022-03-16 17:22:23 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.8411, stu_CELoss: 1.2887 KDLoss: 0.5524 
2022-03-16 17:22:55 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.7530, stu_CELoss: 1.1536 KDLoss: 0.5994 
2022-03-16 17:23:27 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.6508, stu_CELoss: 1.0931 KDLoss: 0.5578 
2022-03-16 17:23:58 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.0090, stu_CELoss: 1.3264 KDLoss: 0.6826 
2022-03-16 17:24:30 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.9452, stu_CELoss: 1.3610 KDLoss: 0.5842 
2022-03-16 17:25:02 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.9691, stu_CELoss: 1.2919 KDLoss: 0.6772 
2022-03-16 17:25:34 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.8655, stu_CELoss: 1.2539 KDLoss: 0.6116 
2022-03-16 17:26:06 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.9193, stu_CELoss: 1.2757 KDLoss: 0.6436 
2022-03-16 17:26:07 - train: epoch 040, train_loss: 1.8422
2022-03-16 17:28:33 - eval: epoch: 040, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.852%, stu_acc5: 90.680%, stu_test_loss: 1.1247
2022-03-16 17:28:35 - until epoch: 040, tea_best_acc1: 78.334%, stu_best_acc1: 71.852%
2022-03-16 17:28:35 - epoch 041 lr: 0.010000000000000002
2022-03-16 17:29:11 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.0174, stu_CELoss: 1.4093 KDLoss: 0.6081 
2022-03-16 17:29:43 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.2134, stu_CELoss: 1.5872 KDLoss: 0.6262 
2022-03-16 17:30:15 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.8831, stu_CELoss: 1.3275 KDLoss: 0.5556 
2022-03-16 17:30:47 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.7367, stu_CELoss: 1.1624 KDLoss: 0.5743 
2022-03-16 17:31:19 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.7332, stu_CELoss: 1.1627 KDLoss: 0.5705 
2022-03-16 17:31:51 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.9902, stu_CELoss: 1.3305 KDLoss: 0.6598 
2022-03-16 17:32:24 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.6695, stu_CELoss: 1.1541 KDLoss: 0.5154 
2022-03-16 17:32:56 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.7163, stu_CELoss: 1.1903 KDLoss: 0.5260 
2022-03-16 17:33:29 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.6710, stu_CELoss: 1.1539 KDLoss: 0.5171 
2022-03-16 17:34:02 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.0047, stu_CELoss: 1.4351 KDLoss: 0.5696 
2022-03-16 17:34:35 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.7385, stu_CELoss: 1.2429 KDLoss: 0.4956 
2022-03-16 17:35:07 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.6849, stu_CELoss: 1.1465 KDLoss: 0.5384 
2022-03-16 17:35:40 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.5985, stu_CELoss: 1.1070 KDLoss: 0.4915 
2022-03-16 17:36:13 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.0043, stu_CELoss: 1.3448 KDLoss: 0.6595 
2022-03-16 17:36:45 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.1381, stu_CELoss: 1.5060 KDLoss: 0.6321 
2022-03-16 17:37:17 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.6639, stu_CELoss: 1.1252 KDLoss: 0.5387 
2022-03-16 17:37:49 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.8534, stu_CELoss: 1.2571 KDLoss: 0.5963 
2022-03-16 17:38:21 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.8136, stu_CELoss: 1.3086 KDLoss: 0.5050 
2022-03-16 17:38:53 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.0087, stu_CELoss: 1.4087 KDLoss: 0.6001 
2022-03-16 17:39:25 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.7908, stu_CELoss: 1.2092 KDLoss: 0.5816 
2022-03-16 17:39:57 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.5938, stu_CELoss: 1.1110 KDLoss: 0.4828 
2022-03-16 17:40:29 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.7587, stu_CELoss: 1.1865 KDLoss: 0.5721 
2022-03-16 17:41:01 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.8008, stu_CELoss: 1.2426 KDLoss: 0.5582 
2022-03-16 17:41:33 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.0566, stu_CELoss: 1.4096 KDLoss: 0.6471 
2022-03-16 17:42:05 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.6659, stu_CELoss: 1.1171 KDLoss: 0.5487 
2022-03-16 17:42:37 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.1549, stu_CELoss: 1.4697 KDLoss: 0.6852 
2022-03-16 17:43:09 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.1066, stu_CELoss: 1.4359 KDLoss: 0.6708 
2022-03-16 17:43:41 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.6896, stu_CELoss: 1.1244 KDLoss: 0.5652 
2022-03-16 17:44:12 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7182, stu_CELoss: 1.2014 KDLoss: 0.5167 
2022-03-16 17:44:44 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.9039, stu_CELoss: 1.3298 KDLoss: 0.5742 
2022-03-16 17:45:16 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.9001, stu_CELoss: 1.2812 KDLoss: 0.6189 
2022-03-16 17:45:48 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.7897, stu_CELoss: 1.2435 KDLoss: 0.5462 
2022-03-16 17:46:20 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.7360, stu_CELoss: 1.1696 KDLoss: 0.5664 
2022-03-16 17:46:52 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.8149, stu_CELoss: 1.2583 KDLoss: 0.5566 
2022-03-16 17:47:25 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.6583, stu_CELoss: 1.1617 KDLoss: 0.4966 
2022-03-16 17:47:57 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.8266, stu_CELoss: 1.3305 KDLoss: 0.4961 
2022-03-16 17:48:28 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.0096, stu_CELoss: 1.3904 KDLoss: 0.6192 
2022-03-16 17:49:00 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.8856, stu_CELoss: 1.3090 KDLoss: 0.5766 
2022-03-16 17:49:32 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.7921, stu_CELoss: 1.2009 KDLoss: 0.5912 
2022-03-16 17:50:04 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.8564, stu_CELoss: 1.2228 KDLoss: 0.6337 
2022-03-16 17:50:36 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.7587, stu_CELoss: 1.2021 KDLoss: 0.5566 
2022-03-16 17:51:08 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4993, stu_CELoss: 1.0314 KDLoss: 0.4679 
2022-03-16 17:51:40 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.7636, stu_CELoss: 1.1395 KDLoss: 0.6241 
2022-03-16 17:52:12 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.6217, stu_CELoss: 1.1193 KDLoss: 0.5023 
2022-03-16 17:52:44 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.9044, stu_CELoss: 1.3276 KDLoss: 0.5769 
2022-03-16 17:53:16 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.0232, stu_CELoss: 1.3673 KDLoss: 0.6559 
2022-03-16 17:53:48 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.9397, stu_CELoss: 1.3446 KDLoss: 0.5951 
2022-03-16 17:54:20 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.7108, stu_CELoss: 1.1510 KDLoss: 0.5598 
2022-03-16 17:54:52 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.8830, stu_CELoss: 1.2683 KDLoss: 0.6147 
2022-03-16 17:55:23 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.7140, stu_CELoss: 1.1698 KDLoss: 0.5442 
2022-03-16 17:55:25 - train: epoch 041, train_loss: 1.8413
2022-03-16 17:57:52 - eval: epoch: 041, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.904%, stu_acc5: 90.684%, stu_test_loss: 1.1246
2022-03-16 17:57:53 - until epoch: 041, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 17:57:53 - epoch 042 lr: 0.010000000000000002
2022-03-16 17:58:31 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.4717, stu_CELoss: 1.0042 KDLoss: 0.4675 
2022-03-16 17:59:03 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.6448, stu_CELoss: 1.1439 KDLoss: 0.5010 
2022-03-16 17:59:35 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.1472, stu_CELoss: 1.4967 KDLoss: 0.6505 
2022-03-16 18:00:07 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.6509, stu_CELoss: 1.1107 KDLoss: 0.5402 
2022-03-16 18:00:39 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.5847, stu_CELoss: 1.1103 KDLoss: 0.4744 
2022-03-16 18:01:12 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.6819, stu_CELoss: 1.2261 KDLoss: 0.4558 
2022-03-16 18:01:45 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.9204, stu_CELoss: 1.2857 KDLoss: 0.6347 
2022-03-16 18:02:17 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.7726, stu_CELoss: 1.2157 KDLoss: 0.5569 
2022-03-16 18:02:49 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.9033, stu_CELoss: 1.3238 KDLoss: 0.5794 
2022-03-16 18:03:21 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.1411, stu_CELoss: 1.4866 KDLoss: 0.6545 
2022-03-16 18:03:54 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.5598, stu_CELoss: 1.1150 KDLoss: 0.4448 
2022-03-16 18:04:25 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.5420, stu_CELoss: 1.0301 KDLoss: 0.5119 
2022-03-16 18:04:58 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9202, stu_CELoss: 1.3659 KDLoss: 0.5543 
2022-03-16 18:05:30 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1375, stu_CELoss: 1.4824 KDLoss: 0.6551 
2022-03-16 18:06:03 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.6701, stu_CELoss: 1.1517 KDLoss: 0.5185 
2022-03-16 18:06:35 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.0510, stu_CELoss: 1.4235 KDLoss: 0.6275 
2022-03-16 18:07:08 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.8145, stu_CELoss: 1.2812 KDLoss: 0.5333 
2022-03-16 18:07:40 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.5299, stu_CELoss: 1.0588 KDLoss: 0.4712 
2022-03-16 18:08:12 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.8759, stu_CELoss: 1.2514 KDLoss: 0.6245 
2022-03-16 18:08:44 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.8742, stu_CELoss: 1.3139 KDLoss: 0.5603 
2022-03-16 18:09:17 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.6472, stu_CELoss: 1.1083 KDLoss: 0.5389 
2022-03-16 18:09:49 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.6967, stu_CELoss: 1.1399 KDLoss: 0.5568 
2022-03-16 18:10:22 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.8336, stu_CELoss: 1.2557 KDLoss: 0.5779 
2022-03-16 18:10:54 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.7948, stu_CELoss: 1.2114 KDLoss: 0.5834 
2022-03-16 18:11:27 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.9795, stu_CELoss: 1.3949 KDLoss: 0.5846 
2022-03-16 18:11:59 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.9934, stu_CELoss: 1.3579 KDLoss: 0.6355 
2022-03-16 18:12:31 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.6529, stu_CELoss: 1.1441 KDLoss: 0.5088 
2022-03-16 18:13:03 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.6159, stu_CELoss: 1.0951 KDLoss: 0.5207 
2022-03-16 18:13:35 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.7234, stu_CELoss: 1.1980 KDLoss: 0.5255 
2022-03-16 18:14:07 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.7643, stu_CELoss: 1.2020 KDLoss: 0.5623 
2022-03-16 18:14:39 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.8024, stu_CELoss: 1.1866 KDLoss: 0.6158 
2022-03-16 18:15:11 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.0372, stu_CELoss: 1.4403 KDLoss: 0.5969 
2022-03-16 18:15:43 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.2842, stu_CELoss: 1.6487 KDLoss: 0.6354 
2022-03-16 18:16:15 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.6068, stu_CELoss: 1.0793 KDLoss: 0.5275 
2022-03-16 18:16:47 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.0000, stu_CELoss: 1.3686 KDLoss: 0.6314 
2022-03-16 18:17:19 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.8363, stu_CELoss: 1.2739 KDLoss: 0.5624 
2022-03-16 18:17:51 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.9703, stu_CELoss: 1.3167 KDLoss: 0.6536 
2022-03-16 18:18:23 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.6528, stu_CELoss: 1.0938 KDLoss: 0.5590 
2022-03-16 18:18:55 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.9024, stu_CELoss: 1.2874 KDLoss: 0.6151 
2022-03-16 18:19:27 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.7782, stu_CELoss: 1.2244 KDLoss: 0.5538 
2022-03-16 18:20:00 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.7800, stu_CELoss: 1.2015 KDLoss: 0.5785 
2022-03-16 18:20:32 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.0269, stu_CELoss: 1.3941 KDLoss: 0.6328 
2022-03-16 18:21:04 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7761, stu_CELoss: 1.1915 KDLoss: 0.5845 
2022-03-16 18:21:36 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.6645, stu_CELoss: 1.1566 KDLoss: 0.5079 
2022-03-16 18:22:09 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.7559, stu_CELoss: 1.2181 KDLoss: 0.5378 
2022-03-16 18:22:41 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.9396, stu_CELoss: 1.2782 KDLoss: 0.6614 
2022-03-16 18:23:13 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.6694, stu_CELoss: 1.1349 KDLoss: 0.5344 
2022-03-16 18:23:45 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.0418, stu_CELoss: 1.4316 KDLoss: 0.6102 
2022-03-16 18:24:17 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.8085, stu_CELoss: 1.2525 KDLoss: 0.5560 
2022-03-16 18:24:49 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.9899, stu_CELoss: 1.3475 KDLoss: 0.6424 
2022-03-16 18:24:51 - train: epoch 042, train_loss: 1.8421
2022-03-16 18:27:18 - eval: epoch: 042, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.764%, stu_acc5: 90.206%, stu_test_loss: 1.1660
2022-03-16 18:27:19 - until epoch: 042, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 18:27:19 - epoch 043 lr: 0.010000000000000002
2022-03-16 18:27:56 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.7438, stu_CELoss: 1.2159 KDLoss: 0.5279 
2022-03-16 18:28:28 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.1539, stu_CELoss: 1.5749 KDLoss: 0.5790 
2022-03-16 18:29:00 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.5952, stu_CELoss: 1.1383 KDLoss: 0.4569 
2022-03-16 18:29:32 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.6110, stu_CELoss: 1.1151 KDLoss: 0.4959 
2022-03-16 18:30:04 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.7215, stu_CELoss: 1.2101 KDLoss: 0.5113 
2022-03-16 18:30:36 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.7430, stu_CELoss: 1.1878 KDLoss: 0.5553 
2022-03-16 18:31:08 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.7407, stu_CELoss: 1.2183 KDLoss: 0.5225 
2022-03-16 18:31:40 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.7171, stu_CELoss: 1.1847 KDLoss: 0.5324 
2022-03-16 18:32:13 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.6634, stu_CELoss: 1.1371 KDLoss: 0.5263 
2022-03-16 18:32:45 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.1058, stu_CELoss: 1.5188 KDLoss: 0.5870 
2022-03-16 18:33:17 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.8170, stu_CELoss: 1.2856 KDLoss: 0.5314 
2022-03-16 18:33:50 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.7580, stu_CELoss: 1.2817 KDLoss: 0.4763 
2022-03-16 18:34:22 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.9745, stu_CELoss: 1.4285 KDLoss: 0.5460 
2022-03-16 18:34:54 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.7016, stu_CELoss: 1.1753 KDLoss: 0.5263 
2022-03-16 18:35:26 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.7060, stu_CELoss: 1.1100 KDLoss: 0.5960 
2022-03-16 18:35:59 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6608, stu_CELoss: 1.1023 KDLoss: 0.5585 
2022-03-16 18:36:31 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.9282, stu_CELoss: 1.2672 KDLoss: 0.6609 
2022-03-16 18:37:03 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.0015, stu_CELoss: 1.3922 KDLoss: 0.6093 
2022-03-16 18:37:36 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.9291, stu_CELoss: 1.3772 KDLoss: 0.5519 
2022-03-16 18:38:08 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.6685, stu_CELoss: 1.1439 KDLoss: 0.5246 
2022-03-16 18:38:40 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.8209, stu_CELoss: 1.2211 KDLoss: 0.5999 
2022-03-16 18:39:12 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.0825, stu_CELoss: 1.4285 KDLoss: 0.6540 
2022-03-16 18:39:44 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.1621, stu_CELoss: 1.5199 KDLoss: 0.6422 
2022-03-16 18:40:17 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.6373, stu_CELoss: 1.0607 KDLoss: 0.5766 
2022-03-16 18:40:49 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.9474, stu_CELoss: 1.2992 KDLoss: 0.6481 
2022-03-16 18:41:21 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.7083, stu_CELoss: 1.1304 KDLoss: 0.5780 
2022-03-16 18:41:54 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.9389, stu_CELoss: 1.3929 KDLoss: 0.5460 
2022-03-16 18:42:26 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.6926, stu_CELoss: 1.1909 KDLoss: 0.5018 
2022-03-16 18:42:58 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7874, stu_CELoss: 1.2228 KDLoss: 0.5646 
2022-03-16 18:43:30 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.0319, stu_CELoss: 1.4248 KDLoss: 0.6072 
2022-03-16 18:44:02 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.1563, stu_CELoss: 1.4492 KDLoss: 0.7071 
2022-03-16 18:44:34 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.8149, stu_CELoss: 1.2004 KDLoss: 0.6145 
2022-03-16 18:45:06 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.0963, stu_CELoss: 1.4309 KDLoss: 0.6654 
2022-03-16 18:45:38 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.7174, stu_CELoss: 1.1376 KDLoss: 0.5797 
2022-03-16 18:46:09 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.8102, stu_CELoss: 1.2352 KDLoss: 0.5750 
2022-03-16 18:46:41 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.9512, stu_CELoss: 1.3484 KDLoss: 0.6027 
2022-03-16 18:47:13 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.6701, stu_CELoss: 1.1317 KDLoss: 0.5384 
2022-03-16 18:47:45 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.1069, stu_CELoss: 1.5243 KDLoss: 0.5825 
2022-03-16 18:48:17 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.8817, stu_CELoss: 1.2658 KDLoss: 0.6160 
2022-03-16 18:48:49 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.0458, stu_CELoss: 1.3798 KDLoss: 0.6660 
2022-03-16 18:49:21 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.8733, stu_CELoss: 1.2751 KDLoss: 0.5982 
2022-03-16 18:49:53 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.0061, stu_CELoss: 1.3958 KDLoss: 0.6103 
2022-03-16 18:50:25 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.8836, stu_CELoss: 1.2921 KDLoss: 0.5915 
2022-03-16 18:50:57 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.8855, stu_CELoss: 1.2834 KDLoss: 0.6020 
2022-03-16 18:51:29 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.9410, stu_CELoss: 1.2960 KDLoss: 0.6450 
2022-03-16 18:52:00 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.8628, stu_CELoss: 1.2931 KDLoss: 0.5697 
2022-03-16 18:52:32 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.8817, stu_CELoss: 1.2770 KDLoss: 0.6047 
2022-03-16 18:53:03 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.0827, stu_CELoss: 1.3842 KDLoss: 0.6985 
2022-03-16 18:53:35 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.7008, stu_CELoss: 1.1670 KDLoss: 0.5338 
2022-03-16 18:54:07 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.9948, stu_CELoss: 1.3380 KDLoss: 0.6568 
2022-03-16 18:54:08 - train: epoch 043, train_loss: 1.8486
2022-03-16 18:56:36 - eval: epoch: 043, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 68.876%, stu_acc5: 89.046%, stu_test_loss: 1.2537
2022-03-16 18:56:37 - until epoch: 043, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 18:56:37 - epoch 044 lr: 0.010000000000000002
2022-03-16 18:57:15 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.8589, stu_CELoss: 1.2993 KDLoss: 0.5596 
2022-03-16 18:57:48 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.9057, stu_CELoss: 1.3503 KDLoss: 0.5554 
2022-03-16 18:58:19 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.9184, stu_CELoss: 1.2732 KDLoss: 0.6452 
2022-03-16 18:58:51 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.5371, stu_CELoss: 1.0355 KDLoss: 0.5016 
2022-03-16 18:59:23 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.9902, stu_CELoss: 1.3844 KDLoss: 0.6057 
2022-03-16 18:59:55 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.5606, stu_CELoss: 1.0585 KDLoss: 0.5022 
2022-03-16 19:00:27 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.7420, stu_CELoss: 1.1971 KDLoss: 0.5449 
2022-03-16 19:00:59 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.8516, stu_CELoss: 1.2878 KDLoss: 0.5639 
2022-03-16 19:01:31 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.7812, stu_CELoss: 1.2500 KDLoss: 0.5312 
2022-03-16 19:02:03 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.8510, stu_CELoss: 1.3059 KDLoss: 0.5451 
2022-03-16 19:02:36 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5949, stu_CELoss: 1.0644 KDLoss: 0.5305 
2022-03-16 19:03:08 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.7874, stu_CELoss: 1.2796 KDLoss: 0.5078 
2022-03-16 19:03:40 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.8303, stu_CELoss: 1.2864 KDLoss: 0.5439 
2022-03-16 19:04:12 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.6509, stu_CELoss: 1.1292 KDLoss: 0.5217 
2022-03-16 19:04:43 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.8886, stu_CELoss: 1.2732 KDLoss: 0.6154 
2022-03-16 19:05:15 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.5159, stu_CELoss: 1.0140 KDLoss: 0.5018 
2022-03-16 19:05:47 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.6981, stu_CELoss: 1.1523 KDLoss: 0.5458 
2022-03-16 19:06:20 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.9199, stu_CELoss: 1.3098 KDLoss: 0.6101 
2022-03-16 19:06:52 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.1722, stu_CELoss: 1.4642 KDLoss: 0.7080 
2022-03-16 19:07:24 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.3896, stu_CELoss: 0.9302 KDLoss: 0.4594 
2022-03-16 19:07:56 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.8281, stu_CELoss: 1.2770 KDLoss: 0.5512 
2022-03-16 19:08:28 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.0414, stu_CELoss: 1.3842 KDLoss: 0.6572 
2022-03-16 19:08:59 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.1391, stu_CELoss: 1.4751 KDLoss: 0.6640 
2022-03-16 19:09:31 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.7888, stu_CELoss: 1.2156 KDLoss: 0.5732 
2022-03-16 19:10:04 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.0906, stu_CELoss: 1.4123 KDLoss: 0.6783 
2022-03-16 19:10:35 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.0154, stu_CELoss: 1.3517 KDLoss: 0.6637 
2022-03-16 19:11:07 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9997, stu_CELoss: 1.3255 KDLoss: 0.6742 
2022-03-16 19:11:39 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.7341, stu_CELoss: 1.1837 KDLoss: 0.5504 
2022-03-16 19:12:11 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.5966, stu_CELoss: 1.0776 KDLoss: 0.5190 
2022-03-16 19:12:44 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.7232, stu_CELoss: 1.1979 KDLoss: 0.5253 
2022-03-16 19:13:16 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.6163, stu_CELoss: 1.1526 KDLoss: 0.4636 
2022-03-16 19:13:47 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.5459, stu_CELoss: 1.0642 KDLoss: 0.4816 
2022-03-16 19:14:19 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.8110, stu_CELoss: 1.2315 KDLoss: 0.5795 
2022-03-16 19:14:51 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.0065, stu_CELoss: 1.3735 KDLoss: 0.6330 
2022-03-16 19:15:23 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.8472, stu_CELoss: 1.2654 KDLoss: 0.5818 
2022-03-16 19:15:55 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.0493, stu_CELoss: 1.4063 KDLoss: 0.6430 
2022-03-16 19:16:26 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.7436, stu_CELoss: 1.1750 KDLoss: 0.5686 
2022-03-16 19:16:58 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.7268, stu_CELoss: 1.1489 KDLoss: 0.5779 
2022-03-16 19:17:30 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.7851, stu_CELoss: 1.2305 KDLoss: 0.5546 
2022-03-16 19:18:02 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.9102, stu_CELoss: 1.3509 KDLoss: 0.5593 
2022-03-16 19:18:34 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.8032, stu_CELoss: 1.1808 KDLoss: 0.6225 
2022-03-16 19:19:06 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.9522, stu_CELoss: 1.3347 KDLoss: 0.6174 
2022-03-16 19:19:38 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.7729, stu_CELoss: 1.2137 KDLoss: 0.5592 
2022-03-16 19:20:10 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.8403, stu_CELoss: 1.2864 KDLoss: 0.5539 
2022-03-16 19:20:42 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.0823, stu_CELoss: 1.4257 KDLoss: 0.6566 
2022-03-16 19:21:14 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.9598, stu_CELoss: 1.2857 KDLoss: 0.6740 
2022-03-16 19:21:45 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.9272, stu_CELoss: 1.3073 KDLoss: 0.6199 
2022-03-16 19:22:17 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.7611, stu_CELoss: 1.1791 KDLoss: 0.5820 
2022-03-16 19:22:49 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.6163, stu_CELoss: 1.0674 KDLoss: 0.5489 
2022-03-16 19:23:20 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.9529, stu_CELoss: 1.3500 KDLoss: 0.6029 
2022-03-16 19:23:22 - train: epoch 044, train_loss: 1.8478
2022-03-16 19:25:48 - eval: epoch: 044, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.466%, stu_acc5: 90.650%, stu_test_loss: 1.1312
2022-03-16 19:25:50 - until epoch: 044, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 19:25:50 - epoch 045 lr: 0.010000000000000002
2022-03-16 19:26:27 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.6130, stu_CELoss: 1.1186 KDLoss: 0.4944 
2022-03-16 19:26:59 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.6118, stu_CELoss: 1.0684 KDLoss: 0.5434 
2022-03-16 19:27:31 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.7165, stu_CELoss: 1.2183 KDLoss: 0.4982 
2022-03-16 19:28:03 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.6942, stu_CELoss: 1.1746 KDLoss: 0.5196 
2022-03-16 19:28:35 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.1670, stu_CELoss: 1.5695 KDLoss: 0.5975 
2022-03-16 19:29:07 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.1242, stu_CELoss: 1.4270 KDLoss: 0.6971 
2022-03-16 19:29:39 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.7259, stu_CELoss: 1.2300 KDLoss: 0.4959 
2022-03-16 19:30:10 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.8821, stu_CELoss: 1.2638 KDLoss: 0.6183 
2022-03-16 19:30:42 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.7382, stu_CELoss: 1.2020 KDLoss: 0.5362 
2022-03-16 19:31:13 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.6214, stu_CELoss: 1.1133 KDLoss: 0.5081 
2022-03-16 19:31:45 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.8821, stu_CELoss: 1.2770 KDLoss: 0.6051 
2022-03-16 19:32:17 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.9197, stu_CELoss: 1.3205 KDLoss: 0.5992 
2022-03-16 19:32:48 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9339, stu_CELoss: 1.3493 KDLoss: 0.5846 
2022-03-16 19:33:19 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.9855, stu_CELoss: 1.4020 KDLoss: 0.5835 
2022-03-16 19:33:50 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.9483, stu_CELoss: 1.3193 KDLoss: 0.6291 
2022-03-16 19:34:21 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.4947, stu_CELoss: 0.9930 KDLoss: 0.5017 
2022-03-16 19:34:53 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.5520, stu_CELoss: 1.0797 KDLoss: 0.4723 
2022-03-16 19:35:25 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.7858, stu_CELoss: 1.2214 KDLoss: 0.5644 
2022-03-16 19:35:57 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.9347, stu_CELoss: 1.3406 KDLoss: 0.5941 
2022-03-16 19:36:28 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.8761, stu_CELoss: 1.2819 KDLoss: 0.5942 
2022-03-16 19:37:00 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.9528, stu_CELoss: 1.2259 KDLoss: 0.7268 
2022-03-16 19:37:32 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.9313, stu_CELoss: 1.2726 KDLoss: 0.6587 
2022-03-16 19:38:03 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.6202, stu_CELoss: 1.1318 KDLoss: 0.4884 
2022-03-16 19:38:35 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.6794, stu_CELoss: 1.1394 KDLoss: 0.5399 
2022-03-16 19:39:07 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.9856, stu_CELoss: 1.3912 KDLoss: 0.5944 
2022-03-16 19:39:38 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.9662, stu_CELoss: 1.3534 KDLoss: 0.6128 
2022-03-16 19:40:10 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.5694, stu_CELoss: 1.0876 KDLoss: 0.4818 
2022-03-16 19:40:42 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.9236, stu_CELoss: 1.2883 KDLoss: 0.6353 
2022-03-16 19:41:14 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.1421, stu_CELoss: 1.4729 KDLoss: 0.6693 
2022-03-16 19:41:45 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.8836, stu_CELoss: 1.2959 KDLoss: 0.5876 
2022-03-16 19:42:17 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.8960, stu_CELoss: 1.2671 KDLoss: 0.6289 
2022-03-16 19:42:49 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.9794, stu_CELoss: 1.3654 KDLoss: 0.6139 
2022-03-16 19:43:21 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.9775, stu_CELoss: 1.3305 KDLoss: 0.6470 
2022-03-16 19:43:52 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.8023, stu_CELoss: 1.2488 KDLoss: 0.5535 
2022-03-16 19:44:24 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.8113, stu_CELoss: 1.2532 KDLoss: 0.5580 
2022-03-16 19:44:56 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.9316, stu_CELoss: 1.3169 KDLoss: 0.6147 
2022-03-16 19:45:28 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.7304, stu_CELoss: 1.1300 KDLoss: 0.6004 
2022-03-16 19:46:00 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.8078, stu_CELoss: 1.2670 KDLoss: 0.5408 
2022-03-16 19:46:31 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.8756, stu_CELoss: 1.3108 KDLoss: 0.5648 
2022-03-16 19:47:03 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.7814, stu_CELoss: 1.1756 KDLoss: 0.6058 
2022-03-16 19:47:35 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.8685, stu_CELoss: 1.2487 KDLoss: 0.6198 
2022-03-16 19:48:06 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.1167, stu_CELoss: 1.4524 KDLoss: 0.6644 
2022-03-16 19:48:38 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.0638, stu_CELoss: 1.3784 KDLoss: 0.6854 
2022-03-16 19:49:10 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.9737, stu_CELoss: 1.3876 KDLoss: 0.5862 
2022-03-16 19:49:42 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.7614, stu_CELoss: 1.1812 KDLoss: 0.5802 
2022-03-16 19:50:13 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.0591, stu_CELoss: 1.4182 KDLoss: 0.6409 
2022-03-16 19:50:45 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.9240, stu_CELoss: 1.3430 KDLoss: 0.5810 
2022-03-16 19:51:16 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.7619, stu_CELoss: 1.1858 KDLoss: 0.5761 
2022-03-16 19:51:48 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.1069, stu_CELoss: 1.4330 KDLoss: 0.6739 
2022-03-16 19:52:19 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.7956, stu_CELoss: 1.2163 KDLoss: 0.5793 
2022-03-16 19:52:21 - train: epoch 045, train_loss: 1.8509
2022-03-16 19:54:47 - eval: epoch: 045, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.676%, stu_acc5: 90.254%, stu_test_loss: 1.1687
2022-03-16 19:54:48 - until epoch: 045, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 19:54:48 - epoch 046 lr: 0.010000000000000002
2022-03-16 19:55:26 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.4512, stu_CELoss: 0.9679 KDLoss: 0.4832 
2022-03-16 19:55:57 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.7075, stu_CELoss: 1.1834 KDLoss: 0.5241 
2022-03-16 19:56:29 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.9865, stu_CELoss: 1.4014 KDLoss: 0.5851 
2022-03-16 19:57:00 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.6624, stu_CELoss: 1.1000 KDLoss: 0.5624 
2022-03-16 19:57:32 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.8196, stu_CELoss: 1.2371 KDLoss: 0.5825 
2022-03-16 19:58:04 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.6645, stu_CELoss: 1.1378 KDLoss: 0.5268 
2022-03-16 19:58:35 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.6235, stu_CELoss: 1.1041 KDLoss: 0.5194 
2022-03-16 19:59:07 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.1192, stu_CELoss: 1.4140 KDLoss: 0.7052 
2022-03-16 19:59:39 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.7853, stu_CELoss: 1.2225 KDLoss: 0.5628 
2022-03-16 20:00:11 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6255, stu_CELoss: 1.1041 KDLoss: 0.5214 
2022-03-16 20:00:42 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8136, stu_CELoss: 1.2281 KDLoss: 0.5855 
2022-03-16 20:01:14 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.7999, stu_CELoss: 1.2045 KDLoss: 0.5954 
2022-03-16 20:01:46 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.8998, stu_CELoss: 1.3277 KDLoss: 0.5720 
2022-03-16 20:02:18 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.8521, stu_CELoss: 1.2952 KDLoss: 0.5569 
2022-03-16 20:02:49 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.6374, stu_CELoss: 1.0985 KDLoss: 0.5389 
2022-03-16 20:03:21 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.2640, stu_CELoss: 1.6024 KDLoss: 0.6616 
2022-03-16 20:03:53 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.7838, stu_CELoss: 1.2039 KDLoss: 0.5799 
2022-03-16 20:04:25 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.9062, stu_CELoss: 1.3363 KDLoss: 0.5699 
2022-03-16 20:04:56 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.8263, stu_CELoss: 1.2138 KDLoss: 0.6125 
2022-03-16 20:05:28 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.7562, stu_CELoss: 1.1753 KDLoss: 0.5810 
2022-03-16 20:06:00 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.1067, stu_CELoss: 1.4689 KDLoss: 0.6378 
2022-03-16 20:06:32 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.7123, stu_CELoss: 1.1326 KDLoss: 0.5797 
2022-03-16 20:07:03 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.1879, stu_CELoss: 1.5113 KDLoss: 0.6766 
2022-03-16 20:07:35 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 2.0287, stu_CELoss: 1.4284 KDLoss: 0.6002 
2022-03-16 20:08:07 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.9495, stu_CELoss: 1.3175 KDLoss: 0.6320 
2022-03-16 20:08:38 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.1711, stu_CELoss: 1.5016 KDLoss: 0.6694 
2022-03-16 20:09:10 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.6591, stu_CELoss: 1.1121 KDLoss: 0.5470 
2022-03-16 20:09:41 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.6732, stu_CELoss: 1.1335 KDLoss: 0.5397 
2022-03-16 20:10:13 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.7900, stu_CELoss: 1.3118 KDLoss: 0.4782 
2022-03-16 20:10:45 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.7163, stu_CELoss: 1.1773 KDLoss: 0.5390 
2022-03-16 20:11:17 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.7756, stu_CELoss: 1.2431 KDLoss: 0.5324 
2022-03-16 20:11:49 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8592, stu_CELoss: 1.2851 KDLoss: 0.5741 
2022-03-16 20:12:20 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.9647, stu_CELoss: 1.3674 KDLoss: 0.5972 
2022-03-16 20:12:52 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.8159, stu_CELoss: 1.2900 KDLoss: 0.5259 
2022-03-16 20:13:23 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.8360, stu_CELoss: 1.2595 KDLoss: 0.5765 
2022-03-16 20:13:55 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.8385, stu_CELoss: 1.2403 KDLoss: 0.5982 
2022-03-16 20:14:27 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.6645, stu_CELoss: 1.1033 KDLoss: 0.5612 
2022-03-16 20:14:59 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.8053, stu_CELoss: 1.2254 KDLoss: 0.5799 
2022-03-16 20:15:31 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.7419, stu_CELoss: 1.2181 KDLoss: 0.5238 
2022-03-16 20:16:02 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.7054, stu_CELoss: 1.1926 KDLoss: 0.5128 
2022-03-16 20:16:34 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.9748, stu_CELoss: 1.3589 KDLoss: 0.6159 
2022-03-16 20:17:06 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.7644, stu_CELoss: 1.2194 KDLoss: 0.5449 
2022-03-16 20:17:38 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.9578, stu_CELoss: 1.3412 KDLoss: 0.6166 
2022-03-16 20:18:09 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.0936, stu_CELoss: 1.4729 KDLoss: 0.6207 
2022-03-16 20:18:40 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.9237, stu_CELoss: 1.2985 KDLoss: 0.6252 
2022-03-16 20:19:12 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.8069, stu_CELoss: 1.2163 KDLoss: 0.5906 
2022-03-16 20:19:43 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.0072, stu_CELoss: 1.3361 KDLoss: 0.6712 
2022-03-16 20:20:15 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.6512, stu_CELoss: 1.0888 KDLoss: 0.5623 
2022-03-16 20:20:46 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.1454, stu_CELoss: 1.5078 KDLoss: 0.6376 
2022-03-16 20:21:17 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.8747, stu_CELoss: 1.2450 KDLoss: 0.6298 
2022-03-16 20:21:19 - train: epoch 046, train_loss: 1.8518
2022-03-16 20:23:47 - eval: epoch: 046, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.836%, stu_acc5: 90.270%, stu_test_loss: 1.1623
2022-03-16 20:23:48 - until epoch: 046, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 20:23:48 - epoch 047 lr: 0.010000000000000002
2022-03-16 20:24:26 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.0774, stu_CELoss: 1.4594 KDLoss: 0.6181 
2022-03-16 20:24:58 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.0122, stu_CELoss: 1.3779 KDLoss: 0.6343 
2022-03-16 20:25:30 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.5307, stu_CELoss: 1.0193 KDLoss: 0.5114 
2022-03-16 20:26:02 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.5320, stu_CELoss: 1.0393 KDLoss: 0.4926 
2022-03-16 20:26:34 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.7967, stu_CELoss: 1.2010 KDLoss: 0.5957 
2022-03-16 20:27:06 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.6711, stu_CELoss: 1.1241 KDLoss: 0.5470 
2022-03-16 20:27:38 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.8879, stu_CELoss: 1.3103 KDLoss: 0.5776 
2022-03-16 20:28:11 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.8831, stu_CELoss: 1.2788 KDLoss: 0.6044 
2022-03-16 20:28:43 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.8953, stu_CELoss: 1.3646 KDLoss: 0.5306 
2022-03-16 20:29:15 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.8784, stu_CELoss: 1.2493 KDLoss: 0.6291 
2022-03-16 20:29:47 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.1723, stu_CELoss: 1.5012 KDLoss: 0.6711 
2022-03-16 20:30:19 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.8825, stu_CELoss: 1.2574 KDLoss: 0.6250 
2022-03-16 20:30:51 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.8138, stu_CELoss: 1.2323 KDLoss: 0.5815 
2022-03-16 20:31:23 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.9559, stu_CELoss: 1.3452 KDLoss: 0.6107 
2022-03-16 20:31:55 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.9034, stu_CELoss: 1.3157 KDLoss: 0.5876 
2022-03-16 20:32:28 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.7028, stu_CELoss: 1.1774 KDLoss: 0.5254 
2022-03-16 20:33:00 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.5907, stu_CELoss: 1.0662 KDLoss: 0.5245 
2022-03-16 20:33:32 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.0688, stu_CELoss: 1.3723 KDLoss: 0.6965 
2022-03-16 20:34:05 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.7464, stu_CELoss: 1.2238 KDLoss: 0.5226 
2022-03-16 20:34:37 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.1754, stu_CELoss: 1.5011 KDLoss: 0.6743 
2022-03-16 20:35:09 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.7174, stu_CELoss: 1.1759 KDLoss: 0.5415 
2022-03-16 20:35:40 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.9502, stu_CELoss: 1.3718 KDLoss: 0.5784 
2022-03-16 20:36:12 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.8009, stu_CELoss: 1.2931 KDLoss: 0.5078 
2022-03-16 20:36:44 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.8803, stu_CELoss: 1.2917 KDLoss: 0.5886 
2022-03-16 20:37:16 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.9582, stu_CELoss: 1.3501 KDLoss: 0.6080 
2022-03-16 20:37:47 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.2747, stu_CELoss: 1.6301 KDLoss: 0.6445 
2022-03-16 20:38:19 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.5873, stu_CELoss: 1.0467 KDLoss: 0.5406 
2022-03-16 20:38:51 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.9234, stu_CELoss: 1.3553 KDLoss: 0.5681 
2022-03-16 20:39:23 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.8335, stu_CELoss: 1.2615 KDLoss: 0.5720 
2022-03-16 20:39:54 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.9780, stu_CELoss: 1.3609 KDLoss: 0.6170 
2022-03-16 20:40:26 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.7439, stu_CELoss: 1.1776 KDLoss: 0.5663 
2022-03-16 20:40:58 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.8659, stu_CELoss: 1.3046 KDLoss: 0.5613 
2022-03-16 20:41:30 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 2.0134, stu_CELoss: 1.3628 KDLoss: 0.6506 
2022-03-16 20:42:02 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.7020, stu_CELoss: 1.1782 KDLoss: 0.5239 
2022-03-16 20:42:33 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.0749, stu_CELoss: 1.4548 KDLoss: 0.6201 
2022-03-16 20:43:06 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.8500, stu_CELoss: 1.2689 KDLoss: 0.5812 
2022-03-16 20:43:37 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.6719, stu_CELoss: 1.0936 KDLoss: 0.5783 
2022-03-16 20:44:09 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5981, stu_CELoss: 1.1002 KDLoss: 0.4979 
2022-03-16 20:44:41 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.8766, stu_CELoss: 1.2880 KDLoss: 0.5887 
2022-03-16 20:45:13 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.9935, stu_CELoss: 1.3737 KDLoss: 0.6198 
2022-03-16 20:45:44 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.9570, stu_CELoss: 1.3461 KDLoss: 0.6109 
2022-03-16 20:46:16 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.8963, stu_CELoss: 1.2569 KDLoss: 0.6394 
2022-03-16 20:46:48 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.6892, stu_CELoss: 1.1589 KDLoss: 0.5303 
2022-03-16 20:47:20 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.8792, stu_CELoss: 1.3026 KDLoss: 0.5766 
2022-03-16 20:47:52 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.8400, stu_CELoss: 1.2710 KDLoss: 0.5690 
2022-03-16 20:48:23 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.7237, stu_CELoss: 1.1769 KDLoss: 0.5468 
2022-03-16 20:48:56 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.1008, stu_CELoss: 1.4127 KDLoss: 0.6881 
2022-03-16 20:49:27 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.6550, stu_CELoss: 1.0898 KDLoss: 0.5652 
2022-03-16 20:49:59 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.8762, stu_CELoss: 1.2902 KDLoss: 0.5861 
2022-03-16 20:50:30 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.8880, stu_CELoss: 1.2895 KDLoss: 0.5985 
2022-03-16 20:50:32 - train: epoch 047, train_loss: 1.8484
2022-03-16 20:52:59 - eval: epoch: 047, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 70.590%, stu_acc5: 90.262%, stu_test_loss: 1.1731
2022-03-16 20:53:01 - until epoch: 047, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 20:53:01 - epoch 048 lr: 0.010000000000000002
2022-03-16 20:53:39 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9071, stu_CELoss: 1.3600 KDLoss: 0.5471 
2022-03-16 20:54:11 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.1575, stu_CELoss: 1.5184 KDLoss: 0.6391 
2022-03-16 20:54:43 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.7960, stu_CELoss: 1.2363 KDLoss: 0.5597 
2022-03-16 20:55:15 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.7642, stu_CELoss: 1.1828 KDLoss: 0.5814 
2022-03-16 20:55:47 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.9293, stu_CELoss: 1.3251 KDLoss: 0.6041 
2022-03-16 20:56:20 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.8416, stu_CELoss: 1.3094 KDLoss: 0.5322 
2022-03-16 20:56:52 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.6423, stu_CELoss: 1.1376 KDLoss: 0.5047 
2022-03-16 20:57:24 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.9511, stu_CELoss: 1.3740 KDLoss: 0.5771 
2022-03-16 20:57:56 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.9146, stu_CELoss: 1.2886 KDLoss: 0.6260 
2022-03-16 20:58:28 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.8760, stu_CELoss: 1.3014 KDLoss: 0.5746 
2022-03-16 20:59:01 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.9831, stu_CELoss: 1.3863 KDLoss: 0.5968 
2022-03-16 20:59:33 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.1495, stu_CELoss: 1.5599 KDLoss: 0.5896 
2022-03-16 21:00:05 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.8581, stu_CELoss: 1.2993 KDLoss: 0.5588 
2022-03-16 21:00:37 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.8276, stu_CELoss: 1.2420 KDLoss: 0.5856 
2022-03-16 21:01:09 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.9023, stu_CELoss: 1.2589 KDLoss: 0.6433 
2022-03-16 21:01:41 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.9444, stu_CELoss: 1.3662 KDLoss: 0.5782 
2022-03-16 21:02:14 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8472, stu_CELoss: 1.2708 KDLoss: 0.5763 
2022-03-16 21:02:46 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.6824, stu_CELoss: 1.1475 KDLoss: 0.5349 
2022-03-16 21:03:18 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.7488, stu_CELoss: 1.2575 KDLoss: 0.4913 
2022-03-16 21:03:50 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.9396, stu_CELoss: 1.3429 KDLoss: 0.5967 
2022-03-16 21:04:23 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.8050, stu_CELoss: 1.2730 KDLoss: 0.5320 
2022-03-16 21:04:55 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.9800, stu_CELoss: 1.4112 KDLoss: 0.5688 
2022-03-16 21:05:27 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.8192, stu_CELoss: 1.2617 KDLoss: 0.5575 
2022-03-16 21:05:59 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.1034, stu_CELoss: 1.5165 KDLoss: 0.5869 
2022-03-16 21:06:31 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.8625, stu_CELoss: 1.3015 KDLoss: 0.5610 
2022-03-16 21:07:03 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.8545, stu_CELoss: 1.2810 KDLoss: 0.5735 
2022-03-16 21:07:35 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.1575, stu_CELoss: 1.4906 KDLoss: 0.6669 
2022-03-16 21:08:07 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.7943, stu_CELoss: 1.1780 KDLoss: 0.6163 
2022-03-16 21:08:38 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.7584, stu_CELoss: 1.2008 KDLoss: 0.5576 
2022-03-16 21:09:10 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.9171, stu_CELoss: 1.3251 KDLoss: 0.5920 
2022-03-16 21:09:41 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.9901, stu_CELoss: 1.3747 KDLoss: 0.6154 
2022-03-16 21:10:13 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.7352, stu_CELoss: 1.2084 KDLoss: 0.5268 
2022-03-16 21:10:44 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.7150, stu_CELoss: 1.2118 KDLoss: 0.5032 
2022-03-16 21:11:15 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.6759, stu_CELoss: 1.1527 KDLoss: 0.5232 
2022-03-16 21:11:47 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0710, stu_CELoss: 1.4776 KDLoss: 0.5934 
2022-03-16 21:12:18 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.8471, stu_CELoss: 1.2877 KDLoss: 0.5594 
2022-03-16 21:12:50 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.9030, stu_CELoss: 1.2948 KDLoss: 0.6082 
2022-03-16 21:13:22 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.7200, stu_CELoss: 1.1496 KDLoss: 0.5704 
2022-03-16 21:13:54 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.0070, stu_CELoss: 1.3579 KDLoss: 0.6491 
2022-03-16 21:14:26 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.6381, stu_CELoss: 1.0756 KDLoss: 0.5625 
2022-03-16 21:14:57 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.8908, stu_CELoss: 1.2782 KDLoss: 0.6127 
2022-03-16 21:15:28 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.8057, stu_CELoss: 1.2704 KDLoss: 0.5352 
2022-03-16 21:16:00 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.9179, stu_CELoss: 1.3461 KDLoss: 0.5718 
2022-03-16 21:16:31 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.8581, stu_CELoss: 1.2595 KDLoss: 0.5986 
2022-03-16 21:17:03 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.1144, stu_CELoss: 1.4763 KDLoss: 0.6381 
2022-03-16 21:17:34 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.9163, stu_CELoss: 1.3146 KDLoss: 0.6017 
2022-03-16 21:18:06 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.6648, stu_CELoss: 1.1284 KDLoss: 0.5364 
2022-03-16 21:18:38 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.1120, stu_CELoss: 1.4285 KDLoss: 0.6836 
2022-03-16 21:19:10 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.0124, stu_CELoss: 1.4841 KDLoss: 0.5283 
2022-03-16 21:19:42 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.8408, stu_CELoss: 1.2340 KDLoss: 0.6069 
2022-03-16 21:19:43 - train: epoch 048, train_loss: 1.8506
2022-03-16 21:22:13 - eval: epoch: 048, tea_acc1: 78.334%, tea_acc5: 94.064%, tea_test_loss: 0.8656, stu_acc1: 71.372%, stu_acc5: 90.568%, stu_test_loss: 1.1448
2022-03-16 21:22:14 - until epoch: 048, tea_best_acc1: 78.334%, stu_best_acc1: 71.904%
2022-03-16 21:22:14 - epoch 049 lr: 0.010000000000000002
2022-03-16 21:22:53 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.0180, stu_CELoss: 1.3405 KDLoss: 0.6775 
2022-03-16 21:23:26 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.6909, stu_CELoss: 1.1224 KDLoss: 0.5685 
2022-03-16 21:23:59 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.8888, stu_CELoss: 1.2773 KDLoss: 0.6115 
2022-03-16 21:24:32 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.8926, stu_CELoss: 1.2542 KDLoss: 0.6384 
2022-03-16 21:25:05 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.7642, stu_CELoss: 1.2577 KDLoss: 0.5065 
2022-03-16 21:25:38 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.5749, stu_CELoss: 1.0770 KDLoss: 0.4979 
