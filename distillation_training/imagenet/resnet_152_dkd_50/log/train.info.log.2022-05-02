2022-05-01 19:13:59 - teacher: resnet152
2022-05-01 19:13:59 - student: resnet50
2022-05-01 19:13:59 - num_classes: 1000
2022-05-01 19:13:59 - input_image_size: 224
2022-05-01 19:13:59 - scale: 1.1428571428571428
2022-05-01 19:13:59 - teacher_pretrained_model_path: /root/new/simpleAICV-pytorch-ImageNet-COCO-training/pretrained_models/resnet/resnet152-acc78.372.pth
2022-05-01 19:13:59 - student_pretrained_model_path: 
2022-05-01 19:13:59 - freeze_teacher: True
2022-05-01 19:13:59 - loss_list: ['CELoss', 'DKDLoss']
2022-05-01 19:13:59 - alpha: 1.0
2022-05-01 19:13:59 - beta: 0.5
2022-05-01 19:13:59 - T: 1.0
2022-05-01 19:13:59 - train_criterion: {'CELoss': CELoss(
  (loss): CrossEntropyLoss()
), 'DKDLoss': DKDLoss()}
2022-05-01 19:13:59 - loss_name: DKDLoss
2022-05-01 19:13:59 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-05-01 19:13:59 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa954345c10>
2022-05-01 19:13:59 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa954345940>
2022-05-01 19:13:59 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fa954345910>
2022-05-01 19:13:59 - seed: 0
2022-05-01 19:13:59 - batch_size: 256
2022-05-01 19:13:59 - num_workers: 16
2022-05-01 19:13:59 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-05-01 19:13:59 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-05-01 19:13:59 - epochs: 100
2022-05-01 19:13:59 - print_interval: 100
2022-05-01 19:13:59 - sync_bn: False
2022-05-01 19:13:59 - apex: True
2022-05-01 19:13:59 - gpus_type: NVIDIA RTX A5000
2022-05-01 19:13:59 - gpus_num: 2
2022-05-01 19:13:59 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fa946c30430>
2022-05-01 19:13:59 - --------------------parameters--------------------
2022-05-01 19:13:59 - name: teacher.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.0.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.1.weight, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.1.bias, grad: False
2022-05-01 19:13:59 - name: teacher.fc.weight, grad: False
2022-05-01 19:13:59 - name: teacher.fc.bias, grad: False
2022-05-01 19:13:59 - name: student.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.0.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.1.weight, grad: True
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.1.bias, grad: True
2022-05-01 19:13:59 - name: student.fc.weight, grad: True
2022-05-01 19:13:59 - name: student.fc.bias, grad: True
2022-05-01 19:13:59 - --------------------buffers--------------------
2022-05-01 19:13:59 - name: teacher.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.6.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer2.7.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.15.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.16.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.17.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.18.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.19.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.20.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.21.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.22.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.23.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.24.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.25.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.26.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.27.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.28.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.29.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.30.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.31.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.32.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.33.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.34.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer3.35.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: teacher.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.1.running_mean, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.1.running_var, grad: False
2022-05-01 19:13:59 - name: student.layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-05-01 19:13:59 - epoch 001 lr: 0.1
2022-05-01 19:14:38 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 14.7266, stu_CELoss: 6.9080 DKDLoss: 7.8186 
2022-05-01 19:15:10 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 14.9450, stu_CELoss: 6.8784 DKDLoss: 8.0666 
2022-05-01 19:15:42 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 14.3982, stu_CELoss: 6.8962 DKDLoss: 7.5020 
2022-05-01 19:16:12 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 14.6174, stu_CELoss: 6.8221 DKDLoss: 7.7953 
2022-05-01 19:16:43 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 14.2015, stu_CELoss: 6.7456 DKDLoss: 7.4559 
2022-05-01 19:17:13 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 13.8289, stu_CELoss: 6.5473 DKDLoss: 7.2816 
2022-05-01 19:17:44 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 13.7486, stu_CELoss: 6.6042 DKDLoss: 7.1444 
2022-05-01 19:18:15 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 13.6345, stu_CELoss: 6.5003 DKDLoss: 7.1342 
2022-05-01 19:18:46 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 13.2493, stu_CELoss: 6.3893 DKDLoss: 6.8600 
2022-05-01 19:19:17 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 13.4888, stu_CELoss: 6.3808 DKDLoss: 7.1080 
2022-05-01 19:19:48 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 12.7297, stu_CELoss: 6.2163 DKDLoss: 6.5133 
2022-05-01 19:20:18 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 13.1006, stu_CELoss: 6.1275 DKDLoss: 6.9731 
2022-05-01 19:20:49 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 12.8878, stu_CELoss: 6.1287 DKDLoss: 6.7591 
2022-05-01 19:21:20 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 12.4613, stu_CELoss: 6.0057 DKDLoss: 6.4556 
2022-05-01 19:21:51 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 12.5243, stu_CELoss: 5.8432 DKDLoss: 6.6811 
2022-05-01 19:22:22 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 12.3179, stu_CELoss: 5.8870 DKDLoss: 6.4309 
2022-05-01 19:22:53 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 12.1306, stu_CELoss: 5.7365 DKDLoss: 6.3942 
2022-05-01 19:23:23 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 11.8994, stu_CELoss: 5.7042 DKDLoss: 6.1952 
2022-05-01 19:23:54 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 11.7179, stu_CELoss: 5.4872 DKDLoss: 6.2307 
2022-05-01 19:24:25 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 11.5003, stu_CELoss: 5.4498 DKDLoss: 6.0505 
2022-05-01 19:24:56 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 11.3248, stu_CELoss: 5.3832 DKDLoss: 5.9416 
2022-05-01 19:25:27 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 11.2408, stu_CELoss: 5.3331 DKDLoss: 5.9077 
2022-05-01 19:25:58 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 10.9865, stu_CELoss: 5.2787 DKDLoss: 5.7078 
2022-05-01 19:26:29 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 10.8477, stu_CELoss: 5.2277 DKDLoss: 5.6200 
2022-05-01 19:27:00 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 10.8641, stu_CELoss: 5.3044 DKDLoss: 5.5597 
2022-05-01 19:27:31 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 10.9895, stu_CELoss: 5.2702 DKDLoss: 5.7193 
2022-05-01 19:28:02 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 10.7488, stu_CELoss: 5.2424 DKDLoss: 5.5064 
2022-05-01 19:28:33 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 10.3516, stu_CELoss: 5.0250 DKDLoss: 5.3265 
2022-05-01 19:29:04 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 10.1613, stu_CELoss: 4.8709 DKDLoss: 5.2904 
2022-05-01 19:29:35 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 10.4274, stu_CELoss: 4.9618 DKDLoss: 5.4656 
2022-05-01 19:30:06 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 10.6044, stu_CELoss: 5.0147 DKDLoss: 5.5897 
2022-05-01 19:30:38 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 10.5502, stu_CELoss: 5.0606 DKDLoss: 5.4896 
2022-05-01 19:31:08 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 9.6932, stu_CELoss: 4.6798 DKDLoss: 5.0133 
2022-05-01 19:31:39 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 9.6694, stu_CELoss: 4.6436 DKDLoss: 5.0258 
2022-05-01 19:32:10 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 9.7701, stu_CELoss: 4.6946 DKDLoss: 5.0755 
2022-05-01 19:32:41 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 9.8425, stu_CELoss: 4.7105 DKDLoss: 5.1319 
2022-05-01 19:33:13 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 9.5743, stu_CELoss: 4.7117 DKDLoss: 4.8626 
2022-05-01 19:33:44 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 9.3006, stu_CELoss: 4.4453 DKDLoss: 4.8553 
2022-05-01 19:34:15 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 9.6417, stu_CELoss: 4.6811 DKDLoss: 4.9605 
2022-05-01 19:34:46 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 9.5071, stu_CELoss: 4.5646 DKDLoss: 4.9425 
2022-05-01 19:35:17 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 9.3628, stu_CELoss: 4.5487 DKDLoss: 4.8141 
2022-05-01 19:35:49 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 9.2256, stu_CELoss: 4.4158 DKDLoss: 4.8099 
2022-05-01 19:36:20 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 9.2819, stu_CELoss: 4.3840 DKDLoss: 4.8979 
2022-05-01 19:36:51 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 8.5956, stu_CELoss: 4.1596 DKDLoss: 4.4360 
2022-05-01 19:37:22 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 9.2254, stu_CELoss: 4.4340 DKDLoss: 4.7914 
2022-05-01 19:37:54 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 9.2667, stu_CELoss: 4.4859 DKDLoss: 4.7808 
2022-05-01 19:38:25 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 8.3773, stu_CELoss: 4.1731 DKDLoss: 4.2042 
2022-05-01 19:38:56 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 9.1502, stu_CELoss: 4.5594 DKDLoss: 4.5908 
2022-05-01 19:39:27 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 8.5537, stu_CELoss: 4.1517 DKDLoss: 4.4020 
2022-05-01 19:39:59 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 8.8754, stu_CELoss: 4.2538 DKDLoss: 4.6216 
2022-05-01 19:40:00 - train: epoch 001, train_loss: 11.2084
2022-05-01 19:42:25 - eval: epoch: 001, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 20.036%, stu_acc5: 42.812%, stu_test_loss: 3.9876
2022-05-01 19:42:26 - until epoch: 001, tea_best_acc1: 78.330%, stu_best_acc1: 20.036%
2022-05-01 19:42:26 - epoch 002 lr: 0.1
2022-05-01 19:43:03 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 7.9979, stu_CELoss: 3.8803 DKDLoss: 4.1176 
2022-05-01 19:43:34 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 8.1222, stu_CELoss: 3.9364 DKDLoss: 4.1858 
2022-05-01 19:44:05 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 8.4709, stu_CELoss: 4.1470 DKDLoss: 4.3239 
2022-05-01 19:44:36 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 8.4937, stu_CELoss: 4.1883 DKDLoss: 4.3054 
2022-05-01 19:45:08 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 8.0458, stu_CELoss: 3.9541 DKDLoss: 4.0918 
2022-05-01 19:45:40 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 7.6957, stu_CELoss: 3.7557 DKDLoss: 3.9400 
2022-05-01 19:46:11 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 8.6263, stu_CELoss: 4.2366 DKDLoss: 4.3897 
2022-05-01 19:46:42 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 7.9734, stu_CELoss: 3.8130 DKDLoss: 4.1604 
2022-05-01 19:47:14 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 7.8028, stu_CELoss: 3.7233 DKDLoss: 4.0795 
2022-05-01 19:47:45 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 7.8791, stu_CELoss: 3.8190 DKDLoss: 4.0602 
2022-05-01 19:48:17 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 8.1082, stu_CELoss: 3.9872 DKDLoss: 4.1209 
2022-05-01 19:48:48 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 7.6864, stu_CELoss: 3.6720 DKDLoss: 4.0144 
2022-05-01 19:49:19 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 8.0199, stu_CELoss: 3.8549 DKDLoss: 4.1650 
2022-05-01 19:49:50 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 8.1945, stu_CELoss: 3.9422 DKDLoss: 4.2523 
2022-05-01 19:50:22 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 7.7464, stu_CELoss: 3.7521 DKDLoss: 3.9943 
2022-05-01 19:50:53 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 7.4981, stu_CELoss: 3.6953 DKDLoss: 3.8027 
2022-05-01 19:51:24 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 7.5928, stu_CELoss: 3.7492 DKDLoss: 3.8436 
2022-05-01 19:51:56 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 7.5041, stu_CELoss: 3.6824 DKDLoss: 3.8217 
2022-05-01 19:52:27 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 7.1203, stu_CELoss: 3.4082 DKDLoss: 3.7121 
2022-05-01 19:52:59 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 6.9966, stu_CELoss: 3.3297 DKDLoss: 3.6669 
2022-05-01 19:53:30 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 7.4398, stu_CELoss: 3.6847 DKDLoss: 3.7551 
2022-05-01 19:54:02 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 7.2267, stu_CELoss: 3.5273 DKDLoss: 3.6994 
2022-05-01 19:54:33 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 7.3826, stu_CELoss: 3.5620 DKDLoss: 3.8206 
2022-05-01 19:55:04 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 7.0647, stu_CELoss: 3.3853 DKDLoss: 3.6794 
2022-05-01 19:55:36 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 7.0232, stu_CELoss: 3.4119 DKDLoss: 3.6113 
2022-05-01 19:56:07 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 7.1280, stu_CELoss: 3.4176 DKDLoss: 3.7105 
2022-05-01 19:56:38 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 7.6248, stu_CELoss: 3.7260 DKDLoss: 3.8988 
2022-05-01 19:57:08 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 7.5699, stu_CELoss: 3.6816 DKDLoss: 3.8883 
2022-05-01 19:57:40 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 7.0013, stu_CELoss: 3.4170 DKDLoss: 3.5843 
2022-05-01 19:58:10 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 6.7057, stu_CELoss: 3.2436 DKDLoss: 3.4621 
2022-05-01 19:58:42 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 7.4643, stu_CELoss: 3.5241 DKDLoss: 3.9402 
2022-05-01 19:59:13 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 7.3064, stu_CELoss: 3.5445 DKDLoss: 3.7619 
2022-05-01 19:59:45 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 6.9758, stu_CELoss: 3.5481 DKDLoss: 3.4277 
2022-05-01 20:00:16 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 7.1910, stu_CELoss: 3.4579 DKDLoss: 3.7332 
2022-05-01 20:00:47 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 6.9354, stu_CELoss: 3.3807 DKDLoss: 3.5548 
2022-05-01 20:01:18 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 6.7155, stu_CELoss: 3.2899 DKDLoss: 3.4256 
2022-05-01 20:01:49 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 6.9985, stu_CELoss: 3.5409 DKDLoss: 3.4576 
2022-05-01 20:02:20 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 6.2795, stu_CELoss: 3.1592 DKDLoss: 3.1203 
2022-05-01 20:02:51 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 7.0549, stu_CELoss: 3.3926 DKDLoss: 3.6623 
2022-05-01 20:03:23 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 6.5425, stu_CELoss: 3.1354 DKDLoss: 3.4071 
2022-05-01 20:03:55 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 6.9646, stu_CELoss: 3.3261 DKDLoss: 3.6385 
2022-05-01 20:04:27 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 6.8445, stu_CELoss: 3.2983 DKDLoss: 3.5462 
2022-05-01 20:04:59 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 6.4709, stu_CELoss: 3.2711 DKDLoss: 3.1998 
2022-05-01 20:05:31 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 6.4001, stu_CELoss: 3.1870 DKDLoss: 3.2131 
2022-05-01 20:06:02 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 6.3364, stu_CELoss: 3.0542 DKDLoss: 3.2821 
2022-05-01 20:06:34 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 6.5024, stu_CELoss: 3.1539 DKDLoss: 3.3485 
2022-05-01 20:07:06 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 6.3806, stu_CELoss: 3.0058 DKDLoss: 3.3748 
2022-05-01 20:07:37 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 6.5372, stu_CELoss: 3.3552 DKDLoss: 3.1820 
2022-05-01 20:08:08 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 6.5217, stu_CELoss: 3.1403 DKDLoss: 3.3814 
2022-05-01 20:08:40 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 6.4305, stu_CELoss: 3.2465 DKDLoss: 3.1840 
2022-05-01 20:08:41 - train: epoch 002, train_loss: 7.3292
2022-05-01 20:11:07 - eval: epoch: 002, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 33.498%, stu_acc5: 59.664%, stu_test_loss: 3.0949
2022-05-01 20:11:07 - until epoch: 002, tea_best_acc1: 78.330%, stu_best_acc1: 33.498%
2022-05-01 20:11:07 - epoch 003 lr: 0.1
2022-05-01 20:11:44 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 6.1891, stu_CELoss: 3.1129 DKDLoss: 3.0762 
2022-05-01 20:12:14 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 6.4602, stu_CELoss: 3.2310 DKDLoss: 3.2293 
2022-05-01 20:12:44 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 6.6157, stu_CELoss: 3.1762 DKDLoss: 3.4396 
2022-05-01 20:13:15 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 6.3702, stu_CELoss: 3.1626 DKDLoss: 3.2077 
2022-05-01 20:13:45 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 6.2860, stu_CELoss: 3.1447 DKDLoss: 3.1413 
2022-05-01 20:14:16 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 6.3183, stu_CELoss: 3.0591 DKDLoss: 3.2592 
2022-05-01 20:14:46 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 6.3715, stu_CELoss: 3.2476 DKDLoss: 3.1239 
2022-05-01 20:15:17 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 6.5904, stu_CELoss: 3.3036 DKDLoss: 3.2868 
2022-05-01 20:15:47 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 6.0883, stu_CELoss: 3.0086 DKDLoss: 3.0796 
2022-05-01 20:16:18 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 6.0577, stu_CELoss: 3.0985 DKDLoss: 2.9592 
2022-05-01 20:16:48 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 6.1660, stu_CELoss: 3.0171 DKDLoss: 3.1490 
2022-05-01 20:17:19 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 6.0889, stu_CELoss: 2.9945 DKDLoss: 3.0944 
2022-05-01 20:17:49 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 5.8560, stu_CELoss: 2.9357 DKDLoss: 2.9203 
2022-05-01 20:18:20 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 5.8644, stu_CELoss: 2.8808 DKDLoss: 2.9836 
2022-05-01 20:18:51 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 6.3561, stu_CELoss: 3.2398 DKDLoss: 3.1164 
2022-05-01 20:19:22 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 6.1014, stu_CELoss: 2.9324 DKDLoss: 3.1690 
2022-05-01 20:19:53 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 5.9304, stu_CELoss: 2.9512 DKDLoss: 2.9792 
2022-05-01 20:20:25 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 5.9057, stu_CELoss: 2.9691 DKDLoss: 2.9365 
2022-05-01 20:20:55 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 6.2430, stu_CELoss: 2.9919 DKDLoss: 3.2511 
2022-05-01 20:21:26 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 6.3035, stu_CELoss: 3.2390 DKDLoss: 3.0645 
2022-05-01 20:21:57 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 5.9255, stu_CELoss: 3.0096 DKDLoss: 2.9159 
2022-05-01 20:22:28 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 6.5579, stu_CELoss: 3.3973 DKDLoss: 3.1605 
2022-05-01 20:22:59 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 5.6319, stu_CELoss: 2.9147 DKDLoss: 2.7172 
2022-05-01 20:23:30 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 5.7930, stu_CELoss: 2.7749 DKDLoss: 3.0180 
2022-05-01 20:24:01 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 6.0610, stu_CELoss: 3.0402 DKDLoss: 3.0208 
2022-05-01 20:24:32 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 5.9958, stu_CELoss: 3.0905 DKDLoss: 2.9054 
2022-05-01 20:25:03 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 5.9754, stu_CELoss: 3.1266 DKDLoss: 2.8488 
2022-05-01 20:25:34 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 5.9176, stu_CELoss: 2.8749 DKDLoss: 3.0428 
2022-05-01 20:26:06 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 5.9837, stu_CELoss: 3.0096 DKDLoss: 2.9741 
2022-05-01 20:26:37 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 6.1394, stu_CELoss: 3.0083 DKDLoss: 3.1310 
2022-05-01 20:27:08 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 6.4174, stu_CELoss: 3.2828 DKDLoss: 3.1346 
2022-05-01 20:27:39 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 6.1474, stu_CELoss: 3.1046 DKDLoss: 3.0429 
2022-05-01 20:28:11 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 5.8904, stu_CELoss: 2.8863 DKDLoss: 3.0041 
2022-05-01 20:28:41 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 6.3301, stu_CELoss: 3.1908 DKDLoss: 3.1394 
2022-05-01 20:29:13 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 5.9817, stu_CELoss: 2.9219 DKDLoss: 3.0598 
2022-05-01 20:29:44 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 5.7124, stu_CELoss: 2.7307 DKDLoss: 2.9818 
2022-05-01 20:30:16 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 5.8316, stu_CELoss: 2.8748 DKDLoss: 2.9568 
2022-05-01 20:30:47 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 6.3495, stu_CELoss: 3.1851 DKDLoss: 3.1645 
2022-05-01 20:31:19 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 6.4262, stu_CELoss: 3.3219 DKDLoss: 3.1043 
2022-05-01 20:31:50 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 5.4414, stu_CELoss: 2.5840 DKDLoss: 2.8574 
2022-05-01 20:32:21 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 5.4770, stu_CELoss: 2.8868 DKDLoss: 2.5901 
2022-05-01 20:32:53 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 5.6930, stu_CELoss: 2.8980 DKDLoss: 2.7950 
2022-05-01 20:33:25 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 5.3124, stu_CELoss: 2.5234 DKDLoss: 2.7890 
2022-05-01 20:33:56 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 5.3028, stu_CELoss: 2.7704 DKDLoss: 2.5324 
2022-05-01 20:34:28 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 5.6287, stu_CELoss: 2.8018 DKDLoss: 2.8270 
2022-05-01 20:34:59 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 5.4471, stu_CELoss: 2.7027 DKDLoss: 2.7444 
2022-05-01 20:35:31 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 5.8058, stu_CELoss: 2.9640 DKDLoss: 2.8419 
2022-05-01 20:36:03 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 6.1301, stu_CELoss: 3.1451 DKDLoss: 2.9850 
2022-05-01 20:36:34 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 5.7000, stu_CELoss: 2.8936 DKDLoss: 2.8064 
2022-05-01 20:37:05 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 5.8245, stu_CELoss: 2.9455 DKDLoss: 2.8790 
2022-05-01 20:37:06 - train: epoch 003, train_loss: 6.0050
2022-05-01 20:39:31 - eval: epoch: 003, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 34.854%, stu_acc5: 60.144%, stu_test_loss: 3.0959
2022-05-01 20:39:32 - until epoch: 003, tea_best_acc1: 78.330%, stu_best_acc1: 34.854%
2022-05-01 20:39:32 - epoch 004 lr: 0.1
2022-05-01 20:40:09 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 5.8904, stu_CELoss: 3.0939 DKDLoss: 2.7965 
2022-05-01 20:40:40 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 5.5264, stu_CELoss: 2.6625 DKDLoss: 2.8638 
2022-05-01 20:41:12 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 5.6390, stu_CELoss: 2.8030 DKDLoss: 2.8360 
2022-05-01 20:41:43 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 5.3108, stu_CELoss: 2.7277 DKDLoss: 2.5831 
2022-05-01 20:42:14 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 5.4073, stu_CELoss: 2.5997 DKDLoss: 2.8077 
2022-05-01 20:42:46 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 6.0326, stu_CELoss: 3.1056 DKDLoss: 2.9270 
2022-05-01 20:43:17 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 5.6134, stu_CELoss: 2.9830 DKDLoss: 2.6303 
2022-05-01 20:43:49 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 5.5924, stu_CELoss: 2.7939 DKDLoss: 2.7985 
2022-05-01 20:44:20 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 5.1417, stu_CELoss: 2.4423 DKDLoss: 2.6993 
2022-05-01 20:44:52 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 5.6764, stu_CELoss: 2.8340 DKDLoss: 2.8424 
2022-05-01 20:45:23 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 5.6850, stu_CELoss: 2.9080 DKDLoss: 2.7771 
2022-05-01 20:45:55 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 5.5138, stu_CELoss: 2.6730 DKDLoss: 2.8408 
2022-05-01 20:46:26 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 5.3611, stu_CELoss: 2.6860 DKDLoss: 2.6751 
2022-05-01 20:46:57 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 5.4184, stu_CELoss: 2.7955 DKDLoss: 2.6229 
2022-05-01 20:47:29 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 5.2754, stu_CELoss: 2.5688 DKDLoss: 2.7066 
2022-05-01 20:48:01 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 5.5868, stu_CELoss: 2.8178 DKDLoss: 2.7691 
2022-05-01 20:48:33 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 5.4077, stu_CELoss: 2.6651 DKDLoss: 2.7426 
2022-05-01 20:49:05 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 5.7982, stu_CELoss: 2.9742 DKDLoss: 2.8240 
2022-05-01 20:49:37 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 5.6234, stu_CELoss: 2.8104 DKDLoss: 2.8130 
2022-05-01 20:50:09 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 5.4048, stu_CELoss: 2.7756 DKDLoss: 2.6292 
2022-05-01 20:50:40 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 5.5208, stu_CELoss: 2.7558 DKDLoss: 2.7650 
2022-05-01 20:51:12 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 5.2653, stu_CELoss: 2.7171 DKDLoss: 2.5483 
2022-05-01 20:51:43 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 4.8991, stu_CELoss: 2.4318 DKDLoss: 2.4673 
2022-05-01 20:52:15 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 5.0992, stu_CELoss: 2.5245 DKDLoss: 2.5747 
2022-05-01 20:52:47 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 5.0241, stu_CELoss: 2.5181 DKDLoss: 2.5060 
2022-05-01 20:53:19 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 5.3434, stu_CELoss: 2.7035 DKDLoss: 2.6398 
2022-05-01 20:53:51 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 5.1868, stu_CELoss: 2.6034 DKDLoss: 2.5834 
2022-05-01 20:54:23 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 5.2801, stu_CELoss: 2.6103 DKDLoss: 2.6697 
2022-05-01 20:54:55 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 5.6968, stu_CELoss: 2.8031 DKDLoss: 2.8937 
2022-05-01 20:55:26 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 5.5292, stu_CELoss: 2.7853 DKDLoss: 2.7439 
2022-05-01 20:55:57 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 5.2858, stu_CELoss: 2.7211 DKDLoss: 2.5647 
2022-05-01 20:56:29 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 5.8407, stu_CELoss: 2.9090 DKDLoss: 2.9318 
2022-05-01 20:57:00 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 5.2744, stu_CELoss: 2.6853 DKDLoss: 2.5890 
2022-05-01 20:57:31 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 5.4880, stu_CELoss: 2.7234 DKDLoss: 2.7646 
2022-05-01 20:58:03 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 5.1151, stu_CELoss: 2.6423 DKDLoss: 2.4729 
2022-05-01 20:58:35 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 5.2253, stu_CELoss: 2.6684 DKDLoss: 2.5569 
2022-05-01 20:59:07 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 5.2788, stu_CELoss: 2.7010 DKDLoss: 2.5779 
2022-05-01 20:59:39 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 5.0593, stu_CELoss: 2.4987 DKDLoss: 2.5606 
2022-05-01 21:00:10 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 5.0594, stu_CELoss: 2.5278 DKDLoss: 2.5316 
2022-05-01 21:00:42 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 4.8351, stu_CELoss: 2.4636 DKDLoss: 2.3714 
2022-05-01 21:01:14 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 5.0597, stu_CELoss: 2.5538 DKDLoss: 2.5060 
2022-05-01 21:01:45 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 4.9678, stu_CELoss: 2.4679 DKDLoss: 2.4999 
2022-05-01 21:02:16 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 5.2468, stu_CELoss: 2.5611 DKDLoss: 2.6856 
2022-05-01 21:02:48 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 5.2985, stu_CELoss: 2.6074 DKDLoss: 2.6911 
2022-05-01 21:03:19 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 4.6012, stu_CELoss: 2.2183 DKDLoss: 2.3829 
2022-05-01 21:03:51 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 4.9058, stu_CELoss: 2.4439 DKDLoss: 2.4619 
2022-05-01 21:04:22 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 5.1234, stu_CELoss: 2.5402 DKDLoss: 2.5832 
2022-05-01 21:04:54 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 4.6546, stu_CELoss: 2.2561 DKDLoss: 2.3985 
2022-05-01 21:05:25 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 5.2785, stu_CELoss: 2.7547 DKDLoss: 2.5238 
2022-05-01 21:05:56 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 5.3840, stu_CELoss: 2.6790 DKDLoss: 2.7051 
2022-05-01 21:05:57 - train: epoch 004, train_loss: 5.3763
2022-05-01 21:08:23 - eval: epoch: 004, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 37.578%, stu_acc5: 63.056%, stu_test_loss: 2.9318
2022-05-01 21:08:23 - until epoch: 004, tea_best_acc1: 78.330%, stu_best_acc1: 37.578%
2022-05-01 21:08:23 - epoch 005 lr: 0.1
2022-05-01 21:09:01 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 5.2949, stu_CELoss: 2.6928 DKDLoss: 2.6021 
2022-05-01 21:09:32 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 5.3474, stu_CELoss: 2.6960 DKDLoss: 2.6514 
2022-05-01 21:10:03 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 5.4747, stu_CELoss: 2.8223 DKDLoss: 2.6524 
2022-05-01 21:10:34 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 4.6428, stu_CELoss: 2.3775 DKDLoss: 2.2653 
2022-05-01 21:11:04 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 4.9723, stu_CELoss: 2.4925 DKDLoss: 2.4798 
2022-05-01 21:11:35 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 5.1073, stu_CELoss: 2.5911 DKDLoss: 2.5163 
2022-05-01 21:12:06 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 4.9980, stu_CELoss: 2.5922 DKDLoss: 2.4057 
2022-05-01 21:12:37 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 5.4532, stu_CELoss: 2.8474 DKDLoss: 2.6058 
2022-05-01 21:13:08 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 4.8698, stu_CELoss: 2.4836 DKDLoss: 2.3862 
2022-05-01 21:13:39 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 5.2387, stu_CELoss: 2.6351 DKDLoss: 2.6036 
2022-05-01 21:14:10 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 5.1084, stu_CELoss: 2.5038 DKDLoss: 2.6047 
2022-05-01 21:14:41 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 5.2018, stu_CELoss: 2.5909 DKDLoss: 2.6109 
2022-05-01 21:15:12 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 4.6659, stu_CELoss: 2.4139 DKDLoss: 2.2520 
2022-05-01 21:15:43 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 4.8741, stu_CELoss: 2.5585 DKDLoss: 2.3156 
2022-05-01 21:16:14 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 4.8892, stu_CELoss: 2.4557 DKDLoss: 2.4335 
2022-05-01 21:16:46 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 4.4394, stu_CELoss: 2.2371 DKDLoss: 2.2023 
2022-05-01 21:17:17 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 5.1803, stu_CELoss: 2.6684 DKDLoss: 2.5119 
2022-05-01 21:17:48 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 4.8377, stu_CELoss: 2.4270 DKDLoss: 2.4107 
2022-05-01 21:18:20 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 4.8484, stu_CELoss: 2.4460 DKDLoss: 2.4024 
2022-05-01 21:18:51 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 4.8632, stu_CELoss: 2.4831 DKDLoss: 2.3800 
2022-05-01 21:19:23 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 4.6472, stu_CELoss: 2.4272 DKDLoss: 2.2200 
2022-05-01 21:19:55 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 5.1407, stu_CELoss: 2.4207 DKDLoss: 2.7200 
2022-05-01 21:20:27 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 4.4518, stu_CELoss: 2.1753 DKDLoss: 2.2765 
2022-05-01 21:20:59 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 4.8114, stu_CELoss: 2.4235 DKDLoss: 2.3878 
2022-05-01 21:21:31 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 5.4511, stu_CELoss: 2.7267 DKDLoss: 2.7244 
2022-05-01 21:22:01 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 4.7980, stu_CELoss: 2.3936 DKDLoss: 2.4044 
2022-05-01 21:22:33 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 5.1050, stu_CELoss: 2.6065 DKDLoss: 2.4985 
2022-05-01 21:23:04 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 4.9529, stu_CELoss: 2.4509 DKDLoss: 2.5020 
2022-05-01 21:23:35 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 5.0882, stu_CELoss: 2.6281 DKDLoss: 2.4601 
2022-05-01 21:24:07 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 4.8721, stu_CELoss: 2.5586 DKDLoss: 2.3135 
2022-05-01 21:24:38 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 4.9241, stu_CELoss: 2.4843 DKDLoss: 2.4398 
2022-05-01 21:25:09 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 4.9520, stu_CELoss: 2.5344 DKDLoss: 2.4176 
2022-05-01 21:25:41 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 4.8340, stu_CELoss: 2.4236 DKDLoss: 2.4105 
2022-05-01 21:26:13 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 4.6311, stu_CELoss: 2.4263 DKDLoss: 2.2048 
2022-05-01 21:26:45 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 5.0852, stu_CELoss: 2.6449 DKDLoss: 2.4403 
2022-05-01 21:27:16 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 4.8191, stu_CELoss: 2.5087 DKDLoss: 2.3104 
2022-05-01 21:27:48 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 4.8227, stu_CELoss: 2.4393 DKDLoss: 2.3834 
2022-05-01 21:28:19 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 4.9693, stu_CELoss: 2.5360 DKDLoss: 2.4332 
2022-05-01 21:28:51 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 5.0817, stu_CELoss: 2.6084 DKDLoss: 2.4734 
2022-05-01 21:29:22 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 4.6185, stu_CELoss: 2.3788 DKDLoss: 2.2396 
2022-05-01 21:29:53 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 5.1493, stu_CELoss: 2.5948 DKDLoss: 2.5545 
2022-05-01 21:30:24 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 4.8834, stu_CELoss: 2.6223 DKDLoss: 2.2612 
2022-05-01 21:30:56 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 5.0740, stu_CELoss: 2.5855 DKDLoss: 2.4884 
2022-05-01 21:31:27 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 5.0700, stu_CELoss: 2.6723 DKDLoss: 2.3977 
2022-05-01 21:31:59 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 4.9712, stu_CELoss: 2.5154 DKDLoss: 2.4558 
2022-05-01 21:32:31 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 4.7482, stu_CELoss: 2.4333 DKDLoss: 2.3149 
2022-05-01 21:33:03 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 4.5225, stu_CELoss: 2.2634 DKDLoss: 2.2592 
2022-05-01 21:33:35 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 4.5539, stu_CELoss: 2.4290 DKDLoss: 2.1249 
2022-05-01 21:34:06 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 5.1626, stu_CELoss: 2.7200 DKDLoss: 2.4425 
2022-05-01 21:34:37 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 4.5270, stu_CELoss: 2.3603 DKDLoss: 2.1667 
2022-05-01 21:34:39 - train: epoch 005, train_loss: 4.9645
2022-05-01 21:37:04 - eval: epoch: 005, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 43.660%, stu_acc5: 70.162%, stu_test_loss: 2.5098
2022-05-01 21:37:05 - until epoch: 005, tea_best_acc1: 78.330%, stu_best_acc1: 43.660%
2022-05-01 21:37:05 - epoch 006 lr: 0.1
2022-05-01 21:37:42 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 4.4080, stu_CELoss: 2.2566 DKDLoss: 2.1514 
2022-05-01 21:38:13 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 4.5521, stu_CELoss: 2.3973 DKDLoss: 2.1547 
2022-05-01 21:38:45 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 4.7638, stu_CELoss: 2.2983 DKDLoss: 2.4656 
2022-05-01 21:39:16 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 4.6919, stu_CELoss: 2.4276 DKDLoss: 2.2643 
2022-05-01 21:39:48 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 4.5479, stu_CELoss: 2.2780 DKDLoss: 2.2699 
2022-05-01 21:40:20 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 4.4574, stu_CELoss: 2.2358 DKDLoss: 2.2216 
2022-05-01 21:40:52 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 5.0794, stu_CELoss: 2.6378 DKDLoss: 2.4417 
2022-05-01 21:41:23 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 4.6262, stu_CELoss: 2.3113 DKDLoss: 2.3149 
2022-05-01 21:41:55 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 4.5230, stu_CELoss: 2.3349 DKDLoss: 2.1881 
2022-05-01 21:42:27 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 4.8113, stu_CELoss: 2.4455 DKDLoss: 2.3657 
2022-05-01 21:42:59 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 5.0534, stu_CELoss: 2.5677 DKDLoss: 2.4857 
2022-05-01 21:43:31 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 4.9231, stu_CELoss: 2.5341 DKDLoss: 2.3890 
2022-05-01 21:44:03 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 4.7371, stu_CELoss: 2.4509 DKDLoss: 2.2861 
2022-05-01 21:44:35 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 4.9113, stu_CELoss: 2.5538 DKDLoss: 2.3575 
2022-05-01 21:45:07 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 5.2628, stu_CELoss: 2.6625 DKDLoss: 2.6003 
2022-05-01 21:45:39 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 4.4222, stu_CELoss: 2.2154 DKDLoss: 2.2067 
2022-05-01 21:46:11 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 4.7646, stu_CELoss: 2.5595 DKDLoss: 2.2050 
2022-05-01 21:46:43 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 4.6332, stu_CELoss: 2.3994 DKDLoss: 2.2338 
2022-05-01 21:47:15 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 4.5603, stu_CELoss: 2.3216 DKDLoss: 2.2387 
2022-05-01 21:47:46 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 5.0013, stu_CELoss: 2.6030 DKDLoss: 2.3983 
2022-05-01 21:48:18 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 4.6731, stu_CELoss: 2.3401 DKDLoss: 2.3331 
2022-05-01 21:48:50 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 4.5156, stu_CELoss: 2.2111 DKDLoss: 2.3046 
2022-05-01 21:49:22 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 4.4137, stu_CELoss: 2.3514 DKDLoss: 2.0623 
2022-05-01 21:49:54 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 4.8210, stu_CELoss: 2.4207 DKDLoss: 2.4003 
2022-05-01 21:50:26 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 4.5694, stu_CELoss: 2.2794 DKDLoss: 2.2900 
2022-05-01 21:50:57 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 4.8512, stu_CELoss: 2.5580 DKDLoss: 2.2932 
2022-05-01 21:51:29 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 4.7066, stu_CELoss: 2.3864 DKDLoss: 2.3202 
2022-05-01 21:52:01 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 4.1139, stu_CELoss: 2.1190 DKDLoss: 1.9949 
2022-05-01 21:52:33 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 4.6200, stu_CELoss: 2.3935 DKDLoss: 2.2265 
2022-05-01 21:53:04 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 4.7162, stu_CELoss: 2.3848 DKDLoss: 2.3313 
2022-05-01 21:53:36 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 4.5729, stu_CELoss: 2.3498 DKDLoss: 2.2232 
2022-05-01 21:54:07 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 4.5639, stu_CELoss: 2.3161 DKDLoss: 2.2479 
2022-05-01 21:54:39 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 4.7154, stu_CELoss: 2.3019 DKDLoss: 2.4134 
2022-05-01 21:55:10 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 4.8294, stu_CELoss: 2.4309 DKDLoss: 2.3985 
2022-05-01 21:55:42 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 4.5037, stu_CELoss: 2.3618 DKDLoss: 2.1419 
2022-05-01 21:56:13 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 4.5525, stu_CELoss: 2.3675 DKDLoss: 2.1850 
2022-05-01 21:56:45 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 4.5046, stu_CELoss: 2.2749 DKDLoss: 2.2297 
2022-05-01 21:57:17 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 4.2274, stu_CELoss: 2.1764 DKDLoss: 2.0510 
2022-05-01 21:57:48 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 4.6663, stu_CELoss: 2.3935 DKDLoss: 2.2728 
2022-05-01 21:58:20 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 4.5452, stu_CELoss: 2.2979 DKDLoss: 2.2473 
2022-05-01 21:58:51 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 4.7531, stu_CELoss: 2.4434 DKDLoss: 2.3097 
2022-05-01 21:59:23 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 4.3735, stu_CELoss: 2.1961 DKDLoss: 2.1774 
2022-05-01 21:59:55 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 4.7840, stu_CELoss: 2.3993 DKDLoss: 2.3848 
2022-05-01 22:00:26 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 4.7229, stu_CELoss: 2.3937 DKDLoss: 2.3292 
2022-05-01 22:00:58 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 4.6225, stu_CELoss: 2.4285 DKDLoss: 2.1940 
2022-05-01 22:01:30 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 4.4776, stu_CELoss: 2.2512 DKDLoss: 2.2264 
2022-05-01 22:02:02 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 4.1891, stu_CELoss: 2.2073 DKDLoss: 1.9818 
2022-05-01 22:02:34 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 4.5943, stu_CELoss: 2.3497 DKDLoss: 2.2446 
2022-05-01 22:03:06 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 4.4823, stu_CELoss: 2.3987 DKDLoss: 2.0836 
2022-05-01 22:03:38 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 4.4802, stu_CELoss: 2.2263 DKDLoss: 2.2539 
2022-05-01 22:03:39 - train: epoch 006, train_loss: 4.6941
2022-05-01 22:06:06 - eval: epoch: 006, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 48.018%, stu_acc5: 74.008%, stu_test_loss: 2.2743
2022-05-01 22:06:07 - until epoch: 006, tea_best_acc1: 78.330%, stu_best_acc1: 48.018%
2022-05-01 22:06:07 - epoch 007 lr: 0.1
2022-05-01 22:06:44 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 4.3996, stu_CELoss: 2.2085 DKDLoss: 2.1910 
2022-05-01 22:07:16 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 4.6228, stu_CELoss: 2.3382 DKDLoss: 2.2847 
2022-05-01 22:07:48 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 4.8966, stu_CELoss: 2.5374 DKDLoss: 2.3592 
2022-05-01 22:08:19 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 4.6331, stu_CELoss: 2.4168 DKDLoss: 2.2163 
2022-05-01 22:08:51 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 4.1815, stu_CELoss: 2.1230 DKDLoss: 2.0585 
2022-05-01 22:09:22 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 4.9550, stu_CELoss: 2.6258 DKDLoss: 2.3292 
2022-05-01 22:09:54 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 4.5005, stu_CELoss: 2.3189 DKDLoss: 2.1816 
2022-05-01 22:10:25 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 4.4678, stu_CELoss: 2.3190 DKDLoss: 2.1488 
2022-05-01 22:10:57 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 4.5485, stu_CELoss: 2.3482 DKDLoss: 2.2003 
2022-05-01 22:11:29 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 4.3465, stu_CELoss: 2.2555 DKDLoss: 2.0910 
2022-05-01 22:12:01 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 4.6730, stu_CELoss: 2.3734 DKDLoss: 2.2997 
2022-05-01 22:12:32 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 4.3509, stu_CELoss: 2.2029 DKDLoss: 2.1480 
2022-05-01 22:13:03 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 4.1058, stu_CELoss: 2.0537 DKDLoss: 2.0521 
2022-05-01 22:13:35 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 4.8269, stu_CELoss: 2.4352 DKDLoss: 2.3916 
2022-05-01 22:14:07 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 4.6649, stu_CELoss: 2.4083 DKDLoss: 2.2566 
2022-05-01 22:14:38 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 4.5914, stu_CELoss: 2.3818 DKDLoss: 2.2096 
2022-05-01 22:15:10 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 4.3848, stu_CELoss: 2.2113 DKDLoss: 2.1736 
2022-05-01 22:15:41 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 4.5092, stu_CELoss: 2.4018 DKDLoss: 2.1074 
2022-05-01 22:16:13 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 4.5083, stu_CELoss: 2.2876 DKDLoss: 2.2208 
2022-05-01 22:16:45 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.9706, stu_CELoss: 1.9062 DKDLoss: 2.0644 
2022-05-01 22:17:17 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 4.7524, stu_CELoss: 2.5006 DKDLoss: 2.2518 
2022-05-01 22:17:49 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 4.3863, stu_CELoss: 2.2724 DKDLoss: 2.1140 
2022-05-01 22:18:20 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 4.4579, stu_CELoss: 2.3626 DKDLoss: 2.0952 
2022-05-01 22:18:52 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 5.0264, stu_CELoss: 2.5775 DKDLoss: 2.4489 
2022-05-01 22:19:24 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 4.6189, stu_CELoss: 2.3406 DKDLoss: 2.2782 
2022-05-01 22:19:55 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 4.5162, stu_CELoss: 2.2479 DKDLoss: 2.2683 
2022-05-01 22:20:27 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 4.3434, stu_CELoss: 2.2199 DKDLoss: 2.1235 
2022-05-01 22:20:59 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 4.4050, stu_CELoss: 2.3059 DKDLoss: 2.0991 
2022-05-01 22:21:30 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 4.2578, stu_CELoss: 2.1571 DKDLoss: 2.1007 
2022-05-01 22:22:02 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 4.6542, stu_CELoss: 2.4596 DKDLoss: 2.1946 
2022-05-01 22:22:34 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 4.1883, stu_CELoss: 2.1766 DKDLoss: 2.0117 
2022-05-01 22:23:06 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 4.3538, stu_CELoss: 2.1300 DKDLoss: 2.2238 
2022-05-01 22:23:38 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 4.9147, stu_CELoss: 2.5634 DKDLoss: 2.3513 
2022-05-01 22:24:10 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 4.7147, stu_CELoss: 2.4889 DKDLoss: 2.2259 
2022-05-01 22:24:42 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 4.7971, stu_CELoss: 2.4765 DKDLoss: 2.3207 
2022-05-01 22:25:13 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 4.5017, stu_CELoss: 2.3090 DKDLoss: 2.1927 
2022-05-01 22:25:44 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 4.3167, stu_CELoss: 2.1552 DKDLoss: 2.1614 
2022-05-01 22:26:16 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 4.7991, stu_CELoss: 2.5729 DKDLoss: 2.2262 
2022-05-01 22:26:48 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 4.4963, stu_CELoss: 2.3963 DKDLoss: 2.1001 
2022-05-01 22:27:20 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 4.2608, stu_CELoss: 2.2954 DKDLoss: 1.9654 
2022-05-01 22:27:51 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 4.5872, stu_CELoss: 2.2922 DKDLoss: 2.2949 
2022-05-01 22:28:23 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 4.3019, stu_CELoss: 2.1327 DKDLoss: 2.1692 
2022-05-01 22:28:55 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 4.5328, stu_CELoss: 2.3752 DKDLoss: 2.1576 
2022-05-01 22:29:27 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 4.4814, stu_CELoss: 2.2671 DKDLoss: 2.2143 
2022-05-01 22:29:58 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 4.5384, stu_CELoss: 2.3372 DKDLoss: 2.2012 
2022-05-01 22:30:31 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 4.8389, stu_CELoss: 2.3499 DKDLoss: 2.4891 
2022-05-01 22:31:03 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 4.0925, stu_CELoss: 2.0675 DKDLoss: 2.0250 
2022-05-01 22:31:35 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 4.6736, stu_CELoss: 2.4912 DKDLoss: 2.1824 
2022-05-01 22:32:06 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 4.3022, stu_CELoss: 2.2470 DKDLoss: 2.0552 
2022-05-01 22:32:38 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 4.2427, stu_CELoss: 2.1970 DKDLoss: 2.0457 
2022-05-01 22:32:39 - train: epoch 007, train_loss: 4.5027
2022-05-01 22:35:04 - eval: epoch: 007, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 50.842%, stu_acc5: 76.786%, stu_test_loss: 2.1055
2022-05-01 22:35:04 - until epoch: 007, tea_best_acc1: 78.330%, stu_best_acc1: 50.842%
2022-05-01 22:35:04 - epoch 008 lr: 0.1
2022-05-01 22:35:41 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 4.2932, stu_CELoss: 2.2181 DKDLoss: 2.0751 
2022-05-01 22:36:12 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 4.4531, stu_CELoss: 2.3162 DKDLoss: 2.1369 
2022-05-01 22:36:43 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 4.4511, stu_CELoss: 2.2238 DKDLoss: 2.2273 
2022-05-01 22:37:14 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 4.1645, stu_CELoss: 2.1424 DKDLoss: 2.0221 
2022-05-01 22:37:44 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 4.3321, stu_CELoss: 2.1564 DKDLoss: 2.1757 
2022-05-01 22:38:15 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 4.2397, stu_CELoss: 2.1867 DKDLoss: 2.0530 
2022-05-01 22:38:47 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 4.5271, stu_CELoss: 2.3213 DKDLoss: 2.2058 
2022-05-01 22:39:18 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 4.0448, stu_CELoss: 1.9722 DKDLoss: 2.0725 
2022-05-01 22:39:49 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 4.2800, stu_CELoss: 2.2712 DKDLoss: 2.0087 
2022-05-01 22:40:20 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 4.3597, stu_CELoss: 2.2586 DKDLoss: 2.1011 
2022-05-01 22:40:51 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 4.1223, stu_CELoss: 2.1441 DKDLoss: 1.9782 
2022-05-01 22:41:22 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 4.1118, stu_CELoss: 1.9813 DKDLoss: 2.1305 
2022-05-01 22:41:54 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 4.7216, stu_CELoss: 2.3801 DKDLoss: 2.3414 
2022-05-01 22:42:25 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 4.2444, stu_CELoss: 2.1733 DKDLoss: 2.0711 
2022-05-01 22:42:57 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 4.5505, stu_CELoss: 2.3482 DKDLoss: 2.2023 
2022-05-01 22:43:29 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 4.4778, stu_CELoss: 2.3434 DKDLoss: 2.1343 
2022-05-01 22:44:01 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 4.2833, stu_CELoss: 2.2608 DKDLoss: 2.0225 
2022-05-01 22:44:33 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 4.5442, stu_CELoss: 2.3419 DKDLoss: 2.2024 
2022-05-01 22:45:05 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 4.1264, stu_CELoss: 2.0452 DKDLoss: 2.0812 
2022-05-01 22:45:36 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 4.7077, stu_CELoss: 2.5930 DKDLoss: 2.1148 
2022-05-01 22:46:08 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 4.5653, stu_CELoss: 2.2915 DKDLoss: 2.2738 
2022-05-01 22:46:39 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 4.4439, stu_CELoss: 2.3907 DKDLoss: 2.0531 
2022-05-01 22:47:11 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 4.0496, stu_CELoss: 2.1479 DKDLoss: 1.9016 
2022-05-01 22:47:42 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 4.0962, stu_CELoss: 2.1129 DKDLoss: 1.9834 
2022-05-01 22:48:13 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 4.4073, stu_CELoss: 2.2568 DKDLoss: 2.1506 
2022-05-01 22:48:44 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 4.4701, stu_CELoss: 2.2001 DKDLoss: 2.2700 
2022-05-01 22:49:15 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 4.4552, stu_CELoss: 2.3438 DKDLoss: 2.1114 
2022-05-01 22:49:47 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 4.1582, stu_CELoss: 2.2049 DKDLoss: 1.9533 
2022-05-01 22:50:18 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 4.7067, stu_CELoss: 2.5356 DKDLoss: 2.1711 
2022-05-01 22:50:50 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 4.7839, stu_CELoss: 2.5131 DKDLoss: 2.2708 
2022-05-01 22:51:21 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 4.4579, stu_CELoss: 2.2931 DKDLoss: 2.1648 
2022-05-01 22:51:53 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 4.4156, stu_CELoss: 2.3227 DKDLoss: 2.0929 
2022-05-01 22:52:24 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 4.4652, stu_CELoss: 2.2751 DKDLoss: 2.1901 
2022-05-01 22:52:56 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 4.5342, stu_CELoss: 2.3741 DKDLoss: 2.1602 
2022-05-01 22:53:27 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 4.2233, stu_CELoss: 2.1348 DKDLoss: 2.0885 
2022-05-01 22:53:59 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 4.5901, stu_CELoss: 2.4003 DKDLoss: 2.1898 
2022-05-01 22:54:30 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 4.1860, stu_CELoss: 2.1727 DKDLoss: 2.0133 
2022-05-01 22:55:01 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 4.3776, stu_CELoss: 2.2106 DKDLoss: 2.1670 
2022-05-01 22:55:32 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 4.4279, stu_CELoss: 2.3064 DKDLoss: 2.1215 
2022-05-01 22:56:03 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 4.9388, stu_CELoss: 2.5362 DKDLoss: 2.4026 
2022-05-01 22:56:35 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 4.2319, stu_CELoss: 2.1390 DKDLoss: 2.0929 
2022-05-01 22:57:06 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 4.1427, stu_CELoss: 2.1354 DKDLoss: 2.0074 
2022-05-01 22:57:38 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 4.0040, stu_CELoss: 1.9863 DKDLoss: 2.0177 
2022-05-01 22:58:09 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 4.3928, stu_CELoss: 2.3041 DKDLoss: 2.0887 
2022-05-01 22:58:40 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 4.4813, stu_CELoss: 2.3096 DKDLoss: 2.1716 
2022-05-01 22:59:11 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 4.4275, stu_CELoss: 2.2774 DKDLoss: 2.1501 
2022-05-01 22:59:43 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 4.3345, stu_CELoss: 2.2676 DKDLoss: 2.0669 
2022-05-01 23:00:14 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 4.2167, stu_CELoss: 2.1991 DKDLoss: 2.0176 
2022-05-01 23:00:46 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 4.1113, stu_CELoss: 2.1918 DKDLoss: 1.9195 
2022-05-01 23:01:17 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 4.4981, stu_CELoss: 2.3564 DKDLoss: 2.1417 
2022-05-01 23:01:19 - train: epoch 008, train_loss: 4.3616
2022-05-01 23:03:43 - eval: epoch: 008, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 51.104%, stu_acc5: 76.904%, stu_test_loss: 2.0986
2022-05-01 23:03:44 - until epoch: 008, tea_best_acc1: 78.330%, stu_best_acc1: 51.104%
2022-05-01 23:03:44 - epoch 009 lr: 0.1
2022-05-01 23:04:22 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 3.6904, stu_CELoss: 1.8913 DKDLoss: 1.7991 
2022-05-01 23:04:53 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 4.2099, stu_CELoss: 2.1950 DKDLoss: 2.0149 
2022-05-01 23:05:24 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 4.1570, stu_CELoss: 2.2126 DKDLoss: 1.9443 
2022-05-01 23:05:55 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 4.4868, stu_CELoss: 2.3430 DKDLoss: 2.1438 
2022-05-01 23:06:27 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.9660, stu_CELoss: 1.9959 DKDLoss: 1.9701 
2022-05-01 23:06:58 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 3.9268, stu_CELoss: 2.0705 DKDLoss: 1.8563 
2022-05-01 23:07:30 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 3.9627, stu_CELoss: 1.9884 DKDLoss: 1.9743 
2022-05-01 23:08:02 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 4.1669, stu_CELoss: 2.1519 DKDLoss: 2.0150 
2022-05-01 23:08:33 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 4.1458, stu_CELoss: 2.1000 DKDLoss: 2.0457 
2022-05-01 23:09:05 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 4.0411, stu_CELoss: 2.1359 DKDLoss: 1.9053 
2022-05-01 23:09:36 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 4.3315, stu_CELoss: 2.3749 DKDLoss: 1.9567 
2022-05-01 23:10:08 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 4.4145, stu_CELoss: 2.2189 DKDLoss: 2.1956 
2022-05-01 23:10:40 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 4.6074, stu_CELoss: 2.4414 DKDLoss: 2.1660 
2022-05-01 23:11:12 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 3.9459, stu_CELoss: 1.9649 DKDLoss: 1.9810 
2022-05-01 23:11:44 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 4.0722, stu_CELoss: 2.0698 DKDLoss: 2.0024 
2022-05-01 23:12:16 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 4.2504, stu_CELoss: 2.2022 DKDLoss: 2.0483 
2022-05-01 23:12:47 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 4.4723, stu_CELoss: 2.3509 DKDLoss: 2.1215 
2022-05-01 23:13:19 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 4.4510, stu_CELoss: 2.2831 DKDLoss: 2.1679 
2022-05-01 23:13:51 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 3.7949, stu_CELoss: 1.9191 DKDLoss: 1.8758 
2022-05-01 23:14:23 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 4.2861, stu_CELoss: 2.1274 DKDLoss: 2.1587 
2022-05-01 23:14:54 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 4.2490, stu_CELoss: 2.2339 DKDLoss: 2.0152 
2022-05-01 23:15:25 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 4.2956, stu_CELoss: 2.2389 DKDLoss: 2.0566 
2022-05-01 23:15:57 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 3.7013, stu_CELoss: 1.8706 DKDLoss: 1.8307 
2022-05-01 23:16:28 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 4.0837, stu_CELoss: 2.1412 DKDLoss: 1.9426 
2022-05-01 23:16:59 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 4.0503, stu_CELoss: 2.1007 DKDLoss: 1.9496 
2022-05-01 23:17:30 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 4.1808, stu_CELoss: 2.1782 DKDLoss: 2.0026 
2022-05-01 23:18:01 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 4.4820, stu_CELoss: 2.3794 DKDLoss: 2.1026 
2022-05-01 23:18:33 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 4.1813, stu_CELoss: 2.2891 DKDLoss: 1.8922 
2022-05-01 23:19:04 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 3.9456, stu_CELoss: 2.0471 DKDLoss: 1.8985 
2022-05-01 23:19:35 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 4.0166, stu_CELoss: 2.0344 DKDLoss: 1.9822 
2022-05-01 23:20:06 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 4.0765, stu_CELoss: 2.1480 DKDLoss: 1.9285 
2022-05-01 23:20:38 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 4.2748, stu_CELoss: 2.2515 DKDLoss: 2.0233 
2022-05-01 23:21:10 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 4.7441, stu_CELoss: 2.5153 DKDLoss: 2.2288 
2022-05-01 23:21:41 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 4.2535, stu_CELoss: 2.1505 DKDLoss: 2.1030 
2022-05-01 23:22:12 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 4.3001, stu_CELoss: 2.2778 DKDLoss: 2.0223 
2022-05-01 23:22:43 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 3.8964, stu_CELoss: 2.0038 DKDLoss: 1.8926 
2022-05-01 23:23:15 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 4.4694, stu_CELoss: 2.3057 DKDLoss: 2.1636 
2022-05-01 23:23:46 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 4.4538, stu_CELoss: 2.3380 DKDLoss: 2.1157 
2022-05-01 23:24:18 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 3.9844, stu_CELoss: 2.0572 DKDLoss: 1.9272 
2022-05-01 23:24:50 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 4.6689, stu_CELoss: 2.5695 DKDLoss: 2.0994 
2022-05-01 23:25:21 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 4.3543, stu_CELoss: 2.2556 DKDLoss: 2.0987 
2022-05-01 23:25:53 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 3.8444, stu_CELoss: 2.0874 DKDLoss: 1.7571 
2022-05-01 23:26:24 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 3.8511, stu_CELoss: 1.9669 DKDLoss: 1.8843 
2022-05-01 23:26:56 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 4.1801, stu_CELoss: 2.1831 DKDLoss: 1.9971 
2022-05-01 23:27:28 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 4.4268, stu_CELoss: 2.2721 DKDLoss: 2.1547 
2022-05-01 23:27:59 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 4.3094, stu_CELoss: 2.2894 DKDLoss: 2.0200 
2022-05-01 23:28:31 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 4.4778, stu_CELoss: 2.4257 DKDLoss: 2.0521 
2022-05-01 23:29:03 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 4.1982, stu_CELoss: 2.2141 DKDLoss: 1.9841 
2022-05-01 23:29:34 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 4.2024, stu_CELoss: 2.2018 DKDLoss: 2.0007 
2022-05-01 23:30:06 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 4.1230, stu_CELoss: 2.0386 DKDLoss: 2.0844 
2022-05-01 23:30:07 - train: epoch 009, train_loss: 4.2486
2022-05-01 23:32:32 - eval: epoch: 009, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 51.294%, stu_acc5: 76.786%, stu_test_loss: 2.0987
2022-05-01 23:32:33 - until epoch: 009, tea_best_acc1: 78.330%, stu_best_acc1: 51.294%
2022-05-01 23:32:33 - epoch 010 lr: 0.1
2022-05-01 23:33:10 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 4.1189, stu_CELoss: 2.0819 DKDLoss: 2.0371 
2022-05-01 23:33:41 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 4.0775, stu_CELoss: 2.0985 DKDLoss: 1.9790 
2022-05-01 23:34:12 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 4.1883, stu_CELoss: 2.2242 DKDLoss: 1.9641 
2022-05-01 23:34:43 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 4.3825, stu_CELoss: 2.2802 DKDLoss: 2.1024 
2022-05-01 23:35:15 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 4.1776, stu_CELoss: 2.1750 DKDLoss: 2.0026 
2022-05-01 23:35:47 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 4.3879, stu_CELoss: 2.3567 DKDLoss: 2.0311 
2022-05-01 23:36:18 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 4.3334, stu_CELoss: 2.3293 DKDLoss: 2.0041 
2022-05-01 23:36:50 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 4.3960, stu_CELoss: 2.3286 DKDLoss: 2.0674 
2022-05-01 23:37:21 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 3.7121, stu_CELoss: 1.8950 DKDLoss: 1.8171 
2022-05-01 23:37:52 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 3.6108, stu_CELoss: 1.8284 DKDLoss: 1.7825 
2022-05-01 23:38:24 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 3.9491, stu_CELoss: 2.0067 DKDLoss: 1.9425 
2022-05-01 23:38:55 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 4.2732, stu_CELoss: 2.2707 DKDLoss: 2.0025 
2022-05-01 23:39:27 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 3.7570, stu_CELoss: 1.9756 DKDLoss: 1.7814 
2022-05-01 23:39:58 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 4.5122, stu_CELoss: 2.3258 DKDLoss: 2.1863 
2022-05-01 23:40:30 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 3.7929, stu_CELoss: 1.8717 DKDLoss: 1.9213 
2022-05-01 23:41:01 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 4.2497, stu_CELoss: 2.2497 DKDLoss: 2.0000 
2022-05-01 23:41:33 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 4.4385, stu_CELoss: 2.3229 DKDLoss: 2.1156 
2022-05-01 23:42:04 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 4.0720, stu_CELoss: 2.1077 DKDLoss: 1.9642 
2022-05-01 23:42:36 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 3.9525, stu_CELoss: 2.0853 DKDLoss: 1.8672 
2022-05-01 23:43:07 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 4.1082, stu_CELoss: 2.1062 DKDLoss: 2.0019 
2022-05-01 23:43:38 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 3.8217, stu_CELoss: 1.9889 DKDLoss: 1.8329 
2022-05-01 23:44:09 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 4.4042, stu_CELoss: 2.2651 DKDLoss: 2.1391 
2022-05-01 23:44:41 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 4.3148, stu_CELoss: 2.3812 DKDLoss: 1.9336 
2022-05-01 23:45:12 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 4.6785, stu_CELoss: 2.4875 DKDLoss: 2.1910 
2022-05-01 23:45:44 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 4.1664, stu_CELoss: 2.2749 DKDLoss: 1.8915 
2022-05-01 23:46:15 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 4.3572, stu_CELoss: 2.2515 DKDLoss: 2.1056 
2022-05-01 23:46:47 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 3.8717, stu_CELoss: 2.0485 DKDLoss: 1.8232 
2022-05-01 23:47:19 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 4.3062, stu_CELoss: 2.2265 DKDLoss: 2.0797 
2022-05-01 23:47:50 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 4.4694, stu_CELoss: 2.3210 DKDLoss: 2.1484 
2022-05-01 23:48:21 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 4.0147, stu_CELoss: 2.0632 DKDLoss: 1.9515 
2022-05-01 23:48:52 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 4.7456, stu_CELoss: 2.5019 DKDLoss: 2.2436 
2022-05-01 23:49:23 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 4.1626, stu_CELoss: 2.2243 DKDLoss: 1.9383 
2022-05-01 23:49:54 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 4.4907, stu_CELoss: 2.3581 DKDLoss: 2.1326 
2022-05-01 23:50:26 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 4.4577, stu_CELoss: 2.3577 DKDLoss: 2.1001 
2022-05-01 23:50:58 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 4.4388, stu_CELoss: 2.2983 DKDLoss: 2.1405 
2022-05-01 23:51:29 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 4.3050, stu_CELoss: 2.2647 DKDLoss: 2.0403 
2022-05-01 23:52:01 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 4.5396, stu_CELoss: 2.4130 DKDLoss: 2.1266 
2022-05-01 23:52:32 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 4.2477, stu_CELoss: 2.2331 DKDLoss: 2.0146 
2022-05-01 23:53:04 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 3.9138, stu_CELoss: 2.0002 DKDLoss: 1.9136 
2022-05-01 23:53:35 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 3.8353, stu_CELoss: 1.9527 DKDLoss: 1.8825 
2022-05-01 23:54:07 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 3.8359, stu_CELoss: 1.9636 DKDLoss: 1.8723 
2022-05-01 23:54:38 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 4.0460, stu_CELoss: 2.0892 DKDLoss: 1.9568 
2022-05-01 23:55:09 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 4.1410, stu_CELoss: 2.1229 DKDLoss: 2.0181 
2022-05-01 23:55:40 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 4.0282, stu_CELoss: 2.1529 DKDLoss: 1.8752 
2022-05-01 23:56:12 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 4.0779, stu_CELoss: 2.2323 DKDLoss: 1.8456 
2022-05-01 23:56:44 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 4.1372, stu_CELoss: 2.0175 DKDLoss: 2.1197 
2022-05-01 23:57:15 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 4.1490, stu_CELoss: 2.1626 DKDLoss: 1.9864 
2022-05-01 23:57:46 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 4.2982, stu_CELoss: 2.1831 DKDLoss: 2.1151 
2022-05-01 23:58:18 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 4.1388, stu_CELoss: 2.2192 DKDLoss: 1.9196 
2022-05-01 23:58:48 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 4.0325, stu_CELoss: 2.0601 DKDLoss: 1.9724 
2022-05-01 23:58:50 - train: epoch 010, train_loss: 4.1592
2022-05-02 00:01:15 - eval: epoch: 010, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 53.116%, stu_acc5: 78.536%, stu_test_loss: 1.9935
2022-05-02 00:01:16 - until epoch: 010, tea_best_acc1: 78.330%, stu_best_acc1: 53.116%
2022-05-02 00:01:16 - epoch 011 lr: 0.1
2022-05-02 00:01:53 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 4.1365, stu_CELoss: 2.1934 DKDLoss: 1.9431 
2022-05-02 00:02:25 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 4.2463, stu_CELoss: 2.3088 DKDLoss: 1.9375 
2022-05-02 00:02:56 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 4.0934, stu_CELoss: 2.2516 DKDLoss: 1.8419 
2022-05-02 00:03:28 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 4.5619, stu_CELoss: 2.4134 DKDLoss: 2.1486 
2022-05-02 00:03:59 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 3.8011, stu_CELoss: 2.0146 DKDLoss: 1.7866 
2022-05-02 00:04:31 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 4.1705, stu_CELoss: 2.3261 DKDLoss: 1.8444 
2022-05-02 00:05:02 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 4.3357, stu_CELoss: 2.2071 DKDLoss: 2.1286 
2022-05-02 00:05:34 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 4.3571, stu_CELoss: 2.2485 DKDLoss: 2.1086 
2022-05-02 00:06:06 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 4.2444, stu_CELoss: 2.3157 DKDLoss: 1.9287 
2022-05-02 00:06:38 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 3.7141, stu_CELoss: 1.9914 DKDLoss: 1.7227 
2022-05-02 00:07:09 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 3.9280, stu_CELoss: 2.0803 DKDLoss: 1.8477 
2022-05-02 00:07:41 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 4.3452, stu_CELoss: 2.3074 DKDLoss: 2.0378 
2022-05-02 00:08:13 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 4.2320, stu_CELoss: 2.1656 DKDLoss: 2.0664 
2022-05-02 00:08:44 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 4.3914, stu_CELoss: 2.4048 DKDLoss: 1.9866 
2022-05-02 00:09:16 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 4.0436, stu_CELoss: 2.0853 DKDLoss: 1.9583 
2022-05-02 00:09:48 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 4.2047, stu_CELoss: 2.2536 DKDLoss: 1.9511 
2022-05-02 00:10:20 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 4.3970, stu_CELoss: 2.3149 DKDLoss: 2.0820 
2022-05-02 00:10:52 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 4.2494, stu_CELoss: 2.2370 DKDLoss: 2.0124 
2022-05-02 00:11:24 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 4.1228, stu_CELoss: 2.0554 DKDLoss: 2.0674 
2022-05-02 00:11:55 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 3.9526, stu_CELoss: 1.9669 DKDLoss: 1.9857 
2022-05-02 00:12:27 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 3.8137, stu_CELoss: 2.0036 DKDLoss: 1.8100 
2022-05-02 00:12:59 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 3.9473, stu_CELoss: 1.9517 DKDLoss: 1.9956 
2022-05-02 00:13:31 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 4.0987, stu_CELoss: 2.1738 DKDLoss: 1.9249 
2022-05-02 00:14:02 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 3.7966, stu_CELoss: 1.9444 DKDLoss: 1.8522 
2022-05-02 00:14:34 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.8881, stu_CELoss: 2.1139 DKDLoss: 1.7741 
2022-05-02 00:15:05 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 4.1678, stu_CELoss: 2.1526 DKDLoss: 2.0152 
2022-05-02 00:15:36 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 3.9458, stu_CELoss: 2.0235 DKDLoss: 1.9223 
2022-05-02 00:16:08 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 3.8052, stu_CELoss: 1.9548 DKDLoss: 1.8504 
2022-05-02 00:16:39 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 4.0542, stu_CELoss: 2.1660 DKDLoss: 1.8882 
2022-05-02 00:17:11 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 4.3857, stu_CELoss: 2.3984 DKDLoss: 1.9873 
2022-05-02 00:17:42 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 3.9240, stu_CELoss: 2.1036 DKDLoss: 1.8204 
2022-05-02 00:18:14 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 4.0112, stu_CELoss: 2.1246 DKDLoss: 1.8867 
2022-05-02 00:18:46 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 4.1401, stu_CELoss: 2.1704 DKDLoss: 1.9697 
2022-05-02 00:19:18 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 3.6897, stu_CELoss: 2.0354 DKDLoss: 1.6543 
2022-05-02 00:19:50 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 4.0089, stu_CELoss: 2.0194 DKDLoss: 1.9896 
2022-05-02 00:20:21 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.7971, stu_CELoss: 1.9531 DKDLoss: 1.8440 
2022-05-02 00:20:53 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 4.2507, stu_CELoss: 2.3236 DKDLoss: 1.9271 
2022-05-02 00:21:25 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 4.0468, stu_CELoss: 2.1466 DKDLoss: 1.9003 
2022-05-02 00:21:56 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 4.2518, stu_CELoss: 2.1708 DKDLoss: 2.0810 
2022-05-02 00:22:28 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 4.0096, stu_CELoss: 1.9462 DKDLoss: 2.0634 
2022-05-02 00:22:59 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 3.9160, stu_CELoss: 2.0129 DKDLoss: 1.9031 
2022-05-02 00:23:30 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 3.8093, stu_CELoss: 1.9859 DKDLoss: 1.8234 
2022-05-02 00:24:02 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 3.8604, stu_CELoss: 2.0052 DKDLoss: 1.8552 
2022-05-02 00:24:33 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 4.3598, stu_CELoss: 2.4750 DKDLoss: 1.8848 
2022-05-02 00:25:05 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 4.0917, stu_CELoss: 2.1267 DKDLoss: 1.9650 
2022-05-02 00:25:36 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 4.0565, stu_CELoss: 2.1788 DKDLoss: 1.8778 
2022-05-02 00:26:08 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 4.0724, stu_CELoss: 2.0931 DKDLoss: 1.9792 
2022-05-02 00:26:40 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 3.7164, stu_CELoss: 1.8598 DKDLoss: 1.8566 
2022-05-02 00:27:11 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 3.7442, stu_CELoss: 1.9142 DKDLoss: 1.8300 
2022-05-02 00:27:43 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 3.9351, stu_CELoss: 2.1101 DKDLoss: 1.8251 
2022-05-02 00:27:44 - train: epoch 011, train_loss: 4.0861
2022-05-02 00:30:10 - eval: epoch: 011, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 53.102%, stu_acc5: 78.698%, stu_test_loss: 1.9903
2022-05-02 00:30:10 - until epoch: 011, tea_best_acc1: 78.330%, stu_best_acc1: 53.116%
2022-05-02 00:30:10 - epoch 012 lr: 0.1
2022-05-02 00:30:47 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 3.7120, stu_CELoss: 1.9826 DKDLoss: 1.7294 
2022-05-02 00:31:19 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 3.6141, stu_CELoss: 1.7953 DKDLoss: 1.8188 
2022-05-02 00:31:51 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 3.8882, stu_CELoss: 2.0021 DKDLoss: 1.8861 
2022-05-02 00:32:22 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 4.2163, stu_CELoss: 2.2253 DKDLoss: 1.9911 
2022-05-02 00:32:54 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 4.2821, stu_CELoss: 2.2423 DKDLoss: 2.0398 
2022-05-02 00:33:25 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.8570, stu_CELoss: 1.8992 DKDLoss: 1.9578 
2022-05-02 00:33:57 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 3.8327, stu_CELoss: 1.9043 DKDLoss: 1.9284 
2022-05-02 00:34:28 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 4.3901, stu_CELoss: 2.4660 DKDLoss: 1.9241 
2022-05-02 00:35:00 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 3.9652, stu_CELoss: 2.1502 DKDLoss: 1.8150 
2022-05-02 00:35:31 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 3.7783, stu_CELoss: 1.9963 DKDLoss: 1.7820 
2022-05-02 00:36:03 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 4.2445, stu_CELoss: 2.3502 DKDLoss: 1.8944 
2022-05-02 00:36:35 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 3.6066, stu_CELoss: 1.8297 DKDLoss: 1.7769 
2022-05-02 00:37:06 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 3.8525, stu_CELoss: 2.0328 DKDLoss: 1.8198 
2022-05-02 00:37:38 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 4.1592, stu_CELoss: 2.2159 DKDLoss: 1.9432 
2022-05-02 00:38:09 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 3.7033, stu_CELoss: 1.9320 DKDLoss: 1.7712 
2022-05-02 00:38:41 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 4.0448, stu_CELoss: 2.0930 DKDLoss: 1.9518 
2022-05-02 00:39:13 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 3.7423, stu_CELoss: 1.9503 DKDLoss: 1.7921 
2022-05-02 00:39:45 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 4.1710, stu_CELoss: 2.2676 DKDLoss: 1.9034 
2022-05-02 00:40:17 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 4.1894, stu_CELoss: 2.2911 DKDLoss: 1.8983 
2022-05-02 00:40:48 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.9713, stu_CELoss: 2.1368 DKDLoss: 1.8346 
2022-05-02 00:41:20 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 4.2480, stu_CELoss: 2.3260 DKDLoss: 1.9220 
2022-05-02 00:41:53 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 4.2768, stu_CELoss: 2.3354 DKDLoss: 1.9414 
2022-05-02 00:42:25 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 3.9110, stu_CELoss: 2.0188 DKDLoss: 1.8922 
2022-05-02 00:42:57 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 4.1217, stu_CELoss: 2.0475 DKDLoss: 2.0742 
2022-05-02 00:43:29 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 3.9849, stu_CELoss: 1.9951 DKDLoss: 1.9899 
2022-05-02 00:44:01 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 3.6347, stu_CELoss: 1.9368 DKDLoss: 1.6978 
2022-05-02 00:44:32 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 4.1805, stu_CELoss: 2.2218 DKDLoss: 1.9587 
2022-05-02 00:45:04 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 3.9096, stu_CELoss: 2.1599 DKDLoss: 1.7497 
2022-05-02 00:45:36 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 3.9923, stu_CELoss: 2.1955 DKDLoss: 1.7968 
2022-05-02 00:46:08 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 3.7916, stu_CELoss: 1.9470 DKDLoss: 1.8447 
2022-05-02 00:46:40 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 4.1425, stu_CELoss: 2.1657 DKDLoss: 1.9768 
2022-05-02 00:47:11 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 4.0252, stu_CELoss: 2.0855 DKDLoss: 1.9396 
2022-05-02 00:47:43 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 4.0151, stu_CELoss: 2.1179 DKDLoss: 1.8971 
2022-05-02 00:48:14 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 3.9266, stu_CELoss: 2.1227 DKDLoss: 1.8038 
2022-05-02 00:48:45 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 4.2242, stu_CELoss: 2.1512 DKDLoss: 2.0731 
2022-05-02 00:49:17 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 4.0239, stu_CELoss: 2.0248 DKDLoss: 1.9991 
2022-05-02 00:49:49 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 4.1772, stu_CELoss: 2.3018 DKDLoss: 1.8754 
2022-05-02 00:50:21 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 4.3250, stu_CELoss: 2.3452 DKDLoss: 1.9798 
2022-05-02 00:50:52 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 3.7048, stu_CELoss: 1.8757 DKDLoss: 1.8290 
2022-05-02 00:51:24 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 3.7354, stu_CELoss: 2.0685 DKDLoss: 1.6669 
2022-05-02 00:51:56 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 4.1797, stu_CELoss: 2.1646 DKDLoss: 2.0151 
2022-05-02 00:52:28 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 3.9192, stu_CELoss: 2.0286 DKDLoss: 1.8907 
2022-05-02 00:53:00 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 4.2919, stu_CELoss: 2.3382 DKDLoss: 1.9538 
2022-05-02 00:53:31 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 3.7987, stu_CELoss: 2.0587 DKDLoss: 1.7400 
2022-05-02 00:54:03 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 4.1299, stu_CELoss: 2.1292 DKDLoss: 2.0007 
2022-05-02 00:54:35 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 4.1170, stu_CELoss: 2.1525 DKDLoss: 1.9645 
2022-05-02 00:55:07 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 3.9328, stu_CELoss: 2.0259 DKDLoss: 1.9070 
2022-05-02 00:55:38 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 4.1296, stu_CELoss: 2.1317 DKDLoss: 1.9979 
2022-05-02 00:56:10 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 3.6102, stu_CELoss: 1.9324 DKDLoss: 1.6778 
2022-05-02 00:56:41 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 3.9845, stu_CELoss: 2.1359 DKDLoss: 1.8486 
2022-05-02 00:56:43 - train: epoch 012, train_loss: 4.0262
2022-05-02 00:59:07 - eval: epoch: 012, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 54.402%, stu_acc5: 79.504%, stu_test_loss: 1.9353
2022-05-02 00:59:08 - until epoch: 012, tea_best_acc1: 78.330%, stu_best_acc1: 54.402%
2022-05-02 00:59:08 - epoch 013 lr: 0.1
2022-05-02 00:59:45 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 3.6153, stu_CELoss: 1.8582 DKDLoss: 1.7570 
2022-05-02 01:00:16 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 3.8886, stu_CELoss: 2.1314 DKDLoss: 1.7572 
2022-05-02 01:00:48 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 3.8334, stu_CELoss: 2.1405 DKDLoss: 1.6929 
2022-05-02 01:01:19 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 3.7150, stu_CELoss: 1.9569 DKDLoss: 1.7581 
2022-05-02 01:01:50 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 4.1278, stu_CELoss: 2.1618 DKDLoss: 1.9660 
2022-05-02 01:02:21 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 4.1039, stu_CELoss: 2.2240 DKDLoss: 1.8799 
2022-05-02 01:02:53 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 3.7251, stu_CELoss: 1.9006 DKDLoss: 1.8245 
2022-05-02 01:03:25 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 4.1910, stu_CELoss: 2.2364 DKDLoss: 1.9546 
2022-05-02 01:03:56 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 4.0920, stu_CELoss: 2.2183 DKDLoss: 1.8737 
2022-05-02 01:04:28 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 3.6632, stu_CELoss: 1.9383 DKDLoss: 1.7249 
2022-05-02 01:05:00 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 3.9921, stu_CELoss: 2.0700 DKDLoss: 1.9222 
2022-05-02 01:05:31 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 4.2660, stu_CELoss: 2.2707 DKDLoss: 1.9953 
2022-05-02 01:06:03 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 3.8438, stu_CELoss: 1.9783 DKDLoss: 1.8654 
2022-05-02 01:06:35 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 3.7797, stu_CELoss: 2.0342 DKDLoss: 1.7455 
2022-05-02 01:07:06 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.9320, stu_CELoss: 2.0961 DKDLoss: 1.8359 
2022-05-02 01:07:38 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 4.0261, stu_CELoss: 2.1482 DKDLoss: 1.8779 
2022-05-02 01:08:10 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 4.0857, stu_CELoss: 2.0777 DKDLoss: 2.0079 
2022-05-02 01:08:41 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 4.0733, stu_CELoss: 2.1442 DKDLoss: 1.9291 
2022-05-02 01:09:13 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 3.9966, stu_CELoss: 2.1509 DKDLoss: 1.8457 
2022-05-02 01:09:45 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.9781, stu_CELoss: 2.1514 DKDLoss: 1.8267 
2022-05-02 01:10:16 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 4.1405, stu_CELoss: 2.1923 DKDLoss: 1.9482 
2022-05-02 01:10:48 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 3.8715, stu_CELoss: 2.0226 DKDLoss: 1.8489 
2022-05-02 01:11:20 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 3.9235, stu_CELoss: 2.0030 DKDLoss: 1.9204 
2022-05-02 01:11:52 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 4.1892, stu_CELoss: 2.2188 DKDLoss: 1.9703 
2022-05-02 01:12:23 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 3.7213, stu_CELoss: 1.9845 DKDLoss: 1.7368 
2022-05-02 01:12:55 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 4.0721, stu_CELoss: 2.1675 DKDLoss: 1.9046 
2022-05-02 01:13:27 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 3.9570, stu_CELoss: 2.0938 DKDLoss: 1.8632 
2022-05-02 01:13:59 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 4.1238, stu_CELoss: 2.1113 DKDLoss: 2.0125 
2022-05-02 01:14:30 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 4.0046, stu_CELoss: 2.1374 DKDLoss: 1.8672 
2022-05-02 01:15:01 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 3.7230, stu_CELoss: 1.8567 DKDLoss: 1.8662 
2022-05-02 01:15:33 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 3.9574, stu_CELoss: 2.0637 DKDLoss: 1.8937 
2022-05-02 01:16:05 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.9902, stu_CELoss: 2.1577 DKDLoss: 1.8326 
2022-05-02 01:16:36 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 3.6101, stu_CELoss: 1.8618 DKDLoss: 1.7483 
2022-05-02 01:17:08 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 3.9893, stu_CELoss: 2.0549 DKDLoss: 1.9344 
2022-05-02 01:17:39 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 3.9795, stu_CELoss: 2.0975 DKDLoss: 1.8820 
2022-05-02 01:18:10 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 4.0769, stu_CELoss: 2.1436 DKDLoss: 1.9333 
2022-05-02 01:18:42 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 3.8240, stu_CELoss: 1.8933 DKDLoss: 1.9307 
2022-05-02 01:19:14 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 4.2103, stu_CELoss: 2.3328 DKDLoss: 1.8775 
2022-05-02 01:19:45 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 4.3168, stu_CELoss: 2.2513 DKDLoss: 2.0656 
2022-05-02 01:20:16 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 4.0106, stu_CELoss: 2.1034 DKDLoss: 1.9071 
2022-05-02 01:20:48 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 3.7686, stu_CELoss: 1.9329 DKDLoss: 1.8358 
2022-05-02 01:21:20 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 4.2262, stu_CELoss: 2.1745 DKDLoss: 2.0516 
2022-05-02 01:21:51 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.8350, stu_CELoss: 1.9768 DKDLoss: 1.8582 
2022-05-02 01:22:23 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 3.9925, stu_CELoss: 1.9813 DKDLoss: 2.0112 
2022-05-02 01:22:55 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 3.6576, stu_CELoss: 1.9224 DKDLoss: 1.7352 
2022-05-02 01:23:27 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 4.1462, stu_CELoss: 2.0347 DKDLoss: 2.1115 
2022-05-02 01:23:59 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 4.0977, stu_CELoss: 2.1980 DKDLoss: 1.8997 
2022-05-02 01:24:30 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 4.1196, stu_CELoss: 2.2199 DKDLoss: 1.8996 
2022-05-02 01:25:02 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 4.2297, stu_CELoss: 2.2176 DKDLoss: 2.0120 
2022-05-02 01:25:33 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 4.0033, stu_CELoss: 2.0767 DKDLoss: 1.9266 
2022-05-02 01:25:35 - train: epoch 013, train_loss: 3.9753
2022-05-02 01:27:59 - eval: epoch: 013, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 54.230%, stu_acc5: 78.948%, stu_test_loss: 1.9543
2022-05-02 01:28:00 - until epoch: 013, tea_best_acc1: 78.330%, stu_best_acc1: 54.402%
2022-05-02 01:28:00 - epoch 014 lr: 0.1
2022-05-02 01:28:37 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 3.8286, stu_CELoss: 2.0850 DKDLoss: 1.7436 
2022-05-02 01:29:07 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 4.0263, stu_CELoss: 2.1855 DKDLoss: 1.8409 
2022-05-02 01:29:38 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 3.5757, stu_CELoss: 1.9487 DKDLoss: 1.6270 
2022-05-02 01:30:09 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 3.5930, stu_CELoss: 1.8807 DKDLoss: 1.7123 
2022-05-02 01:30:40 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 3.6273, stu_CELoss: 2.0060 DKDLoss: 1.6214 
2022-05-02 01:31:12 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 3.9502, stu_CELoss: 2.2235 DKDLoss: 1.7267 
2022-05-02 01:31:43 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 3.6221, stu_CELoss: 1.9296 DKDLoss: 1.6925 
2022-05-02 01:32:14 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 3.9068, stu_CELoss: 2.0560 DKDLoss: 1.8508 
2022-05-02 01:32:46 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 3.7793, stu_CELoss: 2.0174 DKDLoss: 1.7620 
2022-05-02 01:33:18 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 4.0227, stu_CELoss: 2.1215 DKDLoss: 1.9012 
2022-05-02 01:33:50 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 3.7448, stu_CELoss: 1.9412 DKDLoss: 1.8036 
2022-05-02 01:34:22 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 4.1909, stu_CELoss: 2.2304 DKDLoss: 1.9604 
2022-05-02 01:34:53 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 4.0874, stu_CELoss: 2.1523 DKDLoss: 1.9351 
2022-05-02 01:35:25 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 4.0222, stu_CELoss: 2.1971 DKDLoss: 1.8251 
2022-05-02 01:35:57 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 4.0804, stu_CELoss: 2.1259 DKDLoss: 1.9545 
2022-05-02 01:36:28 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 3.8087, stu_CELoss: 2.0271 DKDLoss: 1.7816 
2022-05-02 01:37:00 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 4.1396, stu_CELoss: 2.2070 DKDLoss: 1.9325 
2022-05-02 01:37:33 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 4.2144, stu_CELoss: 2.1967 DKDLoss: 2.0177 
2022-05-02 01:38:05 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 3.7668, stu_CELoss: 2.0373 DKDLoss: 1.7294 
2022-05-02 01:38:36 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 3.6722, stu_CELoss: 1.8952 DKDLoss: 1.7770 
2022-05-02 01:39:08 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.9746, stu_CELoss: 2.1072 DKDLoss: 1.8674 
2022-05-02 01:39:40 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 3.7283, stu_CELoss: 1.9090 DKDLoss: 1.8193 
2022-05-02 01:40:12 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 3.9541, stu_CELoss: 2.0529 DKDLoss: 1.9013 
2022-05-02 01:40:44 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 4.0498, stu_CELoss: 2.1605 DKDLoss: 1.8893 
2022-05-02 01:41:16 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 3.7644, stu_CELoss: 1.9763 DKDLoss: 1.7880 
2022-05-02 01:41:48 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 3.6625, stu_CELoss: 1.8979 DKDLoss: 1.7646 
2022-05-02 01:42:20 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 3.7585, stu_CELoss: 2.0093 DKDLoss: 1.7491 
2022-05-02 01:42:51 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 4.2408, stu_CELoss: 2.2971 DKDLoss: 1.9437 
2022-05-02 01:43:23 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 4.1255, stu_CELoss: 2.1811 DKDLoss: 1.9444 
2022-05-02 01:43:55 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 4.1331, stu_CELoss: 2.1839 DKDLoss: 1.9492 
2022-05-02 01:44:27 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 3.8008, stu_CELoss: 1.9941 DKDLoss: 1.8067 
2022-05-02 01:44:58 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 4.0357, stu_CELoss: 2.1023 DKDLoss: 1.9335 
2022-05-02 01:45:30 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 3.7706, stu_CELoss: 1.9263 DKDLoss: 1.8444 
2022-05-02 01:46:02 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 3.7279, stu_CELoss: 1.9871 DKDLoss: 1.7407 
2022-05-02 01:46:33 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 3.8702, stu_CELoss: 2.0312 DKDLoss: 1.8391 
2022-05-02 01:47:05 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 3.6884, stu_CELoss: 1.9360 DKDLoss: 1.7524 
2022-05-02 01:47:37 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 4.0132, stu_CELoss: 2.1191 DKDLoss: 1.8941 
2022-05-02 01:48:08 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 4.0715, stu_CELoss: 2.2113 DKDLoss: 1.8602 
2022-05-02 01:48:40 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 4.0217, stu_CELoss: 2.1294 DKDLoss: 1.8924 
2022-05-02 01:49:12 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 4.0262, stu_CELoss: 2.0712 DKDLoss: 1.9550 
2022-05-02 01:49:44 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 4.2866, stu_CELoss: 2.1662 DKDLoss: 2.1204 
2022-05-02 01:50:15 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 4.0519, stu_CELoss: 2.0148 DKDLoss: 2.0371 
2022-05-02 01:50:47 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 3.9465, stu_CELoss: 2.1138 DKDLoss: 1.8327 
2022-05-02 01:51:18 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 3.7614, stu_CELoss: 1.9631 DKDLoss: 1.7983 
2022-05-02 01:51:50 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 3.8077, stu_CELoss: 2.0514 DKDLoss: 1.7563 
2022-05-02 01:52:22 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 3.8021, stu_CELoss: 1.9539 DKDLoss: 1.8481 
2022-05-02 01:52:53 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 3.6849, stu_CELoss: 1.8926 DKDLoss: 1.7923 
2022-05-02 01:53:26 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 4.1744, stu_CELoss: 2.2110 DKDLoss: 1.9634 
2022-05-02 01:53:57 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 4.0509, stu_CELoss: 2.1766 DKDLoss: 1.8742 
2022-05-02 01:54:28 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 4.0202, stu_CELoss: 2.1822 DKDLoss: 1.8380 
2022-05-02 01:54:30 - train: epoch 014, train_loss: 3.9384
2022-05-02 01:56:54 - eval: epoch: 014, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 49.214%, stu_acc5: 74.950%, stu_test_loss: 2.2150
2022-05-02 01:56:54 - until epoch: 014, tea_best_acc1: 78.330%, stu_best_acc1: 54.402%
2022-05-02 01:56:54 - epoch 015 lr: 0.1
2022-05-02 01:57:32 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 3.6154, stu_CELoss: 1.9005 DKDLoss: 1.7149 
2022-05-02 01:58:03 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 4.1013, stu_CELoss: 2.2681 DKDLoss: 1.8332 
2022-05-02 01:58:34 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 4.1472, stu_CELoss: 2.2755 DKDLoss: 1.8717 
2022-05-02 01:59:05 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 3.9797, stu_CELoss: 2.0362 DKDLoss: 1.9435 
2022-05-02 01:59:37 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 3.8024, stu_CELoss: 1.9595 DKDLoss: 1.8429 
2022-05-02 02:00:08 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.8151, stu_CELoss: 2.0788 DKDLoss: 1.7363 
2022-05-02 02:00:39 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 3.7729, stu_CELoss: 1.9836 DKDLoss: 1.7893 
2022-05-02 02:01:10 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 3.6801, stu_CELoss: 1.9591 DKDLoss: 1.7211 
2022-05-02 02:01:42 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 3.5885, stu_CELoss: 1.9296 DKDLoss: 1.6589 
2022-05-02 02:02:13 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 3.8551, stu_CELoss: 2.0523 DKDLoss: 1.8027 
2022-05-02 02:02:45 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 3.8875, stu_CELoss: 2.0962 DKDLoss: 1.7913 
2022-05-02 02:03:16 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 3.8630, stu_CELoss: 2.1365 DKDLoss: 1.7265 
2022-05-02 02:03:47 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 4.0099, stu_CELoss: 2.1917 DKDLoss: 1.8182 
2022-05-02 02:04:19 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 3.4438, stu_CELoss: 1.8952 DKDLoss: 1.5486 
2022-05-02 02:04:50 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 3.7571, stu_CELoss: 1.9759 DKDLoss: 1.7813 
2022-05-02 02:05:21 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 4.1820, stu_CELoss: 2.1901 DKDLoss: 1.9919 
2022-05-02 02:05:53 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 4.2437, stu_CELoss: 2.1728 DKDLoss: 2.0708 
2022-05-02 02:06:25 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 3.7847, stu_CELoss: 2.0184 DKDLoss: 1.7663 
2022-05-02 02:06:57 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 3.8932, stu_CELoss: 1.9663 DKDLoss: 1.9269 
2022-05-02 02:07:28 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 3.5959, stu_CELoss: 1.8887 DKDLoss: 1.7072 
2022-05-02 02:08:00 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 3.8402, stu_CELoss: 1.9908 DKDLoss: 1.8494 
2022-05-02 02:08:32 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 3.9760, stu_CELoss: 2.1274 DKDLoss: 1.8485 
2022-05-02 02:09:03 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 4.1020, stu_CELoss: 2.1299 DKDLoss: 1.9721 
2022-05-02 02:09:34 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 3.6977, stu_CELoss: 2.0081 DKDLoss: 1.6896 
2022-05-02 02:10:06 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.9344, stu_CELoss: 2.1597 DKDLoss: 1.7748 
2022-05-02 02:10:38 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 3.8321, stu_CELoss: 1.9552 DKDLoss: 1.8769 
2022-05-02 02:11:09 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.8159, stu_CELoss: 2.1652 DKDLoss: 1.6507 
2022-05-02 02:11:41 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 3.7667, stu_CELoss: 2.0032 DKDLoss: 1.7635 
2022-05-02 02:12:13 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 3.8422, stu_CELoss: 2.0648 DKDLoss: 1.7773 
2022-05-02 02:12:44 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 3.6067, stu_CELoss: 1.8491 DKDLoss: 1.7575 
2022-05-02 02:13:16 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 3.7494, stu_CELoss: 2.1042 DKDLoss: 1.6452 
2022-05-02 02:13:48 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 3.8273, stu_CELoss: 1.9743 DKDLoss: 1.8530 
2022-05-02 02:14:19 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 3.7252, stu_CELoss: 1.9999 DKDLoss: 1.7253 
2022-05-02 02:14:50 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 3.7381, stu_CELoss: 1.9704 DKDLoss: 1.7676 
2022-05-02 02:15:21 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 4.2243, stu_CELoss: 2.2221 DKDLoss: 2.0022 
2022-05-02 02:15:53 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 4.2208, stu_CELoss: 2.2359 DKDLoss: 1.9848 
2022-05-02 02:16:24 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 3.5698, stu_CELoss: 1.7401 DKDLoss: 1.8296 
2022-05-02 02:16:56 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 3.7984, stu_CELoss: 2.0428 DKDLoss: 1.7555 
2022-05-02 02:17:27 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 4.2325, stu_CELoss: 2.1939 DKDLoss: 2.0387 
2022-05-02 02:17:58 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 3.6937, stu_CELoss: 1.9856 DKDLoss: 1.7081 
2022-05-02 02:18:29 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 4.0348, stu_CELoss: 2.0639 DKDLoss: 1.9709 
2022-05-02 02:19:01 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 3.6790, stu_CELoss: 1.9104 DKDLoss: 1.7686 
2022-05-02 02:19:32 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 3.8823, stu_CELoss: 1.9233 DKDLoss: 1.9590 
2022-05-02 02:20:04 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 3.9194, stu_CELoss: 2.0666 DKDLoss: 1.8528 
2022-05-02 02:20:35 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 3.7899, stu_CELoss: 1.9900 DKDLoss: 1.7998 
2022-05-02 02:21:06 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 3.9859, stu_CELoss: 2.0978 DKDLoss: 1.8881 
2022-05-02 02:21:38 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 3.9321, stu_CELoss: 2.0844 DKDLoss: 1.8477 
2022-05-02 02:22:09 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 4.0590, stu_CELoss: 2.1777 DKDLoss: 1.8813 
2022-05-02 02:22:41 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 3.8712, stu_CELoss: 1.9978 DKDLoss: 1.8734 
2022-05-02 02:23:12 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 3.9690, stu_CELoss: 2.1991 DKDLoss: 1.7698 
2022-05-02 02:23:14 - train: epoch 015, train_loss: 3.9021
2022-05-02 02:25:39 - eval: epoch: 015, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 53.666%, stu_acc5: 79.002%, stu_test_loss: 1.9714
2022-05-02 02:25:39 - until epoch: 015, tea_best_acc1: 78.330%, stu_best_acc1: 54.402%
2022-05-02 02:25:39 - epoch 016 lr: 0.1
2022-05-02 02:26:17 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 3.6640, stu_CELoss: 1.9582 DKDLoss: 1.7059 
2022-05-02 02:26:48 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 3.7125, stu_CELoss: 1.8273 DKDLoss: 1.8852 
2022-05-02 02:27:20 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 3.7465, stu_CELoss: 2.0380 DKDLoss: 1.7085 
2022-05-02 02:27:51 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.8881, stu_CELoss: 2.0457 DKDLoss: 1.8423 
2022-05-02 02:28:23 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 3.8919, stu_CELoss: 2.1002 DKDLoss: 1.7917 
2022-05-02 02:28:54 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 3.5873, stu_CELoss: 1.8214 DKDLoss: 1.7659 
2022-05-02 02:29:26 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 3.6789, stu_CELoss: 1.9617 DKDLoss: 1.7173 
2022-05-02 02:29:58 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 3.6802, stu_CELoss: 1.9271 DKDLoss: 1.7531 
2022-05-02 02:30:29 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.8964, stu_CELoss: 2.0917 DKDLoss: 1.8046 
2022-05-02 02:31:01 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 3.9998, stu_CELoss: 2.0095 DKDLoss: 1.9903 
2022-05-02 02:31:33 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 3.9930, stu_CELoss: 2.0939 DKDLoss: 1.8991 
2022-05-02 02:32:04 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 3.7705, stu_CELoss: 1.9220 DKDLoss: 1.8485 
2022-05-02 02:32:36 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 4.0030, stu_CELoss: 2.1411 DKDLoss: 1.8618 
2022-05-02 02:33:08 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 3.4541, stu_CELoss: 1.9016 DKDLoss: 1.5525 
2022-05-02 02:33:39 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 4.2130, stu_CELoss: 2.2790 DKDLoss: 1.9340 
2022-05-02 02:34:10 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 4.1421, stu_CELoss: 2.3300 DKDLoss: 1.8120 
2022-05-02 02:34:41 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 3.8407, stu_CELoss: 2.0319 DKDLoss: 1.8089 
2022-05-02 02:35:13 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 4.1879, stu_CELoss: 2.2538 DKDLoss: 1.9341 
2022-05-02 02:35:45 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 3.6020, stu_CELoss: 1.9821 DKDLoss: 1.6199 
2022-05-02 02:36:17 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 3.3153, stu_CELoss: 1.6088 DKDLoss: 1.7066 
2022-05-02 02:36:48 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 3.9604, stu_CELoss: 2.1139 DKDLoss: 1.8465 
2022-05-02 02:37:20 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 4.1256, stu_CELoss: 2.1697 DKDLoss: 1.9559 
2022-05-02 02:37:52 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 4.2009, stu_CELoss: 2.2113 DKDLoss: 1.9896 
2022-05-02 02:38:23 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 3.8868, stu_CELoss: 2.0199 DKDLoss: 1.8669 
2022-05-02 02:38:55 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 3.8328, stu_CELoss: 2.0275 DKDLoss: 1.8053 
2022-05-02 02:39:26 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 4.1233, stu_CELoss: 2.1105 DKDLoss: 2.0128 
2022-05-02 02:39:57 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 3.7086, stu_CELoss: 1.9785 DKDLoss: 1.7301 
2022-05-02 02:40:28 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 3.5106, stu_CELoss: 1.8369 DKDLoss: 1.6737 
2022-05-02 02:41:00 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 4.0478, stu_CELoss: 2.0790 DKDLoss: 1.9689 
2022-05-02 02:41:31 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 4.0676, stu_CELoss: 2.1822 DKDLoss: 1.8854 
2022-05-02 02:42:02 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 3.8891, stu_CELoss: 2.0665 DKDLoss: 1.8225 
2022-05-02 02:42:33 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 4.0570, stu_CELoss: 2.1639 DKDLoss: 1.8931 
2022-05-02 02:43:04 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 3.9793, stu_CELoss: 2.1852 DKDLoss: 1.7941 
2022-05-02 02:43:36 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 3.6194, stu_CELoss: 1.9473 DKDLoss: 1.6721 
2022-05-02 02:44:08 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 3.6184, stu_CELoss: 1.8663 DKDLoss: 1.7521 
2022-05-02 02:44:40 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 3.6775, stu_CELoss: 1.9125 DKDLoss: 1.7650 
2022-05-02 02:45:11 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.7569, stu_CELoss: 1.9727 DKDLoss: 1.7842 
2022-05-02 02:45:42 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 4.1402, stu_CELoss: 2.3070 DKDLoss: 1.8332 
2022-05-02 02:46:14 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 3.9644, stu_CELoss: 2.1321 DKDLoss: 1.8323 
2022-05-02 02:46:45 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 3.9730, stu_CELoss: 2.1478 DKDLoss: 1.8252 
2022-05-02 02:47:16 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 3.9350, stu_CELoss: 2.0071 DKDLoss: 1.9279 
2022-05-02 02:47:48 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 3.8426, stu_CELoss: 1.9970 DKDLoss: 1.8456 
2022-05-02 02:48:20 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 3.8157, stu_CELoss: 1.9418 DKDLoss: 1.8740 
2022-05-02 02:48:52 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 3.5981, stu_CELoss: 1.9363 DKDLoss: 1.6618 
2022-05-02 02:49:23 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 4.0438, stu_CELoss: 2.1694 DKDLoss: 1.8743 
2022-05-02 02:49:54 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 3.7197, stu_CELoss: 1.9677 DKDLoss: 1.7520 
2022-05-02 02:50:26 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.8255, stu_CELoss: 2.1489 DKDLoss: 1.6767 
2022-05-02 02:50:57 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 3.8202, stu_CELoss: 2.0681 DKDLoss: 1.7522 
2022-05-02 02:51:28 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 3.9753, stu_CELoss: 2.0721 DKDLoss: 1.9032 
2022-05-02 02:51:59 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 4.1043, stu_CELoss: 2.1333 DKDLoss: 1.9711 
2022-05-02 02:52:01 - train: epoch 016, train_loss: 3.8735
2022-05-02 02:54:26 - eval: epoch: 016, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 51.350%, stu_acc5: 76.448%, stu_test_loss: 2.1318
2022-05-02 02:54:26 - until epoch: 016, tea_best_acc1: 78.330%, stu_best_acc1: 54.402%
2022-05-02 02:54:26 - epoch 017 lr: 0.1
2022-05-02 02:55:03 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 3.8692, stu_CELoss: 2.0675 DKDLoss: 1.8016 
2022-05-02 02:55:34 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 4.1098, stu_CELoss: 2.1916 DKDLoss: 1.9182 
2022-05-02 02:56:05 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 4.0584, stu_CELoss: 2.1993 DKDLoss: 1.8590 
2022-05-02 02:56:36 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 3.4676, stu_CELoss: 1.7377 DKDLoss: 1.7298 
2022-05-02 02:57:08 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 3.7474, stu_CELoss: 2.0804 DKDLoss: 1.6671 
2022-05-02 02:57:39 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 4.2586, stu_CELoss: 2.1945 DKDLoss: 2.0642 
2022-05-02 02:58:10 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 3.8294, stu_CELoss: 2.0313 DKDLoss: 1.7981 
2022-05-02 02:58:41 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 3.6112, stu_CELoss: 1.9356 DKDLoss: 1.6756 
2022-05-02 02:59:12 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 3.8653, stu_CELoss: 1.9727 DKDLoss: 1.8926 
2022-05-02 02:59:44 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 3.9957, stu_CELoss: 2.1293 DKDLoss: 1.8664 
2022-05-02 03:00:16 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 4.0849, stu_CELoss: 2.2828 DKDLoss: 1.8021 
2022-05-02 03:00:47 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 3.6541, stu_CELoss: 1.9064 DKDLoss: 1.7477 
2022-05-02 03:01:19 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 4.0154, stu_CELoss: 2.2208 DKDLoss: 1.7946 
2022-05-02 03:01:51 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 3.9898, stu_CELoss: 2.0932 DKDLoss: 1.8966 
2022-05-02 03:02:22 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 3.6731, stu_CELoss: 1.9304 DKDLoss: 1.7427 
2022-05-02 03:02:55 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 3.8412, stu_CELoss: 2.0478 DKDLoss: 1.7934 
2022-05-02 03:03:26 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 3.7134, stu_CELoss: 1.8932 DKDLoss: 1.8202 
2022-05-02 03:03:57 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 3.9044, stu_CELoss: 2.1479 DKDLoss: 1.7565 
2022-05-02 03:04:29 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 3.7130, stu_CELoss: 1.9466 DKDLoss: 1.7664 
2022-05-02 03:05:01 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 3.8249, stu_CELoss: 2.0320 DKDLoss: 1.7928 
2022-05-02 03:05:33 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 3.9401, stu_CELoss: 2.0892 DKDLoss: 1.8509 
2022-05-02 03:06:04 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 3.4949, stu_CELoss: 1.7666 DKDLoss: 1.7283 
2022-05-02 03:06:35 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 3.7967, stu_CELoss: 2.0631 DKDLoss: 1.7336 
2022-05-02 03:07:07 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 3.7919, stu_CELoss: 2.0564 DKDLoss: 1.7355 
2022-05-02 03:07:38 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 3.8773, stu_CELoss: 2.0658 DKDLoss: 1.8115 
2022-05-02 03:08:09 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 3.9438, stu_CELoss: 2.0263 DKDLoss: 1.9175 
2022-05-02 03:08:40 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 3.7388, stu_CELoss: 1.9929 DKDLoss: 1.7459 
2022-05-02 03:09:12 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.7621, stu_CELoss: 1.9366 DKDLoss: 1.8256 
2022-05-02 03:09:44 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 4.3438, stu_CELoss: 2.3699 DKDLoss: 1.9739 
2022-05-02 03:10:15 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 3.9756, stu_CELoss: 2.0826 DKDLoss: 1.8930 
2022-05-02 03:10:47 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.6762, stu_CELoss: 1.9853 DKDLoss: 1.6909 
2022-05-02 03:11:18 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 3.6093, stu_CELoss: 1.8650 DKDLoss: 1.7442 
2022-05-02 03:11:50 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 3.8229, stu_CELoss: 2.1893 DKDLoss: 1.6336 
2022-05-02 03:12:21 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 3.8613, stu_CELoss: 1.9885 DKDLoss: 1.8727 
2022-05-02 03:12:53 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 4.1003, stu_CELoss: 2.1570 DKDLoss: 1.9434 
2022-05-02 03:13:24 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 4.0391, stu_CELoss: 2.1120 DKDLoss: 1.9270 
2022-05-02 03:13:55 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 3.5057, stu_CELoss: 1.8738 DKDLoss: 1.6319 
2022-05-02 03:14:27 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 4.4222, stu_CELoss: 2.3972 DKDLoss: 2.0251 
2022-05-02 03:14:58 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 3.7023, stu_CELoss: 1.8982 DKDLoss: 1.8041 
2022-05-02 03:15:29 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 3.6143, stu_CELoss: 2.0116 DKDLoss: 1.6027 
2022-05-02 03:16:01 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 4.1989, stu_CELoss: 2.1969 DKDLoss: 2.0019 
2022-05-02 03:16:32 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 3.9415, stu_CELoss: 2.0679 DKDLoss: 1.8736 
2022-05-02 03:17:03 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 3.7287, stu_CELoss: 1.9175 DKDLoss: 1.8112 
2022-05-02 03:17:35 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 3.8952, stu_CELoss: 2.1132 DKDLoss: 1.7821 
2022-05-02 03:18:06 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 3.9509, stu_CELoss: 2.0850 DKDLoss: 1.8658 
2022-05-02 03:18:38 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 3.6887, stu_CELoss: 1.9109 DKDLoss: 1.7778 
2022-05-02 03:19:09 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 4.3139, stu_CELoss: 2.2975 DKDLoss: 2.0164 
2022-05-02 03:19:41 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 3.9220, stu_CELoss: 2.1304 DKDLoss: 1.7915 
2022-05-02 03:20:12 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 3.8883, stu_CELoss: 2.0693 DKDLoss: 1.8190 
2022-05-02 03:20:43 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 3.7533, stu_CELoss: 2.0150 DKDLoss: 1.7384 
2022-05-02 03:20:44 - train: epoch 017, train_loss: 3.8466
2022-05-02 03:23:09 - eval: epoch: 017, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 55.608%, stu_acc5: 80.382%, stu_test_loss: 1.8807
2022-05-02 03:23:10 - until epoch: 017, tea_best_acc1: 78.330%, stu_best_acc1: 55.608%
2022-05-02 03:23:10 - epoch 018 lr: 0.1
2022-05-02 03:23:47 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 3.6374, stu_CELoss: 1.8598 DKDLoss: 1.7775 
2022-05-02 03:24:18 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 4.2003, stu_CELoss: 2.3505 DKDLoss: 1.8498 
2022-05-02 03:24:50 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 4.1267, stu_CELoss: 2.1611 DKDLoss: 1.9656 
2022-05-02 03:25:21 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 3.9273, stu_CELoss: 2.0943 DKDLoss: 1.8330 
2022-05-02 03:25:53 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 3.9886, stu_CELoss: 2.1139 DKDLoss: 1.8747 
2022-05-02 03:26:25 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 3.9932, stu_CELoss: 2.1729 DKDLoss: 1.8203 
2022-05-02 03:26:56 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 3.3904, stu_CELoss: 1.6993 DKDLoss: 1.6911 
2022-05-02 03:27:28 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 3.9946, stu_CELoss: 2.0758 DKDLoss: 1.9187 
2022-05-02 03:28:00 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 4.0251, stu_CELoss: 2.2726 DKDLoss: 1.7525 
2022-05-02 03:28:32 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 3.7056, stu_CELoss: 1.9332 DKDLoss: 1.7724 
2022-05-02 03:29:04 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.9808, stu_CELoss: 2.0881 DKDLoss: 1.8927 
2022-05-02 03:29:35 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 3.4599, stu_CELoss: 1.8224 DKDLoss: 1.6376 
2022-05-02 03:30:07 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 4.0988, stu_CELoss: 2.1995 DKDLoss: 1.8993 
2022-05-02 03:30:38 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 3.9969, stu_CELoss: 2.1795 DKDLoss: 1.8174 
2022-05-02 03:31:10 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 4.0857, stu_CELoss: 2.2338 DKDLoss: 1.8519 
2022-05-02 03:31:41 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 3.9824, stu_CELoss: 2.0406 DKDLoss: 1.9418 
2022-05-02 03:32:13 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 3.8560, stu_CELoss: 2.1258 DKDLoss: 1.7302 
2022-05-02 03:32:44 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 3.4333, stu_CELoss: 1.8194 DKDLoss: 1.6139 
2022-05-02 03:33:16 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 3.9493, stu_CELoss: 2.0640 DKDLoss: 1.8853 
2022-05-02 03:33:48 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 4.3398, stu_CELoss: 2.4001 DKDLoss: 1.9397 
2022-05-02 03:34:20 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.9721, stu_CELoss: 2.1417 DKDLoss: 1.8304 
2022-05-02 03:34:52 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 4.0226, stu_CELoss: 2.1113 DKDLoss: 1.9113 
2022-05-02 03:35:24 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 3.8610, stu_CELoss: 2.0452 DKDLoss: 1.8158 
2022-05-02 03:35:55 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 3.8014, stu_CELoss: 2.0998 DKDLoss: 1.7016 
2022-05-02 03:36:28 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 3.6202, stu_CELoss: 1.8508 DKDLoss: 1.7694 
2022-05-02 03:37:00 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 3.7103, stu_CELoss: 1.9447 DKDLoss: 1.7655 
2022-05-02 03:37:32 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 3.8225, stu_CELoss: 2.1015 DKDLoss: 1.7211 
2022-05-02 03:38:03 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 3.5073, stu_CELoss: 1.8456 DKDLoss: 1.6618 
2022-05-02 03:38:35 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 3.8240, stu_CELoss: 2.0637 DKDLoss: 1.7602 
2022-05-02 03:39:07 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 3.7004, stu_CELoss: 1.9941 DKDLoss: 1.7063 
2022-05-02 03:39:38 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 4.1391, stu_CELoss: 2.3773 DKDLoss: 1.7618 
2022-05-02 03:40:11 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 3.7970, stu_CELoss: 2.0463 DKDLoss: 1.7508 
2022-05-02 03:40:42 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 3.9504, stu_CELoss: 2.0564 DKDLoss: 1.8940 
2022-05-02 03:41:15 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 3.8262, stu_CELoss: 2.1514 DKDLoss: 1.6749 
2022-05-02 03:41:47 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 3.7705, stu_CELoss: 2.0363 DKDLoss: 1.7342 
2022-05-02 03:42:18 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 3.8616, stu_CELoss: 2.0087 DKDLoss: 1.8529 
2022-05-02 03:42:50 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 4.1449, stu_CELoss: 2.1860 DKDLoss: 1.9589 
2022-05-02 03:43:21 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 3.9782, stu_CELoss: 2.1709 DKDLoss: 1.8074 
2022-05-02 03:43:53 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 3.9019, stu_CELoss: 2.2079 DKDLoss: 1.6941 
2022-05-02 03:44:24 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 4.0547, stu_CELoss: 2.2219 DKDLoss: 1.8328 
2022-05-02 03:44:56 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 3.9783, stu_CELoss: 2.2549 DKDLoss: 1.7234 
2022-05-02 03:45:28 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 3.7073, stu_CELoss: 1.9680 DKDLoss: 1.7393 
2022-05-02 03:45:59 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 3.9628, stu_CELoss: 2.2170 DKDLoss: 1.7458 
2022-05-02 03:46:31 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.6640, stu_CELoss: 1.8740 DKDLoss: 1.7900 
2022-05-02 03:47:02 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 4.1717, stu_CELoss: 2.2078 DKDLoss: 1.9640 
2022-05-02 03:47:34 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 3.9809, stu_CELoss: 2.0871 DKDLoss: 1.8938 
2022-05-02 03:48:07 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 3.9347, stu_CELoss: 2.1039 DKDLoss: 1.8307 
2022-05-02 03:48:38 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 3.6298, stu_CELoss: 1.9497 DKDLoss: 1.6801 
2022-05-02 03:49:10 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 3.7147, stu_CELoss: 1.9395 DKDLoss: 1.7752 
2022-05-02 03:49:41 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 3.8683, stu_CELoss: 2.1649 DKDLoss: 1.7033 
2022-05-02 03:49:42 - train: epoch 018, train_loss: 3.8176
2022-05-02 03:52:07 - eval: epoch: 018, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 55.096%, stu_acc5: 80.068%, stu_test_loss: 1.8854
2022-05-02 03:52:08 - until epoch: 018, tea_best_acc1: 78.330%, stu_best_acc1: 55.608%
2022-05-02 03:52:08 - epoch 019 lr: 0.1
2022-05-02 03:52:45 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 3.4128, stu_CELoss: 1.7648 DKDLoss: 1.6480 
2022-05-02 03:53:16 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 3.5675, stu_CELoss: 1.8538 DKDLoss: 1.7137 
2022-05-02 03:53:48 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 3.9672, stu_CELoss: 2.1453 DKDLoss: 1.8219 
2022-05-02 03:54:19 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 3.9099, stu_CELoss: 2.0999 DKDLoss: 1.8100 
2022-05-02 03:54:50 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 3.6250, stu_CELoss: 1.9418 DKDLoss: 1.6832 
2022-05-02 03:55:21 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 3.9008, stu_CELoss: 2.1372 DKDLoss: 1.7636 
2022-05-02 03:55:53 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 3.5805, stu_CELoss: 1.9542 DKDLoss: 1.6262 
2022-05-02 03:56:25 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 3.7147, stu_CELoss: 2.0253 DKDLoss: 1.6893 
2022-05-02 03:56:56 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 3.6702, stu_CELoss: 1.9329 DKDLoss: 1.7373 
2022-05-02 03:57:28 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 3.9660, stu_CELoss: 2.1851 DKDLoss: 1.7810 
2022-05-02 03:57:59 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 3.7096, stu_CELoss: 1.9523 DKDLoss: 1.7574 
2022-05-02 03:58:30 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 3.7974, stu_CELoss: 2.0703 DKDLoss: 1.7271 
2022-05-02 03:59:02 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 3.7469, stu_CELoss: 2.1211 DKDLoss: 1.6258 
2022-05-02 03:59:33 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 3.6063, stu_CELoss: 1.9273 DKDLoss: 1.6791 
2022-05-02 04:00:05 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 4.2036, stu_CELoss: 2.2563 DKDLoss: 1.9473 
2022-05-02 04:00:37 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 3.5762, stu_CELoss: 1.7939 DKDLoss: 1.7822 
2022-05-02 04:01:09 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.9436, stu_CELoss: 2.1704 DKDLoss: 1.7732 
2022-05-02 04:01:41 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 3.7296, stu_CELoss: 1.9831 DKDLoss: 1.7465 
2022-05-02 04:02:13 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 4.1234, stu_CELoss: 2.1655 DKDLoss: 1.9578 
2022-05-02 04:02:45 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 3.6621, stu_CELoss: 1.9513 DKDLoss: 1.7108 
2022-05-02 04:03:17 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 3.7914, stu_CELoss: 1.9866 DKDLoss: 1.8047 
2022-05-02 04:03:48 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 3.9221, stu_CELoss: 2.0722 DKDLoss: 1.8499 
2022-05-02 04:04:20 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 3.9187, stu_CELoss: 1.9882 DKDLoss: 1.9305 
2022-05-02 04:04:51 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 4.0477, stu_CELoss: 2.1166 DKDLoss: 1.9311 
2022-05-02 04:05:23 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 3.6619, stu_CELoss: 2.0132 DKDLoss: 1.6487 
2022-05-02 04:05:54 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 3.6038, stu_CELoss: 1.8687 DKDLoss: 1.7351 
2022-05-02 04:06:26 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 3.7439, stu_CELoss: 1.9413 DKDLoss: 1.8026 
2022-05-02 04:06:57 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 3.5662, stu_CELoss: 1.8952 DKDLoss: 1.6710 
2022-05-02 04:07:29 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 3.9217, stu_CELoss: 2.0146 DKDLoss: 1.9071 
2022-05-02 04:08:01 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 4.0624, stu_CELoss: 2.2102 DKDLoss: 1.8522 
2022-05-02 04:08:33 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 3.8524, stu_CELoss: 2.0238 DKDLoss: 1.8287 
2022-05-02 04:09:05 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 3.6089, stu_CELoss: 1.8754 DKDLoss: 1.7335 
2022-05-02 04:09:37 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 3.8377, stu_CELoss: 2.0618 DKDLoss: 1.7760 
2022-05-02 04:10:08 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 3.9730, stu_CELoss: 2.0692 DKDLoss: 1.9038 
2022-05-02 04:10:39 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 4.1269, stu_CELoss: 2.2191 DKDLoss: 1.9078 
2022-05-02 04:11:11 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 3.3741, stu_CELoss: 1.7022 DKDLoss: 1.6719 
2022-05-02 04:11:42 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 3.7034, stu_CELoss: 2.0279 DKDLoss: 1.6754 
2022-05-02 04:12:13 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 4.0207, stu_CELoss: 2.0674 DKDLoss: 1.9532 
2022-05-02 04:12:45 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 3.7802, stu_CELoss: 2.0380 DKDLoss: 1.7423 
2022-05-02 04:13:17 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 3.6533, stu_CELoss: 1.9947 DKDLoss: 1.6587 
2022-05-02 04:13:48 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 3.9380, stu_CELoss: 2.1568 DKDLoss: 1.7812 
2022-05-02 04:14:20 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 3.8495, stu_CELoss: 2.0397 DKDLoss: 1.8098 
2022-05-02 04:14:52 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 3.8296, stu_CELoss: 2.0730 DKDLoss: 1.7566 
2022-05-02 04:15:23 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 4.1262, stu_CELoss: 2.1934 DKDLoss: 1.9328 
2022-05-02 04:15:55 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 4.0068, stu_CELoss: 2.2559 DKDLoss: 1.7509 
2022-05-02 04:16:27 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 3.6795, stu_CELoss: 1.9081 DKDLoss: 1.7714 
2022-05-02 04:16:58 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 3.6845, stu_CELoss: 1.9339 DKDLoss: 1.7506 
2022-05-02 04:17:29 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 3.7510, stu_CELoss: 2.0713 DKDLoss: 1.6796 
2022-05-02 04:18:01 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 3.4396, stu_CELoss: 1.8505 DKDLoss: 1.5890 
2022-05-02 04:18:32 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 3.8032, stu_CELoss: 1.9707 DKDLoss: 1.8325 
2022-05-02 04:18:33 - train: epoch 019, train_loss: 3.8040
2022-05-02 04:20:58 - eval: epoch: 019, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 54.588%, stu_acc5: 79.538%, stu_test_loss: 1.9264
2022-05-02 04:20:59 - until epoch: 019, tea_best_acc1: 78.330%, stu_best_acc1: 55.608%
2022-05-02 04:20:59 - epoch 020 lr: 0.1
2022-05-02 04:21:36 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 3.9020, stu_CELoss: 2.1176 DKDLoss: 1.7844 
2022-05-02 04:22:07 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 3.3473, stu_CELoss: 1.7117 DKDLoss: 1.6356 
2022-05-02 04:22:39 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 3.6255, stu_CELoss: 1.8908 DKDLoss: 1.7347 
2022-05-02 04:23:10 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 3.6252, stu_CELoss: 1.8499 DKDLoss: 1.7753 
2022-05-02 04:23:42 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 3.6644, stu_CELoss: 1.9248 DKDLoss: 1.7396 
2022-05-02 04:24:14 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 3.7799, stu_CELoss: 2.0247 DKDLoss: 1.7552 
2022-05-02 04:24:45 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 3.7209, stu_CELoss: 1.9349 DKDLoss: 1.7860 
2022-05-02 04:25:17 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 4.0923, stu_CELoss: 2.1325 DKDLoss: 1.9598 
2022-05-02 04:25:49 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 4.0478, stu_CELoss: 2.1008 DKDLoss: 1.9470 
2022-05-02 04:26:21 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 3.7801, stu_CELoss: 2.0761 DKDLoss: 1.7040 
2022-05-02 04:26:53 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 3.4773, stu_CELoss: 1.8413 DKDLoss: 1.6360 
2022-05-02 04:27:24 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 3.6263, stu_CELoss: 1.8515 DKDLoss: 1.7748 
2022-05-02 04:27:56 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 3.6972, stu_CELoss: 1.9948 DKDLoss: 1.7024 
2022-05-02 04:28:28 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 3.7465, stu_CELoss: 1.9688 DKDLoss: 1.7777 
2022-05-02 04:29:00 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 3.5841, stu_CELoss: 1.8477 DKDLoss: 1.7364 
2022-05-02 04:29:32 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 3.9813, stu_CELoss: 2.1259 DKDLoss: 1.8554 
2022-05-02 04:30:04 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 3.6205, stu_CELoss: 1.9644 DKDLoss: 1.6562 
2022-05-02 04:30:36 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 3.9004, stu_CELoss: 2.0973 DKDLoss: 1.8031 
2022-05-02 04:31:08 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 3.8873, stu_CELoss: 2.0178 DKDLoss: 1.8695 
2022-05-02 04:31:39 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 3.6178, stu_CELoss: 1.8614 DKDLoss: 1.7564 
2022-05-02 04:32:10 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 4.0639, stu_CELoss: 2.2032 DKDLoss: 1.8608 
2022-05-02 04:32:42 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.3550, stu_CELoss: 1.7966 DKDLoss: 1.5584 
2022-05-02 04:33:13 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 3.7201, stu_CELoss: 1.9898 DKDLoss: 1.7302 
2022-05-02 04:33:45 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 3.9854, stu_CELoss: 2.1711 DKDLoss: 1.8143 
2022-05-02 04:34:16 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 3.6605, stu_CELoss: 1.9681 DKDLoss: 1.6925 
2022-05-02 04:34:48 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 3.5449, stu_CELoss: 1.9409 DKDLoss: 1.6040 
2022-05-02 04:35:19 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 3.5922, stu_CELoss: 1.8676 DKDLoss: 1.7246 
2022-05-02 04:35:50 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 4.0925, stu_CELoss: 2.1595 DKDLoss: 1.9330 
2022-05-02 04:36:22 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 3.8780, stu_CELoss: 2.0890 DKDLoss: 1.7889 
2022-05-02 04:36:54 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 3.8638, stu_CELoss: 2.0825 DKDLoss: 1.7814 
2022-05-02 04:37:26 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.8990, stu_CELoss: 2.0432 DKDLoss: 1.8558 
2022-05-02 04:37:57 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 4.0331, stu_CELoss: 2.0778 DKDLoss: 1.9553 
2022-05-02 04:38:28 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 3.5944, stu_CELoss: 1.7924 DKDLoss: 1.8020 
2022-05-02 04:39:00 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 3.8045, stu_CELoss: 2.0120 DKDLoss: 1.7924 
2022-05-02 04:39:32 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 3.5028, stu_CELoss: 1.8077 DKDLoss: 1.6951 
2022-05-02 04:40:03 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 3.6813, stu_CELoss: 1.9747 DKDLoss: 1.7066 
2022-05-02 04:40:35 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 3.7971, stu_CELoss: 1.9933 DKDLoss: 1.8038 
2022-05-02 04:41:06 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 3.6180, stu_CELoss: 1.9283 DKDLoss: 1.6898 
2022-05-02 04:41:37 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 4.1981, stu_CELoss: 2.2912 DKDLoss: 1.9069 
2022-05-02 04:42:08 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 3.9408, stu_CELoss: 2.0419 DKDLoss: 1.8990 
2022-05-02 04:42:39 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 3.7325, stu_CELoss: 2.0168 DKDLoss: 1.7157 
2022-05-02 04:43:11 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 3.9133, stu_CELoss: 2.0589 DKDLoss: 1.8544 
2022-05-02 04:43:43 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 3.8388, stu_CELoss: 2.1770 DKDLoss: 1.6619 
2022-05-02 04:44:14 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 3.8872, stu_CELoss: 2.0827 DKDLoss: 1.8044 
2022-05-02 04:44:45 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 3.7675, stu_CELoss: 1.9685 DKDLoss: 1.7989 
2022-05-02 04:45:17 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 3.6327, stu_CELoss: 1.9847 DKDLoss: 1.6479 
2022-05-02 04:45:48 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 3.5497, stu_CELoss: 1.8519 DKDLoss: 1.6978 
2022-05-02 04:46:20 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 3.4315, stu_CELoss: 1.8647 DKDLoss: 1.5668 
2022-05-02 04:46:53 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 3.9403, stu_CELoss: 2.0273 DKDLoss: 1.9130 
2022-05-02 04:47:24 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 3.6025, stu_CELoss: 1.9120 DKDLoss: 1.6905 
2022-05-02 04:47:26 - train: epoch 020, train_loss: 3.7824
2022-05-02 04:49:50 - eval: epoch: 020, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 56.498%, stu_acc5: 80.730%, stu_test_loss: 1.8491
2022-05-02 04:49:51 - until epoch: 020, tea_best_acc1: 78.330%, stu_best_acc1: 56.498%
2022-05-02 04:49:51 - epoch 021 lr: 0.1
2022-05-02 04:50:28 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 3.7455, stu_CELoss: 2.0246 DKDLoss: 1.7208 
2022-05-02 04:50:59 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 3.9692, stu_CELoss: 2.0400 DKDLoss: 1.9292 
2022-05-02 04:51:31 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 3.3645, stu_CELoss: 1.7260 DKDLoss: 1.6385 
2022-05-02 04:52:03 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 3.7609, stu_CELoss: 2.0429 DKDLoss: 1.7181 
2022-05-02 04:52:34 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 3.2610, stu_CELoss: 1.6534 DKDLoss: 1.6076 
2022-05-02 04:53:06 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 3.6106, stu_CELoss: 1.9223 DKDLoss: 1.6883 
2022-05-02 04:53:38 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.4301, stu_CELoss: 1.7593 DKDLoss: 1.6708 
2022-05-02 04:54:09 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 3.6655, stu_CELoss: 1.9384 DKDLoss: 1.7271 
2022-05-02 04:54:41 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 3.7317, stu_CELoss: 1.9369 DKDLoss: 1.7948 
2022-05-02 04:55:12 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 3.5953, stu_CELoss: 1.8983 DKDLoss: 1.6969 
2022-05-02 04:55:44 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 3.6656, stu_CELoss: 2.0101 DKDLoss: 1.6555 
2022-05-02 04:56:14 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 3.5031, stu_CELoss: 1.9401 DKDLoss: 1.5629 
2022-05-02 04:56:46 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.4291, stu_CELoss: 1.7721 DKDLoss: 1.6570 
2022-05-02 04:57:18 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 3.3971, stu_CELoss: 1.6942 DKDLoss: 1.7030 
2022-05-02 04:57:49 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 3.8067, stu_CELoss: 1.9979 DKDLoss: 1.8088 
2022-05-02 04:58:21 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 3.7642, stu_CELoss: 1.9821 DKDLoss: 1.7821 
2022-05-02 04:58:52 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 3.7000, stu_CELoss: 1.9762 DKDLoss: 1.7238 
2022-05-02 04:59:24 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 3.7402, stu_CELoss: 2.0505 DKDLoss: 1.6897 
2022-05-02 04:59:55 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 3.8942, stu_CELoss: 2.1482 DKDLoss: 1.7460 
2022-05-02 05:00:27 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 3.8846, stu_CELoss: 2.1062 DKDLoss: 1.7784 
2022-05-02 05:00:58 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 3.4091, stu_CELoss: 1.8428 DKDLoss: 1.5663 
2022-05-02 05:01:30 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.8838, stu_CELoss: 2.0381 DKDLoss: 1.8456 
2022-05-02 05:02:02 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 3.8866, stu_CELoss: 2.1357 DKDLoss: 1.7508 
2022-05-02 05:02:34 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 3.4717, stu_CELoss: 1.7810 DKDLoss: 1.6907 
2022-05-02 05:03:05 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 3.5881, stu_CELoss: 1.9257 DKDLoss: 1.6624 
2022-05-02 05:03:37 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 3.8021, stu_CELoss: 2.0645 DKDLoss: 1.7376 
2022-05-02 05:04:09 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 3.7385, stu_CELoss: 1.8251 DKDLoss: 1.9134 
2022-05-02 05:04:41 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 3.7473, stu_CELoss: 2.0199 DKDLoss: 1.7273 
2022-05-02 05:05:13 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 3.7093, stu_CELoss: 1.9981 DKDLoss: 1.7112 
2022-05-02 05:05:45 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 4.1955, stu_CELoss: 2.1994 DKDLoss: 1.9962 
2022-05-02 05:06:16 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 4.0529, stu_CELoss: 2.1386 DKDLoss: 1.9143 
2022-05-02 05:06:48 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 3.6745, stu_CELoss: 1.8849 DKDLoss: 1.7897 
2022-05-02 05:07:19 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 4.0184, stu_CELoss: 2.2364 DKDLoss: 1.7821 
2022-05-02 05:07:50 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 4.0562, stu_CELoss: 2.2735 DKDLoss: 1.7827 
2022-05-02 05:08:22 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 3.9185, stu_CELoss: 2.0524 DKDLoss: 1.8662 
2022-05-02 05:08:53 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 4.0717, stu_CELoss: 2.1466 DKDLoss: 1.9251 
2022-05-02 05:09:24 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 3.5278, stu_CELoss: 1.8857 DKDLoss: 1.6421 
2022-05-02 05:09:56 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 3.6708, stu_CELoss: 1.9857 DKDLoss: 1.6850 
2022-05-02 05:10:28 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 3.6122, stu_CELoss: 1.8656 DKDLoss: 1.7466 
2022-05-02 05:10:59 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 3.8509, stu_CELoss: 2.1246 DKDLoss: 1.7263 
2022-05-02 05:11:31 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 3.4738, stu_CELoss: 1.7900 DKDLoss: 1.6838 
2022-05-02 05:12:03 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 3.9339, stu_CELoss: 2.1208 DKDLoss: 1.8132 
2022-05-02 05:12:34 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 3.9824, stu_CELoss: 2.2000 DKDLoss: 1.7824 
2022-05-02 05:13:06 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 4.0287, stu_CELoss: 2.1382 DKDLoss: 1.8905 
2022-05-02 05:13:37 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 3.9621, stu_CELoss: 2.1227 DKDLoss: 1.8395 
2022-05-02 05:14:08 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 3.9821, stu_CELoss: 2.1171 DKDLoss: 1.8650 
2022-05-02 05:14:40 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 3.8604, stu_CELoss: 2.1882 DKDLoss: 1.6722 
2022-05-02 05:15:12 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 3.6740, stu_CELoss: 1.9618 DKDLoss: 1.7122 
2022-05-02 05:15:43 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 3.4921, stu_CELoss: 1.7986 DKDLoss: 1.6935 
2022-05-02 05:16:14 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 3.8015, stu_CELoss: 1.9712 DKDLoss: 1.8303 
2022-05-02 05:16:16 - train: epoch 021, train_loss: 3.7671
2022-05-02 05:18:42 - eval: epoch: 021, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 56.026%, stu_acc5: 80.906%, stu_test_loss: 1.8463
2022-05-02 05:18:42 - until epoch: 021, tea_best_acc1: 78.330%, stu_best_acc1: 56.498%
2022-05-02 05:18:42 - epoch 022 lr: 0.1
2022-05-02 05:19:19 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 3.3297, stu_CELoss: 1.7069 DKDLoss: 1.6229 
2022-05-02 05:19:50 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 3.5370, stu_CELoss: 1.9034 DKDLoss: 1.6336 
2022-05-02 05:20:22 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 3.3453, stu_CELoss: 1.8016 DKDLoss: 1.5437 
2022-05-02 05:20:53 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 3.5833, stu_CELoss: 1.8931 DKDLoss: 1.6902 
2022-05-02 05:21:25 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 3.6584, stu_CELoss: 1.9363 DKDLoss: 1.7221 
2022-05-02 05:21:56 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 3.7748, stu_CELoss: 2.0263 DKDLoss: 1.7484 
2022-05-02 05:22:27 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 3.8648, stu_CELoss: 2.0787 DKDLoss: 1.7861 
2022-05-02 05:22:58 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 3.8576, stu_CELoss: 2.1357 DKDLoss: 1.7219 
2022-05-02 05:23:30 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 3.8638, stu_CELoss: 2.0802 DKDLoss: 1.7836 
2022-05-02 05:24:02 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 3.8986, stu_CELoss: 2.1269 DKDLoss: 1.7717 
2022-05-02 05:24:33 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.6084, stu_CELoss: 1.8656 DKDLoss: 1.7428 
2022-05-02 05:25:05 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 3.1580, stu_CELoss: 1.5759 DKDLoss: 1.5821 
2022-05-02 05:25:37 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 4.0445, stu_CELoss: 2.1060 DKDLoss: 1.9386 
2022-05-02 05:26:09 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 3.8050, stu_CELoss: 1.8856 DKDLoss: 1.9194 
2022-05-02 05:26:41 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 3.9014, stu_CELoss: 2.1102 DKDLoss: 1.7913 
2022-05-02 05:27:12 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 3.5075, stu_CELoss: 1.8081 DKDLoss: 1.6994 
2022-05-02 05:27:44 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 3.7711, stu_CELoss: 2.1113 DKDLoss: 1.6598 
2022-05-02 05:28:16 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.9426, stu_CELoss: 2.1203 DKDLoss: 1.8223 
2022-05-02 05:28:48 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 3.9437, stu_CELoss: 2.0007 DKDLoss: 1.9431 
2022-05-02 05:29:19 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 3.5797, stu_CELoss: 1.8997 DKDLoss: 1.6800 
2022-05-02 05:29:51 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 3.3803, stu_CELoss: 1.8248 DKDLoss: 1.5555 
2022-05-02 05:30:22 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 3.4174, stu_CELoss: 1.7846 DKDLoss: 1.6328 
2022-05-02 05:30:54 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 4.1072, stu_CELoss: 2.1533 DKDLoss: 1.9539 
2022-05-02 05:31:25 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 4.2656, stu_CELoss: 2.2706 DKDLoss: 1.9950 
2022-05-02 05:31:56 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 3.7138, stu_CELoss: 2.0066 DKDLoss: 1.7072 
2022-05-02 05:32:28 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 3.4731, stu_CELoss: 1.8775 DKDLoss: 1.5956 
2022-05-02 05:32:59 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 3.6187, stu_CELoss: 1.9563 DKDLoss: 1.6624 
2022-05-02 05:33:31 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 4.1648, stu_CELoss: 2.1451 DKDLoss: 2.0197 
2022-05-02 05:34:03 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 3.9064, stu_CELoss: 1.9345 DKDLoss: 1.9719 
2022-05-02 05:34:35 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.6764, stu_CELoss: 1.9468 DKDLoss: 1.7296 
2022-05-02 05:35:06 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.9046, stu_CELoss: 2.1360 DKDLoss: 1.7687 
2022-05-02 05:35:38 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 3.5156, stu_CELoss: 1.9264 DKDLoss: 1.5892 
2022-05-02 05:36:10 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 3.7898, stu_CELoss: 1.9922 DKDLoss: 1.7976 
2022-05-02 05:36:41 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 3.6967, stu_CELoss: 1.9420 DKDLoss: 1.7547 
2022-05-02 05:37:13 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.8088, stu_CELoss: 2.0750 DKDLoss: 1.7338 
2022-05-02 05:37:45 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 3.7706, stu_CELoss: 1.9562 DKDLoss: 1.8144 
2022-05-02 05:38:16 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 3.7591, stu_CELoss: 1.9727 DKDLoss: 1.7864 
2022-05-02 05:38:48 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 3.9120, stu_CELoss: 2.1093 DKDLoss: 1.8027 
2022-05-02 05:39:20 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 3.8075, stu_CELoss: 2.0152 DKDLoss: 1.7923 
2022-05-02 05:39:52 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 3.6020, stu_CELoss: 2.0444 DKDLoss: 1.5576 
2022-05-02 05:40:24 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 3.7698, stu_CELoss: 2.0026 DKDLoss: 1.7672 
2022-05-02 05:40:56 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 3.8532, stu_CELoss: 2.0559 DKDLoss: 1.7973 
2022-05-02 05:41:27 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 3.8633, stu_CELoss: 2.0604 DKDLoss: 1.8028 
2022-05-02 05:41:59 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 3.6436, stu_CELoss: 1.9460 DKDLoss: 1.6975 
2022-05-02 05:42:30 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 3.6887, stu_CELoss: 1.9748 DKDLoss: 1.7139 
2022-05-02 05:43:01 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 3.9063, stu_CELoss: 2.1972 DKDLoss: 1.7091 
2022-05-02 05:43:33 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 3.8151, stu_CELoss: 2.0634 DKDLoss: 1.7517 
2022-05-02 05:44:04 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 3.2923, stu_CELoss: 1.6564 DKDLoss: 1.6359 
2022-05-02 05:44:36 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 3.6440, stu_CELoss: 1.9767 DKDLoss: 1.6673 
2022-05-02 05:45:07 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 3.6092, stu_CELoss: 1.8966 DKDLoss: 1.7126 
2022-05-02 05:45:09 - train: epoch 022, train_loss: 3.7504
2022-05-02 05:47:34 - eval: epoch: 022, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 57.750%, stu_acc5: 82.010%, stu_test_loss: 1.7663
2022-05-02 05:47:35 - until epoch: 022, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 05:47:35 - epoch 023 lr: 0.1
2022-05-02 05:48:13 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 3.7368, stu_CELoss: 1.9924 DKDLoss: 1.7445 
2022-05-02 05:48:43 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 3.3959, stu_CELoss: 1.8682 DKDLoss: 1.5277 
2022-05-02 05:49:15 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 3.6456, stu_CELoss: 1.9856 DKDLoss: 1.6601 
2022-05-02 05:49:47 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 3.5242, stu_CELoss: 1.8769 DKDLoss: 1.6472 
2022-05-02 05:50:18 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 3.7465, stu_CELoss: 1.9889 DKDLoss: 1.7576 
2022-05-02 05:50:50 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 3.9164, stu_CELoss: 2.0233 DKDLoss: 1.8931 
2022-05-02 05:51:21 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 3.7289, stu_CELoss: 1.9748 DKDLoss: 1.7541 
2022-05-02 05:51:53 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 3.7127, stu_CELoss: 1.9939 DKDLoss: 1.7188 
2022-05-02 05:52:25 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 3.6512, stu_CELoss: 1.9897 DKDLoss: 1.6615 
2022-05-02 05:52:57 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 3.6759, stu_CELoss: 1.8501 DKDLoss: 1.8257 
2022-05-02 05:53:29 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 3.8763, stu_CELoss: 2.0076 DKDLoss: 1.8687 
2022-05-02 05:54:01 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 3.4397, stu_CELoss: 1.8751 DKDLoss: 1.5646 
2022-05-02 05:54:33 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 3.5120, stu_CELoss: 1.8366 DKDLoss: 1.6754 
2022-05-02 05:55:04 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 3.7972, stu_CELoss: 2.0160 DKDLoss: 1.7812 
2022-05-02 05:55:36 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 3.5112, stu_CELoss: 1.8579 DKDLoss: 1.6533 
2022-05-02 05:56:07 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 3.5669, stu_CELoss: 1.8944 DKDLoss: 1.6724 
2022-05-02 05:56:39 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 3.6033, stu_CELoss: 1.8151 DKDLoss: 1.7882 
2022-05-02 05:57:11 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 3.8114, stu_CELoss: 2.1511 DKDLoss: 1.6603 
2022-05-02 05:57:43 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 3.9098, stu_CELoss: 2.1762 DKDLoss: 1.7336 
2022-05-02 05:58:15 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 3.4374, stu_CELoss: 1.7704 DKDLoss: 1.6671 
2022-05-02 05:58:47 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 3.6191, stu_CELoss: 1.8757 DKDLoss: 1.7434 
2022-05-02 05:59:19 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 3.4383, stu_CELoss: 1.8569 DKDLoss: 1.5814 
2022-05-02 05:59:50 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 3.6398, stu_CELoss: 1.8734 DKDLoss: 1.7664 
2022-05-02 06:00:22 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 3.8136, stu_CELoss: 1.9780 DKDLoss: 1.8356 
2022-05-02 06:00:54 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 4.0096, stu_CELoss: 2.0585 DKDLoss: 1.9510 
2022-05-02 06:01:27 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 4.0047, stu_CELoss: 2.1128 DKDLoss: 1.8919 
2022-05-02 06:01:58 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 3.7907, stu_CELoss: 2.0887 DKDLoss: 1.7020 
2022-05-02 06:02:30 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 4.0751, stu_CELoss: 2.1777 DKDLoss: 1.8974 
2022-05-02 06:03:02 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 3.8765, stu_CELoss: 1.9723 DKDLoss: 1.9042 
2022-05-02 06:03:34 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 3.7803, stu_CELoss: 2.0504 DKDLoss: 1.7299 
2022-05-02 06:04:06 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 4.3415, stu_CELoss: 2.3061 DKDLoss: 2.0354 
2022-05-02 06:04:38 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 3.8155, stu_CELoss: 2.0795 DKDLoss: 1.7360 
2022-05-02 06:05:09 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 3.6088, stu_CELoss: 1.9380 DKDLoss: 1.6708 
2022-05-02 06:05:41 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 3.7777, stu_CELoss: 1.9712 DKDLoss: 1.8065 
2022-05-02 06:06:13 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 3.5155, stu_CELoss: 1.9182 DKDLoss: 1.5973 
2022-05-02 06:06:45 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 3.6452, stu_CELoss: 1.9410 DKDLoss: 1.7042 
2022-05-02 06:07:17 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 3.6015, stu_CELoss: 1.9070 DKDLoss: 1.6946 
2022-05-02 06:07:49 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 4.0015, stu_CELoss: 2.1896 DKDLoss: 1.8120 
2022-05-02 06:08:20 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 3.9098, stu_CELoss: 2.1465 DKDLoss: 1.7633 
2022-05-02 06:08:52 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 3.5602, stu_CELoss: 1.9190 DKDLoss: 1.6412 
2022-05-02 06:09:24 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 3.6492, stu_CELoss: 1.8584 DKDLoss: 1.7908 
2022-05-02 06:09:55 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 3.4017, stu_CELoss: 1.6944 DKDLoss: 1.7073 
2022-05-02 06:10:27 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 3.4869, stu_CELoss: 1.8559 DKDLoss: 1.6309 
2022-05-02 06:10:59 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 3.3666, stu_CELoss: 1.7744 DKDLoss: 1.5922 
2022-05-02 06:11:31 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 3.5220, stu_CELoss: 1.9008 DKDLoss: 1.6211 
2022-05-02 06:12:02 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 3.8475, stu_CELoss: 2.0750 DKDLoss: 1.7725 
2022-05-02 06:12:34 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 3.6803, stu_CELoss: 1.9604 DKDLoss: 1.7199 
2022-05-02 06:13:06 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 3.6177, stu_CELoss: 1.9696 DKDLoss: 1.6481 
2022-05-02 06:13:38 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 3.4887, stu_CELoss: 1.8522 DKDLoss: 1.6364 
2022-05-02 06:14:10 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 4.1421, stu_CELoss: 2.1783 DKDLoss: 1.9638 
2022-05-02 06:14:11 - train: epoch 023, train_loss: 3.7326
2022-05-02 06:16:37 - eval: epoch: 023, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 55.514%, stu_acc5: 80.216%, stu_test_loss: 1.8916
2022-05-02 06:16:38 - until epoch: 023, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 06:16:38 - epoch 024 lr: 0.1
2022-05-02 06:17:15 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 4.0875, stu_CELoss: 2.2199 DKDLoss: 1.8676 
2022-05-02 06:17:46 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 3.4032, stu_CELoss: 1.8046 DKDLoss: 1.5986 
2022-05-02 06:18:18 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 3.7034, stu_CELoss: 2.0398 DKDLoss: 1.6636 
2022-05-02 06:18:50 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 3.8040, stu_CELoss: 2.0422 DKDLoss: 1.7617 
2022-05-02 06:19:21 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 3.9251, stu_CELoss: 2.0967 DKDLoss: 1.8285 
2022-05-02 06:19:53 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 3.9256, stu_CELoss: 2.0518 DKDLoss: 1.8738 
2022-05-02 06:20:24 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 3.4713, stu_CELoss: 1.7812 DKDLoss: 1.6902 
2022-05-02 06:20:56 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 3.5135, stu_CELoss: 1.7677 DKDLoss: 1.7457 
2022-05-02 06:21:27 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 3.6419, stu_CELoss: 1.8703 DKDLoss: 1.7716 
2022-05-02 06:21:58 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 3.5533, stu_CELoss: 1.9017 DKDLoss: 1.6516 
2022-05-02 06:22:30 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 3.4386, stu_CELoss: 1.7887 DKDLoss: 1.6500 
2022-05-02 06:23:02 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 3.4993, stu_CELoss: 1.8948 DKDLoss: 1.6045 
2022-05-02 06:23:34 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 4.0131, stu_CELoss: 2.2105 DKDLoss: 1.8027 
2022-05-02 06:24:06 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 3.3357, stu_CELoss: 1.7767 DKDLoss: 1.5589 
2022-05-02 06:24:37 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 3.7866, stu_CELoss: 2.0420 DKDLoss: 1.7447 
2022-05-02 06:25:09 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 3.6340, stu_CELoss: 1.9403 DKDLoss: 1.6936 
2022-05-02 06:25:40 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 3.6139, stu_CELoss: 1.8921 DKDLoss: 1.7218 
2022-05-02 06:26:11 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 3.9863, stu_CELoss: 2.1246 DKDLoss: 1.8617 
2022-05-02 06:26:43 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 3.5075, stu_CELoss: 1.8966 DKDLoss: 1.6109 
2022-05-02 06:27:15 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 3.8205, stu_CELoss: 1.9732 DKDLoss: 1.8473 
2022-05-02 06:27:47 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 3.5525, stu_CELoss: 1.9900 DKDLoss: 1.5624 
2022-05-02 06:28:18 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 3.6375, stu_CELoss: 1.9808 DKDLoss: 1.6566 
2022-05-02 06:28:49 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 3.8715, stu_CELoss: 2.0240 DKDLoss: 1.8475 
2022-05-02 06:29:21 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 4.0827, stu_CELoss: 2.2201 DKDLoss: 1.8626 
2022-05-02 06:29:52 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 3.8854, stu_CELoss: 2.0686 DKDLoss: 1.8168 
2022-05-02 06:30:24 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 4.0329, stu_CELoss: 2.2978 DKDLoss: 1.7351 
2022-05-02 06:30:55 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 3.8938, stu_CELoss: 2.1293 DKDLoss: 1.7645 
2022-05-02 06:31:26 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 3.8529, stu_CELoss: 2.1382 DKDLoss: 1.7147 
2022-05-02 06:31:58 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 3.6502, stu_CELoss: 1.8663 DKDLoss: 1.7839 
2022-05-02 06:32:29 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 3.4189, stu_CELoss: 1.8140 DKDLoss: 1.6049 
2022-05-02 06:33:01 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 3.3051, stu_CELoss: 1.7286 DKDLoss: 1.5765 
2022-05-02 06:33:32 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 3.9773, stu_CELoss: 2.0434 DKDLoss: 1.9339 
2022-05-02 06:34:04 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 3.6998, stu_CELoss: 1.8946 DKDLoss: 1.8052 
2022-05-02 06:34:36 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 3.3467, stu_CELoss: 1.8463 DKDLoss: 1.5004 
2022-05-02 06:35:07 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 3.9285, stu_CELoss: 2.1184 DKDLoss: 1.8100 
2022-05-02 06:35:38 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 3.7372, stu_CELoss: 1.9791 DKDLoss: 1.7581 
2022-05-02 06:36:10 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 3.4350, stu_CELoss: 1.8645 DKDLoss: 1.5706 
2022-05-02 06:36:41 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 3.7590, stu_CELoss: 2.0869 DKDLoss: 1.6721 
2022-05-02 06:37:14 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.6989, stu_CELoss: 1.9363 DKDLoss: 1.7627 
2022-05-02 06:37:46 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 3.8566, stu_CELoss: 2.0370 DKDLoss: 1.8196 
2022-05-02 06:38:17 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 3.6527, stu_CELoss: 1.9036 DKDLoss: 1.7492 
2022-05-02 06:38:49 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 3.6637, stu_CELoss: 1.9657 DKDLoss: 1.6980 
2022-05-02 06:39:21 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 3.4687, stu_CELoss: 1.8633 DKDLoss: 1.6054 
2022-05-02 06:39:52 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 3.8705, stu_CELoss: 2.1098 DKDLoss: 1.7608 
2022-05-02 06:40:24 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 3.4895, stu_CELoss: 1.7563 DKDLoss: 1.7332 
2022-05-02 06:40:55 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 3.8715, stu_CELoss: 2.1410 DKDLoss: 1.7305 
2022-05-02 06:41:26 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 3.9028, stu_CELoss: 1.9978 DKDLoss: 1.9050 
2022-05-02 06:41:58 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 3.3125, stu_CELoss: 1.6970 DKDLoss: 1.6155 
2022-05-02 06:42:29 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 3.8646, stu_CELoss: 2.1397 DKDLoss: 1.7250 
2022-05-02 06:43:00 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 4.0184, stu_CELoss: 2.1452 DKDLoss: 1.8732 
2022-05-02 06:43:01 - train: epoch 024, train_loss: 3.7226
2022-05-02 06:45:26 - eval: epoch: 024, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 57.338%, stu_acc5: 81.720%, stu_test_loss: 1.7882
2022-05-02 06:45:27 - until epoch: 024, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 06:45:27 - epoch 025 lr: 0.1
2022-05-02 06:46:04 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 3.7091, stu_CELoss: 1.9976 DKDLoss: 1.7115 
2022-05-02 06:46:35 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 3.1649, stu_CELoss: 1.6342 DKDLoss: 1.5306 
2022-05-02 06:47:06 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 3.5683, stu_CELoss: 1.8398 DKDLoss: 1.7285 
2022-05-02 06:47:38 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 3.6539, stu_CELoss: 2.0302 DKDLoss: 1.6237 
2022-05-02 06:48:09 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 3.5268, stu_CELoss: 1.9181 DKDLoss: 1.6086 
2022-05-02 06:48:40 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 3.4870, stu_CELoss: 1.9379 DKDLoss: 1.5491 
2022-05-02 06:49:11 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 3.7001, stu_CELoss: 1.9583 DKDLoss: 1.7418 
2022-05-02 06:49:42 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 3.5135, stu_CELoss: 1.8677 DKDLoss: 1.6458 
2022-05-02 06:50:14 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 3.8416, stu_CELoss: 2.0288 DKDLoss: 1.8127 
2022-05-02 06:50:45 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 3.5922, stu_CELoss: 1.9821 DKDLoss: 1.6101 
2022-05-02 06:51:17 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 3.4118, stu_CELoss: 1.8314 DKDLoss: 1.5803 
2022-05-02 06:51:48 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 3.8847, stu_CELoss: 2.1082 DKDLoss: 1.7765 
2022-05-02 06:52:19 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 3.5713, stu_CELoss: 1.9164 DKDLoss: 1.6549 
2022-05-02 06:52:50 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 3.6589, stu_CELoss: 1.8705 DKDLoss: 1.7885 
2022-05-02 06:53:22 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 3.6603, stu_CELoss: 2.0968 DKDLoss: 1.5635 
2022-05-02 06:53:53 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 3.3287, stu_CELoss: 1.7623 DKDLoss: 1.5663 
2022-05-02 06:54:24 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 3.6184, stu_CELoss: 1.9832 DKDLoss: 1.6352 
2022-05-02 06:54:56 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 3.3921, stu_CELoss: 1.7358 DKDLoss: 1.6563 
2022-05-02 06:55:27 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 3.4645, stu_CELoss: 1.8969 DKDLoss: 1.5676 
2022-05-02 06:55:59 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 3.6712, stu_CELoss: 1.9276 DKDLoss: 1.7436 
2022-05-02 06:56:30 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 3.2692, stu_CELoss: 1.6575 DKDLoss: 1.6117 
2022-05-02 06:57:02 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 3.6863, stu_CELoss: 2.0424 DKDLoss: 1.6439 
2022-05-02 06:57:33 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 3.8232, stu_CELoss: 1.9811 DKDLoss: 1.8421 
2022-05-02 06:58:05 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 3.7433, stu_CELoss: 1.9531 DKDLoss: 1.7902 
2022-05-02 06:58:36 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 3.6703, stu_CELoss: 1.9649 DKDLoss: 1.7053 
2022-05-02 06:59:07 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 3.6951, stu_CELoss: 2.0085 DKDLoss: 1.6866 
2022-05-02 06:59:39 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 3.7538, stu_CELoss: 2.1236 DKDLoss: 1.6302 
2022-05-02 07:00:10 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 3.7625, stu_CELoss: 1.9854 DKDLoss: 1.7771 
2022-05-02 07:00:42 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 3.9086, stu_CELoss: 2.1600 DKDLoss: 1.7486 
2022-05-02 07:01:13 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 3.9550, stu_CELoss: 2.0193 DKDLoss: 1.9356 
2022-05-02 07:01:44 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 3.9068, stu_CELoss: 2.0629 DKDLoss: 1.8439 
2022-05-02 07:02:16 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 3.8648, stu_CELoss: 2.1215 DKDLoss: 1.7434 
2022-05-02 07:02:48 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 3.6057, stu_CELoss: 1.8637 DKDLoss: 1.7420 
2022-05-02 07:03:19 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 3.8484, stu_CELoss: 1.9981 DKDLoss: 1.8503 
2022-05-02 07:03:52 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 3.6088, stu_CELoss: 1.9020 DKDLoss: 1.7069 
2022-05-02 07:04:23 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 3.8489, stu_CELoss: 2.0742 DKDLoss: 1.7747 
2022-05-02 07:04:55 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 3.6095, stu_CELoss: 1.9419 DKDLoss: 1.6676 
2022-05-02 07:05:27 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 3.8676, stu_CELoss: 2.1220 DKDLoss: 1.7456 
2022-05-02 07:05:58 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 4.1696, stu_CELoss: 2.2769 DKDLoss: 1.8927 
2022-05-02 07:06:30 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 3.9111, stu_CELoss: 2.0424 DKDLoss: 1.8687 
2022-05-02 07:07:02 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 3.7866, stu_CELoss: 2.0666 DKDLoss: 1.7200 
2022-05-02 07:07:33 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 3.6849, stu_CELoss: 1.9763 DKDLoss: 1.7086 
2022-05-02 07:08:05 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 3.6670, stu_CELoss: 1.8975 DKDLoss: 1.7694 
2022-05-02 07:08:36 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 3.4888, stu_CELoss: 1.8738 DKDLoss: 1.6150 
2022-05-02 07:09:08 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 3.7115, stu_CELoss: 1.9897 DKDLoss: 1.7217 
2022-05-02 07:09:39 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 3.6440, stu_CELoss: 1.9840 DKDLoss: 1.6600 
2022-05-02 07:10:10 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 3.7137, stu_CELoss: 1.9888 DKDLoss: 1.7249 
2022-05-02 07:10:42 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 3.3817, stu_CELoss: 1.8756 DKDLoss: 1.5061 
2022-05-02 07:11:13 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 3.4687, stu_CELoss: 1.9053 DKDLoss: 1.5635 
2022-05-02 07:11:44 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 3.9542, stu_CELoss: 2.2186 DKDLoss: 1.7356 
2022-05-02 07:11:46 - train: epoch 025, train_loss: 3.7113
2022-05-02 07:14:10 - eval: epoch: 025, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 54.560%, stu_acc5: 79.320%, stu_test_loss: 1.9353
2022-05-02 07:14:10 - until epoch: 025, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 07:14:10 - epoch 026 lr: 0.1
2022-05-02 07:14:47 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 3.8406, stu_CELoss: 1.9434 DKDLoss: 1.8972 
2022-05-02 07:15:19 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 3.6661, stu_CELoss: 1.9286 DKDLoss: 1.7375 
2022-05-02 07:15:50 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 3.7441, stu_CELoss: 2.0709 DKDLoss: 1.6733 
2022-05-02 07:16:21 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 3.3991, stu_CELoss: 1.7864 DKDLoss: 1.6127 
2022-05-02 07:16:52 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 3.6727, stu_CELoss: 1.9464 DKDLoss: 1.7262 
2022-05-02 07:17:23 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 4.3042, stu_CELoss: 2.3461 DKDLoss: 1.9580 
2022-05-02 07:17:55 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 3.5201, stu_CELoss: 1.9308 DKDLoss: 1.5893 
2022-05-02 07:18:26 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 3.4257, stu_CELoss: 1.7854 DKDLoss: 1.6403 
2022-05-02 07:18:57 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 4.1380, stu_CELoss: 2.4154 DKDLoss: 1.7226 
2022-05-02 07:19:28 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 3.7447, stu_CELoss: 1.9791 DKDLoss: 1.7657 
2022-05-02 07:19:59 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 3.8154, stu_CELoss: 2.0079 DKDLoss: 1.8075 
2022-05-02 07:20:31 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 3.7901, stu_CELoss: 2.0289 DKDLoss: 1.7612 
2022-05-02 07:21:02 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 3.4439, stu_CELoss: 1.9116 DKDLoss: 1.5323 
2022-05-02 07:21:34 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 3.8147, stu_CELoss: 2.0473 DKDLoss: 1.7674 
2022-05-02 07:22:06 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 3.3996, stu_CELoss: 1.8283 DKDLoss: 1.5713 
2022-05-02 07:22:37 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 3.5099, stu_CELoss: 1.9444 DKDLoss: 1.5655 
2022-05-02 07:23:09 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 3.5641, stu_CELoss: 1.8928 DKDLoss: 1.6713 
2022-05-02 07:23:41 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 3.8720, stu_CELoss: 2.2149 DKDLoss: 1.6571 
2022-05-02 07:24:13 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.9032, stu_CELoss: 2.1486 DKDLoss: 1.7546 
2022-05-02 07:24:44 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 3.9248, stu_CELoss: 2.0727 DKDLoss: 1.8521 
2022-05-02 07:25:16 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 3.6753, stu_CELoss: 1.8967 DKDLoss: 1.7786 
2022-05-02 07:25:48 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 3.8509, stu_CELoss: 2.0123 DKDLoss: 1.8386 
2022-05-02 07:26:20 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 3.7253, stu_CELoss: 1.9367 DKDLoss: 1.7886 
2022-05-02 07:26:52 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 3.8092, stu_CELoss: 2.0577 DKDLoss: 1.7515 
2022-05-02 07:27:24 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 3.9589, stu_CELoss: 2.1552 DKDLoss: 1.8037 
2022-05-02 07:27:55 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 3.7512, stu_CELoss: 2.0113 DKDLoss: 1.7398 
2022-05-02 07:28:27 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 3.7746, stu_CELoss: 2.1422 DKDLoss: 1.6324 
2022-05-02 07:28:58 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 3.7615, stu_CELoss: 2.0224 DKDLoss: 1.7391 
2022-05-02 07:29:30 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 4.2272, stu_CELoss: 2.2635 DKDLoss: 1.9638 
2022-05-02 07:30:01 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 3.7436, stu_CELoss: 2.0101 DKDLoss: 1.7335 
2022-05-02 07:30:32 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 3.8281, stu_CELoss: 2.0972 DKDLoss: 1.7308 
2022-05-02 07:31:03 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 3.7241, stu_CELoss: 2.0503 DKDLoss: 1.6739 
2022-05-02 07:31:34 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 3.6758, stu_CELoss: 1.9740 DKDLoss: 1.7018 
2022-05-02 07:32:05 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 3.7442, stu_CELoss: 1.9612 DKDLoss: 1.7830 
2022-05-02 07:32:36 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 3.7695, stu_CELoss: 2.1302 DKDLoss: 1.6394 
2022-05-02 07:33:08 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 3.3599, stu_CELoss: 1.7651 DKDLoss: 1.5948 
2022-05-02 07:33:39 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 3.4585, stu_CELoss: 1.8727 DKDLoss: 1.5858 
2022-05-02 07:34:11 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 4.0572, stu_CELoss: 2.1811 DKDLoss: 1.8761 
2022-05-02 07:34:42 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 3.8917, stu_CELoss: 2.0689 DKDLoss: 1.8228 
2022-05-02 07:35:14 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 3.5949, stu_CELoss: 1.9341 DKDLoss: 1.6608 
2022-05-02 07:35:46 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 3.8209, stu_CELoss: 2.0206 DKDLoss: 1.8004 
2022-05-02 07:36:17 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 3.7659, stu_CELoss: 2.0672 DKDLoss: 1.6987 
2022-05-02 07:36:49 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 4.0547, stu_CELoss: 2.1992 DKDLoss: 1.8555 
2022-05-02 07:37:20 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 3.6972, stu_CELoss: 2.1131 DKDLoss: 1.5840 
2022-05-02 07:37:51 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 4.0441, stu_CELoss: 2.3502 DKDLoss: 1.6939 
2022-05-02 07:38:22 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 3.7864, stu_CELoss: 2.0212 DKDLoss: 1.7653 
2022-05-02 07:38:54 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 3.4935, stu_CELoss: 1.8127 DKDLoss: 1.6809 
2022-05-02 07:39:25 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 3.4265, stu_CELoss: 1.9079 DKDLoss: 1.5186 
2022-05-02 07:39:56 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 3.8596, stu_CELoss: 1.9993 DKDLoss: 1.8603 
2022-05-02 07:40:27 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 3.7542, stu_CELoss: 2.0822 DKDLoss: 1.6720 
2022-05-02 07:40:29 - train: epoch 026, train_loss: 3.6985
2022-05-02 07:42:54 - eval: epoch: 026, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 56.094%, stu_acc5: 80.954%, stu_test_loss: 1.8396
2022-05-02 07:42:54 - until epoch: 026, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 07:42:54 - epoch 027 lr: 0.1
2022-05-02 07:43:32 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 3.9085, stu_CELoss: 2.0150 DKDLoss: 1.8935 
2022-05-02 07:44:03 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 3.5656, stu_CELoss: 1.9431 DKDLoss: 1.6224 
2022-05-02 07:44:35 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 3.8878, stu_CELoss: 2.1065 DKDLoss: 1.7813 
2022-05-02 07:45:06 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 3.9699, stu_CELoss: 2.1474 DKDLoss: 1.8225 
2022-05-02 07:45:38 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 3.5012, stu_CELoss: 1.7908 DKDLoss: 1.7104 
2022-05-02 07:46:09 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 3.6133, stu_CELoss: 1.9517 DKDLoss: 1.6616 
2022-05-02 07:46:41 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 3.5087, stu_CELoss: 1.9314 DKDLoss: 1.5772 
2022-05-02 07:47:12 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 3.5313, stu_CELoss: 1.8762 DKDLoss: 1.6551 
2022-05-02 07:47:44 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 3.7328, stu_CELoss: 1.9463 DKDLoss: 1.7864 
2022-05-02 07:48:16 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 3.6169, stu_CELoss: 1.9027 DKDLoss: 1.7142 
2022-05-02 07:48:48 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 3.4047, stu_CELoss: 1.8581 DKDLoss: 1.5467 
2022-05-02 07:49:20 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.9801, stu_CELoss: 2.0906 DKDLoss: 1.8895 
2022-05-02 07:49:52 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 3.4899, stu_CELoss: 1.8094 DKDLoss: 1.6805 
2022-05-02 07:50:23 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 3.9571, stu_CELoss: 2.0791 DKDLoss: 1.8780 
2022-05-02 07:50:55 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 3.7981, stu_CELoss: 2.0474 DKDLoss: 1.7507 
2022-05-02 07:51:26 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 3.8275, stu_CELoss: 2.0622 DKDLoss: 1.7653 
2022-05-02 07:51:58 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 3.7494, stu_CELoss: 1.9314 DKDLoss: 1.8180 
2022-05-02 07:52:29 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 3.6470, stu_CELoss: 1.8285 DKDLoss: 1.8184 
2022-05-02 07:53:01 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 3.6651, stu_CELoss: 1.9899 DKDLoss: 1.6752 
2022-05-02 07:53:33 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 3.5380, stu_CELoss: 1.9082 DKDLoss: 1.6298 
2022-05-02 07:54:05 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 4.0060, stu_CELoss: 2.2168 DKDLoss: 1.7893 
2022-05-02 07:54:37 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 3.9353, stu_CELoss: 2.1556 DKDLoss: 1.7798 
2022-05-02 07:55:09 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 4.1174, stu_CELoss: 2.2614 DKDLoss: 1.8560 
2022-05-02 07:55:41 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 3.4679, stu_CELoss: 1.8730 DKDLoss: 1.5949 
2022-05-02 07:56:13 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 3.6668, stu_CELoss: 1.9406 DKDLoss: 1.7262 
2022-05-02 07:56:45 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 3.6921, stu_CELoss: 2.0748 DKDLoss: 1.6173 
2022-05-02 07:57:16 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 3.5706, stu_CELoss: 1.8864 DKDLoss: 1.6842 
2022-05-02 07:57:48 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 3.6983, stu_CELoss: 2.0180 DKDLoss: 1.6803 
2022-05-02 07:58:18 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 3.8884, stu_CELoss: 2.0458 DKDLoss: 1.8426 
2022-05-02 07:58:50 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 3.5248, stu_CELoss: 1.9144 DKDLoss: 1.6105 
2022-05-02 07:59:21 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 3.3468, stu_CELoss: 1.7662 DKDLoss: 1.5806 
2022-05-02 07:59:53 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 3.5466, stu_CELoss: 1.7635 DKDLoss: 1.7831 
2022-05-02 08:00:24 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 3.6384, stu_CELoss: 2.0098 DKDLoss: 1.6287 
2022-05-02 08:00:56 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 3.5554, stu_CELoss: 1.8188 DKDLoss: 1.7367 
2022-05-02 08:01:28 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 4.0065, stu_CELoss: 2.1846 DKDLoss: 1.8219 
2022-05-02 08:01:59 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 3.8007, stu_CELoss: 2.0907 DKDLoss: 1.7101 
2022-05-02 08:02:30 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 3.7185, stu_CELoss: 1.9743 DKDLoss: 1.7442 
2022-05-02 08:03:01 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 3.3103, stu_CELoss: 1.6957 DKDLoss: 1.6146 
2022-05-02 08:03:33 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 3.7347, stu_CELoss: 1.9559 DKDLoss: 1.7788 
2022-05-02 08:04:05 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 4.0202, stu_CELoss: 2.1470 DKDLoss: 1.8733 
2022-05-02 08:04:36 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 3.5545, stu_CELoss: 1.9013 DKDLoss: 1.6532 
2022-05-02 08:05:08 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 3.7108, stu_CELoss: 1.9572 DKDLoss: 1.7536 
2022-05-02 08:05:40 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 3.2592, stu_CELoss: 1.7686 DKDLoss: 1.4906 
2022-05-02 08:06:11 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 3.6765, stu_CELoss: 1.9961 DKDLoss: 1.6804 
2022-05-02 08:06:42 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 3.7138, stu_CELoss: 2.0101 DKDLoss: 1.7037 
2022-05-02 08:07:14 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.5332, stu_CELoss: 1.8539 DKDLoss: 1.6793 
2022-05-02 08:07:47 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 4.0583, stu_CELoss: 2.2237 DKDLoss: 1.8346 
2022-05-02 08:08:18 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 3.8300, stu_CELoss: 2.0800 DKDLoss: 1.7500 
2022-05-02 08:08:49 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 3.5302, stu_CELoss: 1.9095 DKDLoss: 1.6207 
2022-05-02 08:09:21 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 3.5991, stu_CELoss: 1.9285 DKDLoss: 1.6706 
2022-05-02 08:09:22 - train: epoch 027, train_loss: 3.6881
2022-05-02 08:11:47 - eval: epoch: 027, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 57.354%, stu_acc5: 81.642%, stu_test_loss: 1.7922
2022-05-02 08:11:48 - until epoch: 027, tea_best_acc1: 78.330%, stu_best_acc1: 57.750%
2022-05-02 08:11:48 - epoch 028 lr: 0.1
2022-05-02 08:12:25 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 3.3351, stu_CELoss: 1.7453 DKDLoss: 1.5897 
2022-05-02 08:12:56 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 3.6506, stu_CELoss: 1.9496 DKDLoss: 1.7010 
2022-05-02 08:13:27 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 3.7198, stu_CELoss: 2.0613 DKDLoss: 1.6585 
2022-05-02 08:13:59 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 3.6071, stu_CELoss: 1.9577 DKDLoss: 1.6494 
2022-05-02 08:14:30 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 3.3481, stu_CELoss: 1.7628 DKDLoss: 1.5853 
2022-05-02 08:15:00 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 3.6200, stu_CELoss: 2.0406 DKDLoss: 1.5794 
2022-05-02 08:15:32 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.9396, stu_CELoss: 2.1610 DKDLoss: 1.7786 
2022-05-02 08:16:03 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 3.3758, stu_CELoss: 1.7703 DKDLoss: 1.6055 
2022-05-02 08:16:34 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 3.4805, stu_CELoss: 1.8706 DKDLoss: 1.6098 
2022-05-02 08:17:05 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 3.7453, stu_CELoss: 1.9583 DKDLoss: 1.7870 
2022-05-02 08:17:36 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.6446, stu_CELoss: 1.8927 DKDLoss: 1.7520 
2022-05-02 08:18:08 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 3.6992, stu_CELoss: 1.9566 DKDLoss: 1.7426 
2022-05-02 08:18:39 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 3.6874, stu_CELoss: 2.0256 DKDLoss: 1.6619 
2022-05-02 08:19:11 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 4.0672, stu_CELoss: 2.1435 DKDLoss: 1.9237 
2022-05-02 08:19:43 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 3.6589, stu_CELoss: 1.9808 DKDLoss: 1.6780 
2022-05-02 08:20:15 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 3.6759, stu_CELoss: 1.9276 DKDLoss: 1.7483 
2022-05-02 08:20:46 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 3.7276, stu_CELoss: 2.0423 DKDLoss: 1.6853 
2022-05-02 08:21:18 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 3.4220, stu_CELoss: 1.8101 DKDLoss: 1.6119 
2022-05-02 08:21:49 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 3.6283, stu_CELoss: 1.9254 DKDLoss: 1.7030 
2022-05-02 08:22:20 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 3.8732, stu_CELoss: 2.0331 DKDLoss: 1.8401 
2022-05-02 08:22:52 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 4.0126, stu_CELoss: 2.2166 DKDLoss: 1.7961 
2022-05-02 08:23:23 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 3.7449, stu_CELoss: 1.9789 DKDLoss: 1.7660 
2022-05-02 08:23:54 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 3.5788, stu_CELoss: 1.9050 DKDLoss: 1.6739 
2022-05-02 08:24:26 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 4.0607, stu_CELoss: 2.0815 DKDLoss: 1.9792 
2022-05-02 08:24:58 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 3.9607, stu_CELoss: 2.1224 DKDLoss: 1.8383 
2022-05-02 08:25:29 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 3.5611, stu_CELoss: 1.8984 DKDLoss: 1.6627 
2022-05-02 08:26:00 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 3.5457, stu_CELoss: 1.8790 DKDLoss: 1.6667 
2022-05-02 08:26:32 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 3.9865, stu_CELoss: 2.1869 DKDLoss: 1.7996 
2022-05-02 08:27:04 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 3.8297, stu_CELoss: 2.1819 DKDLoss: 1.6477 
2022-05-02 08:27:35 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 3.7194, stu_CELoss: 2.0105 DKDLoss: 1.7089 
2022-05-02 08:28:06 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 3.6795, stu_CELoss: 1.9825 DKDLoss: 1.6969 
2022-05-02 08:28:37 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 3.3855, stu_CELoss: 1.7938 DKDLoss: 1.5917 
2022-05-02 08:29:09 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 4.0258, stu_CELoss: 2.1660 DKDLoss: 1.8598 
2022-05-02 08:29:40 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 3.8616, stu_CELoss: 2.1446 DKDLoss: 1.7170 
2022-05-02 08:30:12 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 3.7197, stu_CELoss: 1.9280 DKDLoss: 1.7917 
2022-05-02 08:30:43 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 3.5082, stu_CELoss: 1.9137 DKDLoss: 1.5945 
2022-05-02 08:31:15 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 3.5825, stu_CELoss: 1.8581 DKDLoss: 1.7244 
2022-05-02 08:31:46 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 3.2677, stu_CELoss: 1.6956 DKDLoss: 1.5721 
2022-05-02 08:32:17 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 3.9349, stu_CELoss: 2.1675 DKDLoss: 1.7674 
2022-05-02 08:32:49 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 3.7147, stu_CELoss: 2.0815 DKDLoss: 1.6332 
2022-05-02 08:33:20 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 3.7692, stu_CELoss: 2.0247 DKDLoss: 1.7445 
2022-05-02 08:33:51 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 3.8637, stu_CELoss: 2.1016 DKDLoss: 1.7621 
2022-05-02 08:34:23 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 3.5049, stu_CELoss: 1.9023 DKDLoss: 1.6026 
2022-05-02 08:34:55 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 3.6573, stu_CELoss: 1.9503 DKDLoss: 1.7070 
2022-05-02 08:35:27 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 3.7684, stu_CELoss: 2.0619 DKDLoss: 1.7065 
2022-05-02 08:35:59 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 3.5803, stu_CELoss: 2.0415 DKDLoss: 1.5387 
2022-05-02 08:36:30 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 3.8323, stu_CELoss: 2.1651 DKDLoss: 1.6673 
2022-05-02 08:37:01 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 3.4964, stu_CELoss: 1.9018 DKDLoss: 1.5946 
2022-05-02 08:37:32 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 3.9782, stu_CELoss: 2.0848 DKDLoss: 1.8934 
2022-05-02 08:38:03 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 3.7473, stu_CELoss: 2.0238 DKDLoss: 1.7234 
2022-05-02 08:38:05 - train: epoch 028, train_loss: 3.6782
2022-05-02 08:40:30 - eval: epoch: 028, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 58.480%, stu_acc5: 82.504%, stu_test_loss: 1.7326
2022-05-02 08:40:31 - until epoch: 028, tea_best_acc1: 78.330%, stu_best_acc1: 58.480%
2022-05-02 08:40:31 - epoch 029 lr: 0.1
2022-05-02 08:41:08 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 3.7909, stu_CELoss: 1.9827 DKDLoss: 1.8082 
2022-05-02 08:41:38 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 3.5089, stu_CELoss: 1.8164 DKDLoss: 1.6925 
2022-05-02 08:42:09 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 3.8239, stu_CELoss: 2.1898 DKDLoss: 1.6341 
2022-05-02 08:42:39 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 4.1477, stu_CELoss: 2.2168 DKDLoss: 1.9308 
2022-05-02 08:43:10 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 3.9430, stu_CELoss: 2.1850 DKDLoss: 1.7580 
2022-05-02 08:43:40 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 3.6112, stu_CELoss: 1.9533 DKDLoss: 1.6579 
2022-05-02 08:44:11 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 3.2041, stu_CELoss: 1.5781 DKDLoss: 1.6261 
2022-05-02 08:44:41 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 3.5293, stu_CELoss: 1.9374 DKDLoss: 1.5919 
2022-05-02 08:45:12 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 3.6302, stu_CELoss: 1.9801 DKDLoss: 1.6501 
2022-05-02 08:45:43 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 3.5605, stu_CELoss: 1.9958 DKDLoss: 1.5647 
2022-05-02 08:46:14 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 3.6286, stu_CELoss: 1.9416 DKDLoss: 1.6870 
2022-05-02 08:46:45 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 3.9570, stu_CELoss: 2.0990 DKDLoss: 1.8580 
2022-05-02 08:47:16 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 3.4661, stu_CELoss: 1.9133 DKDLoss: 1.5528 
2022-05-02 08:47:47 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 3.5955, stu_CELoss: 1.9107 DKDLoss: 1.6848 
2022-05-02 08:48:18 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 3.6498, stu_CELoss: 1.9524 DKDLoss: 1.6974 
2022-05-02 08:48:49 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 3.7609, stu_CELoss: 2.0212 DKDLoss: 1.7397 
2022-05-02 08:49:20 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 3.3938, stu_CELoss: 1.8215 DKDLoss: 1.5723 
2022-05-02 08:49:51 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 3.9182, stu_CELoss: 2.1347 DKDLoss: 1.7835 
2022-05-02 08:50:22 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 3.3619, stu_CELoss: 1.8660 DKDLoss: 1.4959 
2022-05-02 08:50:53 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 3.5951, stu_CELoss: 1.8998 DKDLoss: 1.6953 
2022-05-02 08:51:25 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 3.6940, stu_CELoss: 1.9960 DKDLoss: 1.6979 
2022-05-02 08:51:57 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 3.9426, stu_CELoss: 2.1089 DKDLoss: 1.8337 
2022-05-02 08:52:29 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 3.7121, stu_CELoss: 2.0114 DKDLoss: 1.7007 
2022-05-02 08:53:01 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 3.4542, stu_CELoss: 1.8037 DKDLoss: 1.6504 
2022-05-02 08:53:32 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 3.5353, stu_CELoss: 1.8853 DKDLoss: 1.6500 
2022-05-02 08:54:04 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 3.8447, stu_CELoss: 2.0946 DKDLoss: 1.7501 
2022-05-02 08:54:35 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 3.6549, stu_CELoss: 1.9975 DKDLoss: 1.6574 
2022-05-02 08:55:06 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 3.5788, stu_CELoss: 1.9057 DKDLoss: 1.6731 
2022-05-02 08:55:37 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 3.6031, stu_CELoss: 1.9724 DKDLoss: 1.6307 
2022-05-02 08:56:08 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 3.8641, stu_CELoss: 2.0483 DKDLoss: 1.8157 
2022-05-02 08:56:40 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 3.8479, stu_CELoss: 2.0764 DKDLoss: 1.7715 
2022-05-02 08:57:11 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 3.7218, stu_CELoss: 2.0177 DKDLoss: 1.7041 
2022-05-02 08:57:42 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 3.5544, stu_CELoss: 1.9306 DKDLoss: 1.6237 
2022-05-02 08:58:13 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 3.6982, stu_CELoss: 1.9599 DKDLoss: 1.7383 
2022-05-02 08:58:45 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.7269, stu_CELoss: 2.0346 DKDLoss: 1.6923 
2022-05-02 08:59:16 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 3.5925, stu_CELoss: 1.8516 DKDLoss: 1.7409 
2022-05-02 08:59:47 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 3.4637, stu_CELoss: 1.7960 DKDLoss: 1.6677 
2022-05-02 09:00:18 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 3.5080, stu_CELoss: 1.9188 DKDLoss: 1.5892 
2022-05-02 09:00:50 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 3.6435, stu_CELoss: 1.9775 DKDLoss: 1.6660 
2022-05-02 09:01:21 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 3.7215, stu_CELoss: 1.9993 DKDLoss: 1.7222 
2022-05-02 09:01:52 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 3.8701, stu_CELoss: 2.0865 DKDLoss: 1.7836 
2022-05-02 09:02:24 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 3.7012, stu_CELoss: 1.9788 DKDLoss: 1.7224 
2022-05-02 09:02:55 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 3.9156, stu_CELoss: 2.1675 DKDLoss: 1.7481 
2022-05-02 09:03:27 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 3.7872, stu_CELoss: 2.0404 DKDLoss: 1.7468 
2022-05-02 09:03:58 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 3.7813, stu_CELoss: 2.1522 DKDLoss: 1.6291 
2022-05-02 09:04:30 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 3.9711, stu_CELoss: 2.0827 DKDLoss: 1.8884 
2022-05-02 09:05:01 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 3.3815, stu_CELoss: 1.7435 DKDLoss: 1.6380 
2022-05-02 09:05:33 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 3.6716, stu_CELoss: 1.9256 DKDLoss: 1.7460 
2022-05-02 09:06:04 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 4.0934, stu_CELoss: 2.1988 DKDLoss: 1.8946 
2022-05-02 09:06:35 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 3.3410, stu_CELoss: 1.8011 DKDLoss: 1.5398 
2022-05-02 09:06:37 - train: epoch 029, train_loss: 3.6711
2022-05-02 09:09:03 - eval: epoch: 029, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 56.320%, stu_acc5: 81.318%, stu_test_loss: 1.8231
2022-05-02 09:09:04 - until epoch: 029, tea_best_acc1: 78.330%, stu_best_acc1: 58.480%
2022-05-02 09:09:04 - epoch 030 lr: 0.1
2022-05-02 09:09:41 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 3.6858, stu_CELoss: 2.0538 DKDLoss: 1.6319 
2022-05-02 09:10:13 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 3.5499, stu_CELoss: 1.8353 DKDLoss: 1.7146 
2022-05-02 09:10:44 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 3.5210, stu_CELoss: 1.8305 DKDLoss: 1.6905 
2022-05-02 09:11:15 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 3.6744, stu_CELoss: 1.9627 DKDLoss: 1.7117 
2022-05-02 09:11:47 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.8683, stu_CELoss: 2.0524 DKDLoss: 1.8159 
2022-05-02 09:12:19 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 3.5128, stu_CELoss: 1.7861 DKDLoss: 1.7268 
2022-05-02 09:12:50 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 3.4074, stu_CELoss: 1.8146 DKDLoss: 1.5929 
2022-05-02 09:13:21 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 3.4976, stu_CELoss: 1.9424 DKDLoss: 1.5552 
2022-05-02 09:13:52 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 3.6189, stu_CELoss: 1.9442 DKDLoss: 1.6747 
2022-05-02 09:14:23 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 3.5642, stu_CELoss: 1.8494 DKDLoss: 1.7147 
2022-05-02 09:14:54 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 3.3204, stu_CELoss: 1.8072 DKDLoss: 1.5132 
2022-05-02 09:15:26 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 3.6757, stu_CELoss: 1.9976 DKDLoss: 1.6781 
2022-05-02 09:15:57 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 3.1973, stu_CELoss: 1.7354 DKDLoss: 1.4619 
2022-05-02 09:16:28 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 3.3204, stu_CELoss: 1.7352 DKDLoss: 1.5852 
2022-05-02 09:17:00 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 3.4881, stu_CELoss: 1.8172 DKDLoss: 1.6709 
2022-05-02 09:17:31 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 3.7627, stu_CELoss: 1.9548 DKDLoss: 1.8079 
2022-05-02 09:18:02 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 3.6757, stu_CELoss: 2.0760 DKDLoss: 1.5996 
2022-05-02 09:18:33 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 3.9306, stu_CELoss: 2.0822 DKDLoss: 1.8484 
2022-05-02 09:19:05 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 3.6253, stu_CELoss: 1.9940 DKDLoss: 1.6313 
2022-05-02 09:19:36 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 3.5232, stu_CELoss: 1.9627 DKDLoss: 1.5605 
2022-05-02 09:20:08 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 3.7413, stu_CELoss: 2.0409 DKDLoss: 1.7004 
2022-05-02 09:20:40 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 3.6509, stu_CELoss: 1.9941 DKDLoss: 1.6568 
2022-05-02 09:21:11 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 3.6554, stu_CELoss: 1.9070 DKDLoss: 1.7483 
2022-05-02 09:21:42 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 3.9452, stu_CELoss: 2.1881 DKDLoss: 1.7571 
2022-05-02 09:22:14 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 3.7731, stu_CELoss: 2.1853 DKDLoss: 1.5877 
2022-05-02 09:22:45 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 3.7668, stu_CELoss: 2.0829 DKDLoss: 1.6839 
2022-05-02 09:23:17 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 3.4340, stu_CELoss: 1.7974 DKDLoss: 1.6365 
2022-05-02 09:23:48 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 3.6259, stu_CELoss: 1.9906 DKDLoss: 1.6353 
2022-05-02 09:24:19 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 3.5546, stu_CELoss: 1.8510 DKDLoss: 1.7036 
2022-05-02 09:24:50 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 3.8370, stu_CELoss: 2.1224 DKDLoss: 1.7146 
2022-05-02 09:25:21 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 3.9273, stu_CELoss: 2.0043 DKDLoss: 1.9230 
2022-05-02 09:25:53 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 3.6579, stu_CELoss: 1.8790 DKDLoss: 1.7789 
2022-05-02 09:26:24 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.8745, stu_CELoss: 2.0263 DKDLoss: 1.8482 
2022-05-02 09:26:55 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 3.6616, stu_CELoss: 1.8309 DKDLoss: 1.8307 
2022-05-02 09:27:27 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 3.6303, stu_CELoss: 2.0429 DKDLoss: 1.5875 
2022-05-02 09:27:58 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 3.7954, stu_CELoss: 1.9939 DKDLoss: 1.8015 
2022-05-02 09:28:29 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 3.6438, stu_CELoss: 1.9541 DKDLoss: 1.6897 
2022-05-02 09:29:00 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 3.6902, stu_CELoss: 1.9093 DKDLoss: 1.7809 
2022-05-02 09:29:31 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 3.3777, stu_CELoss: 1.7986 DKDLoss: 1.5790 
2022-05-02 09:30:02 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 3.5685, stu_CELoss: 1.9747 DKDLoss: 1.5938 
2022-05-02 09:30:33 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 3.6061, stu_CELoss: 1.9239 DKDLoss: 1.6822 
2022-05-02 09:31:05 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.9021, stu_CELoss: 2.0979 DKDLoss: 1.8041 
2022-05-02 09:31:36 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 3.6287, stu_CELoss: 1.9462 DKDLoss: 1.6825 
2022-05-02 09:32:07 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 3.6099, stu_CELoss: 1.9200 DKDLoss: 1.6899 
2022-05-02 09:32:38 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 3.8697, stu_CELoss: 2.0575 DKDLoss: 1.8123 
2022-05-02 09:33:09 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 3.5698, stu_CELoss: 1.8759 DKDLoss: 1.6938 
2022-05-02 09:33:40 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 3.6061, stu_CELoss: 1.8939 DKDLoss: 1.7122 
2022-05-02 09:34:11 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 3.4979, stu_CELoss: 1.8506 DKDLoss: 1.6474 
2022-05-02 09:34:42 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 3.8041, stu_CELoss: 2.0867 DKDLoss: 1.7173 
2022-05-02 09:35:13 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 3.7560, stu_CELoss: 2.0149 DKDLoss: 1.7411 
2022-05-02 09:35:15 - train: epoch 030, train_loss: 3.6629
2022-05-02 09:37:40 - eval: epoch: 030, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 56.048%, stu_acc5: 80.826%, stu_test_loss: 1.8448
2022-05-02 09:37:41 - until epoch: 030, tea_best_acc1: 78.330%, stu_best_acc1: 58.480%
2022-05-02 09:37:41 - epoch 031 lr: 0.010000000000000002
2022-05-02 09:38:18 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.9987, stu_CELoss: 1.6797 DKDLoss: 1.3190 
2022-05-02 09:38:49 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.9219, stu_CELoss: 1.6293 DKDLoss: 1.2926 
2022-05-02 09:39:20 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.9668, stu_CELoss: 1.6079 DKDLoss: 1.3590 
2022-05-02 09:39:50 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.7321, stu_CELoss: 1.5718 DKDLoss: 1.1602 
2022-05-02 09:40:22 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.7560, stu_CELoss: 1.6100 DKDLoss: 1.1460 
2022-05-02 09:40:54 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.6359, stu_CELoss: 1.5203 DKDLoss: 1.1156 
2022-05-02 09:41:25 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.8027, stu_CELoss: 1.6229 DKDLoss: 1.1798 
2022-05-02 09:41:56 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.6116, stu_CELoss: 1.4987 DKDLoss: 1.1129 
2022-05-02 09:42:28 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.6917, stu_CELoss: 1.5473 DKDLoss: 1.1444 
2022-05-02 09:42:59 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.7395, stu_CELoss: 1.6279 DKDLoss: 1.1116 
2022-05-02 09:43:31 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.8985, stu_CELoss: 1.6303 DKDLoss: 1.2681 
2022-05-02 09:44:03 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.9206, stu_CELoss: 1.6081 DKDLoss: 1.3125 
2022-05-02 09:44:35 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 2.1592, stu_CELoss: 1.1956 DKDLoss: 0.9637 
2022-05-02 09:45:06 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.4805, stu_CELoss: 1.4595 DKDLoss: 1.0210 
2022-05-02 09:45:38 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.8181, stu_CELoss: 1.5793 DKDLoss: 1.2388 
2022-05-02 09:46:09 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.6031, stu_CELoss: 1.5301 DKDLoss: 1.0730 
2022-05-02 09:46:41 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.6271, stu_CELoss: 1.5146 DKDLoss: 1.1125 
2022-05-02 09:47:13 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.3118, stu_CELoss: 1.1851 DKDLoss: 1.1267 
2022-05-02 09:47:45 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.5872, stu_CELoss: 1.4499 DKDLoss: 1.1373 
2022-05-02 09:48:16 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.4721, stu_CELoss: 1.4192 DKDLoss: 1.0529 
2022-05-02 09:48:48 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.3756, stu_CELoss: 1.3465 DKDLoss: 1.0291 
2022-05-02 09:49:19 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.3411, stu_CELoss: 1.3040 DKDLoss: 1.0371 
2022-05-02 09:49:51 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.4202, stu_CELoss: 1.4490 DKDLoss: 0.9712 
2022-05-02 09:50:22 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.3628, stu_CELoss: 1.3786 DKDLoss: 0.9842 
2022-05-02 09:50:54 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.7143, stu_CELoss: 1.6739 DKDLoss: 1.0403 
2022-05-02 09:51:26 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.5219, stu_CELoss: 1.4778 DKDLoss: 1.0442 
2022-05-02 09:51:58 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.5837, stu_CELoss: 1.5239 DKDLoss: 1.0598 
2022-05-02 09:52:29 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.4353, stu_CELoss: 1.4203 DKDLoss: 1.0150 
2022-05-02 09:53:01 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.6022, stu_CELoss: 1.6121 DKDLoss: 0.9901 
2022-05-02 09:53:32 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 3.0367, stu_CELoss: 1.7936 DKDLoss: 1.2431 
2022-05-02 09:54:04 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.5326, stu_CELoss: 1.5024 DKDLoss: 1.0302 
2022-05-02 09:54:36 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.7176, stu_CELoss: 1.5117 DKDLoss: 1.2059 
2022-05-02 09:55:08 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.3264, stu_CELoss: 1.4069 DKDLoss: 0.9195 
2022-05-02 09:55:39 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.6498, stu_CELoss: 1.6514 DKDLoss: 0.9985 
2022-05-02 09:56:11 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.7312, stu_CELoss: 1.5721 DKDLoss: 1.1591 
2022-05-02 09:56:42 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.4742, stu_CELoss: 1.4713 DKDLoss: 1.0029 
2022-05-02 09:57:14 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.4751, stu_CELoss: 1.4009 DKDLoss: 1.0742 
2022-05-02 09:57:46 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.3902, stu_CELoss: 1.3882 DKDLoss: 1.0020 
2022-05-02 09:58:18 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.3523, stu_CELoss: 1.3305 DKDLoss: 1.0219 
2022-05-02 09:58:49 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.3561, stu_CELoss: 1.3239 DKDLoss: 1.0322 
2022-05-02 09:59:21 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.3865, stu_CELoss: 1.4844 DKDLoss: 0.9021 
2022-05-02 09:59:53 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.6011, stu_CELoss: 1.5281 DKDLoss: 1.0731 
2022-05-02 10:00:24 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.2686, stu_CELoss: 1.4153 DKDLoss: 0.8533 
2022-05-02 10:00:56 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.6556, stu_CELoss: 1.6525 DKDLoss: 1.0031 
2022-05-02 10:01:28 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.3498, stu_CELoss: 1.3538 DKDLoss: 0.9960 
2022-05-02 10:01:59 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.3405, stu_CELoss: 1.3638 DKDLoss: 0.9767 
2022-05-02 10:02:31 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.2287, stu_CELoss: 1.2730 DKDLoss: 0.9557 
2022-05-02 10:03:03 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.2175, stu_CELoss: 1.3151 DKDLoss: 0.9024 
2022-05-02 10:03:35 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.4571, stu_CELoss: 1.4695 DKDLoss: 0.9876 
2022-05-02 10:04:06 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.1054, stu_CELoss: 1.2859 DKDLoss: 0.8194 
2022-05-02 10:04:08 - train: epoch 031, train_loss: 2.5351
2022-05-02 10:06:33 - eval: epoch: 031, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.182%, stu_acc5: 90.272%, stu_test_loss: 1.1509
2022-05-02 10:06:34 - until epoch: 031, tea_best_acc1: 78.330%, stu_best_acc1: 71.182%
2022-05-02 10:10:45 - epoch 032 lr: 0.010000000000000002
2022-05-02 10:11:23 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 2.3173, stu_CELoss: 1.4465 DKDLoss: 0.8707 
2022-05-02 10:11:53 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.3491, stu_CELoss: 1.4073 DKDLoss: 0.9418 
2022-05-02 10:12:23 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 2.2325, stu_CELoss: 1.3219 DKDLoss: 0.9106 
2022-05-02 10:12:54 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.2576, stu_CELoss: 1.3548 DKDLoss: 0.9028 
2022-05-02 10:13:24 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.3370, stu_CELoss: 1.3173 DKDLoss: 1.0197 
2022-05-02 10:13:55 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.5298, stu_CELoss: 1.4779 DKDLoss: 1.0519 
2022-05-02 10:14:26 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.1603, stu_CELoss: 1.2364 DKDLoss: 0.9238 
2022-05-02 10:14:57 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.2509, stu_CELoss: 1.3070 DKDLoss: 0.9439 
2022-05-02 10:15:28 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.3420, stu_CELoss: 1.3475 DKDLoss: 0.9945 
2022-05-02 10:15:58 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.4423, stu_CELoss: 1.4727 DKDLoss: 0.9696 
2022-05-02 10:16:29 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.4012, stu_CELoss: 1.4275 DKDLoss: 0.9738 
2022-05-02 10:17:00 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.3633, stu_CELoss: 1.3949 DKDLoss: 0.9684 
2022-05-02 10:17:31 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.3856, stu_CELoss: 1.4308 DKDLoss: 0.9548 
2022-05-02 10:18:02 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.5373, stu_CELoss: 1.5121 DKDLoss: 1.0252 
2022-05-02 10:18:33 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.3013, stu_CELoss: 1.3065 DKDLoss: 0.9948 
2022-05-02 10:19:04 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.5818, stu_CELoss: 1.5176 DKDLoss: 1.0642 
2022-05-02 10:19:35 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.2725, stu_CELoss: 1.2600 DKDLoss: 1.0125 
2022-05-02 10:20:06 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.5570, stu_CELoss: 1.4903 DKDLoss: 1.0666 
2022-05-02 10:20:37 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 2.1464, stu_CELoss: 1.1512 DKDLoss: 0.9953 
2022-05-02 10:21:07 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.0944, stu_CELoss: 1.1616 DKDLoss: 0.9328 
2022-05-02 10:21:38 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 2.0560, stu_CELoss: 1.2259 DKDLoss: 0.8301 
2022-05-02 10:22:08 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.2954, stu_CELoss: 1.3070 DKDLoss: 0.9884 
2022-05-02 10:22:39 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.2506, stu_CELoss: 1.3409 DKDLoss: 0.9097 
2022-05-02 10:23:10 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.3627, stu_CELoss: 1.3545 DKDLoss: 1.0082 
2022-05-02 10:23:42 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.1978, stu_CELoss: 1.3521 DKDLoss: 0.8457 
2022-05-02 10:24:13 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 2.0548, stu_CELoss: 1.1702 DKDLoss: 0.8847 
2022-05-02 10:24:44 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.3953, stu_CELoss: 1.4032 DKDLoss: 0.9921 
2022-05-02 10:25:15 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.3268, stu_CELoss: 1.4931 DKDLoss: 0.8337 
2022-05-02 10:25:46 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.0474, stu_CELoss: 1.1532 DKDLoss: 0.8942 
2022-05-02 10:26:17 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.2038, stu_CELoss: 1.3397 DKDLoss: 0.8642 
2022-05-02 10:26:49 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.4078, stu_CELoss: 1.5291 DKDLoss: 0.8787 
2022-05-02 10:27:20 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.2555, stu_CELoss: 1.3414 DKDLoss: 0.9140 
2022-05-02 10:27:52 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.1642, stu_CELoss: 1.1972 DKDLoss: 0.9670 
2022-05-02 10:28:23 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 2.2282, stu_CELoss: 1.2266 DKDLoss: 1.0016 
2022-05-02 10:28:54 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.0757, stu_CELoss: 1.1619 DKDLoss: 0.9138 
2022-05-02 10:29:26 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.3099, stu_CELoss: 1.3971 DKDLoss: 0.9128 
2022-05-02 10:29:57 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.4647, stu_CELoss: 1.4497 DKDLoss: 1.0150 
2022-05-02 10:30:28 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 2.0908, stu_CELoss: 1.2659 DKDLoss: 0.8249 
2022-05-02 10:30:59 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.2242, stu_CELoss: 1.3245 DKDLoss: 0.8997 
2022-05-02 10:31:31 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.2069, stu_CELoss: 1.2992 DKDLoss: 0.9077 
2022-05-02 10:32:02 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.1613, stu_CELoss: 1.3129 DKDLoss: 0.8484 
2022-05-02 10:32:33 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 2.0506, stu_CELoss: 1.2358 DKDLoss: 0.8148 
2022-05-02 10:33:05 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.4365, stu_CELoss: 1.4599 DKDLoss: 0.9767 
2022-05-02 10:33:36 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.1096, stu_CELoss: 1.3239 DKDLoss: 0.7857 
2022-05-02 10:34:08 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.4801, stu_CELoss: 1.4745 DKDLoss: 1.0056 
2022-05-02 10:34:40 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.3386, stu_CELoss: 1.3489 DKDLoss: 0.9897 
2022-05-02 10:35:11 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.3266, stu_CELoss: 1.3237 DKDLoss: 1.0028 
2022-05-02 10:35:43 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 2.1329, stu_CELoss: 1.2520 DKDLoss: 0.8809 
2022-05-02 10:36:15 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.4436, stu_CELoss: 1.4789 DKDLoss: 0.9647 
2022-05-02 10:36:46 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.3145, stu_CELoss: 1.3431 DKDLoss: 0.9714 
2022-05-02 10:36:48 - train: epoch 032, train_loss: 2.2856
2022-05-02 10:39:14 - eval: epoch: 032, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.682%, stu_acc5: 90.782%, stu_test_loss: 1.1163
2022-05-02 10:39:14 - until epoch: 032, tea_best_acc1: 78.330%, stu_best_acc1: 71.682%
2022-05-02 10:39:14 - epoch 033 lr: 0.010000000000000002
2022-05-02 10:39:52 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 2.1596, stu_CELoss: 1.2557 DKDLoss: 0.9039 
2022-05-02 10:40:23 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.2344, stu_CELoss: 1.2734 DKDLoss: 0.9610 
2022-05-02 10:40:54 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.2105, stu_CELoss: 1.2844 DKDLoss: 0.9261 
2022-05-02 10:41:25 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 2.0115, stu_CELoss: 1.2161 DKDLoss: 0.7954 
2022-05-02 10:41:57 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.2909, stu_CELoss: 1.4203 DKDLoss: 0.8706 
2022-05-02 10:42:28 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 2.1891, stu_CELoss: 1.3340 DKDLoss: 0.8551 
2022-05-02 10:43:00 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.3230, stu_CELoss: 1.3962 DKDLoss: 0.9268 
2022-05-02 10:43:32 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.4294, stu_CELoss: 1.4719 DKDLoss: 0.9575 
2022-05-02 10:44:03 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.0901, stu_CELoss: 1.1779 DKDLoss: 0.9122 
2022-05-02 10:44:35 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.1690, stu_CELoss: 1.2514 DKDLoss: 0.9176 
2022-05-02 10:45:07 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 2.0316, stu_CELoss: 1.2772 DKDLoss: 0.7544 
2022-05-02 10:45:39 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.4931, stu_CELoss: 1.5374 DKDLoss: 0.9557 
2022-05-02 10:46:11 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.0916, stu_CELoss: 1.2657 DKDLoss: 0.8259 
2022-05-02 10:46:43 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.4754, stu_CELoss: 1.4881 DKDLoss: 0.9873 
2022-05-02 10:47:15 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.5995, stu_CELoss: 1.5667 DKDLoss: 1.0328 
2022-05-02 10:47:47 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.4430, stu_CELoss: 1.4052 DKDLoss: 1.0378 
2022-05-02 10:48:19 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 2.1452, stu_CELoss: 1.2603 DKDLoss: 0.8849 
2022-05-02 10:48:51 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.3445, stu_CELoss: 1.4746 DKDLoss: 0.8699 
2022-05-02 10:49:23 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 2.1576, stu_CELoss: 1.3360 DKDLoss: 0.8215 
2022-05-02 10:49:54 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 2.1002, stu_CELoss: 1.1776 DKDLoss: 0.9227 
2022-05-02 10:50:25 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.1693, stu_CELoss: 1.2563 DKDLoss: 0.9130 
2022-05-02 10:50:57 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0602, stu_CELoss: 1.3081 DKDLoss: 0.7521 
2022-05-02 10:51:29 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.2558, stu_CELoss: 1.3619 DKDLoss: 0.8939 
2022-05-02 10:52:01 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.2394, stu_CELoss: 1.3411 DKDLoss: 0.8983 
2022-05-02 10:52:32 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.9447, stu_CELoss: 1.1076 DKDLoss: 0.8370 
2022-05-02 10:53:04 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.9540, stu_CELoss: 1.1831 DKDLoss: 0.7709 
2022-05-02 10:53:35 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.1660, stu_CELoss: 1.4013 DKDLoss: 0.7647 
2022-05-02 10:54:07 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.3770, stu_CELoss: 1.3880 DKDLoss: 0.9890 
2022-05-02 10:54:38 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.9689, stu_CELoss: 1.1223 DKDLoss: 0.8466 
2022-05-02 10:55:10 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.1888, stu_CELoss: 1.3543 DKDLoss: 0.8345 
2022-05-02 10:55:43 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.1344, stu_CELoss: 1.2742 DKDLoss: 0.8602 
2022-05-02 10:56:14 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.1865, stu_CELoss: 1.3055 DKDLoss: 0.8810 
2022-05-02 10:56:46 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 2.1144, stu_CELoss: 1.2888 DKDLoss: 0.8257 
2022-05-02 10:57:18 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.9620, stu_CELoss: 1.1192 DKDLoss: 0.8427 
2022-05-02 10:57:50 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.9259, stu_CELoss: 1.1334 DKDLoss: 0.7925 
2022-05-02 10:58:21 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.3988, stu_CELoss: 1.4322 DKDLoss: 0.9666 
2022-05-02 10:58:53 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.7803, stu_CELoss: 1.0055 DKDLoss: 0.7748 
2022-05-02 10:59:25 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.9877, stu_CELoss: 1.2156 DKDLoss: 0.7721 
2022-05-02 10:59:56 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.3063, stu_CELoss: 1.3801 DKDLoss: 0.9263 
2022-05-02 11:00:28 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.1278, stu_CELoss: 1.3064 DKDLoss: 0.8214 
2022-05-02 11:01:00 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.2119, stu_CELoss: 1.3336 DKDLoss: 0.8783 
2022-05-02 11:01:32 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 2.1048, stu_CELoss: 1.2188 DKDLoss: 0.8859 
2022-05-02 11:02:03 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.2503, stu_CELoss: 1.3090 DKDLoss: 0.9412 
2022-05-02 11:02:35 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0921, stu_CELoss: 1.2494 DKDLoss: 0.8427 
2022-05-02 11:03:06 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.5557, stu_CELoss: 1.5884 DKDLoss: 0.9673 
2022-05-02 11:03:38 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.1332, stu_CELoss: 1.3431 DKDLoss: 0.7901 
2022-05-02 11:04:09 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.2194, stu_CELoss: 1.3221 DKDLoss: 0.8973 
2022-05-02 11:04:41 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.4852, stu_CELoss: 1.5674 DKDLoss: 0.9178 
2022-05-02 11:05:13 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.9214, stu_CELoss: 1.1861 DKDLoss: 0.7353 
2022-05-02 11:05:45 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.3436, stu_CELoss: 1.3528 DKDLoss: 0.9908 
2022-05-02 11:05:46 - train: epoch 033, train_loss: 2.1880
2022-05-02 11:08:12 - eval: epoch: 033, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.102%, stu_acc5: 90.944%, stu_test_loss: 1.1051
2022-05-02 11:08:12 - until epoch: 033, tea_best_acc1: 78.330%, stu_best_acc1: 72.102%
2022-05-02 11:08:12 - epoch 034 lr: 0.010000000000000002
2022-05-02 11:08:50 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 2.0115, stu_CELoss: 1.1540 DKDLoss: 0.8575 
2022-05-02 11:09:20 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.0321, stu_CELoss: 1.3122 DKDLoss: 0.7199 
2022-05-02 11:09:50 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 2.0027, stu_CELoss: 1.1822 DKDLoss: 0.8205 
2022-05-02 11:10:21 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.9246, stu_CELoss: 1.1361 DKDLoss: 0.7885 
2022-05-02 11:10:52 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.9851, stu_CELoss: 1.2387 DKDLoss: 0.7464 
2022-05-02 11:11:23 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.4175, stu_CELoss: 1.4734 DKDLoss: 0.9440 
2022-05-02 11:11:53 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 2.0158, stu_CELoss: 1.1702 DKDLoss: 0.8455 
2022-05-02 11:12:25 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.2111, stu_CELoss: 1.3779 DKDLoss: 0.8332 
2022-05-02 11:12:56 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.1486, stu_CELoss: 1.2668 DKDLoss: 0.8819 
2022-05-02 11:13:29 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.1323, stu_CELoss: 1.3598 DKDLoss: 0.7726 
2022-05-02 11:14:01 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.0389, stu_CELoss: 1.1747 DKDLoss: 0.8642 
2022-05-02 11:14:33 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.1637, stu_CELoss: 1.2842 DKDLoss: 0.8796 
2022-05-02 11:15:05 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 2.0248, stu_CELoss: 1.1528 DKDLoss: 0.8720 
2022-05-02 11:15:37 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.0913, stu_CELoss: 1.2805 DKDLoss: 0.8108 
2022-05-02 11:16:09 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.1146, stu_CELoss: 1.3452 DKDLoss: 0.7695 
2022-05-02 11:16:41 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.1698, stu_CELoss: 1.3069 DKDLoss: 0.8628 
2022-05-02 11:17:13 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.0999, stu_CELoss: 1.2498 DKDLoss: 0.8501 
2022-05-02 11:17:44 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1853, stu_CELoss: 1.2783 DKDLoss: 0.9070 
2022-05-02 11:18:16 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.3016, stu_CELoss: 1.4320 DKDLoss: 0.8696 
2022-05-02 11:18:48 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.9993, stu_CELoss: 1.1649 DKDLoss: 0.8344 
2022-05-02 11:19:20 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.3522, stu_CELoss: 1.4373 DKDLoss: 0.9149 
2022-05-02 11:19:52 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.9114, stu_CELoss: 1.1182 DKDLoss: 0.7932 
2022-05-02 11:20:23 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.1603, stu_CELoss: 1.3739 DKDLoss: 0.7864 
2022-05-02 11:20:55 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 2.0087, stu_CELoss: 1.1210 DKDLoss: 0.8877 
2022-05-02 11:21:27 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.2689, stu_CELoss: 1.3205 DKDLoss: 0.9484 
2022-05-02 11:21:59 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.1916, stu_CELoss: 1.3750 DKDLoss: 0.8166 
2022-05-02 11:22:31 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.1637, stu_CELoss: 1.3117 DKDLoss: 0.8520 
2022-05-02 11:23:03 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.9747, stu_CELoss: 1.1749 DKDLoss: 0.7998 
2022-05-02 11:23:35 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.8825, stu_CELoss: 1.1306 DKDLoss: 0.7519 
2022-05-02 11:24:07 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.9226, stu_CELoss: 1.1765 DKDLoss: 0.7461 
2022-05-02 11:24:39 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 2.1676, stu_CELoss: 1.3371 DKDLoss: 0.8305 
2022-05-02 11:25:11 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.0752, stu_CELoss: 1.2577 DKDLoss: 0.8175 
2022-05-02 11:25:43 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 2.0642, stu_CELoss: 1.1863 DKDLoss: 0.8779 
2022-05-02 11:26:15 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.4149, stu_CELoss: 1.5205 DKDLoss: 0.8944 
2022-05-02 11:26:47 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.8954, stu_CELoss: 1.0842 DKDLoss: 0.8112 
2022-05-02 11:27:19 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.8145, stu_CELoss: 1.0821 DKDLoss: 0.7324 
2022-05-02 11:27:51 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.9946, stu_CELoss: 1.1692 DKDLoss: 0.8254 
2022-05-02 11:28:23 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.9900, stu_CELoss: 1.1734 DKDLoss: 0.8166 
2022-05-02 11:28:54 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.3090, stu_CELoss: 1.4733 DKDLoss: 0.8357 
2022-05-02 11:29:26 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 2.0392, stu_CELoss: 1.2422 DKDLoss: 0.7970 
2022-05-02 11:29:58 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.3820, stu_CELoss: 1.4789 DKDLoss: 0.9031 
2022-05-02 11:30:30 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.1256, stu_CELoss: 1.2745 DKDLoss: 0.8510 
2022-05-02 11:31:02 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.1534, stu_CELoss: 1.2993 DKDLoss: 0.8540 
2022-05-02 11:31:34 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 2.1289, stu_CELoss: 1.3570 DKDLoss: 0.7719 
2022-05-02 11:32:06 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.1788, stu_CELoss: 1.3106 DKDLoss: 0.8682 
2022-05-02 11:32:37 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.3254, stu_CELoss: 1.3797 DKDLoss: 0.9457 
2022-05-02 11:33:09 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.0694, stu_CELoss: 1.2106 DKDLoss: 0.8588 
2022-05-02 11:33:41 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.2204, stu_CELoss: 1.2859 DKDLoss: 0.9345 
2022-05-02 11:34:13 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.3168, stu_CELoss: 1.4392 DKDLoss: 0.8776 
2022-05-02 11:34:45 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 2.0236, stu_CELoss: 1.2677 DKDLoss: 0.7560 
2022-05-02 11:34:46 - train: epoch 034, train_loss: 2.1300
2022-05-02 11:37:12 - eval: epoch: 034, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.302%, stu_acc5: 91.022%, stu_test_loss: 1.1006
2022-05-02 11:37:13 - until epoch: 034, tea_best_acc1: 78.330%, stu_best_acc1: 72.302%
2022-05-02 11:37:13 - epoch 035 lr: 0.010000000000000002
2022-05-02 11:37:52 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.9929, stu_CELoss: 1.2102 DKDLoss: 0.7827 
2022-05-02 11:38:23 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.8460, stu_CELoss: 1.0165 DKDLoss: 0.8295 
2022-05-02 11:38:53 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.2234, stu_CELoss: 1.2604 DKDLoss: 0.9630 
2022-05-02 11:39:24 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.0980, stu_CELoss: 1.2664 DKDLoss: 0.8316 
2022-05-02 11:39:55 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.9272, stu_CELoss: 1.1321 DKDLoss: 0.7951 
2022-05-02 11:40:26 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.1658, stu_CELoss: 1.2590 DKDLoss: 0.9068 
2022-05-02 11:40:58 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.1844, stu_CELoss: 1.4110 DKDLoss: 0.7734 
2022-05-02 11:41:29 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.9381, stu_CELoss: 1.0910 DKDLoss: 0.8470 
2022-05-02 11:42:00 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.3190, stu_CELoss: 1.4080 DKDLoss: 0.9111 
2022-05-02 11:42:31 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.0149, stu_CELoss: 1.3530 DKDLoss: 0.6620 
2022-05-02 11:43:03 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.2440, stu_CELoss: 1.3721 DKDLoss: 0.8718 
2022-05-02 11:43:35 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 2.1376, stu_CELoss: 1.2111 DKDLoss: 0.9265 
2022-05-02 11:44:06 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 2.2242, stu_CELoss: 1.4233 DKDLoss: 0.8009 
2022-05-02 11:44:38 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.0719, stu_CELoss: 1.2339 DKDLoss: 0.8380 
2022-05-02 11:45:10 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.1077, stu_CELoss: 1.2935 DKDLoss: 0.8143 
2022-05-02 11:45:42 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 2.1830, stu_CELoss: 1.3161 DKDLoss: 0.8669 
2022-05-02 11:46:13 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.9967, stu_CELoss: 1.2179 DKDLoss: 0.7788 
2022-05-02 11:46:45 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.2459, stu_CELoss: 1.3726 DKDLoss: 0.8733 
2022-05-02 11:47:17 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.0250, stu_CELoss: 1.1882 DKDLoss: 0.8368 
2022-05-02 11:47:49 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.3558, stu_CELoss: 1.4610 DKDLoss: 0.8948 
2022-05-02 11:48:21 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.1319, stu_CELoss: 1.2396 DKDLoss: 0.8923 
2022-05-02 11:48:52 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.0575, stu_CELoss: 1.2448 DKDLoss: 0.8127 
2022-05-02 11:49:24 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.1417, stu_CELoss: 1.3723 DKDLoss: 0.7694 
2022-05-02 11:49:56 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.1353, stu_CELoss: 1.3416 DKDLoss: 0.7938 
2022-05-02 11:50:29 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.1278, stu_CELoss: 1.2665 DKDLoss: 0.8613 
2022-05-02 11:51:01 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.1697, stu_CELoss: 1.3163 DKDLoss: 0.8533 
2022-05-02 11:51:33 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.0500, stu_CELoss: 1.1873 DKDLoss: 0.8627 
2022-05-02 11:52:05 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.1960, stu_CELoss: 1.3468 DKDLoss: 0.8492 
2022-05-02 11:52:36 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 2.0683, stu_CELoss: 1.2457 DKDLoss: 0.8226 
2022-05-02 11:53:08 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.8029, stu_CELoss: 1.0606 DKDLoss: 0.7424 
2022-05-02 11:53:40 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.2096, stu_CELoss: 1.3513 DKDLoss: 0.8582 
2022-05-02 11:54:12 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.9802, stu_CELoss: 1.1465 DKDLoss: 0.8337 
2022-05-02 11:54:44 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 2.1100, stu_CELoss: 1.2542 DKDLoss: 0.8558 
2022-05-02 11:55:16 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.0163, stu_CELoss: 1.2200 DKDLoss: 0.7963 
2022-05-02 11:55:48 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.6747, stu_CELoss: 0.9980 DKDLoss: 0.6767 
2022-05-02 11:56:20 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.2446, stu_CELoss: 1.3270 DKDLoss: 0.9177 
2022-05-02 11:56:51 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.9194, stu_CELoss: 1.0884 DKDLoss: 0.8309 
2022-05-02 11:57:23 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.9582, stu_CELoss: 1.2164 DKDLoss: 0.7418 
2022-05-02 11:57:55 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.9419, stu_CELoss: 1.1614 DKDLoss: 0.7805 
2022-05-02 11:58:27 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.9169, stu_CELoss: 1.1397 DKDLoss: 0.7771 
2022-05-02 11:58:59 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.2929, stu_CELoss: 1.3358 DKDLoss: 0.9571 
2022-05-02 11:59:31 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.9913, stu_CELoss: 1.2438 DKDLoss: 0.7475 
2022-05-02 12:00:03 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.2058, stu_CELoss: 1.3248 DKDLoss: 0.8810 
2022-05-02 12:00:34 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.3219, stu_CELoss: 1.4678 DKDLoss: 0.8542 
2022-05-02 12:01:06 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.1129, stu_CELoss: 1.2680 DKDLoss: 0.8449 
2022-05-02 12:01:38 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.1314, stu_CELoss: 1.2230 DKDLoss: 0.9085 
2022-05-02 12:02:10 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.2729, stu_CELoss: 1.3837 DKDLoss: 0.8892 
2022-05-02 12:02:42 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.0783, stu_CELoss: 1.1920 DKDLoss: 0.8863 
2022-05-02 12:03:13 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.1451, stu_CELoss: 1.2383 DKDLoss: 0.9068 
2022-05-02 12:03:45 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 2.0633, stu_CELoss: 1.3055 DKDLoss: 0.7579 
2022-05-02 12:03:47 - train: epoch 035, train_loss: 2.0883
2022-05-02 12:06:12 - eval: epoch: 035, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.198%, stu_acc5: 91.072%, stu_test_loss: 1.1035
2022-05-02 12:06:12 - until epoch: 035, tea_best_acc1: 78.330%, stu_best_acc1: 72.302%
2022-05-02 12:06:12 - epoch 036 lr: 0.010000000000000002
2022-05-02 12:06:50 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.2433, stu_CELoss: 1.4521 DKDLoss: 0.7912 
2022-05-02 12:07:21 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.7608, stu_CELoss: 1.0169 DKDLoss: 0.7439 
2022-05-02 12:07:52 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.9205, stu_CELoss: 1.1983 DKDLoss: 0.7223 
2022-05-02 12:08:24 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.9779, stu_CELoss: 1.2128 DKDLoss: 0.7652 
2022-05-02 12:08:55 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9764, stu_CELoss: 1.1910 DKDLoss: 0.7855 
2022-05-02 12:09:26 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 2.0520, stu_CELoss: 1.2410 DKDLoss: 0.8110 
2022-05-02 12:09:57 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.9652, stu_CELoss: 1.1172 DKDLoss: 0.8479 
2022-05-02 12:10:29 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.7532, stu_CELoss: 0.9994 DKDLoss: 0.7538 
2022-05-02 12:11:00 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.9275, stu_CELoss: 1.1411 DKDLoss: 0.7863 
2022-05-02 12:11:32 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 2.0412, stu_CELoss: 1.2538 DKDLoss: 0.7874 
2022-05-02 12:12:03 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.9549, stu_CELoss: 1.1343 DKDLoss: 0.8206 
2022-05-02 12:12:34 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 2.1495, stu_CELoss: 1.3736 DKDLoss: 0.7760 
2022-05-02 12:13:05 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.1355, stu_CELoss: 1.2707 DKDLoss: 0.8648 
2022-05-02 12:13:36 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.2104, stu_CELoss: 1.4216 DKDLoss: 0.7888 
2022-05-02 12:14:08 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.1692, stu_CELoss: 1.2212 DKDLoss: 0.9481 
2022-05-02 12:14:40 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.9740, stu_CELoss: 1.1473 DKDLoss: 0.8267 
2022-05-02 12:15:11 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 2.1274, stu_CELoss: 1.3705 DKDLoss: 0.7569 
2022-05-02 12:15:43 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 2.0626, stu_CELoss: 1.2598 DKDLoss: 0.8028 
2022-05-02 12:16:15 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.1521, stu_CELoss: 1.3056 DKDLoss: 0.8465 
2022-05-02 12:16:47 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.9920, stu_CELoss: 1.2085 DKDLoss: 0.7835 
2022-05-02 12:17:18 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.9442, stu_CELoss: 1.2155 DKDLoss: 0.7287 
2022-05-02 12:17:50 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.1215, stu_CELoss: 1.3217 DKDLoss: 0.7999 
2022-05-02 12:18:22 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.1403, stu_CELoss: 1.3984 DKDLoss: 0.7420 
2022-05-02 12:18:53 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.0349, stu_CELoss: 1.2621 DKDLoss: 0.7728 
2022-05-02 12:19:24 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.9124, stu_CELoss: 1.1434 DKDLoss: 0.7690 
2022-05-02 12:19:56 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.0700, stu_CELoss: 1.3134 DKDLoss: 0.7566 
2022-05-02 12:20:27 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.8425, stu_CELoss: 1.0798 DKDLoss: 0.7627 
2022-05-02 12:20:58 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.8895, stu_CELoss: 1.1528 DKDLoss: 0.7367 
2022-05-02 12:21:30 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.7605, stu_CELoss: 1.0333 DKDLoss: 0.7272 
2022-05-02 12:22:01 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.9557, stu_CELoss: 1.1726 DKDLoss: 0.7832 
2022-05-02 12:22:33 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.1575, stu_CELoss: 1.2843 DKDLoss: 0.8732 
2022-05-02 12:23:04 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.2886, stu_CELoss: 1.4260 DKDLoss: 0.8626 
2022-05-02 12:23:36 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.8240, stu_CELoss: 1.1131 DKDLoss: 0.7109 
2022-05-02 12:24:08 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.8245, stu_CELoss: 1.1183 DKDLoss: 0.7061 
2022-05-02 12:24:39 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.3388, stu_CELoss: 1.4594 DKDLoss: 0.8794 
2022-05-02 12:25:11 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.1501, stu_CELoss: 1.3252 DKDLoss: 0.8249 
2022-05-02 12:25:42 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.9334, stu_CELoss: 1.1621 DKDLoss: 0.7713 
2022-05-02 12:26:13 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.0256, stu_CELoss: 1.2325 DKDLoss: 0.7931 
2022-05-02 12:26:45 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.1306, stu_CELoss: 1.2151 DKDLoss: 0.9155 
2022-05-02 12:27:16 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.1594, stu_CELoss: 1.3392 DKDLoss: 0.8201 
2022-05-02 12:27:48 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 2.1974, stu_CELoss: 1.3078 DKDLoss: 0.8896 
2022-05-02 12:28:20 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.1523, stu_CELoss: 1.3035 DKDLoss: 0.8488 
2022-05-02 12:28:51 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.8876, stu_CELoss: 1.1418 DKDLoss: 0.7458 
2022-05-02 12:29:22 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.1812, stu_CELoss: 1.3673 DKDLoss: 0.8139 
2022-05-02 12:29:54 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.9927, stu_CELoss: 1.2580 DKDLoss: 0.7347 
2022-05-02 12:30:25 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.9547, stu_CELoss: 1.1162 DKDLoss: 0.8385 
2022-05-02 12:30:56 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 2.0684, stu_CELoss: 1.2285 DKDLoss: 0.8399 
2022-05-02 12:31:28 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 2.0722, stu_CELoss: 1.2611 DKDLoss: 0.8111 
2022-05-02 12:31:59 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.9152, stu_CELoss: 1.0900 DKDLoss: 0.8253 
2022-05-02 12:32:31 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.9473, stu_CELoss: 1.1215 DKDLoss: 0.8259 
2022-05-02 12:32:32 - train: epoch 036, train_loss: 2.0582
2022-05-02 12:34:57 - eval: epoch: 036, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.510%, stu_acc5: 91.090%, stu_test_loss: 1.0887
2022-05-02 12:34:58 - until epoch: 036, tea_best_acc1: 78.330%, stu_best_acc1: 72.510%
2022-05-02 12:34:58 - epoch 037 lr: 0.010000000000000002
2022-05-02 12:35:37 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.7671, stu_CELoss: 1.0741 DKDLoss: 0.6930 
2022-05-02 12:36:08 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.7890, stu_CELoss: 1.0872 DKDLoss: 0.7018 
2022-05-02 12:36:40 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.8731, stu_CELoss: 1.0705 DKDLoss: 0.8027 
2022-05-02 12:37:12 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 2.0858, stu_CELoss: 1.2778 DKDLoss: 0.8080 
2022-05-02 12:37:43 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.0170, stu_CELoss: 1.2340 DKDLoss: 0.7830 
2022-05-02 12:38:15 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.2697, stu_CELoss: 1.3665 DKDLoss: 0.9033 
2022-05-02 12:38:46 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.1138, stu_CELoss: 1.2700 DKDLoss: 0.8438 
2022-05-02 12:39:18 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.8734, stu_CELoss: 1.1074 DKDLoss: 0.7660 
2022-05-02 12:39:50 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.2214, stu_CELoss: 1.4072 DKDLoss: 0.8141 
2022-05-02 12:40:22 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 2.1255, stu_CELoss: 1.3330 DKDLoss: 0.7925 
2022-05-02 12:40:54 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.9000, stu_CELoss: 1.1352 DKDLoss: 0.7647 
2022-05-02 12:41:26 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.1570, stu_CELoss: 1.3361 DKDLoss: 0.8209 
2022-05-02 12:41:58 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.1294, stu_CELoss: 1.3704 DKDLoss: 0.7591 
2022-05-02 12:42:30 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.3257, stu_CELoss: 1.4278 DKDLoss: 0.8979 
2022-05-02 12:43:01 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 2.0849, stu_CELoss: 1.2615 DKDLoss: 0.8234 
2022-05-02 12:43:33 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.8829, stu_CELoss: 1.1117 DKDLoss: 0.7712 
2022-05-02 12:44:05 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.0990, stu_CELoss: 1.2761 DKDLoss: 0.8229 
2022-05-02 12:44:36 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.2006, stu_CELoss: 1.3258 DKDLoss: 0.8747 
2022-05-02 12:45:08 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 2.0857, stu_CELoss: 1.2797 DKDLoss: 0.8060 
2022-05-02 12:45:39 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.2305, stu_CELoss: 1.4135 DKDLoss: 0.8170 
2022-05-02 12:46:11 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.0164, stu_CELoss: 1.2720 DKDLoss: 0.7444 
2022-05-02 12:46:42 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 2.0040, stu_CELoss: 1.1943 DKDLoss: 0.8097 
2022-05-02 12:47:13 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.8131, stu_CELoss: 1.0847 DKDLoss: 0.7284 
2022-05-02 12:47:44 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.9983, stu_CELoss: 1.2655 DKDLoss: 0.7328 
2022-05-02 12:48:15 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.8936, stu_CELoss: 1.0763 DKDLoss: 0.8173 
2022-05-02 12:48:47 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.2098, stu_CELoss: 1.4028 DKDLoss: 0.8070 
2022-05-02 12:49:19 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.2506, stu_CELoss: 1.4177 DKDLoss: 0.8330 
2022-05-02 12:49:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 2.0351, stu_CELoss: 1.2693 DKDLoss: 0.7658 
2022-05-02 12:50:23 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 2.1203, stu_CELoss: 1.3571 DKDLoss: 0.7632 
2022-05-02 12:50:54 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.2729, stu_CELoss: 1.3388 DKDLoss: 0.9341 
2022-05-02 12:51:26 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.8805, stu_CELoss: 1.1169 DKDLoss: 0.7637 
2022-05-02 12:51:58 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.9053, stu_CELoss: 1.1100 DKDLoss: 0.7952 
2022-05-02 12:52:29 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.8072, stu_CELoss: 1.0562 DKDLoss: 0.7510 
2022-05-02 12:53:00 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.6683, stu_CELoss: 0.9807 DKDLoss: 0.6876 
2022-05-02 12:53:31 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.9524, stu_CELoss: 1.1549 DKDLoss: 0.7975 
2022-05-02 12:54:03 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.1397, stu_CELoss: 1.3329 DKDLoss: 0.8068 
2022-05-02 12:54:35 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.9462, stu_CELoss: 1.2698 DKDLoss: 0.6764 
2022-05-02 12:55:06 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.1055, stu_CELoss: 1.2144 DKDLoss: 0.8911 
2022-05-02 12:55:38 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.9825, stu_CELoss: 1.2320 DKDLoss: 0.7504 
2022-05-02 12:56:09 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.9124, stu_CELoss: 1.1255 DKDLoss: 0.7868 
2022-05-02 12:56:40 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.9470, stu_CELoss: 1.1310 DKDLoss: 0.8160 
2022-05-02 12:57:12 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.1379, stu_CELoss: 1.3019 DKDLoss: 0.8360 
2022-05-02 12:57:44 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.1940, stu_CELoss: 1.3425 DKDLoss: 0.8516 
2022-05-02 12:58:15 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.7716, stu_CELoss: 1.0466 DKDLoss: 0.7250 
2022-05-02 12:58:47 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.9927, stu_CELoss: 1.1680 DKDLoss: 0.8247 
2022-05-02 12:59:18 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.1207, stu_CELoss: 1.2094 DKDLoss: 0.9113 
2022-05-02 12:59:50 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 2.3209, stu_CELoss: 1.4230 DKDLoss: 0.8979 
2022-05-02 13:00:22 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 2.1348, stu_CELoss: 1.3141 DKDLoss: 0.8207 
2022-05-02 13:00:54 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.8684, stu_CELoss: 1.1236 DKDLoss: 0.7448 
2022-05-02 13:01:26 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.9328, stu_CELoss: 1.2169 DKDLoss: 0.7159 
2022-05-02 13:01:27 - train: epoch 037, train_loss: 2.0374
2022-05-02 13:03:53 - eval: epoch: 037, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.966%, stu_acc5: 91.382%, stu_test_loss: 1.0703
2022-05-02 13:03:54 - until epoch: 037, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 13:03:54 - epoch 038 lr: 0.010000000000000002
2022-05-02 13:04:32 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.7801, stu_CELoss: 1.0152 DKDLoss: 0.7649 
2022-05-02 13:05:04 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.9721, stu_CELoss: 1.1977 DKDLoss: 0.7744 
2022-05-02 13:05:36 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.7010, stu_CELoss: 0.9604 DKDLoss: 0.7406 
2022-05-02 13:06:07 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.8887, stu_CELoss: 1.1157 DKDLoss: 0.7729 
2022-05-02 13:06:39 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.8525, stu_CELoss: 1.1119 DKDLoss: 0.7406 
2022-05-02 13:07:10 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.3614, stu_CELoss: 1.5338 DKDLoss: 0.8277 
2022-05-02 13:07:42 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.8643, stu_CELoss: 1.1033 DKDLoss: 0.7610 
2022-05-02 13:08:14 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.8555, stu_CELoss: 1.0616 DKDLoss: 0.7939 
2022-05-02 13:08:46 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.0861, stu_CELoss: 1.3544 DKDLoss: 0.7317 
2022-05-02 13:09:18 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.4645, stu_CELoss: 1.5273 DKDLoss: 0.9372 
2022-05-02 13:09:50 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.8840, stu_CELoss: 1.1092 DKDLoss: 0.7747 
2022-05-02 13:10:22 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.1999, stu_CELoss: 1.3440 DKDLoss: 0.8559 
2022-05-02 13:10:54 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.0621, stu_CELoss: 1.2058 DKDLoss: 0.8563 
2022-05-02 13:11:25 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 2.0942, stu_CELoss: 1.3051 DKDLoss: 0.7891 
2022-05-02 13:11:57 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 2.0913, stu_CELoss: 1.2747 DKDLoss: 0.8166 
2022-05-02 13:12:28 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.2372, stu_CELoss: 1.3626 DKDLoss: 0.8746 
2022-05-02 13:13:00 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.2173, stu_CELoss: 1.3739 DKDLoss: 0.8434 
2022-05-02 13:13:32 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.1632, stu_CELoss: 1.3452 DKDLoss: 0.8180 
2022-05-02 13:14:03 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.8078, stu_CELoss: 1.0807 DKDLoss: 0.7272 
2022-05-02 13:14:35 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.0876, stu_CELoss: 1.3501 DKDLoss: 0.7375 
2022-05-02 13:15:07 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.9227, stu_CELoss: 1.1808 DKDLoss: 0.7419 
2022-05-02 13:15:40 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 2.0050, stu_CELoss: 1.2172 DKDLoss: 0.7878 
2022-05-02 13:16:12 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.1608, stu_CELoss: 1.3003 DKDLoss: 0.8605 
2022-05-02 13:16:44 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.1765, stu_CELoss: 1.3749 DKDLoss: 0.8017 
2022-05-02 13:17:15 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.7921, stu_CELoss: 1.0083 DKDLoss: 0.7838 
2022-05-02 13:17:47 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 2.0899, stu_CELoss: 1.2734 DKDLoss: 0.8164 
2022-05-02 13:18:19 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.7362, stu_CELoss: 1.0667 DKDLoss: 0.6694 
2022-05-02 13:18:51 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.3414, stu_CELoss: 1.5017 DKDLoss: 0.8398 
2022-05-02 13:19:23 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.9647, stu_CELoss: 1.2102 DKDLoss: 0.7545 
2022-05-02 13:19:54 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 2.0119, stu_CELoss: 1.2378 DKDLoss: 0.7741 
2022-05-02 13:20:26 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.2726, stu_CELoss: 1.4722 DKDLoss: 0.8004 
2022-05-02 13:20:57 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.5566, stu_CELoss: 0.8606 DKDLoss: 0.6960 
2022-05-02 13:21:29 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 2.1048, stu_CELoss: 1.2400 DKDLoss: 0.8648 
2022-05-02 13:22:01 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 2.2108, stu_CELoss: 1.3331 DKDLoss: 0.8777 
2022-05-02 13:22:32 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.2241, stu_CELoss: 1.3256 DKDLoss: 0.8985 
2022-05-02 13:23:04 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.0622, stu_CELoss: 1.1915 DKDLoss: 0.8707 
2022-05-02 13:23:35 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.8857, stu_CELoss: 1.1154 DKDLoss: 0.7702 
2022-05-02 13:24:07 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.2119, stu_CELoss: 1.3097 DKDLoss: 0.9022 
2022-05-02 13:24:39 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.9908, stu_CELoss: 1.1802 DKDLoss: 0.8106 
2022-05-02 13:25:10 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.0007, stu_CELoss: 1.2907 DKDLoss: 0.7100 
2022-05-02 13:25:42 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 2.1189, stu_CELoss: 1.3180 DKDLoss: 0.8009 
2022-05-02 13:26:14 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.8596, stu_CELoss: 1.0117 DKDLoss: 0.8479 
2022-05-02 13:26:45 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.0659, stu_CELoss: 1.2550 DKDLoss: 0.8109 
2022-05-02 13:27:17 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 2.2276, stu_CELoss: 1.3893 DKDLoss: 0.8382 
2022-05-02 13:27:49 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.2885, stu_CELoss: 1.3826 DKDLoss: 0.9059 
2022-05-02 13:28:21 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.2400, stu_CELoss: 1.3144 DKDLoss: 0.9256 
2022-05-02 13:28:53 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.8181, stu_CELoss: 1.0286 DKDLoss: 0.7895 
2022-05-02 13:29:25 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.9135, stu_CELoss: 1.1029 DKDLoss: 0.8106 
2022-05-02 13:29:57 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.0473, stu_CELoss: 1.3359 DKDLoss: 0.7114 
2022-05-02 13:30:28 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7740, stu_CELoss: 0.9248 DKDLoss: 0.8492 
2022-05-02 13:30:30 - train: epoch 038, train_loss: 2.0252
2022-05-02 13:32:56 - eval: epoch: 038, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.814%, stu_acc5: 91.240%, stu_test_loss: 1.0810
2022-05-02 13:32:57 - until epoch: 038, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 13:32:57 - epoch 039 lr: 0.010000000000000002
2022-05-02 13:33:35 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 2.0054, stu_CELoss: 1.2432 DKDLoss: 0.7622 
2022-05-02 13:34:06 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.1910, stu_CELoss: 1.4199 DKDLoss: 0.7711 
2022-05-02 13:34:38 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.9725, stu_CELoss: 1.2018 DKDLoss: 0.7707 
2022-05-02 13:35:10 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.0395, stu_CELoss: 1.2865 DKDLoss: 0.7530 
2022-05-02 13:35:43 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.0084, stu_CELoss: 1.2729 DKDLoss: 0.7355 
2022-05-02 13:36:14 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 2.0118, stu_CELoss: 1.2203 DKDLoss: 0.7915 
2022-05-02 13:36:46 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.1089, stu_CELoss: 1.2374 DKDLoss: 0.8716 
2022-05-02 13:37:17 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.8076, stu_CELoss: 1.0601 DKDLoss: 0.7475 
2022-05-02 13:37:49 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.1729, stu_CELoss: 1.3309 DKDLoss: 0.8421 
2022-05-02 13:38:20 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.9433, stu_CELoss: 1.1657 DKDLoss: 0.7776 
2022-05-02 13:38:53 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.9433, stu_CELoss: 1.2310 DKDLoss: 0.7123 
2022-05-02 13:39:24 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.9833, stu_CELoss: 1.1579 DKDLoss: 0.8254 
2022-05-02 13:39:56 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.0313, stu_CELoss: 1.2329 DKDLoss: 0.7984 
2022-05-02 13:40:28 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.0249, stu_CELoss: 1.2148 DKDLoss: 0.8101 
2022-05-02 13:41:00 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.7790, stu_CELoss: 1.0400 DKDLoss: 0.7390 
2022-05-02 13:41:32 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 2.1620, stu_CELoss: 1.3153 DKDLoss: 0.8467 
2022-05-02 13:42:04 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.8172, stu_CELoss: 1.0849 DKDLoss: 0.7323 
2022-05-02 13:42:36 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 2.1688, stu_CELoss: 1.3214 DKDLoss: 0.8474 
2022-05-02 13:43:08 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.9814, stu_CELoss: 1.1530 DKDLoss: 0.8284 
2022-05-02 13:43:40 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 2.1744, stu_CELoss: 1.2975 DKDLoss: 0.8769 
2022-05-02 13:44:11 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.0574, stu_CELoss: 1.3540 DKDLoss: 0.7034 
2022-05-02 13:44:43 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.9767, stu_CELoss: 1.2452 DKDLoss: 0.7315 
2022-05-02 13:45:14 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.3171, stu_CELoss: 1.5079 DKDLoss: 0.8092 
2022-05-02 13:45:46 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.0883, stu_CELoss: 1.2402 DKDLoss: 0.8481 
2022-05-02 13:46:17 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.9799, stu_CELoss: 1.2569 DKDLoss: 0.7229 
2022-05-02 13:46:49 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 2.1560, stu_CELoss: 1.3532 DKDLoss: 0.8029 
2022-05-02 13:47:20 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.0979, stu_CELoss: 1.2964 DKDLoss: 0.8015 
2022-05-02 13:47:52 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 2.1909, stu_CELoss: 1.2986 DKDLoss: 0.8923 
2022-05-02 13:48:24 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.8894, stu_CELoss: 1.1552 DKDLoss: 0.7342 
2022-05-02 13:48:55 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.1854, stu_CELoss: 1.3482 DKDLoss: 0.8372 
2022-05-02 13:49:27 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.8713, stu_CELoss: 1.0715 DKDLoss: 0.7998 
2022-05-02 13:49:59 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.1519, stu_CELoss: 1.3471 DKDLoss: 0.8048 
2022-05-02 13:50:31 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.0308, stu_CELoss: 1.1697 DKDLoss: 0.8610 
2022-05-02 13:51:03 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.0712, stu_CELoss: 1.1492 DKDLoss: 0.9219 
2022-05-02 13:51:35 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.1456, stu_CELoss: 1.3092 DKDLoss: 0.8364 
2022-05-02 13:52:07 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.4588, stu_CELoss: 1.6452 DKDLoss: 0.8136 
2022-05-02 13:52:39 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.7911, stu_CELoss: 1.0483 DKDLoss: 0.7428 
2022-05-02 13:53:10 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 2.0028, stu_CELoss: 1.2301 DKDLoss: 0.7726 
2022-05-02 13:53:42 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.9488, stu_CELoss: 1.1142 DKDLoss: 0.8346 
2022-05-02 13:54:14 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.8443, stu_CELoss: 1.1386 DKDLoss: 0.7058 
2022-05-02 13:54:46 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 2.3116, stu_CELoss: 1.4679 DKDLoss: 0.8437 
2022-05-02 13:55:17 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.0223, stu_CELoss: 1.2208 DKDLoss: 0.8015 
2022-05-02 13:55:49 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.8617, stu_CELoss: 1.0879 DKDLoss: 0.7738 
2022-05-02 13:56:20 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 2.0396, stu_CELoss: 1.2152 DKDLoss: 0.8243 
2022-05-02 13:56:52 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.8349, stu_CELoss: 1.1352 DKDLoss: 0.6997 
2022-05-02 13:57:23 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.2254, stu_CELoss: 1.4229 DKDLoss: 0.8024 
2022-05-02 13:57:55 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7428, stu_CELoss: 0.9745 DKDLoss: 0.7683 
2022-05-02 13:58:27 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.7857, stu_CELoss: 1.0910 DKDLoss: 0.6947 
2022-05-02 13:58:59 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.7325, stu_CELoss: 1.0054 DKDLoss: 0.7270 
2022-05-02 13:59:30 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.7761, stu_CELoss: 1.0757 DKDLoss: 0.7004 
2022-05-02 13:59:32 - train: epoch 039, train_loss: 2.0149
2022-05-02 14:01:58 - eval: epoch: 039, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.586%, stu_acc5: 91.244%, stu_test_loss: 1.0856
2022-05-02 14:01:58 - until epoch: 039, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 14:01:58 - epoch 040 lr: 0.010000000000000002
2022-05-02 14:02:37 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.1270, stu_CELoss: 1.3940 DKDLoss: 0.7331 
2022-05-02 14:03:09 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.9177, stu_CELoss: 1.1993 DKDLoss: 0.7184 
2022-05-02 14:03:40 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.2124, stu_CELoss: 1.3854 DKDLoss: 0.8271 
2022-05-02 14:04:12 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.6699, stu_CELoss: 1.0008 DKDLoss: 0.6690 
2022-05-02 14:04:44 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.7629, stu_CELoss: 1.0393 DKDLoss: 0.7236 
2022-05-02 14:05:15 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.1386, stu_CELoss: 1.3071 DKDLoss: 0.8315 
2022-05-02 14:05:47 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.0893, stu_CELoss: 1.2533 DKDLoss: 0.8360 
2022-05-02 14:06:19 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.9727, stu_CELoss: 1.1819 DKDLoss: 0.7908 
2022-05-02 14:06:51 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.0071, stu_CELoss: 1.1749 DKDLoss: 0.8322 
2022-05-02 14:07:22 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.9100, stu_CELoss: 1.1500 DKDLoss: 0.7600 
2022-05-02 14:07:54 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.9944, stu_CELoss: 1.1796 DKDLoss: 0.8148 
2022-05-02 14:08:26 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 2.0915, stu_CELoss: 1.3401 DKDLoss: 0.7513 
2022-05-02 14:08:57 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 2.0164, stu_CELoss: 1.2316 DKDLoss: 0.7849 
2022-05-02 14:09:29 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 2.0618, stu_CELoss: 1.2789 DKDLoss: 0.7829 
2022-05-02 14:10:01 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.8490, stu_CELoss: 1.1526 DKDLoss: 0.6964 
2022-05-02 14:10:33 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.1616, stu_CELoss: 1.2952 DKDLoss: 0.8664 
2022-05-02 14:11:05 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.8213, stu_CELoss: 1.0740 DKDLoss: 0.7474 
2022-05-02 14:11:37 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 2.0344, stu_CELoss: 1.2316 DKDLoss: 0.8028 
2022-05-02 14:12:08 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.8883, stu_CELoss: 1.1517 DKDLoss: 0.7366 
2022-05-02 14:12:40 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.9354, stu_CELoss: 1.1691 DKDLoss: 0.7663 
2022-05-02 14:13:12 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.9882, stu_CELoss: 1.2155 DKDLoss: 0.7727 
2022-05-02 14:13:44 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 2.0378, stu_CELoss: 1.2005 DKDLoss: 0.8373 
2022-05-02 14:14:16 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.0497, stu_CELoss: 1.1526 DKDLoss: 0.8971 
2022-05-02 14:14:48 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.1396, stu_CELoss: 1.2388 DKDLoss: 0.9008 
2022-05-02 14:15:20 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.1486, stu_CELoss: 1.2416 DKDLoss: 0.9071 
2022-05-02 14:15:51 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.8246, stu_CELoss: 1.1029 DKDLoss: 0.7216 
2022-05-02 14:16:23 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.1589, stu_CELoss: 1.2484 DKDLoss: 0.9106 
2022-05-02 14:16:55 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.9560, stu_CELoss: 1.1674 DKDLoss: 0.7886 
2022-05-02 14:17:27 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.2812, stu_CELoss: 1.4412 DKDLoss: 0.8401 
2022-05-02 14:17:59 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.9285, stu_CELoss: 1.1210 DKDLoss: 0.8074 
2022-05-02 14:18:31 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.2740, stu_CELoss: 1.3810 DKDLoss: 0.8930 
2022-05-02 14:19:03 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.1065, stu_CELoss: 1.2926 DKDLoss: 0.8139 
2022-05-02 14:19:35 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.1629, stu_CELoss: 1.3284 DKDLoss: 0.8345 
2022-05-02 14:20:06 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.9218, stu_CELoss: 1.1219 DKDLoss: 0.7998 
2022-05-02 14:20:38 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.1302, stu_CELoss: 1.3266 DKDLoss: 0.8036 
2022-05-02 14:21:10 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.1503, stu_CELoss: 1.3075 DKDLoss: 0.8428 
2022-05-02 14:21:42 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 2.1429, stu_CELoss: 1.2831 DKDLoss: 0.8598 
2022-05-02 14:22:14 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 2.0128, stu_CELoss: 1.1752 DKDLoss: 0.8376 
2022-05-02 14:22:45 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.1797, stu_CELoss: 1.3213 DKDLoss: 0.8584 
2022-05-02 14:23:18 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.0239, stu_CELoss: 1.2901 DKDLoss: 0.7337 
2022-05-02 14:23:49 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 2.0807, stu_CELoss: 1.2629 DKDLoss: 0.8178 
2022-05-02 14:24:21 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.7787, stu_CELoss: 1.0178 DKDLoss: 0.7608 
2022-05-02 14:24:53 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 2.1345, stu_CELoss: 1.3760 DKDLoss: 0.7585 
2022-05-02 14:25:25 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.8126, stu_CELoss: 1.0470 DKDLoss: 0.7655 
2022-05-02 14:25:57 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.9602, stu_CELoss: 1.0958 DKDLoss: 0.8644 
2022-05-02 14:26:28 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.1238, stu_CELoss: 1.2960 DKDLoss: 0.8278 
2022-05-02 14:27:00 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.2601, stu_CELoss: 1.4167 DKDLoss: 0.8435 
2022-05-02 14:27:32 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.9387, stu_CELoss: 1.0630 DKDLoss: 0.8758 
2022-05-02 14:28:04 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.0054, stu_CELoss: 1.2460 DKDLoss: 0.7595 
2022-05-02 14:28:36 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 2.1949, stu_CELoss: 1.4249 DKDLoss: 0.7700 
2022-05-02 14:28:37 - train: epoch 040, train_loss: 2.0161
2022-05-02 14:31:03 - eval: epoch: 040, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.904%, stu_acc5: 91.302%, stu_test_loss: 1.0768
2022-05-02 14:31:04 - until epoch: 040, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 14:31:04 - epoch 041 lr: 0.010000000000000002
2022-05-02 14:31:42 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.3458, stu_CELoss: 1.5248 DKDLoss: 0.8211 
2022-05-02 14:32:13 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.1432, stu_CELoss: 1.3259 DKDLoss: 0.8173 
2022-05-02 14:32:45 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.8755, stu_CELoss: 1.0750 DKDLoss: 0.8005 
2022-05-02 14:33:16 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.0489, stu_CELoss: 1.2087 DKDLoss: 0.8403 
2022-05-02 14:33:47 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.7892, stu_CELoss: 0.9986 DKDLoss: 0.7905 
2022-05-02 14:34:18 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.0550, stu_CELoss: 1.2506 DKDLoss: 0.8044 
2022-05-02 14:34:49 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 2.1510, stu_CELoss: 1.3105 DKDLoss: 0.8405 
2022-05-02 14:35:21 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.0013, stu_CELoss: 1.2943 DKDLoss: 0.7071 
2022-05-02 14:35:52 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.8010, stu_CELoss: 1.0772 DKDLoss: 0.7238 
2022-05-02 14:36:24 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.0307, stu_CELoss: 1.2747 DKDLoss: 0.7561 
2022-05-02 14:36:55 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 2.0142, stu_CELoss: 1.2587 DKDLoss: 0.7555 
2022-05-02 14:37:26 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.8092, stu_CELoss: 1.0710 DKDLoss: 0.7383 
2022-05-02 14:37:58 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.8269, stu_CELoss: 1.1154 DKDLoss: 0.7115 
2022-05-02 14:38:30 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.1292, stu_CELoss: 1.3951 DKDLoss: 0.7341 
2022-05-02 14:39:02 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.2850, stu_CELoss: 1.4074 DKDLoss: 0.8777 
2022-05-02 14:39:34 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.1023, stu_CELoss: 1.3507 DKDLoss: 0.7516 
2022-05-02 14:40:06 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.1184, stu_CELoss: 1.3108 DKDLoss: 0.8076 
2022-05-02 14:40:38 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.8227, stu_CELoss: 1.1134 DKDLoss: 0.7093 
2022-05-02 14:41:10 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.0436, stu_CELoss: 1.3084 DKDLoss: 0.7352 
2022-05-02 14:41:41 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.9649, stu_CELoss: 1.1096 DKDLoss: 0.8553 
2022-05-02 14:42:13 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.6924, stu_CELoss: 0.9735 DKDLoss: 0.7188 
2022-05-02 14:42:45 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.8166, stu_CELoss: 1.0990 DKDLoss: 0.7176 
2022-05-02 14:43:16 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.7458, stu_CELoss: 1.0861 DKDLoss: 0.6597 
2022-05-02 14:43:48 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.1884, stu_CELoss: 1.3199 DKDLoss: 0.8685 
2022-05-02 14:44:20 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 2.1281, stu_CELoss: 1.3185 DKDLoss: 0.8096 
2022-05-02 14:44:51 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.3726, stu_CELoss: 1.5010 DKDLoss: 0.8717 
2022-05-02 14:45:23 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.1194, stu_CELoss: 1.3477 DKDLoss: 0.7716 
2022-05-02 14:45:55 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.8332, stu_CELoss: 1.0840 DKDLoss: 0.7493 
2022-05-02 14:46:27 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 2.0599, stu_CELoss: 1.3012 DKDLoss: 0.7587 
2022-05-02 14:46:59 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.0749, stu_CELoss: 1.2677 DKDLoss: 0.8072 
2022-05-02 14:47:31 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.1057, stu_CELoss: 1.2214 DKDLoss: 0.8844 
2022-05-02 14:48:02 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 2.0820, stu_CELoss: 1.2926 DKDLoss: 0.7894 
2022-05-02 14:48:34 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.9227, stu_CELoss: 1.1601 DKDLoss: 0.7626 
2022-05-02 14:49:06 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.8168, stu_CELoss: 1.0578 DKDLoss: 0.7589 
2022-05-02 14:49:37 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.9663, stu_CELoss: 1.1747 DKDLoss: 0.7917 
2022-05-02 14:50:08 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.9910, stu_CELoss: 1.2598 DKDLoss: 0.7312 
2022-05-02 14:50:40 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.0987, stu_CELoss: 1.3449 DKDLoss: 0.7538 
2022-05-02 14:51:11 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.0583, stu_CELoss: 1.2622 DKDLoss: 0.7961 
2022-05-02 14:51:43 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.9315, stu_CELoss: 1.2100 DKDLoss: 0.7215 
2022-05-02 14:52:15 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.9726, stu_CELoss: 1.0945 DKDLoss: 0.8780 
2022-05-02 14:52:47 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.1820, stu_CELoss: 1.3392 DKDLoss: 0.8429 
2022-05-02 14:53:18 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.6796, stu_CELoss: 0.9739 DKDLoss: 0.7057 
2022-05-02 14:53:50 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.8685, stu_CELoss: 1.0567 DKDLoss: 0.8117 
2022-05-02 14:54:22 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.9888, stu_CELoss: 1.2385 DKDLoss: 0.7503 
2022-05-02 14:54:54 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.9576, stu_CELoss: 1.1886 DKDLoss: 0.7690 
2022-05-02 14:55:26 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.1794, stu_CELoss: 1.3002 DKDLoss: 0.8792 
2022-05-02 14:55:58 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.1368, stu_CELoss: 1.2699 DKDLoss: 0.8670 
2022-05-02 14:56:30 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.9532, stu_CELoss: 1.1452 DKDLoss: 0.8080 
2022-05-02 14:57:02 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.9513, stu_CELoss: 1.2300 DKDLoss: 0.7213 
2022-05-02 14:57:33 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.2279, stu_CELoss: 1.3405 DKDLoss: 0.8874 
2022-05-02 14:57:35 - train: epoch 041, train_loss: 2.0151
2022-05-02 15:00:00 - eval: epoch: 041, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.392%, stu_acc5: 91.296%, stu_test_loss: 1.0890
2022-05-02 15:00:01 - until epoch: 041, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 15:00:01 - epoch 042 lr: 0.010000000000000002
2022-05-02 15:00:38 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.7011, stu_CELoss: 0.9708 DKDLoss: 0.7304 
2022-05-02 15:01:09 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 2.0207, stu_CELoss: 1.1936 DKDLoss: 0.8271 
2022-05-02 15:01:40 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.1516, stu_CELoss: 1.3381 DKDLoss: 0.8135 
2022-05-02 15:02:11 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 2.0071, stu_CELoss: 1.2760 DKDLoss: 0.7311 
2022-05-02 15:02:43 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.8946, stu_CELoss: 1.1319 DKDLoss: 0.7627 
2022-05-02 15:03:15 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.9984, stu_CELoss: 1.2370 DKDLoss: 0.7614 
2022-05-02 15:03:47 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.1175, stu_CELoss: 1.3194 DKDLoss: 0.7981 
2022-05-02 15:04:19 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.9590, stu_CELoss: 1.2596 DKDLoss: 0.6994 
2022-05-02 15:04:50 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.0129, stu_CELoss: 1.1840 DKDLoss: 0.8289 
2022-05-02 15:05:22 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.2145, stu_CELoss: 1.3789 DKDLoss: 0.8355 
2022-05-02 15:05:53 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.7495, stu_CELoss: 0.9834 DKDLoss: 0.7660 
2022-05-02 15:06:25 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.7420, stu_CELoss: 1.0532 DKDLoss: 0.6888 
2022-05-02 15:06:56 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9334, stu_CELoss: 1.1654 DKDLoss: 0.7680 
2022-05-02 15:07:28 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.2857, stu_CELoss: 1.4164 DKDLoss: 0.8693 
2022-05-02 15:08:00 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.9526, stu_CELoss: 1.1873 DKDLoss: 0.7652 
2022-05-02 15:08:32 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.1214, stu_CELoss: 1.2861 DKDLoss: 0.8354 
2022-05-02 15:09:03 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.1089, stu_CELoss: 1.2177 DKDLoss: 0.8912 
2022-05-02 15:09:35 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.8744, stu_CELoss: 1.1230 DKDLoss: 0.7513 
2022-05-02 15:10:06 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.1873, stu_CELoss: 1.2720 DKDLoss: 0.9153 
2022-05-02 15:10:38 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.0458, stu_CELoss: 1.2087 DKDLoss: 0.8371 
2022-05-02 15:11:10 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.7181, stu_CELoss: 1.0089 DKDLoss: 0.7092 
2022-05-02 15:11:42 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.8108, stu_CELoss: 1.1009 DKDLoss: 0.7098 
2022-05-02 15:12:14 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 2.2223, stu_CELoss: 1.3076 DKDLoss: 0.9147 
2022-05-02 15:12:45 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.9839, stu_CELoss: 1.1235 DKDLoss: 0.8604 
2022-05-02 15:13:17 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.8729, stu_CELoss: 1.0947 DKDLoss: 0.7783 
2022-05-02 15:13:49 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.1065, stu_CELoss: 1.2449 DKDLoss: 0.8616 
2022-05-02 15:14:20 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.9805, stu_CELoss: 1.1505 DKDLoss: 0.8300 
2022-05-02 15:14:52 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 2.0351, stu_CELoss: 1.2072 DKDLoss: 0.8279 
2022-05-02 15:15:24 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 2.0573, stu_CELoss: 1.2628 DKDLoss: 0.7944 
2022-05-02 15:15:56 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 2.0048, stu_CELoss: 1.2109 DKDLoss: 0.7939 
2022-05-02 15:16:28 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.0226, stu_CELoss: 1.1861 DKDLoss: 0.8364 
2022-05-02 15:16:59 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.1646, stu_CELoss: 1.2628 DKDLoss: 0.9017 
2022-05-02 15:17:31 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.2676, stu_CELoss: 1.4916 DKDLoss: 0.7760 
2022-05-02 15:18:03 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.9178, stu_CELoss: 1.1554 DKDLoss: 0.7625 
2022-05-02 15:18:34 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.0716, stu_CELoss: 1.2780 DKDLoss: 0.7935 
2022-05-02 15:19:06 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.2050, stu_CELoss: 1.4074 DKDLoss: 0.7976 
2022-05-02 15:19:38 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.9002, stu_CELoss: 1.1328 DKDLoss: 0.7673 
2022-05-02 15:20:10 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.7707, stu_CELoss: 0.9732 DKDLoss: 0.7975 
2022-05-02 15:20:41 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.0891, stu_CELoss: 1.2542 DKDLoss: 0.8349 
2022-05-02 15:21:13 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.9054, stu_CELoss: 1.1791 DKDLoss: 0.7263 
2022-05-02 15:21:44 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.1251, stu_CELoss: 1.2308 DKDLoss: 0.8942 
2022-05-02 15:22:16 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.3539, stu_CELoss: 1.4771 DKDLoss: 0.8769 
2022-05-02 15:22:48 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7775, stu_CELoss: 1.0606 DKDLoss: 0.7169 
2022-05-02 15:23:20 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.8647, stu_CELoss: 1.1796 DKDLoss: 0.6850 
2022-05-02 15:23:52 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.9433, stu_CELoss: 1.2025 DKDLoss: 0.7408 
2022-05-02 15:24:23 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.0667, stu_CELoss: 1.2402 DKDLoss: 0.8265 
2022-05-02 15:24:55 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.9719, stu_CELoss: 1.2429 DKDLoss: 0.7291 
2022-05-02 15:25:27 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.0406, stu_CELoss: 1.2652 DKDLoss: 0.7754 
2022-05-02 15:25:59 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.8727, stu_CELoss: 1.0910 DKDLoss: 0.7816 
2022-05-02 15:26:31 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.0130, stu_CELoss: 1.2572 DKDLoss: 0.7558 
2022-05-02 15:26:33 - train: epoch 042, train_loss: 2.0129
2022-05-02 15:28:58 - eval: epoch: 042, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.506%, stu_acc5: 91.224%, stu_test_loss: 1.0891
2022-05-02 15:28:58 - until epoch: 042, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 15:28:58 - epoch 043 lr: 0.010000000000000002
2022-05-02 15:29:36 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.1600, stu_CELoss: 1.4090 DKDLoss: 0.7510 
2022-05-02 15:30:08 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.7716, stu_CELoss: 1.0837 DKDLoss: 0.6879 
2022-05-02 15:30:39 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.6605, stu_CELoss: 0.9396 DKDLoss: 0.7210 
2022-05-02 15:31:11 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.6939, stu_CELoss: 0.9242 DKDLoss: 0.7698 
2022-05-02 15:31:42 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.9891, stu_CELoss: 1.2177 DKDLoss: 0.7714 
2022-05-02 15:32:14 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.9698, stu_CELoss: 1.2398 DKDLoss: 0.7299 
2022-05-02 15:32:45 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 2.0807, stu_CELoss: 1.2950 DKDLoss: 0.7857 
2022-05-02 15:33:17 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.9841, stu_CELoss: 1.2537 DKDLoss: 0.7304 
2022-05-02 15:33:48 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 2.0865, stu_CELoss: 1.3315 DKDLoss: 0.7550 
2022-05-02 15:34:20 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.1635, stu_CELoss: 1.3369 DKDLoss: 0.8266 
2022-05-02 15:34:51 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.9688, stu_CELoss: 1.1934 DKDLoss: 0.7754 
2022-05-02 15:35:24 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.6665, stu_CELoss: 1.0054 DKDLoss: 0.6611 
2022-05-02 15:35:56 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.1781, stu_CELoss: 1.4000 DKDLoss: 0.7781 
2022-05-02 15:36:27 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 2.0898, stu_CELoss: 1.3065 DKDLoss: 0.7833 
2022-05-02 15:36:58 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.7727, stu_CELoss: 0.9857 DKDLoss: 0.7870 
2022-05-02 15:37:30 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.9094, stu_CELoss: 1.1351 DKDLoss: 0.7743 
2022-05-02 15:38:03 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.2011, stu_CELoss: 1.3945 DKDLoss: 0.8066 
2022-05-02 15:38:34 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.2219, stu_CELoss: 1.3516 DKDLoss: 0.8703 
2022-05-02 15:39:06 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 2.0199, stu_CELoss: 1.2974 DKDLoss: 0.7225 
2022-05-02 15:39:37 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.9113, stu_CELoss: 1.1370 DKDLoss: 0.7742 
2022-05-02 15:40:09 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.7572, stu_CELoss: 1.0565 DKDLoss: 0.7007 
2022-05-02 15:40:40 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.1352, stu_CELoss: 1.2332 DKDLoss: 0.9020 
2022-05-02 15:41:11 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.1676, stu_CELoss: 1.3427 DKDLoss: 0.8250 
2022-05-02 15:41:43 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.7721, stu_CELoss: 1.0252 DKDLoss: 0.7469 
2022-05-02 15:42:15 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.3430, stu_CELoss: 1.5084 DKDLoss: 0.8346 
2022-05-02 15:42:46 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.8289, stu_CELoss: 1.0527 DKDLoss: 0.7761 
2022-05-02 15:43:18 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.9947, stu_CELoss: 1.2125 DKDLoss: 0.7823 
2022-05-02 15:43:49 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.8515, stu_CELoss: 1.1342 DKDLoss: 0.7174 
2022-05-02 15:44:21 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 2.0034, stu_CELoss: 1.0774 DKDLoss: 0.9260 
2022-05-02 15:44:53 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.2130, stu_CELoss: 1.4367 DKDLoss: 0.7764 
2022-05-02 15:45:25 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.2573, stu_CELoss: 1.3485 DKDLoss: 0.9088 
2022-05-02 15:45:57 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.9906, stu_CELoss: 1.1517 DKDLoss: 0.8388 
2022-05-02 15:46:29 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.9731, stu_CELoss: 1.1724 DKDLoss: 0.8007 
2022-05-02 15:47:01 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.9095, stu_CELoss: 1.1097 DKDLoss: 0.7998 
2022-05-02 15:47:33 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.1902, stu_CELoss: 1.3501 DKDLoss: 0.8401 
2022-05-02 15:48:04 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.2229, stu_CELoss: 1.3468 DKDLoss: 0.8761 
2022-05-02 15:48:36 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 2.0890, stu_CELoss: 1.2993 DKDLoss: 0.7898 
2022-05-02 15:49:08 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.2099, stu_CELoss: 1.3571 DKDLoss: 0.8528 
2022-05-02 15:49:40 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.9481, stu_CELoss: 1.1060 DKDLoss: 0.8421 
2022-05-02 15:50:11 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.3173, stu_CELoss: 1.3772 DKDLoss: 0.9401 
2022-05-02 15:50:43 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.0999, stu_CELoss: 1.2925 DKDLoss: 0.8073 
2022-05-02 15:51:14 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.1393, stu_CELoss: 1.2863 DKDLoss: 0.8530 
2022-05-02 15:51:46 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.0955, stu_CELoss: 1.2700 DKDLoss: 0.8255 
2022-05-02 15:52:18 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.0881, stu_CELoss: 1.2209 DKDLoss: 0.8672 
2022-05-02 15:52:49 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.9094, stu_CELoss: 1.0809 DKDLoss: 0.8285 
2022-05-02 15:53:20 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.1608, stu_CELoss: 1.3027 DKDLoss: 0.8582 
2022-05-02 15:53:52 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.0798, stu_CELoss: 1.2622 DKDLoss: 0.8177 
2022-05-02 15:54:24 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.3821, stu_CELoss: 1.4221 DKDLoss: 0.9600 
2022-05-02 15:54:56 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.9574, stu_CELoss: 1.1904 DKDLoss: 0.7670 
2022-05-02 15:55:28 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.9368, stu_CELoss: 1.1192 DKDLoss: 0.8176 
2022-05-02 15:55:29 - train: epoch 043, train_loss: 2.0181
2022-05-02 15:57:55 - eval: epoch: 043, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.088%, stu_acc5: 90.884%, stu_test_loss: 1.1134
2022-05-02 15:57:55 - until epoch: 043, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 15:57:55 - epoch 044 lr: 0.010000000000000002
2022-05-02 15:58:33 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.0164, stu_CELoss: 1.1284 DKDLoss: 0.8879 
2022-05-02 15:59:05 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.0535, stu_CELoss: 1.2284 DKDLoss: 0.8251 
2022-05-02 15:59:36 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.9447, stu_CELoss: 1.1723 DKDLoss: 0.7724 
2022-05-02 16:00:07 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.6657, stu_CELoss: 0.9825 DKDLoss: 0.6833 
2022-05-02 16:00:39 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.1263, stu_CELoss: 1.2501 DKDLoss: 0.8762 
2022-05-02 16:01:10 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.9767, stu_CELoss: 1.2270 DKDLoss: 0.7496 
2022-05-02 16:01:42 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.8621, stu_CELoss: 1.1609 DKDLoss: 0.7012 
2022-05-02 16:02:13 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.9254, stu_CELoss: 1.2028 DKDLoss: 0.7226 
2022-05-02 16:02:44 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.9473, stu_CELoss: 1.2250 DKDLoss: 0.7223 
2022-05-02 16:03:15 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 2.0557, stu_CELoss: 1.2598 DKDLoss: 0.7959 
2022-05-02 16:03:47 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.8465, stu_CELoss: 1.1387 DKDLoss: 0.7078 
2022-05-02 16:04:18 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 2.0087, stu_CELoss: 1.2005 DKDLoss: 0.8082 
2022-05-02 16:04:50 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.2811, stu_CELoss: 1.4530 DKDLoss: 0.8281 
2022-05-02 16:05:22 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.9761, stu_CELoss: 1.1866 DKDLoss: 0.7894 
2022-05-02 16:05:53 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.2678, stu_CELoss: 1.3506 DKDLoss: 0.9172 
2022-05-02 16:06:24 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 2.0040, stu_CELoss: 1.1770 DKDLoss: 0.8269 
2022-05-02 16:06:55 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.9769, stu_CELoss: 1.1266 DKDLoss: 0.8503 
2022-05-02 16:07:27 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.9909, stu_CELoss: 1.2513 DKDLoss: 0.7396 
2022-05-02 16:07:58 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.2633, stu_CELoss: 1.3755 DKDLoss: 0.8878 
2022-05-02 16:08:29 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.7498, stu_CELoss: 1.0563 DKDLoss: 0.6934 
2022-05-02 16:09:00 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.0741, stu_CELoss: 1.3139 DKDLoss: 0.7602 
2022-05-02 16:09:32 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.9118, stu_CELoss: 1.1416 DKDLoss: 0.7702 
2022-05-02 16:10:03 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.2416, stu_CELoss: 1.4630 DKDLoss: 0.7786 
2022-05-02 16:10:35 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.9143, stu_CELoss: 1.1607 DKDLoss: 0.7536 
2022-05-02 16:11:06 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.2025, stu_CELoss: 1.3060 DKDLoss: 0.8965 
2022-05-02 16:11:38 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.1284, stu_CELoss: 1.3434 DKDLoss: 0.7849 
2022-05-02 16:12:10 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 2.1733, stu_CELoss: 1.3486 DKDLoss: 0.8247 
2022-05-02 16:12:41 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 2.0135, stu_CELoss: 1.1990 DKDLoss: 0.8145 
2022-05-02 16:13:13 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.6876, stu_CELoss: 0.9808 DKDLoss: 0.7068 
2022-05-02 16:13:44 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.9464, stu_CELoss: 1.1991 DKDLoss: 0.7473 
2022-05-02 16:14:15 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.0751, stu_CELoss: 1.2930 DKDLoss: 0.7821 
2022-05-02 16:14:45 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.7863, stu_CELoss: 1.0981 DKDLoss: 0.6882 
2022-05-02 16:15:16 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.9224, stu_CELoss: 1.1994 DKDLoss: 0.7230 
2022-05-02 16:15:47 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.1405, stu_CELoss: 1.3118 DKDLoss: 0.8287 
2022-05-02 16:16:19 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.9946, stu_CELoss: 1.2271 DKDLoss: 0.7675 
2022-05-02 16:16:50 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.2230, stu_CELoss: 1.3780 DKDLoss: 0.8451 
2022-05-02 16:17:21 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.8673, stu_CELoss: 1.1080 DKDLoss: 0.7593 
2022-05-02 16:17:53 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.7581, stu_CELoss: 1.0737 DKDLoss: 0.6845 
2022-05-02 16:18:24 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.2369, stu_CELoss: 1.4630 DKDLoss: 0.7739 
2022-05-02 16:18:55 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.0638, stu_CELoss: 1.1721 DKDLoss: 0.8917 
2022-05-02 16:19:27 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.8985, stu_CELoss: 1.0889 DKDLoss: 0.8097 
2022-05-02 16:19:59 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.0554, stu_CELoss: 1.2476 DKDLoss: 0.8078 
2022-05-02 16:20:31 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.2000, stu_CELoss: 1.3373 DKDLoss: 0.8628 
2022-05-02 16:21:02 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.0407, stu_CELoss: 1.2909 DKDLoss: 0.7498 
2022-05-02 16:21:33 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.1065, stu_CELoss: 1.1889 DKDLoss: 0.9176 
2022-05-02 16:22:05 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.9312, stu_CELoss: 1.1143 DKDLoss: 0.8169 
2022-05-02 16:22:36 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.1794, stu_CELoss: 1.2893 DKDLoss: 0.8901 
2022-05-02 16:23:08 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.9859, stu_CELoss: 1.1158 DKDLoss: 0.8700 
2022-05-02 16:23:40 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.8821, stu_CELoss: 1.1547 DKDLoss: 0.7274 
2022-05-02 16:24:11 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.0435, stu_CELoss: 1.2215 DKDLoss: 0.8220 
2022-05-02 16:24:13 - train: epoch 044, train_loss: 2.0210
2022-05-02 16:26:37 - eval: epoch: 044, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.306%, stu_acc5: 91.198%, stu_test_loss: 1.0964
2022-05-02 16:26:37 - until epoch: 044, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 16:26:37 - epoch 045 lr: 0.010000000000000002
2022-05-02 16:27:16 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.6862, stu_CELoss: 0.9818 DKDLoss: 0.7044 
2022-05-02 16:27:47 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 2.0298, stu_CELoss: 1.2245 DKDLoss: 0.8053 
2022-05-02 16:28:18 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.9611, stu_CELoss: 1.1877 DKDLoss: 0.7734 
2022-05-02 16:28:49 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.9451, stu_CELoss: 1.1452 DKDLoss: 0.7999 
2022-05-02 16:29:20 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.2619, stu_CELoss: 1.4335 DKDLoss: 0.8284 
2022-05-02 16:29:52 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.1854, stu_CELoss: 1.3864 DKDLoss: 0.7990 
2022-05-02 16:30:23 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.6821, stu_CELoss: 0.9377 DKDLoss: 0.7444 
2022-05-02 16:30:55 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.8702, stu_CELoss: 1.1364 DKDLoss: 0.7338 
2022-05-02 16:31:25 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.9162, stu_CELoss: 1.1396 DKDLoss: 0.7766 
2022-05-02 16:31:57 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 2.1043, stu_CELoss: 1.2015 DKDLoss: 0.9028 
2022-05-02 16:32:27 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.1677, stu_CELoss: 1.3354 DKDLoss: 0.8322 
2022-05-02 16:32:58 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.0440, stu_CELoss: 1.2287 DKDLoss: 0.8153 
2022-05-02 16:33:29 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 2.0778, stu_CELoss: 1.1880 DKDLoss: 0.8899 
2022-05-02 16:34:01 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.0403, stu_CELoss: 1.2059 DKDLoss: 0.8344 
2022-05-02 16:34:31 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 2.1416, stu_CELoss: 1.3425 DKDLoss: 0.7990 
2022-05-02 16:35:02 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.8375, stu_CELoss: 1.0451 DKDLoss: 0.7923 
2022-05-02 16:35:33 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.8720, stu_CELoss: 1.1611 DKDLoss: 0.7109 
2022-05-02 16:36:04 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.9341, stu_CELoss: 1.1329 DKDLoss: 0.8012 
2022-05-02 16:36:35 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.0789, stu_CELoss: 1.2823 DKDLoss: 0.7966 
2022-05-02 16:37:06 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.0564, stu_CELoss: 1.2655 DKDLoss: 0.7909 
2022-05-02 16:37:37 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.1378, stu_CELoss: 1.3082 DKDLoss: 0.8296 
2022-05-02 16:38:08 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.1078, stu_CELoss: 1.2660 DKDLoss: 0.8419 
2022-05-02 16:38:39 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.9618, stu_CELoss: 1.1698 DKDLoss: 0.7920 
2022-05-02 16:39:10 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.9298, stu_CELoss: 1.1755 DKDLoss: 0.7542 
2022-05-02 16:39:41 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 2.0745, stu_CELoss: 1.2686 DKDLoss: 0.8059 
2022-05-02 16:40:13 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.7944, stu_CELoss: 1.0378 DKDLoss: 0.7567 
2022-05-02 16:40:43 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.8595, stu_CELoss: 1.0644 DKDLoss: 0.7951 
2022-05-02 16:41:15 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.9616, stu_CELoss: 1.1174 DKDLoss: 0.8442 
2022-05-02 16:41:45 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.0386, stu_CELoss: 1.2511 DKDLoss: 0.7875 
2022-05-02 16:42:17 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.2153, stu_CELoss: 1.2965 DKDLoss: 0.9188 
2022-05-02 16:42:49 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.1462, stu_CELoss: 1.2732 DKDLoss: 0.8731 
2022-05-02 16:43:19 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.0913, stu_CELoss: 1.2669 DKDLoss: 0.8244 
2022-05-02 16:43:51 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.9414, stu_CELoss: 1.1579 DKDLoss: 0.7835 
2022-05-02 16:44:22 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.2076, stu_CELoss: 1.3887 DKDLoss: 0.8189 
2022-05-02 16:44:53 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.0427, stu_CELoss: 1.2279 DKDLoss: 0.8148 
2022-05-02 16:45:25 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.0911, stu_CELoss: 1.2223 DKDLoss: 0.8687 
2022-05-02 16:45:56 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 2.0787, stu_CELoss: 1.2217 DKDLoss: 0.8570 
2022-05-02 16:46:27 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.1018, stu_CELoss: 1.2071 DKDLoss: 0.8946 
2022-05-02 16:46:58 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.1006, stu_CELoss: 1.2867 DKDLoss: 0.8139 
2022-05-02 16:47:29 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.0754, stu_CELoss: 1.2425 DKDLoss: 0.8329 
2022-05-02 16:48:01 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.8953, stu_CELoss: 1.1514 DKDLoss: 0.7439 
2022-05-02 16:48:32 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.1734, stu_CELoss: 1.3596 DKDLoss: 0.8138 
2022-05-02 16:49:02 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.1752, stu_CELoss: 1.3283 DKDLoss: 0.8469 
2022-05-02 16:49:33 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.9474, stu_CELoss: 1.1520 DKDLoss: 0.7955 
2022-05-02 16:50:04 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.9447, stu_CELoss: 1.1096 DKDLoss: 0.8351 
2022-05-02 16:50:36 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.4120, stu_CELoss: 1.4546 DKDLoss: 0.9573 
2022-05-02 16:51:07 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 2.1265, stu_CELoss: 1.3484 DKDLoss: 0.7780 
2022-05-02 16:51:38 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.9417, stu_CELoss: 1.1122 DKDLoss: 0.8295 
2022-05-02 16:52:09 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.1500, stu_CELoss: 1.2748 DKDLoss: 0.8751 
2022-05-02 16:52:40 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 2.0014, stu_CELoss: 1.2254 DKDLoss: 0.7760 
2022-05-02 16:52:42 - train: epoch 045, train_loss: 2.0247
2022-05-02 16:55:07 - eval: epoch: 045, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.918%, stu_acc5: 90.924%, stu_test_loss: 1.1155
2022-05-02 16:55:08 - until epoch: 045, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 16:55:08 - epoch 046 lr: 0.010000000000000002
2022-05-02 16:55:46 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.8928, stu_CELoss: 1.1153 DKDLoss: 0.7775 
2022-05-02 16:56:18 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.7239, stu_CELoss: 1.0049 DKDLoss: 0.7190 
2022-05-02 16:56:49 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 2.0507, stu_CELoss: 1.2550 DKDLoss: 0.7957 
2022-05-02 16:57:20 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.9580, stu_CELoss: 1.1407 DKDLoss: 0.8174 
2022-05-02 16:57:51 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.8500, stu_CELoss: 1.1445 DKDLoss: 0.7055 
2022-05-02 16:58:23 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.9418, stu_CELoss: 1.1993 DKDLoss: 0.7425 
2022-05-02 16:58:54 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.7723, stu_CELoss: 1.0473 DKDLoss: 0.7250 
2022-05-02 16:59:26 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.1710, stu_CELoss: 1.2433 DKDLoss: 0.9277 
2022-05-02 16:59:57 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 2.1140, stu_CELoss: 1.2690 DKDLoss: 0.8450 
2022-05-02 17:00:29 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6662, stu_CELoss: 0.9307 DKDLoss: 0.7354 
2022-05-02 17:01:00 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8960, stu_CELoss: 1.1221 DKDLoss: 0.7740 
2022-05-02 17:01:31 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.7796, stu_CELoss: 1.0308 DKDLoss: 0.7488 
2022-05-02 17:02:03 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 2.0188, stu_CELoss: 1.1647 DKDLoss: 0.8542 
2022-05-02 17:02:34 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.0502, stu_CELoss: 1.2972 DKDLoss: 0.7530 
2022-05-02 17:03:06 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 2.0761, stu_CELoss: 1.2093 DKDLoss: 0.8668 
2022-05-02 17:03:38 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.2499, stu_CELoss: 1.3757 DKDLoss: 0.8742 
2022-05-02 17:04:11 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.9926, stu_CELoss: 1.1569 DKDLoss: 0.8357 
2022-05-02 17:04:43 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.1791, stu_CELoss: 1.3516 DKDLoss: 0.8275 
2022-05-02 17:05:15 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.9459, stu_CELoss: 1.1470 DKDLoss: 0.7990 
2022-05-02 17:05:47 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.9247, stu_CELoss: 1.1258 DKDLoss: 0.7989 
2022-05-02 17:06:18 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.2099, stu_CELoss: 1.3836 DKDLoss: 0.8263 
2022-05-02 17:06:50 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.9528, stu_CELoss: 1.1291 DKDLoss: 0.8237 
2022-05-02 17:07:22 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.0988, stu_CELoss: 1.2598 DKDLoss: 0.8390 
2022-05-02 17:07:54 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.9509, stu_CELoss: 1.2611 DKDLoss: 0.6898 
2022-05-02 17:08:26 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.8837, stu_CELoss: 1.1728 DKDLoss: 0.7109 
2022-05-02 17:08:58 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.1565, stu_CELoss: 1.3225 DKDLoss: 0.8340 
2022-05-02 17:09:30 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 2.0812, stu_CELoss: 1.3039 DKDLoss: 0.7772 
2022-05-02 17:10:01 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 2.1217, stu_CELoss: 1.2309 DKDLoss: 0.8908 
2022-05-02 17:10:33 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.0737, stu_CELoss: 1.3231 DKDLoss: 0.7506 
2022-05-02 17:11:05 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.7571, stu_CELoss: 0.9871 DKDLoss: 0.7700 
2022-05-02 17:11:36 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.7497, stu_CELoss: 1.0298 DKDLoss: 0.7200 
2022-05-02 17:12:08 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.0397, stu_CELoss: 1.1991 DKDLoss: 0.8406 
2022-05-02 17:12:39 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.3490, stu_CELoss: 1.4204 DKDLoss: 0.9286 
2022-05-02 17:13:11 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.8304, stu_CELoss: 1.1175 DKDLoss: 0.7129 
2022-05-02 17:13:43 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.1998, stu_CELoss: 1.2722 DKDLoss: 0.9276 
2022-05-02 17:14:15 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.8034, stu_CELoss: 1.0303 DKDLoss: 0.7731 
2022-05-02 17:14:46 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.9608, stu_CELoss: 1.1607 DKDLoss: 0.8000 
2022-05-02 17:15:18 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.9378, stu_CELoss: 1.1309 DKDLoss: 0.8069 
2022-05-02 17:15:50 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.9440, stu_CELoss: 1.2261 DKDLoss: 0.7179 
2022-05-02 17:16:22 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.7064, stu_CELoss: 0.9554 DKDLoss: 0.7510 
2022-05-02 17:16:53 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.9917, stu_CELoss: 1.2706 DKDLoss: 0.7211 
2022-05-02 17:17:24 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.7646, stu_CELoss: 1.0378 DKDLoss: 0.7268 
2022-05-02 17:17:56 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.0979, stu_CELoss: 1.3122 DKDLoss: 0.7857 
2022-05-02 17:18:28 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.2513, stu_CELoss: 1.3361 DKDLoss: 0.9151 
2022-05-02 17:19:00 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 2.0029, stu_CELoss: 1.1671 DKDLoss: 0.8357 
2022-05-02 17:19:31 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9437, stu_CELoss: 1.2255 DKDLoss: 0.7182 
2022-05-02 17:20:03 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.0968, stu_CELoss: 1.1863 DKDLoss: 0.9105 
2022-05-02 17:20:35 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.8352, stu_CELoss: 1.1166 DKDLoss: 0.7186 
2022-05-02 17:21:07 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.1727, stu_CELoss: 1.4021 DKDLoss: 0.7705 
2022-05-02 17:21:38 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.7214, stu_CELoss: 0.9810 DKDLoss: 0.7405 
2022-05-02 17:21:40 - train: epoch 046, train_loss: 2.0241
2022-05-02 17:24:05 - eval: epoch: 046, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.444%, stu_acc5: 91.226%, stu_test_loss: 1.0958
2022-05-02 17:24:06 - until epoch: 046, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 17:24:06 - epoch 047 lr: 0.010000000000000002
2022-05-02 17:24:44 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.9051, stu_CELoss: 1.1130 DKDLoss: 0.7921 
2022-05-02 17:25:16 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.1095, stu_CELoss: 1.2821 DKDLoss: 0.8274 
2022-05-02 17:25:49 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.7331, stu_CELoss: 0.9411 DKDLoss: 0.7920 
2022-05-02 17:26:22 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.6846, stu_CELoss: 0.9338 DKDLoss: 0.7508 
2022-05-02 17:26:54 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 2.0293, stu_CELoss: 1.3042 DKDLoss: 0.7251 
2022-05-02 17:27:25 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.1067, stu_CELoss: 1.2564 DKDLoss: 0.8503 
2022-05-02 17:27:57 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.9488, stu_CELoss: 1.1877 DKDLoss: 0.7611 
2022-05-02 17:28:29 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.9691, stu_CELoss: 1.2152 DKDLoss: 0.7539 
2022-05-02 17:29:01 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.0353, stu_CELoss: 1.2404 DKDLoss: 0.7949 
2022-05-02 17:29:33 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 2.0284, stu_CELoss: 1.2396 DKDLoss: 0.7888 
2022-05-02 17:30:04 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.3553, stu_CELoss: 1.5276 DKDLoss: 0.8277 
2022-05-02 17:30:35 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.8857, stu_CELoss: 1.1624 DKDLoss: 0.7233 
2022-05-02 17:31:07 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.7854, stu_CELoss: 1.0665 DKDLoss: 0.7188 
2022-05-02 17:31:39 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.1820, stu_CELoss: 1.2895 DKDLoss: 0.8925 
2022-05-02 17:32:10 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 2.1343, stu_CELoss: 1.2264 DKDLoss: 0.9079 
2022-05-02 17:32:41 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.9782, stu_CELoss: 1.2545 DKDLoss: 0.7237 
2022-05-02 17:33:13 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 2.0052, stu_CELoss: 1.0489 DKDLoss: 0.9563 
2022-05-02 17:33:44 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.2817, stu_CELoss: 1.4370 DKDLoss: 0.8447 
2022-05-02 17:34:16 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.8232, stu_CELoss: 1.0636 DKDLoss: 0.7595 
2022-05-02 17:34:48 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.0410, stu_CELoss: 1.2339 DKDLoss: 0.8071 
2022-05-02 17:35:19 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.1504, stu_CELoss: 1.2728 DKDLoss: 0.8775 
2022-05-02 17:35:50 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.2462, stu_CELoss: 1.4491 DKDLoss: 0.7971 
2022-05-02 17:36:22 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.9292, stu_CELoss: 1.1186 DKDLoss: 0.8106 
2022-05-02 17:36:54 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.7999, stu_CELoss: 0.9769 DKDLoss: 0.8230 
2022-05-02 17:37:25 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 2.2766, stu_CELoss: 1.3881 DKDLoss: 0.8885 
2022-05-02 17:37:57 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.3770, stu_CELoss: 1.5079 DKDLoss: 0.8690 
2022-05-02 17:38:28 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.8312, stu_CELoss: 1.1231 DKDLoss: 0.7081 
2022-05-02 17:38:59 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.9592, stu_CELoss: 1.2141 DKDLoss: 0.7451 
2022-05-02 17:39:31 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.9926, stu_CELoss: 1.2773 DKDLoss: 0.7153 
2022-05-02 17:40:03 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.1967, stu_CELoss: 1.3295 DKDLoss: 0.8672 
2022-05-02 17:40:34 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 2.0138, stu_CELoss: 1.2363 DKDLoss: 0.7775 
2022-05-02 17:41:06 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.1652, stu_CELoss: 1.3576 DKDLoss: 0.8076 
2022-05-02 17:41:37 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.9901, stu_CELoss: 1.2091 DKDLoss: 0.7810 
2022-05-02 17:42:09 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.8781, stu_CELoss: 1.1410 DKDLoss: 0.7370 
2022-05-02 17:42:41 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.0354, stu_CELoss: 1.2467 DKDLoss: 0.7886 
2022-05-02 17:43:12 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.9787, stu_CELoss: 1.1773 DKDLoss: 0.8014 
2022-05-02 17:43:44 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.1676, stu_CELoss: 1.3129 DKDLoss: 0.8547 
2022-05-02 17:44:15 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.9641, stu_CELoss: 1.1986 DKDLoss: 0.7655 
2022-05-02 17:44:46 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 2.0566, stu_CELoss: 1.2410 DKDLoss: 0.8156 
2022-05-02 17:45:18 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.0722, stu_CELoss: 1.2091 DKDLoss: 0.8631 
2022-05-02 17:45:50 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.1949, stu_CELoss: 1.4058 DKDLoss: 0.7891 
2022-05-02 17:46:21 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 2.0537, stu_CELoss: 1.2483 DKDLoss: 0.8053 
2022-05-02 17:46:52 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.7956, stu_CELoss: 1.0010 DKDLoss: 0.7946 
2022-05-02 17:47:24 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.8321, stu_CELoss: 1.1466 DKDLoss: 0.6854 
2022-05-02 17:47:56 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.9841, stu_CELoss: 1.1813 DKDLoss: 0.8028 
2022-05-02 17:48:27 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 2.3485, stu_CELoss: 1.4296 DKDLoss: 0.9189 
2022-05-02 17:48:58 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.3368, stu_CELoss: 1.4664 DKDLoss: 0.8705 
2022-05-02 17:49:30 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.7681, stu_CELoss: 0.9535 DKDLoss: 0.8147 
2022-05-02 17:50:01 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.9090, stu_CELoss: 1.0941 DKDLoss: 0.8149 
2022-05-02 17:50:32 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.9713, stu_CELoss: 1.2119 DKDLoss: 0.7594 
2022-05-02 17:50:34 - train: epoch 047, train_loss: 2.0245
2022-05-02 17:52:59 - eval: epoch: 047, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.882%, stu_acc5: 90.816%, stu_test_loss: 1.1161
2022-05-02 17:53:00 - until epoch: 047, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 17:53:00 - epoch 048 lr: 0.010000000000000002
2022-05-02 17:53:38 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9581, stu_CELoss: 1.2126 DKDLoss: 0.7455 
2022-05-02 17:54:10 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.4032, stu_CELoss: 1.5567 DKDLoss: 0.8465 
2022-05-02 17:54:41 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 2.0631, stu_CELoss: 1.1768 DKDLoss: 0.8863 
2022-05-02 17:55:12 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 2.0118, stu_CELoss: 1.2214 DKDLoss: 0.7904 
2022-05-02 17:55:43 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.8637, stu_CELoss: 1.1078 DKDLoss: 0.7560 
2022-05-02 17:56:15 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.1205, stu_CELoss: 1.3295 DKDLoss: 0.7911 
2022-05-02 17:56:47 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.7998, stu_CELoss: 1.0877 DKDLoss: 0.7121 
2022-05-02 17:57:18 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.0547, stu_CELoss: 1.2403 DKDLoss: 0.8144 
2022-05-02 17:57:50 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.0661, stu_CELoss: 1.2701 DKDLoss: 0.7959 
2022-05-02 17:58:21 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.9259, stu_CELoss: 1.1445 DKDLoss: 0.7814 
2022-05-02 17:58:53 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.2242, stu_CELoss: 1.3796 DKDLoss: 0.8446 
2022-05-02 17:59:25 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.0766, stu_CELoss: 1.3082 DKDLoss: 0.7684 
2022-05-02 17:59:56 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.9719, stu_CELoss: 1.1507 DKDLoss: 0.8212 
2022-05-02 18:00:28 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.8408, stu_CELoss: 1.0699 DKDLoss: 0.7708 
2022-05-02 18:01:00 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.0766, stu_CELoss: 1.2759 DKDLoss: 0.8007 
2022-05-02 18:01:31 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.6805, stu_CELoss: 0.9956 DKDLoss: 0.6850 
2022-05-02 18:02:03 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.9344, stu_CELoss: 1.0891 DKDLoss: 0.8454 
2022-05-02 18:02:34 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 2.1807, stu_CELoss: 1.2803 DKDLoss: 0.9005 
2022-05-02 18:03:06 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.1215, stu_CELoss: 1.3111 DKDLoss: 0.8104 
2022-05-02 18:03:37 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.2997, stu_CELoss: 1.3812 DKDLoss: 0.9185 
2022-05-02 18:04:09 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.9050, stu_CELoss: 1.2210 DKDLoss: 0.6840 
2022-05-02 18:04:41 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.4975, stu_CELoss: 1.5635 DKDLoss: 0.9339 
2022-05-02 18:05:12 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.8960, stu_CELoss: 1.1364 DKDLoss: 0.7596 
2022-05-02 18:05:44 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.3045, stu_CELoss: 1.4554 DKDLoss: 0.8491 
2022-05-02 18:06:15 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.2501, stu_CELoss: 1.3573 DKDLoss: 0.8929 
2022-05-02 18:06:47 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.1597, stu_CELoss: 1.3124 DKDLoss: 0.8474 
2022-05-02 18:07:18 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.2527, stu_CELoss: 1.3770 DKDLoss: 0.8756 
2022-05-02 18:07:50 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.9034, stu_CELoss: 1.0786 DKDLoss: 0.8248 
2022-05-02 18:08:21 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.1537, stu_CELoss: 1.2885 DKDLoss: 0.8652 
2022-05-02 18:08:53 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.9200, stu_CELoss: 1.1603 DKDLoss: 0.7597 
2022-05-02 18:09:24 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 2.2221, stu_CELoss: 1.3791 DKDLoss: 0.8429 
2022-05-02 18:09:56 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.8910, stu_CELoss: 1.1373 DKDLoss: 0.7536 
2022-05-02 18:10:27 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.2249, stu_CELoss: 1.3820 DKDLoss: 0.8429 
2022-05-02 18:10:58 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 2.0586, stu_CELoss: 1.1915 DKDLoss: 0.8671 
2022-05-02 18:11:30 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0292, stu_CELoss: 1.3586 DKDLoss: 0.6705 
2022-05-02 18:12:02 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.0395, stu_CELoss: 1.2650 DKDLoss: 0.7744 
2022-05-02 18:12:33 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.1347, stu_CELoss: 1.3275 DKDLoss: 0.8072 
2022-05-02 18:13:05 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.0379, stu_CELoss: 1.2807 DKDLoss: 0.7572 
2022-05-02 18:13:36 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.3613, stu_CELoss: 1.5315 DKDLoss: 0.8298 
2022-05-02 18:14:08 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.8032, stu_CELoss: 0.9908 DKDLoss: 0.8124 
2022-05-02 18:14:39 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.1849, stu_CELoss: 1.3048 DKDLoss: 0.8802 
2022-05-02 18:15:11 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.8395, stu_CELoss: 1.1019 DKDLoss: 0.7376 
2022-05-02 18:15:43 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 2.0413, stu_CELoss: 1.1719 DKDLoss: 0.8693 
2022-05-02 18:16:15 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.9463, stu_CELoss: 1.1814 DKDLoss: 0.7649 
2022-05-02 18:16:46 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.2174, stu_CELoss: 1.4228 DKDLoss: 0.7946 
2022-05-02 18:17:18 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 2.0617, stu_CELoss: 1.2233 DKDLoss: 0.8385 
2022-05-02 18:17:50 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.9158, stu_CELoss: 1.1319 DKDLoss: 0.7838 
2022-05-02 18:18:22 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.4068, stu_CELoss: 1.5660 DKDLoss: 0.8408 
2022-05-02 18:18:54 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.0322, stu_CELoss: 1.2581 DKDLoss: 0.7741 
2022-05-02 18:19:25 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 2.1483, stu_CELoss: 1.2773 DKDLoss: 0.8710 
2022-05-02 18:19:27 - train: epoch 048, train_loss: 2.0256
2022-05-02 18:21:53 - eval: epoch: 048, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.924%, stu_acc5: 90.790%, stu_test_loss: 1.1246
2022-05-02 18:21:53 - until epoch: 048, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 18:21:53 - epoch 049 lr: 0.010000000000000002
2022-05-02 18:22:31 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.9946, stu_CELoss: 1.1767 DKDLoss: 0.8179 
2022-05-02 18:23:02 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.9925, stu_CELoss: 1.2158 DKDLoss: 0.7766 
2022-05-02 18:23:33 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.1442, stu_CELoss: 1.3123 DKDLoss: 0.8319 
2022-05-02 18:24:03 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.9377, stu_CELoss: 1.1076 DKDLoss: 0.8301 
2022-05-02 18:24:34 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.1647, stu_CELoss: 1.3406 DKDLoss: 0.8241 
2022-05-02 18:25:05 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.9399, stu_CELoss: 1.1699 DKDLoss: 0.7700 
2022-05-02 18:25:37 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.0377, stu_CELoss: 1.2180 DKDLoss: 0.8197 
2022-05-02 18:26:08 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.2329, stu_CELoss: 1.3805 DKDLoss: 0.8523 
2022-05-02 18:26:39 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.6077, stu_CELoss: 0.9210 DKDLoss: 0.6868 
2022-05-02 18:27:10 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.9624, stu_CELoss: 1.1801 DKDLoss: 0.7823 
2022-05-02 18:27:42 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.9288, stu_CELoss: 1.1133 DKDLoss: 0.8155 
2022-05-02 18:28:14 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.8983, stu_CELoss: 1.0792 DKDLoss: 0.8191 
2022-05-02 18:28:45 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.2753, stu_CELoss: 1.4236 DKDLoss: 0.8517 
2022-05-02 18:29:17 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.2833, stu_CELoss: 1.4068 DKDLoss: 0.8765 
2022-05-02 18:29:48 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.8527, stu_CELoss: 1.0939 DKDLoss: 0.7588 
2022-05-02 18:30:20 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.0129, stu_CELoss: 1.1967 DKDLoss: 0.8162 
2022-05-02 18:30:51 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.9648, stu_CELoss: 1.1330 DKDLoss: 0.8318 
2022-05-02 18:31:22 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 2.0490, stu_CELoss: 1.1309 DKDLoss: 0.9181 
2022-05-02 18:31:53 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 2.0300, stu_CELoss: 1.2110 DKDLoss: 0.8190 
2022-05-02 18:32:24 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.9902, stu_CELoss: 1.2021 DKDLoss: 0.7881 
2022-05-02 18:32:55 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.8668, stu_CELoss: 1.0842 DKDLoss: 0.7827 
2022-05-02 18:33:26 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.9982, stu_CELoss: 1.2248 DKDLoss: 0.7734 
2022-05-02 18:33:57 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.8755, stu_CELoss: 1.0545 DKDLoss: 0.8210 
2022-05-02 18:34:28 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.0414, stu_CELoss: 1.2019 DKDLoss: 0.8395 
2022-05-02 18:35:00 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.2389, stu_CELoss: 1.3082 DKDLoss: 0.9307 
2022-05-02 18:35:31 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 2.0278, stu_CELoss: 1.2570 DKDLoss: 0.7708 
2022-05-02 18:36:03 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.8919, stu_CELoss: 1.1082 DKDLoss: 0.7837 
2022-05-02 18:36:34 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.1605, stu_CELoss: 1.3249 DKDLoss: 0.8355 
2022-05-02 18:37:05 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.9306, stu_CELoss: 1.1376 DKDLoss: 0.7930 
2022-05-02 18:37:37 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 2.2086, stu_CELoss: 1.2729 DKDLoss: 0.9357 
2022-05-02 18:38:08 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.9645, stu_CELoss: 1.2258 DKDLoss: 0.7387 
2022-05-02 18:38:39 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.4366, stu_CELoss: 1.5696 DKDLoss: 0.8670 
2022-05-02 18:39:11 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.8826, stu_CELoss: 1.1089 DKDLoss: 0.7737 
2022-05-02 18:39:42 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.0150, stu_CELoss: 1.2142 DKDLoss: 0.8008 
2022-05-02 18:40:13 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.1575, stu_CELoss: 1.3132 DKDLoss: 0.8444 
2022-05-02 18:40:45 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.0295, stu_CELoss: 1.2348 DKDLoss: 0.7947 
2022-05-02 18:41:16 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.9225, stu_CELoss: 1.0467 DKDLoss: 0.8759 
2022-05-02 18:41:48 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.1855, stu_CELoss: 1.3191 DKDLoss: 0.8665 
2022-05-02 18:42:19 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.3006, stu_CELoss: 1.4294 DKDLoss: 0.8712 
2022-05-02 18:42:51 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.7954, stu_CELoss: 1.1236 DKDLoss: 0.6717 
2022-05-02 18:43:22 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.0013, stu_CELoss: 1.2059 DKDLoss: 0.7954 
2022-05-02 18:43:53 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.1593, stu_CELoss: 1.3461 DKDLoss: 0.8132 
2022-05-02 18:44:24 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.3948, stu_CELoss: 1.4356 DKDLoss: 0.9592 
2022-05-02 18:44:56 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.0129, stu_CELoss: 1.1580 DKDLoss: 0.8549 
2022-05-02 18:45:27 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 2.0843, stu_CELoss: 1.3014 DKDLoss: 0.7829 
2022-05-02 18:45:58 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.8852, stu_CELoss: 1.1037 DKDLoss: 0.7815 
2022-05-02 18:46:29 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.1253, stu_CELoss: 1.3157 DKDLoss: 0.8097 
2022-05-02 18:47:01 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.0336, stu_CELoss: 1.1988 DKDLoss: 0.8348 
2022-05-02 18:47:32 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 2.0166, stu_CELoss: 1.0778 DKDLoss: 0.9388 
2022-05-02 18:48:03 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 2.0192, stu_CELoss: 1.2172 DKDLoss: 0.8021 
2022-05-02 18:48:05 - train: epoch 049, train_loss: 2.0260
2022-05-02 18:50:30 - eval: epoch: 049, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.148%, stu_acc5: 91.064%, stu_test_loss: 1.1050
2022-05-02 18:50:30 - until epoch: 049, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 18:50:30 - epoch 050 lr: 0.010000000000000002
2022-05-02 18:51:08 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.9832, stu_CELoss: 1.2238 DKDLoss: 0.7594 
2022-05-02 18:51:39 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.9968, stu_CELoss: 1.1944 DKDLoss: 0.8025 
2022-05-02 18:52:09 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.9351, stu_CELoss: 1.1305 DKDLoss: 0.8045 
2022-05-02 18:52:40 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 2.0119, stu_CELoss: 1.2052 DKDLoss: 0.8067 
2022-05-02 18:53:11 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.6749, stu_CELoss: 0.9524 DKDLoss: 0.7225 
2022-05-02 18:53:42 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.2476, stu_CELoss: 1.3389 DKDLoss: 0.9087 
2022-05-02 18:54:14 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.9601, stu_CELoss: 1.1162 DKDLoss: 0.8439 
2022-05-02 18:54:45 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.9299, stu_CELoss: 1.0919 DKDLoss: 0.8380 
2022-05-02 18:55:16 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.1123, stu_CELoss: 1.2174 DKDLoss: 0.8949 
2022-05-02 18:55:48 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.1056, stu_CELoss: 1.2720 DKDLoss: 0.8336 
2022-05-02 18:56:19 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.0221, stu_CELoss: 1.2757 DKDLoss: 0.7464 
2022-05-02 18:56:50 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.9595, stu_CELoss: 1.1347 DKDLoss: 0.8247 
2022-05-02 18:57:22 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.7952, stu_CELoss: 1.0916 DKDLoss: 0.7036 
2022-05-02 18:57:53 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.9555, stu_CELoss: 1.2068 DKDLoss: 0.7486 
2022-05-02 18:58:24 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 2.0831, stu_CELoss: 1.1535 DKDLoss: 0.9296 
2022-05-02 18:58:55 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.1292, stu_CELoss: 1.2511 DKDLoss: 0.8781 
2022-05-02 18:59:26 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.1986, stu_CELoss: 1.2983 DKDLoss: 0.9003 
2022-05-02 18:59:58 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.1877, stu_CELoss: 1.2498 DKDLoss: 0.9378 
2022-05-02 19:00:29 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 2.0496, stu_CELoss: 1.2736 DKDLoss: 0.7761 
2022-05-02 19:01:01 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.9578, stu_CELoss: 1.2163 DKDLoss: 0.7414 
2022-05-02 19:01:32 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.9485, stu_CELoss: 1.2169 DKDLoss: 0.7316 
2022-05-02 19:02:04 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.1282, stu_CELoss: 1.2820 DKDLoss: 0.8462 
2022-05-02 19:02:35 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.9489, stu_CELoss: 1.1173 DKDLoss: 0.8316 
2022-05-02 19:03:06 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.3700, stu_CELoss: 1.4484 DKDLoss: 0.9216 
2022-05-02 19:03:38 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.9081, stu_CELoss: 1.1930 DKDLoss: 0.7151 
2022-05-02 19:04:10 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.9105, stu_CELoss: 1.1126 DKDLoss: 0.7980 
2022-05-02 19:04:41 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 2.0121, stu_CELoss: 1.1947 DKDLoss: 0.8174 
2022-05-02 19:05:13 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.0229, stu_CELoss: 1.2179 DKDLoss: 0.8050 
2022-05-02 19:05:44 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.3688, stu_CELoss: 1.4584 DKDLoss: 0.9104 
2022-05-02 19:06:16 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.0519, stu_CELoss: 1.1012 DKDLoss: 0.9507 
2022-05-02 19:06:48 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.1012, stu_CELoss: 1.3011 DKDLoss: 0.8001 
2022-05-02 19:07:19 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.0807, stu_CELoss: 1.2856 DKDLoss: 0.7951 
2022-05-02 19:07:51 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.9351, stu_CELoss: 1.1414 DKDLoss: 0.7936 
2022-05-02 19:08:23 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.1420, stu_CELoss: 1.2743 DKDLoss: 0.8677 
2022-05-02 19:08:54 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 2.0569, stu_CELoss: 1.2658 DKDLoss: 0.7911 
2022-05-02 19:09:26 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.0243, stu_CELoss: 1.2543 DKDLoss: 0.7700 
2022-05-02 19:09:57 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.3325, stu_CELoss: 1.3959 DKDLoss: 0.9366 
2022-05-02 19:10:28 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.8027, stu_CELoss: 1.0590 DKDLoss: 0.7437 
2022-05-02 19:11:00 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.8964, stu_CELoss: 1.0979 DKDLoss: 0.7985 
2022-05-02 19:11:31 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.0807, stu_CELoss: 1.3555 DKDLoss: 0.7252 
2022-05-02 19:12:03 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 2.0965, stu_CELoss: 1.3018 DKDLoss: 0.7947 
2022-05-02 19:12:34 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.2078, stu_CELoss: 1.3721 DKDLoss: 0.8357 
2022-05-02 19:13:06 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.2276, stu_CELoss: 1.3494 DKDLoss: 0.8782 
2022-05-02 19:13:37 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.9619, stu_CELoss: 1.1929 DKDLoss: 0.7690 
2022-05-02 19:14:09 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 2.2508, stu_CELoss: 1.3725 DKDLoss: 0.8783 
2022-05-02 19:14:41 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 2.1481, stu_CELoss: 1.3410 DKDLoss: 0.8070 
2022-05-02 19:15:12 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.0661, stu_CELoss: 1.2573 DKDLoss: 0.8088 
2022-05-02 19:15:44 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.9523, stu_CELoss: 1.2434 DKDLoss: 0.7089 
2022-05-02 19:16:16 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.7936, stu_CELoss: 0.9923 DKDLoss: 0.8014 
2022-05-02 19:16:47 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.7828, stu_CELoss: 1.0702 DKDLoss: 0.7127 
2022-05-02 19:16:49 - train: epoch 050, train_loss: 2.0277
2022-05-02 19:19:14 - eval: epoch: 050, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.352%, stu_acc5: 91.026%, stu_test_loss: 1.0979
2022-05-02 19:19:15 - until epoch: 050, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 19:19:15 - epoch 051 lr: 0.010000000000000002
2022-05-02 19:19:53 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.0469, stu_CELoss: 1.2190 DKDLoss: 0.8280 
2022-05-02 19:20:25 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.1853, stu_CELoss: 1.2524 DKDLoss: 0.9330 
2022-05-02 19:20:56 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.8830, stu_CELoss: 1.0824 DKDLoss: 0.8006 
2022-05-02 19:21:28 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.6740, stu_CELoss: 0.9983 DKDLoss: 0.6756 
2022-05-02 19:21:59 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.1013, stu_CELoss: 1.2925 DKDLoss: 0.8088 
2022-05-02 19:22:31 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.8423, stu_CELoss: 1.0970 DKDLoss: 0.7453 
2022-05-02 19:23:02 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.0305, stu_CELoss: 1.2519 DKDLoss: 0.7785 
2022-05-02 19:23:34 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.0095, stu_CELoss: 1.1885 DKDLoss: 0.8210 
2022-05-02 19:24:06 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.8036, stu_CELoss: 1.1016 DKDLoss: 0.7020 
2022-05-02 19:24:38 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.4914, stu_CELoss: 1.5550 DKDLoss: 0.9364 
2022-05-02 19:25:10 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.2851, stu_CELoss: 1.4514 DKDLoss: 0.8337 
2022-05-02 19:25:42 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.8312, stu_CELoss: 1.0730 DKDLoss: 0.7583 
2022-05-02 19:26:14 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.9175, stu_CELoss: 1.0866 DKDLoss: 0.8309 
2022-05-02 19:26:45 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.9105, stu_CELoss: 1.2090 DKDLoss: 0.7015 
2022-05-02 19:27:17 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.8963, stu_CELoss: 1.1540 DKDLoss: 0.7423 
2022-05-02 19:27:49 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.8638, stu_CELoss: 1.1456 DKDLoss: 0.7182 
2022-05-02 19:28:21 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.0282, stu_CELoss: 1.2197 DKDLoss: 0.8085 
2022-05-02 19:28:53 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 2.1211, stu_CELoss: 1.3104 DKDLoss: 0.8107 
2022-05-02 19:29:25 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 2.0578, stu_CELoss: 1.1946 DKDLoss: 0.8632 
2022-05-02 19:29:57 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.7921, stu_CELoss: 1.0451 DKDLoss: 0.7470 
2022-05-02 19:30:30 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.9258, stu_CELoss: 1.1569 DKDLoss: 0.7689 
2022-05-02 19:31:01 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.9485, stu_CELoss: 1.2326 DKDLoss: 0.7159 
2022-05-02 19:31:33 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.3993, stu_CELoss: 1.4146 DKDLoss: 0.9847 
2022-05-02 19:32:05 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.0883, stu_CELoss: 1.2625 DKDLoss: 0.8258 
2022-05-02 19:32:37 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.9131, stu_CELoss: 1.1027 DKDLoss: 0.8104 
2022-05-02 19:33:09 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.9862, stu_CELoss: 1.1631 DKDLoss: 0.8231 
2022-05-02 19:33:41 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.9953, stu_CELoss: 1.2056 DKDLoss: 0.7897 
2022-05-02 19:34:12 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.8648, stu_CELoss: 1.0683 DKDLoss: 0.7965 
2022-05-02 19:34:44 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.9185, stu_CELoss: 1.1068 DKDLoss: 0.8117 
2022-05-02 19:35:16 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.9401, stu_CELoss: 1.1768 DKDLoss: 0.7634 
2022-05-02 19:35:48 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.8726, stu_CELoss: 1.1676 DKDLoss: 0.7050 
2022-05-02 19:36:19 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.9060, stu_CELoss: 1.2094 DKDLoss: 0.6965 
2022-05-02 19:36:52 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.1453, stu_CELoss: 1.2888 DKDLoss: 0.8564 
2022-05-02 19:37:23 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 2.0987, stu_CELoss: 1.2866 DKDLoss: 0.8122 
2022-05-02 19:37:55 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.0140, stu_CELoss: 1.1810 DKDLoss: 0.8330 
2022-05-02 19:38:26 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 2.0883, stu_CELoss: 1.2498 DKDLoss: 0.8385 
2022-05-02 19:38:58 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.1539, stu_CELoss: 1.3065 DKDLoss: 0.8474 
2022-05-02 19:39:30 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.9357, stu_CELoss: 1.1844 DKDLoss: 0.7513 
2022-05-02 19:40:01 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.0976, stu_CELoss: 1.2548 DKDLoss: 0.8428 
2022-05-02 19:40:33 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.9354, stu_CELoss: 1.1498 DKDLoss: 0.7856 
2022-05-02 19:41:05 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.0321, stu_CELoss: 1.2340 DKDLoss: 0.7981 
2022-05-02 19:41:37 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.1983, stu_CELoss: 1.3250 DKDLoss: 0.8734 
2022-05-02 19:42:09 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.2639, stu_CELoss: 1.3870 DKDLoss: 0.8769 
2022-05-02 19:42:41 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.2384, stu_CELoss: 1.3543 DKDLoss: 0.8841 
2022-05-02 19:43:13 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 2.0059, stu_CELoss: 1.1042 DKDLoss: 0.9017 
2022-05-02 19:43:46 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.0800, stu_CELoss: 1.2955 DKDLoss: 0.7845 
2022-05-02 19:44:17 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.8953, stu_CELoss: 1.1547 DKDLoss: 0.7406 
2022-05-02 19:44:49 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.1975, stu_CELoss: 1.3637 DKDLoss: 0.8338 
2022-05-02 19:45:21 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.9405, stu_CELoss: 1.1093 DKDLoss: 0.8312 
2022-05-02 19:45:52 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.9899, stu_CELoss: 1.2209 DKDLoss: 0.7690 
2022-05-02 19:45:54 - train: epoch 051, train_loss: 2.0235
2022-05-02 19:48:19 - eval: epoch: 051, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.056%, stu_acc5: 90.908%, stu_test_loss: 1.1119
2022-05-02 19:48:20 - until epoch: 051, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 19:48:20 - epoch 052 lr: 0.010000000000000002
2022-05-02 19:48:58 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.9175, stu_CELoss: 1.2302 DKDLoss: 0.6873 
2022-05-02 19:49:30 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.0998, stu_CELoss: 1.3627 DKDLoss: 0.7371 
2022-05-02 19:50:01 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.8817, stu_CELoss: 1.1225 DKDLoss: 0.7592 
2022-05-02 19:50:33 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.1148, stu_CELoss: 1.3010 DKDLoss: 0.8138 
2022-05-02 19:51:05 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.9774, stu_CELoss: 1.2018 DKDLoss: 0.7756 
2022-05-02 19:51:37 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.8256, stu_CELoss: 1.0454 DKDLoss: 0.7802 
2022-05-02 19:52:09 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.9125, stu_CELoss: 1.1510 DKDLoss: 0.7615 
2022-05-02 19:52:41 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 2.0450, stu_CELoss: 1.2121 DKDLoss: 0.8330 
2022-05-02 19:53:12 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.8458, stu_CELoss: 1.1048 DKDLoss: 0.7410 
2022-05-02 19:53:44 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.1086, stu_CELoss: 1.2983 DKDLoss: 0.8103 
2022-05-02 19:54:16 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 2.2696, stu_CELoss: 1.4192 DKDLoss: 0.8505 
2022-05-02 19:54:48 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.8608, stu_CELoss: 1.1052 DKDLoss: 0.7556 
2022-05-02 19:55:20 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.9692, stu_CELoss: 1.1249 DKDLoss: 0.8443 
2022-05-02 19:55:52 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.2962, stu_CELoss: 1.3758 DKDLoss: 0.9204 
2022-05-02 19:56:23 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.7232, stu_CELoss: 1.0242 DKDLoss: 0.6990 
2022-05-02 19:56:55 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.9855, stu_CELoss: 1.2128 DKDLoss: 0.7727 
2022-05-02 19:57:27 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.7895, stu_CELoss: 1.0501 DKDLoss: 0.7395 
2022-05-02 19:57:59 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.7576, stu_CELoss: 1.0264 DKDLoss: 0.7313 
2022-05-02 19:58:31 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.9649, stu_CELoss: 1.1060 DKDLoss: 0.8589 
2022-05-02 19:59:02 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.0240, stu_CELoss: 1.1906 DKDLoss: 0.8334 
2022-05-02 19:59:34 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.9626, stu_CELoss: 1.1912 DKDLoss: 0.7713 
2022-05-02 20:00:05 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.0825, stu_CELoss: 1.2772 DKDLoss: 0.8053 
2022-05-02 20:00:37 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.9765, stu_CELoss: 1.2226 DKDLoss: 0.7540 
2022-05-02 20:01:09 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.7975, stu_CELoss: 1.0605 DKDLoss: 0.7370 
2022-05-02 20:01:41 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 2.0129, stu_CELoss: 1.3076 DKDLoss: 0.7053 
2022-05-02 20:02:12 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.7449, stu_CELoss: 1.0478 DKDLoss: 0.6971 
2022-05-02 20:02:44 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.7858, stu_CELoss: 1.0299 DKDLoss: 0.7559 
2022-05-02 20:03:15 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.3192, stu_CELoss: 1.4407 DKDLoss: 0.8784 
2022-05-02 20:03:47 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.8360, stu_CELoss: 1.0967 DKDLoss: 0.7394 
2022-05-02 20:04:18 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.9936, stu_CELoss: 1.1677 DKDLoss: 0.8259 
2022-05-02 20:04:50 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 2.1567, stu_CELoss: 1.2837 DKDLoss: 0.8731 
2022-05-02 20:05:22 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.9203, stu_CELoss: 1.2088 DKDLoss: 0.7115 
2022-05-02 20:05:53 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 2.1378, stu_CELoss: 1.3429 DKDLoss: 0.7949 
2022-05-02 20:06:25 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.3238, stu_CELoss: 1.4908 DKDLoss: 0.8331 
2022-05-02 20:06:56 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.2570, stu_CELoss: 1.3428 DKDLoss: 0.9142 
2022-05-02 20:07:28 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.1224, stu_CELoss: 1.2536 DKDLoss: 0.8689 
2022-05-02 20:08:00 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.9943, stu_CELoss: 1.1649 DKDLoss: 0.8294 
2022-05-02 20:08:31 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.9776, stu_CELoss: 1.1923 DKDLoss: 0.7852 
2022-05-02 20:09:03 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.9525, stu_CELoss: 1.1322 DKDLoss: 0.8203 
2022-05-02 20:09:34 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.2544, stu_CELoss: 1.4109 DKDLoss: 0.8436 
2022-05-02 20:10:06 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.8030, stu_CELoss: 0.9773 DKDLoss: 0.8256 
2022-05-02 20:10:38 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.2104, stu_CELoss: 1.3526 DKDLoss: 0.8579 
2022-05-02 20:11:09 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.9729, stu_CELoss: 1.1428 DKDLoss: 0.8301 
2022-05-02 20:11:40 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.0597, stu_CELoss: 1.2065 DKDLoss: 0.8532 
2022-05-02 20:12:11 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 2.0519, stu_CELoss: 1.1410 DKDLoss: 0.9109 
2022-05-02 20:12:43 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 2.0735, stu_CELoss: 1.2847 DKDLoss: 0.7888 
2022-05-02 20:13:14 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.2249, stu_CELoss: 1.3586 DKDLoss: 0.8663 
2022-05-02 20:13:46 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.8311, stu_CELoss: 1.0494 DKDLoss: 0.7816 
2022-05-02 20:14:17 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.0650, stu_CELoss: 1.2753 DKDLoss: 0.7897 
2022-05-02 20:14:49 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 2.0461, stu_CELoss: 1.2068 DKDLoss: 0.8393 
2022-05-02 20:14:50 - train: epoch 052, train_loss: 2.0224
2022-05-02 20:17:16 - eval: epoch: 052, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.256%, stu_acc5: 90.550%, stu_test_loss: 1.1500
2022-05-02 20:17:17 - until epoch: 052, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 20:17:17 - epoch 053 lr: 0.010000000000000002
2022-05-02 20:17:55 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.1854, stu_CELoss: 1.3267 DKDLoss: 0.8587 
2022-05-02 20:18:26 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.9189, stu_CELoss: 1.1245 DKDLoss: 0.7944 
2022-05-02 20:18:58 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.8190, stu_CELoss: 1.0243 DKDLoss: 0.7947 
2022-05-02 20:19:29 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.0918, stu_CELoss: 1.2509 DKDLoss: 0.8409 
2022-05-02 20:20:00 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.8984, stu_CELoss: 1.1060 DKDLoss: 0.7923 
2022-05-02 20:20:31 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.9762, stu_CELoss: 1.1978 DKDLoss: 0.7785 
2022-05-02 20:21:01 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 2.0451, stu_CELoss: 1.2792 DKDLoss: 0.7659 
2022-05-02 20:21:33 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.2530, stu_CELoss: 1.4223 DKDLoss: 0.8307 
2022-05-02 20:22:05 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.9088, stu_CELoss: 1.0944 DKDLoss: 0.8144 
2022-05-02 20:22:37 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.1835, stu_CELoss: 1.3056 DKDLoss: 0.8780 
2022-05-02 20:23:08 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.8713, stu_CELoss: 1.0685 DKDLoss: 0.8028 
2022-05-02 20:23:40 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.9661, stu_CELoss: 1.1906 DKDLoss: 0.7755 
2022-05-02 20:24:12 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.2325, stu_CELoss: 1.3317 DKDLoss: 0.9008 
2022-05-02 20:24:44 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.9987, stu_CELoss: 1.1307 DKDLoss: 0.8680 
2022-05-02 20:25:16 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.9268, stu_CELoss: 1.1221 DKDLoss: 0.8046 
2022-05-02 20:25:48 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.9358, stu_CELoss: 1.1758 DKDLoss: 0.7600 
2022-05-02 20:26:20 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.1077, stu_CELoss: 1.2621 DKDLoss: 0.8456 
2022-05-02 20:26:52 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.3442, stu_CELoss: 1.5013 DKDLoss: 0.8429 
2022-05-02 20:27:24 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.9921, stu_CELoss: 1.1810 DKDLoss: 0.8111 
2022-05-02 20:27:56 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.0016, stu_CELoss: 1.1860 DKDLoss: 0.8157 
2022-05-02 20:28:28 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.3640, stu_CELoss: 1.4546 DKDLoss: 0.9094 
2022-05-02 20:29:00 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 2.0539, stu_CELoss: 1.2588 DKDLoss: 0.7951 
2022-05-02 20:29:32 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.9141, stu_CELoss: 1.0918 DKDLoss: 0.8223 
2022-05-02 20:30:05 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.9469, stu_CELoss: 1.1085 DKDLoss: 0.8384 
2022-05-02 20:30:37 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.0785, stu_CELoss: 1.2042 DKDLoss: 0.8743 
2022-05-02 20:31:09 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.2479, stu_CELoss: 1.4675 DKDLoss: 0.7804 
2022-05-02 20:31:40 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.2454, stu_CELoss: 1.3206 DKDLoss: 0.9247 
2022-05-02 20:32:12 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.2503, stu_CELoss: 1.2893 DKDLoss: 0.9610 
2022-05-02 20:32:44 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.9069, stu_CELoss: 1.1150 DKDLoss: 0.7919 
2022-05-02 20:33:16 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.8755, stu_CELoss: 1.0803 DKDLoss: 0.7952 
2022-05-02 20:33:47 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.1255, stu_CELoss: 1.3290 DKDLoss: 0.7966 
2022-05-02 20:34:19 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.3727, stu_CELoss: 1.3915 DKDLoss: 0.9812 
2022-05-02 20:34:51 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.2439, stu_CELoss: 1.3735 DKDLoss: 0.8704 
2022-05-02 20:35:22 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.8026, stu_CELoss: 1.0713 DKDLoss: 0.7313 
2022-05-02 20:35:54 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.9974, stu_CELoss: 1.2259 DKDLoss: 0.7715 
2022-05-02 20:36:26 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.9267, stu_CELoss: 1.2092 DKDLoss: 0.7175 
2022-05-02 20:36:57 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.0348, stu_CELoss: 1.2053 DKDLoss: 0.8295 
2022-05-02 20:37:29 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.0785, stu_CELoss: 1.2612 DKDLoss: 0.8173 
2022-05-02 20:38:01 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.3407, stu_CELoss: 1.5180 DKDLoss: 0.8227 
2022-05-02 20:38:32 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 2.0870, stu_CELoss: 1.2362 DKDLoss: 0.8508 
2022-05-02 20:39:04 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.1206, stu_CELoss: 1.3115 DKDLoss: 0.8092 
2022-05-02 20:39:37 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.3057, stu_CELoss: 1.4016 DKDLoss: 0.9042 
2022-05-02 20:40:08 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.3189, stu_CELoss: 1.4764 DKDLoss: 0.8426 
2022-05-02 20:40:40 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.0470, stu_CELoss: 1.2657 DKDLoss: 0.7813 
2022-05-02 20:41:12 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.0983, stu_CELoss: 1.2491 DKDLoss: 0.8492 
2022-05-02 20:41:44 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 2.0244, stu_CELoss: 1.2623 DKDLoss: 0.7621 
2022-05-02 20:42:16 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.0783, stu_CELoss: 1.2652 DKDLoss: 0.8131 
2022-05-02 20:42:47 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.2628, stu_CELoss: 1.3947 DKDLoss: 0.8681 
2022-05-02 20:43:18 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.1262, stu_CELoss: 1.2968 DKDLoss: 0.8294 
2022-05-02 20:43:49 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.8656, stu_CELoss: 1.0159 DKDLoss: 0.8497 
2022-05-02 20:43:51 - train: epoch 053, train_loss: 2.0219
2022-05-02 20:46:17 - eval: epoch: 053, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.434%, stu_acc5: 91.282%, stu_test_loss: 1.0946
2022-05-02 20:46:18 - until epoch: 053, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 20:46:18 - epoch 054 lr: 0.010000000000000002
2022-05-02 20:46:57 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.9903, stu_CELoss: 1.2030 DKDLoss: 0.7872 
2022-05-02 20:47:28 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.2092, stu_CELoss: 1.3444 DKDLoss: 0.8648 
2022-05-02 20:47:59 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.7672, stu_CELoss: 1.0107 DKDLoss: 0.7566 
2022-05-02 20:48:30 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.9814, stu_CELoss: 1.2100 DKDLoss: 0.7714 
2022-05-02 20:49:01 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.8982, stu_CELoss: 1.1129 DKDLoss: 0.7853 
2022-05-02 20:49:33 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.0421, stu_CELoss: 1.1769 DKDLoss: 0.8652 
2022-05-02 20:50:04 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.3573, stu_CELoss: 1.4516 DKDLoss: 0.9057 
2022-05-02 20:50:36 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.1768, stu_CELoss: 1.2955 DKDLoss: 0.8814 
2022-05-02 20:51:08 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.5698, stu_CELoss: 0.9141 DKDLoss: 0.6557 
2022-05-02 20:51:39 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.6887, stu_CELoss: 0.9975 DKDLoss: 0.6912 
2022-05-02 20:52:11 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.7796, stu_CELoss: 1.0652 DKDLoss: 0.7144 
2022-05-02 20:52:43 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.0705, stu_CELoss: 1.2935 DKDLoss: 0.7769 
2022-05-02 20:53:14 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.9904, stu_CELoss: 1.1500 DKDLoss: 0.8404 
2022-05-02 20:53:46 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.0582, stu_CELoss: 1.1982 DKDLoss: 0.8600 
2022-05-02 20:54:18 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.8178, stu_CELoss: 1.1108 DKDLoss: 0.7070 
2022-05-02 20:54:50 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.6721, stu_CELoss: 0.9686 DKDLoss: 0.7035 
2022-05-02 20:55:21 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.7926, stu_CELoss: 1.0850 DKDLoss: 0.7076 
2022-05-02 20:55:53 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.8432, stu_CELoss: 1.0528 DKDLoss: 0.7903 
2022-05-02 20:56:25 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.0943, stu_CELoss: 1.2901 DKDLoss: 0.8043 
2022-05-02 20:56:56 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.8592, stu_CELoss: 1.1107 DKDLoss: 0.7485 
2022-05-02 20:57:28 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.7882, stu_CELoss: 0.9849 DKDLoss: 0.8033 
2022-05-02 20:58:00 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.9980, stu_CELoss: 1.2598 DKDLoss: 0.7381 
2022-05-02 20:58:32 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.7013, stu_CELoss: 0.9104 DKDLoss: 0.7909 
2022-05-02 20:59:04 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.8949, stu_CELoss: 1.1043 DKDLoss: 0.7906 
2022-05-02 20:59:36 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.0590, stu_CELoss: 1.1634 DKDLoss: 0.8956 
2022-05-02 21:00:07 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.9702, stu_CELoss: 1.1060 DKDLoss: 0.8642 
2022-05-02 21:00:39 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.9554, stu_CELoss: 1.1760 DKDLoss: 0.7794 
2022-05-02 21:01:11 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.2295, stu_CELoss: 1.3129 DKDLoss: 0.9166 
2022-05-02 21:01:43 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.7364, stu_CELoss: 1.0171 DKDLoss: 0.7192 
2022-05-02 21:02:15 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.2017, stu_CELoss: 1.3573 DKDLoss: 0.8444 
2022-05-02 21:02:47 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.2500, stu_CELoss: 1.3557 DKDLoss: 0.8943 
2022-05-02 21:03:18 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.2573, stu_CELoss: 1.4529 DKDLoss: 0.8044 
2022-05-02 21:03:50 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.9056, stu_CELoss: 1.1197 DKDLoss: 0.7859 
2022-05-02 21:04:22 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 2.1187, stu_CELoss: 1.2107 DKDLoss: 0.9080 
2022-05-02 21:04:53 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.2082, stu_CELoss: 1.3751 DKDLoss: 0.8331 
2022-05-02 21:05:25 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.0097, stu_CELoss: 1.1255 DKDLoss: 0.8843 
2022-05-02 21:05:58 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.8067, stu_CELoss: 1.0316 DKDLoss: 0.7751 
2022-05-02 21:06:30 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.7915, stu_CELoss: 1.0052 DKDLoss: 0.7863 
2022-05-02 21:07:02 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.0640, stu_CELoss: 1.2490 DKDLoss: 0.8150 
2022-05-02 21:07:33 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 2.0360, stu_CELoss: 1.2401 DKDLoss: 0.7958 
2022-05-02 21:08:06 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.0466, stu_CELoss: 1.2309 DKDLoss: 0.8157 
2022-05-02 21:08:38 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.9696, stu_CELoss: 1.1852 DKDLoss: 0.7844 
2022-05-02 21:09:10 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.9745, stu_CELoss: 1.2610 DKDLoss: 0.7136 
2022-05-02 21:09:43 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.8535, stu_CELoss: 1.0562 DKDLoss: 0.7974 
2022-05-02 21:10:15 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.9215, stu_CELoss: 1.1455 DKDLoss: 0.7760 
2022-05-02 21:10:47 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.1145, stu_CELoss: 1.2288 DKDLoss: 0.8857 
2022-05-02 21:11:18 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.3717, stu_CELoss: 1.4522 DKDLoss: 0.9194 
2022-05-02 21:11:50 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.1425, stu_CELoss: 1.3850 DKDLoss: 0.7576 
2022-05-02 21:12:22 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.9396, stu_CELoss: 1.1068 DKDLoss: 0.8328 
2022-05-02 21:12:54 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.9167, stu_CELoss: 1.1193 DKDLoss: 0.7974 
2022-05-02 21:12:56 - train: epoch 054, train_loss: 2.0157
2022-05-02 21:15:22 - eval: epoch: 054, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.412%, stu_acc5: 90.974%, stu_test_loss: 1.0978
2022-05-02 21:15:23 - until epoch: 054, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 21:15:23 - epoch 055 lr: 0.010000000000000002
2022-05-02 21:16:01 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 2.1156, stu_CELoss: 1.3253 DKDLoss: 0.7903 
2022-05-02 21:16:32 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.7906, stu_CELoss: 0.9731 DKDLoss: 0.8175 
2022-05-02 21:17:04 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.9093, stu_CELoss: 1.1187 DKDLoss: 0.7907 
2022-05-02 21:17:35 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.8448, stu_CELoss: 1.0760 DKDLoss: 0.7688 
2022-05-02 21:18:07 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.7023, stu_CELoss: 0.9161 DKDLoss: 0.7862 
2022-05-02 21:18:38 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.9842, stu_CELoss: 1.2308 DKDLoss: 0.7534 
2022-05-02 21:19:09 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.9226, stu_CELoss: 1.1200 DKDLoss: 0.8026 
2022-05-02 21:19:41 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.6132, stu_CELoss: 0.9300 DKDLoss: 0.6832 
2022-05-02 21:20:12 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.8977, stu_CELoss: 1.0998 DKDLoss: 0.7978 
2022-05-02 21:20:44 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 2.1015, stu_CELoss: 1.2991 DKDLoss: 0.8024 
2022-05-02 21:21:16 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 2.0103, stu_CELoss: 1.1879 DKDLoss: 0.8224 
2022-05-02 21:21:47 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 2.1050, stu_CELoss: 1.3264 DKDLoss: 0.7786 
2022-05-02 21:22:19 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.1424, stu_CELoss: 1.3508 DKDLoss: 0.7916 
2022-05-02 21:22:51 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.9291, stu_CELoss: 1.0927 DKDLoss: 0.8365 
2022-05-02 21:23:23 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.7255, stu_CELoss: 0.9719 DKDLoss: 0.7536 
2022-05-02 21:23:55 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.1054, stu_CELoss: 1.2602 DKDLoss: 0.8452 
2022-05-02 21:24:27 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.9558, stu_CELoss: 1.1793 DKDLoss: 0.7765 
2022-05-02 21:24:59 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.8711, stu_CELoss: 1.0734 DKDLoss: 0.7978 
2022-05-02 21:25:30 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.9579, stu_CELoss: 1.1278 DKDLoss: 0.8301 
2022-05-02 21:26:02 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 2.0140, stu_CELoss: 1.1936 DKDLoss: 0.8204 
2022-05-02 21:26:33 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.8452, stu_CELoss: 1.1057 DKDLoss: 0.7395 
2022-05-02 21:27:05 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.1883, stu_CELoss: 1.3611 DKDLoss: 0.8272 
2022-05-02 21:27:37 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.8578, stu_CELoss: 1.1281 DKDLoss: 0.7297 
2022-05-02 21:28:09 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.9497, stu_CELoss: 1.1527 DKDLoss: 0.7970 
2022-05-02 21:28:40 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.9834, stu_CELoss: 1.1948 DKDLoss: 0.7886 
2022-05-02 21:29:12 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 2.0756, stu_CELoss: 1.2312 DKDLoss: 0.8445 
2022-05-02 21:29:43 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 2.0142, stu_CELoss: 1.2012 DKDLoss: 0.8130 
2022-05-02 21:30:15 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.4577, stu_CELoss: 1.5235 DKDLoss: 0.9342 
2022-05-02 21:30:46 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.9859, stu_CELoss: 1.1433 DKDLoss: 0.8427 
2022-05-02 21:31:18 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.1242, stu_CELoss: 1.2814 DKDLoss: 0.8428 
2022-05-02 21:31:49 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.0993, stu_CELoss: 1.2989 DKDLoss: 0.8004 
2022-05-02 21:32:21 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.9747, stu_CELoss: 1.1814 DKDLoss: 0.7932 
2022-05-02 21:32:52 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.8688, stu_CELoss: 1.0319 DKDLoss: 0.8369 
2022-05-02 21:33:23 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.0586, stu_CELoss: 1.2755 DKDLoss: 0.7831 
2022-05-02 21:33:55 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.8636, stu_CELoss: 1.1200 DKDLoss: 0.7436 
2022-05-02 21:34:27 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.1029, stu_CELoss: 1.2228 DKDLoss: 0.8802 
2022-05-02 21:34:59 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.9908, stu_CELoss: 1.1365 DKDLoss: 0.8543 
2022-05-02 21:35:31 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.9214, stu_CELoss: 1.1565 DKDLoss: 0.7649 
2022-05-02 21:36:02 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.4083, stu_CELoss: 1.4882 DKDLoss: 0.9200 
2022-05-02 21:36:34 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.9834, stu_CELoss: 1.2337 DKDLoss: 0.7497 
2022-05-02 21:37:06 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.9731, stu_CELoss: 1.1558 DKDLoss: 0.8172 
2022-05-02 21:37:38 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.0910, stu_CELoss: 1.2941 DKDLoss: 0.7968 
2022-05-02 21:38:10 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.1751, stu_CELoss: 1.3306 DKDLoss: 0.8445 
2022-05-02 21:38:42 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.1831, stu_CELoss: 1.3423 DKDLoss: 0.8408 
2022-05-02 21:39:14 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.9087, stu_CELoss: 1.1807 DKDLoss: 0.7280 
2022-05-02 21:39:46 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 2.1271, stu_CELoss: 1.2719 DKDLoss: 0.8553 
2022-05-02 21:40:17 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.8983, stu_CELoss: 1.1428 DKDLoss: 0.7555 
2022-05-02 21:40:49 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.0978, stu_CELoss: 1.1996 DKDLoss: 0.8982 
2022-05-02 21:41:21 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.1311, stu_CELoss: 1.2892 DKDLoss: 0.8419 
2022-05-02 21:41:53 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 2.0395, stu_CELoss: 1.1966 DKDLoss: 0.8429 
2022-05-02 21:41:55 - train: epoch 055, train_loss: 2.0142
2022-05-02 21:44:20 - eval: epoch: 055, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.168%, stu_acc5: 90.996%, stu_test_loss: 1.1023
2022-05-02 21:44:21 - until epoch: 055, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 21:44:21 - epoch 056 lr: 0.010000000000000002
2022-05-02 21:44:59 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.9904, stu_CELoss: 1.2011 DKDLoss: 0.7893 
2022-05-02 21:45:30 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.0981, stu_CELoss: 1.2590 DKDLoss: 0.8391 
2022-05-02 21:46:01 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.7520, stu_CELoss: 0.9574 DKDLoss: 0.7946 
2022-05-02 21:46:32 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.9154, stu_CELoss: 1.0997 DKDLoss: 0.8157 
2022-05-02 21:47:03 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.8720, stu_CELoss: 1.1079 DKDLoss: 0.7641 
2022-05-02 21:47:35 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 2.0019, stu_CELoss: 1.2218 DKDLoss: 0.7801 
2022-05-02 21:48:07 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.9458, stu_CELoss: 1.1738 DKDLoss: 0.7720 
2022-05-02 21:48:39 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.1552, stu_CELoss: 1.3118 DKDLoss: 0.8434 
2022-05-02 21:49:11 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.2195, stu_CELoss: 1.3888 DKDLoss: 0.8307 
2022-05-02 21:49:43 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.8927, stu_CELoss: 1.2161 DKDLoss: 0.6766 
2022-05-02 21:50:15 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.8917, stu_CELoss: 1.1022 DKDLoss: 0.7895 
2022-05-02 21:50:47 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.0847, stu_CELoss: 1.2788 DKDLoss: 0.8060 
2022-05-02 21:51:18 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.0835, stu_CELoss: 1.2560 DKDLoss: 0.8275 
2022-05-02 21:51:49 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.9535, stu_CELoss: 1.1485 DKDLoss: 0.8050 
2022-05-02 21:52:22 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.4187, stu_CELoss: 1.4828 DKDLoss: 0.9359 
2022-05-02 21:52:53 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 2.0077, stu_CELoss: 1.2399 DKDLoss: 0.7679 
2022-05-02 21:53:25 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.1473, stu_CELoss: 1.3111 DKDLoss: 0.8362 
2022-05-02 21:53:57 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.1243, stu_CELoss: 1.2621 DKDLoss: 0.8623 
2022-05-02 21:54:30 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.0940, stu_CELoss: 1.2972 DKDLoss: 0.7968 
2022-05-02 21:55:01 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 2.1047, stu_CELoss: 1.2335 DKDLoss: 0.8712 
2022-05-02 21:55:33 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.0480, stu_CELoss: 1.1665 DKDLoss: 0.8815 
2022-05-02 21:56:05 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.2071, stu_CELoss: 1.4038 DKDLoss: 0.8033 
2022-05-02 21:56:37 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 2.0047, stu_CELoss: 1.1561 DKDLoss: 0.8486 
2022-05-02 21:57:09 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.1735, stu_CELoss: 1.3683 DKDLoss: 0.8053 
2022-05-02 21:57:41 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.1661, stu_CELoss: 1.4097 DKDLoss: 0.7564 
2022-05-02 21:58:13 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.8663, stu_CELoss: 1.0817 DKDLoss: 0.7845 
2022-05-02 21:58:44 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.8128, stu_CELoss: 1.0230 DKDLoss: 0.7898 
2022-05-02 21:59:16 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.8197, stu_CELoss: 1.0352 DKDLoss: 0.7845 
2022-05-02 21:59:48 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 2.0185, stu_CELoss: 1.2793 DKDLoss: 0.7392 
2022-05-02 22:00:20 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.0181, stu_CELoss: 1.2240 DKDLoss: 0.7941 
2022-05-02 22:00:52 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.7942, stu_CELoss: 1.0606 DKDLoss: 0.7336 
2022-05-02 22:01:24 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.8502, stu_CELoss: 1.1190 DKDLoss: 0.7311 
2022-05-02 22:01:56 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.0955, stu_CELoss: 1.2686 DKDLoss: 0.8268 
2022-05-02 22:02:27 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.8304, stu_CELoss: 1.0585 DKDLoss: 0.7719 
2022-05-02 22:02:58 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 2.1050, stu_CELoss: 1.2302 DKDLoss: 0.8747 
2022-05-02 22:03:29 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.7828, stu_CELoss: 0.9981 DKDLoss: 0.7848 
2022-05-02 22:04:01 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.0078, stu_CELoss: 1.1973 DKDLoss: 0.8105 
2022-05-02 22:04:33 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.9000, stu_CELoss: 1.1575 DKDLoss: 0.7425 
2022-05-02 22:05:04 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.0079, stu_CELoss: 1.2325 DKDLoss: 0.7754 
2022-05-02 22:05:36 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 2.0485, stu_CELoss: 1.3174 DKDLoss: 0.7311 
2022-05-02 22:06:07 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.3848, stu_CELoss: 1.5295 DKDLoss: 0.8553 
2022-05-02 22:06:39 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.0211, stu_CELoss: 1.2231 DKDLoss: 0.7980 
2022-05-02 22:07:10 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 2.0309, stu_CELoss: 1.3184 DKDLoss: 0.7125 
2022-05-02 22:07:42 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.0664, stu_CELoss: 1.2734 DKDLoss: 0.7930 
2022-05-02 22:08:14 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.0460, stu_CELoss: 1.2385 DKDLoss: 0.8074 
2022-05-02 22:08:46 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.9746, stu_CELoss: 1.1606 DKDLoss: 0.8140 
2022-05-02 22:09:18 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.0730, stu_CELoss: 1.2930 DKDLoss: 0.7800 
2022-05-02 22:09:50 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.1507, stu_CELoss: 1.2544 DKDLoss: 0.8963 
2022-05-02 22:10:21 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.1269, stu_CELoss: 1.2550 DKDLoss: 0.8719 
2022-05-02 22:10:53 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 2.1008, stu_CELoss: 1.2489 DKDLoss: 0.8519 
2022-05-02 22:10:54 - train: epoch 056, train_loss: 2.0098
2022-05-02 22:13:20 - eval: epoch: 056, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.368%, stu_acc5: 91.074%, stu_test_loss: 1.0958
2022-05-02 22:13:21 - until epoch: 056, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 22:13:21 - epoch 057 lr: 0.010000000000000002
2022-05-02 22:13:59 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.8602, stu_CELoss: 1.1466 DKDLoss: 0.7136 
2022-05-02 22:14:31 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.1401, stu_CELoss: 1.3890 DKDLoss: 0.7512 
2022-05-02 22:15:02 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.9864, stu_CELoss: 1.2011 DKDLoss: 0.7854 
2022-05-02 22:15:34 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 2.1357, stu_CELoss: 1.3248 DKDLoss: 0.8109 
2022-05-02 22:16:05 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.7087, stu_CELoss: 1.0136 DKDLoss: 0.6950 
2022-05-02 22:16:37 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.9131, stu_CELoss: 1.1708 DKDLoss: 0.7424 
2022-05-02 22:17:07 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.6092, stu_CELoss: 0.9549 DKDLoss: 0.6544 
2022-05-02 22:17:39 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.9100, stu_CELoss: 1.1670 DKDLoss: 0.7430 
2022-05-02 22:18:11 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 2.0915, stu_CELoss: 1.2129 DKDLoss: 0.8786 
2022-05-02 22:18:43 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.8699, stu_CELoss: 1.0756 DKDLoss: 0.7943 
2022-05-02 22:19:15 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.9650, stu_CELoss: 1.1963 DKDLoss: 0.7688 
2022-05-02 22:19:47 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.7843, stu_CELoss: 1.0017 DKDLoss: 0.7826 
2022-05-02 22:20:18 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.8852, stu_CELoss: 1.1309 DKDLoss: 0.7543 
2022-05-02 22:20:50 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.8485, stu_CELoss: 1.1280 DKDLoss: 0.7205 
2022-05-02 22:21:21 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.9037, stu_CELoss: 1.1449 DKDLoss: 0.7588 
2022-05-02 22:21:52 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.0623, stu_CELoss: 1.2360 DKDLoss: 0.8263 
2022-05-02 22:22:24 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.2640, stu_CELoss: 1.4231 DKDLoss: 0.8409 
2022-05-02 22:22:57 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.2728, stu_CELoss: 1.3278 DKDLoss: 0.9450 
2022-05-02 22:23:30 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.0939, stu_CELoss: 1.2965 DKDLoss: 0.7975 
2022-05-02 22:24:04 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.9497, stu_CELoss: 1.1919 DKDLoss: 0.7578 
2022-05-02 22:24:37 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 2.0212, stu_CELoss: 1.2155 DKDLoss: 0.8058 
2022-05-02 22:25:11 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.0773, stu_CELoss: 1.2329 DKDLoss: 0.8444 
2022-05-02 22:25:45 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.9429, stu_CELoss: 1.1255 DKDLoss: 0.8175 
2022-05-02 22:26:18 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.8429, stu_CELoss: 1.0368 DKDLoss: 0.8061 
2022-05-02 22:26:52 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.1755, stu_CELoss: 1.2745 DKDLoss: 0.9010 
2022-05-02 22:27:26 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.8619, stu_CELoss: 1.1279 DKDLoss: 0.7340 
2022-05-02 22:27:59 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.7393, stu_CELoss: 0.9441 DKDLoss: 0.7952 
2022-05-02 22:28:33 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.8030, stu_CELoss: 1.0784 DKDLoss: 0.7246 
2022-05-02 22:29:06 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.1860, stu_CELoss: 1.3040 DKDLoss: 0.8820 
2022-05-02 22:29:40 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.3752, stu_CELoss: 1.4502 DKDLoss: 0.9251 
2022-05-02 22:30:14 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.1249, stu_CELoss: 1.2387 DKDLoss: 0.8863 
2022-05-02 22:30:47 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.0493, stu_CELoss: 1.2403 DKDLoss: 0.8090 
2022-05-02 22:31:21 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.0368, stu_CELoss: 1.2218 DKDLoss: 0.8150 
2022-05-02 22:31:55 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.0315, stu_CELoss: 1.1560 DKDLoss: 0.8755 
2022-05-02 22:32:29 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.1794, stu_CELoss: 1.3450 DKDLoss: 0.8344 
2022-05-02 22:33:03 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.0098, stu_CELoss: 1.2126 DKDLoss: 0.7972 
2022-05-02 22:33:37 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.9811, stu_CELoss: 1.2871 DKDLoss: 0.6939 
2022-05-02 22:34:11 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.0628, stu_CELoss: 1.1968 DKDLoss: 0.8660 
2022-05-02 22:34:45 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.2422, stu_CELoss: 1.4198 DKDLoss: 0.8224 
2022-05-02 22:35:19 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.8487, stu_CELoss: 1.1246 DKDLoss: 0.7241 
2022-05-02 22:35:53 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.1889, stu_CELoss: 1.4451 DKDLoss: 0.7438 
2022-05-02 22:36:27 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 2.0007, stu_CELoss: 1.2823 DKDLoss: 0.7184 
2022-05-02 22:37:00 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.7807, stu_CELoss: 1.0253 DKDLoss: 0.7555 
2022-05-02 22:37:34 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.0112, stu_CELoss: 1.2649 DKDLoss: 0.7463 
2022-05-02 22:38:08 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.0341, stu_CELoss: 1.2014 DKDLoss: 0.8327 
2022-05-02 22:38:42 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.9025, stu_CELoss: 1.1080 DKDLoss: 0.7945 
2022-05-02 22:39:16 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.9499, stu_CELoss: 1.1333 DKDLoss: 0.8166 
2022-05-02 22:39:50 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.1988, stu_CELoss: 1.3754 DKDLoss: 0.8233 
2022-05-02 22:40:23 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.2409, stu_CELoss: 1.3570 DKDLoss: 0.8838 
2022-05-02 22:40:56 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.0178, stu_CELoss: 1.1872 DKDLoss: 0.8306 
2022-05-02 22:40:58 - train: epoch 057, train_loss: 2.0050
2022-05-02 22:43:30 - eval: epoch: 057, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 71.752%, stu_acc5: 90.816%, stu_test_loss: 1.1242
2022-05-02 22:43:30 - until epoch: 057, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 22:43:30 - epoch 058 lr: 0.010000000000000002
2022-05-02 22:44:10 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.8260, stu_CELoss: 1.1151 DKDLoss: 0.7109 
2022-05-02 22:44:44 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.0828, stu_CELoss: 1.2710 DKDLoss: 0.8118 
2022-05-02 22:45:17 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.9055, stu_CELoss: 1.1693 DKDLoss: 0.7362 
2022-05-02 22:45:51 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.9588, stu_CELoss: 1.1968 DKDLoss: 0.7620 
2022-05-02 22:46:24 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.8091, stu_CELoss: 1.0594 DKDLoss: 0.7497 
2022-05-02 22:46:58 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.2000, stu_CELoss: 1.3495 DKDLoss: 0.8504 
2022-05-02 22:47:31 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 2.0293, stu_CELoss: 1.2154 DKDLoss: 0.8139 
2022-05-02 22:48:05 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.9893, stu_CELoss: 1.2454 DKDLoss: 0.7440 
2022-05-02 22:48:38 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.7678, stu_CELoss: 1.0758 DKDLoss: 0.6921 
2022-05-02 22:49:12 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.8451, stu_CELoss: 1.0404 DKDLoss: 0.8047 
2022-05-02 22:49:45 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.7187, stu_CELoss: 1.0313 DKDLoss: 0.6874 
2022-05-02 22:50:19 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.7936, stu_CELoss: 1.0749 DKDLoss: 0.7187 
2022-05-02 22:50:53 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.3193, stu_CELoss: 1.3711 DKDLoss: 0.9482 
2022-05-02 22:51:27 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.0518, stu_CELoss: 1.2349 DKDLoss: 0.8170 
2022-05-02 22:52:01 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.8088, stu_CELoss: 1.0514 DKDLoss: 0.7575 
2022-05-02 22:52:34 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.8947, stu_CELoss: 1.1375 DKDLoss: 0.7572 
2022-05-02 22:53:08 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.1281, stu_CELoss: 1.2531 DKDLoss: 0.8750 
2022-05-02 22:53:42 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.2632, stu_CELoss: 1.3879 DKDLoss: 0.8753 
2022-05-02 22:54:16 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.0392, stu_CELoss: 1.1558 DKDLoss: 0.8834 
2022-05-02 22:54:49 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.4462, stu_CELoss: 1.4743 DKDLoss: 0.9719 
2022-05-02 22:55:23 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.8835, stu_CELoss: 1.1442 DKDLoss: 0.7393 
2022-05-02 22:55:57 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 2.1162, stu_CELoss: 1.2690 DKDLoss: 0.8471 
2022-05-02 22:56:31 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.9636, stu_CELoss: 1.2401 DKDLoss: 0.7235 
2022-05-02 22:57:05 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.9488, stu_CELoss: 1.1267 DKDLoss: 0.8221 
2022-05-02 22:57:39 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.9204, stu_CELoss: 1.1256 DKDLoss: 0.7948 
2022-05-02 22:58:13 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.8855, stu_CELoss: 1.1363 DKDLoss: 0.7492 
2022-05-02 22:58:47 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.1510, stu_CELoss: 1.3164 DKDLoss: 0.8346 
2022-05-02 22:59:21 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.9046, stu_CELoss: 1.1387 DKDLoss: 0.7659 
2022-05-02 22:59:55 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.7426, stu_CELoss: 0.9708 DKDLoss: 0.7718 
2022-05-02 23:00:29 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.0936, stu_CELoss: 1.2943 DKDLoss: 0.7993 
2022-05-02 23:01:02 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 2.1326, stu_CELoss: 1.2251 DKDLoss: 0.9076 
2022-05-02 23:01:36 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.8296, stu_CELoss: 1.1140 DKDLoss: 0.7156 
2022-05-02 23:02:10 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 2.0431, stu_CELoss: 1.2093 DKDLoss: 0.8337 
2022-05-02 23:02:44 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.9348, stu_CELoss: 1.0783 DKDLoss: 0.8565 
2022-05-02 23:03:18 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 2.0607, stu_CELoss: 1.2652 DKDLoss: 0.7955 
2022-05-02 23:03:52 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.9632, stu_CELoss: 1.1887 DKDLoss: 0.7745 
2022-05-02 23:04:25 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.0767, stu_CELoss: 1.2614 DKDLoss: 0.8152 
2022-05-02 23:04:59 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.0896, stu_CELoss: 1.2306 DKDLoss: 0.8590 
2022-05-02 23:05:33 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.8922, stu_CELoss: 1.1707 DKDLoss: 0.7215 
2022-05-02 23:06:07 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.8469, stu_CELoss: 1.0685 DKDLoss: 0.7783 
2022-05-02 23:06:41 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.8823, stu_CELoss: 1.0780 DKDLoss: 0.8043 
2022-05-02 23:07:15 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.8190, stu_CELoss: 1.0864 DKDLoss: 0.7326 
2022-05-02 23:07:49 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.2070, stu_CELoss: 1.3567 DKDLoss: 0.8503 
2022-05-02 23:08:23 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 2.0271, stu_CELoss: 1.1700 DKDLoss: 0.8571 
2022-05-02 23:08:57 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.9619, stu_CELoss: 1.2285 DKDLoss: 0.7334 
2022-05-02 23:09:31 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.9405, stu_CELoss: 1.1729 DKDLoss: 0.7676 
2022-05-02 23:10:05 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.9291, stu_CELoss: 1.1069 DKDLoss: 0.8222 
2022-05-02 23:10:39 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.2277, stu_CELoss: 1.2991 DKDLoss: 0.9286 
2022-05-02 23:11:12 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.9002, stu_CELoss: 1.1551 DKDLoss: 0.7451 
2022-05-02 23:11:46 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.7569, stu_CELoss: 1.0225 DKDLoss: 0.7344 
2022-05-02 23:11:47 - train: epoch 058, train_loss: 2.0012
2022-05-02 23:14:20 - eval: epoch: 058, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.420%, stu_acc5: 91.004%, stu_test_loss: 1.1026
2022-05-02 23:14:20 - until epoch: 058, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 23:14:20 - epoch 059 lr: 0.010000000000000002
2022-05-02 23:15:01 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.1133, stu_CELoss: 1.3750 DKDLoss: 0.7383 
2022-05-02 23:15:34 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 2.1679, stu_CELoss: 1.2873 DKDLoss: 0.8805 
2022-05-02 23:16:07 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.9191, stu_CELoss: 1.1357 DKDLoss: 0.7834 
2022-05-02 23:16:40 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.9125, stu_CELoss: 1.2320 DKDLoss: 0.6805 
2022-05-02 23:17:14 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.3379, stu_CELoss: 1.4441 DKDLoss: 0.8938 
2022-05-02 23:17:48 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 2.1368, stu_CELoss: 1.3156 DKDLoss: 0.8212 
2022-05-02 23:18:21 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.9148, stu_CELoss: 1.1753 DKDLoss: 0.7395 
2022-05-02 23:18:55 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.8996, stu_CELoss: 1.1238 DKDLoss: 0.7757 
2022-05-02 23:19:29 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.9277, stu_CELoss: 1.1525 DKDLoss: 0.7753 
2022-05-02 23:20:03 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.1465, stu_CELoss: 1.2879 DKDLoss: 0.8586 
2022-05-02 23:20:37 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.0552, stu_CELoss: 1.2782 DKDLoss: 0.7770 
2022-05-02 23:21:11 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.9002, stu_CELoss: 1.1936 DKDLoss: 0.7066 
2022-05-02 23:21:45 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.1104, stu_CELoss: 1.3232 DKDLoss: 0.7872 
2022-05-02 23:22:19 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.1146, stu_CELoss: 1.3174 DKDLoss: 0.7972 
2022-05-02 23:22:53 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.7844, stu_CELoss: 1.0507 DKDLoss: 0.7338 
2022-05-02 23:23:26 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.8480, stu_CELoss: 1.0861 DKDLoss: 0.7619 
2022-05-02 23:24:00 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.7866, stu_CELoss: 1.0859 DKDLoss: 0.7007 
2022-05-02 23:24:34 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.8646, stu_CELoss: 1.1193 DKDLoss: 0.7453 
2022-05-02 23:25:08 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.9056, stu_CELoss: 1.1656 DKDLoss: 0.7400 
2022-05-02 23:25:41 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.6090, stu_CELoss: 0.8979 DKDLoss: 0.7110 
2022-05-02 23:26:15 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.0286, stu_CELoss: 1.2116 DKDLoss: 0.8170 
2022-05-02 23:26:49 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 2.0125, stu_CELoss: 1.2258 DKDLoss: 0.7867 
2022-05-02 23:27:22 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.9937, stu_CELoss: 1.2081 DKDLoss: 0.7857 
2022-05-02 23:27:56 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.1194, stu_CELoss: 1.2826 DKDLoss: 0.8368 
2022-05-02 23:28:30 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.2822, stu_CELoss: 1.4451 DKDLoss: 0.8371 
2022-05-02 23:29:04 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.8957, stu_CELoss: 1.1320 DKDLoss: 0.7637 
2022-05-02 23:29:37 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.8899, stu_CELoss: 1.0775 DKDLoss: 0.8124 
2022-05-02 23:30:11 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.2169, stu_CELoss: 1.3356 DKDLoss: 0.8813 
2022-05-02 23:30:45 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.9963, stu_CELoss: 1.1742 DKDLoss: 0.8221 
2022-05-02 23:31:19 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.2846, stu_CELoss: 1.4992 DKDLoss: 0.7854 
2022-05-02 23:31:53 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 2.0071, stu_CELoss: 1.1445 DKDLoss: 0.8627 
2022-05-02 23:32:27 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.1591, stu_CELoss: 1.3482 DKDLoss: 0.8109 
2022-05-02 23:33:01 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.0079, stu_CELoss: 1.2046 DKDLoss: 0.8033 
2022-05-02 23:33:35 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.3911, stu_CELoss: 1.4740 DKDLoss: 0.9171 
2022-05-02 23:34:09 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.8740, stu_CELoss: 1.0989 DKDLoss: 0.7751 
2022-05-02 23:34:43 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.1456, stu_CELoss: 1.3163 DKDLoss: 0.8293 
2022-05-02 23:35:17 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.0442, stu_CELoss: 1.1759 DKDLoss: 0.8683 
2022-05-02 23:35:51 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 2.0090, stu_CELoss: 1.2611 DKDLoss: 0.7479 
2022-05-02 23:36:25 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.9470, stu_CELoss: 1.1300 DKDLoss: 0.8170 
2022-05-02 23:36:59 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.1369, stu_CELoss: 1.3447 DKDLoss: 0.7922 
2022-05-02 23:37:33 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.8539, stu_CELoss: 1.0738 DKDLoss: 0.7802 
2022-05-02 23:38:07 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.0086, stu_CELoss: 1.1545 DKDLoss: 0.8541 
2022-05-02 23:38:41 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.3189, stu_CELoss: 1.4109 DKDLoss: 0.9080 
2022-05-02 23:39:15 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.0450, stu_CELoss: 1.1827 DKDLoss: 0.8623 
2022-05-02 23:39:49 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.1906, stu_CELoss: 1.2772 DKDLoss: 0.9134 
2022-05-02 23:40:23 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.1684, stu_CELoss: 1.3080 DKDLoss: 0.8605 
2022-05-02 23:40:57 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 2.0340, stu_CELoss: 1.2643 DKDLoss: 0.7698 
2022-05-02 23:41:31 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.7532, stu_CELoss: 1.0713 DKDLoss: 0.6819 
2022-05-02 23:42:05 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.9517, stu_CELoss: 1.1574 DKDLoss: 0.7943 
2022-05-02 23:42:38 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.2127, stu_CELoss: 1.3561 DKDLoss: 0.8567 
2022-05-02 23:42:40 - train: epoch 059, train_loss: 1.9993
2022-05-02 23:45:12 - eval: epoch: 059, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.080%, stu_acc5: 91.180%, stu_test_loss: 1.1035
2022-05-02 23:45:12 - until epoch: 059, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-02 23:45:12 - epoch 060 lr: 0.010000000000000002
2022-05-02 23:45:53 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.0614, stu_CELoss: 1.2641 DKDLoss: 0.7973 
2022-05-02 23:46:26 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.8402, stu_CELoss: 1.0522 DKDLoss: 0.7880 
2022-05-02 23:46:59 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.7415, stu_CELoss: 0.9766 DKDLoss: 0.7650 
2022-05-02 23:47:33 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.3417, stu_CELoss: 1.4502 DKDLoss: 0.8915 
2022-05-02 23:48:06 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.3423, stu_CELoss: 1.4676 DKDLoss: 0.8746 
2022-05-02 23:48:40 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.1748, stu_CELoss: 1.3814 DKDLoss: 0.7934 
2022-05-02 23:49:13 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.2048, stu_CELoss: 1.3332 DKDLoss: 0.8716 
2022-05-02 23:49:47 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.1617, stu_CELoss: 1.2710 DKDLoss: 0.8907 
2022-05-02 23:50:20 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 2.0463, stu_CELoss: 1.2617 DKDLoss: 0.7846 
2022-05-02 23:50:53 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.7633, stu_CELoss: 0.9789 DKDLoss: 0.7844 
2022-05-02 23:51:27 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.9566, stu_CELoss: 1.1366 DKDLoss: 0.8200 
2022-05-02 23:52:01 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.8082, stu_CELoss: 1.0825 DKDLoss: 0.7257 
2022-05-02 23:52:35 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.9140, stu_CELoss: 1.1585 DKDLoss: 0.7555 
2022-05-02 23:53:08 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.9909, stu_CELoss: 1.2310 DKDLoss: 0.7599 
2022-05-02 23:53:42 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.8651, stu_CELoss: 1.0577 DKDLoss: 0.8075 
2022-05-02 23:54:16 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 2.0508, stu_CELoss: 1.2147 DKDLoss: 0.8361 
2022-05-02 23:54:50 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.7830, stu_CELoss: 1.0173 DKDLoss: 0.7658 
2022-05-02 23:55:24 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.0497, stu_CELoss: 1.2234 DKDLoss: 0.8263 
2022-05-02 23:55:58 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.0830, stu_CELoss: 1.1602 DKDLoss: 0.9228 
2022-05-02 23:56:32 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.8150, stu_CELoss: 1.1431 DKDLoss: 0.6719 
2022-05-02 23:57:06 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.0983, stu_CELoss: 1.2970 DKDLoss: 0.8012 
2022-05-02 23:57:40 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.0152, stu_CELoss: 1.1993 DKDLoss: 0.8159 
2022-05-02 23:58:14 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7964, stu_CELoss: 1.0782 DKDLoss: 0.7181 
2022-05-02 23:58:48 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.9701, stu_CELoss: 1.1687 DKDLoss: 0.8014 
2022-05-02 23:59:22 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.0057, stu_CELoss: 1.1720 DKDLoss: 0.8336 
2022-05-02 23:59:56 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.9405, stu_CELoss: 1.1901 DKDLoss: 0.7504 
2022-05-03 00:00:29 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.9502, stu_CELoss: 1.1760 DKDLoss: 0.7742 
2022-05-03 00:01:04 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.9002, stu_CELoss: 1.1694 DKDLoss: 0.7309 
2022-05-03 00:01:38 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 2.0606, stu_CELoss: 1.2252 DKDLoss: 0.8353 
2022-05-03 00:02:12 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.1142, stu_CELoss: 1.2316 DKDLoss: 0.8826 
2022-05-03 00:02:45 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.1801, stu_CELoss: 1.3405 DKDLoss: 0.8396 
2022-05-03 00:03:19 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.9761, stu_CELoss: 1.2164 DKDLoss: 0.7597 
2022-05-03 00:03:53 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.6510, stu_CELoss: 0.9688 DKDLoss: 0.6822 
2022-05-03 00:04:27 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.9751, stu_CELoss: 1.1864 DKDLoss: 0.7887 
2022-05-03 00:05:01 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.1506, stu_CELoss: 1.3105 DKDLoss: 0.8401 
2022-05-03 00:05:35 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.9618, stu_CELoss: 1.2110 DKDLoss: 0.7508 
2022-05-03 00:06:09 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.1630, stu_CELoss: 1.2612 DKDLoss: 0.9019 
2022-05-03 00:06:43 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.1117, stu_CELoss: 1.3261 DKDLoss: 0.7856 
2022-05-03 00:07:17 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.0790, stu_CELoss: 1.3045 DKDLoss: 0.7745 
2022-05-03 00:07:51 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 2.2053, stu_CELoss: 1.3233 DKDLoss: 0.8820 
2022-05-03 00:08:25 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.0721, stu_CELoss: 1.3528 DKDLoss: 0.7193 
2022-05-03 00:08:59 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.1068, stu_CELoss: 1.1842 DKDLoss: 0.9226 
2022-05-03 00:09:33 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.7617, stu_CELoss: 1.0726 DKDLoss: 0.6891 
2022-05-03 00:10:07 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.0255, stu_CELoss: 1.2320 DKDLoss: 0.7935 
2022-05-03 00:10:41 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.0018, stu_CELoss: 1.2536 DKDLoss: 0.7482 
2022-05-03 00:11:15 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 2.0414, stu_CELoss: 1.1880 DKDLoss: 0.8534 
2022-05-03 00:11:49 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.7939, stu_CELoss: 1.0064 DKDLoss: 0.7875 
2022-05-03 00:12:23 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.7778, stu_CELoss: 1.0919 DKDLoss: 0.6859 
2022-05-03 00:12:57 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.8021, stu_CELoss: 1.1004 DKDLoss: 0.7017 
2022-05-03 00:13:30 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.1562, stu_CELoss: 1.3382 DKDLoss: 0.8179 
2022-05-03 00:13:32 - train: epoch 060, train_loss: 1.9932
2022-05-03 00:16:04 - eval: epoch: 060, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 72.470%, stu_acc5: 91.238%, stu_test_loss: 1.0867
2022-05-03 00:16:05 - until epoch: 060, tea_best_acc1: 78.330%, stu_best_acc1: 72.966%
2022-05-03 00:16:05 - epoch 061 lr: 0.0010000000000000002
2022-05-03 00:16:45 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.7714, stu_CELoss: 1.0815 DKDLoss: 0.6900 
2022-05-03 00:17:18 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.8398, stu_CELoss: 1.1465 DKDLoss: 0.6933 
2022-05-03 00:17:52 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.6155, stu_CELoss: 1.0109 DKDLoss: 0.6047 
2022-05-03 00:18:25 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.0811, stu_CELoss: 1.4270 DKDLoss: 0.6541 
2022-05-03 00:18:59 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.5816, stu_CELoss: 0.9517 DKDLoss: 0.6299 
2022-05-03 00:19:32 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.7966, stu_CELoss: 1.1349 DKDLoss: 0.6617 
2022-05-03 00:20:06 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.5299, stu_CELoss: 0.9023 DKDLoss: 0.6276 
2022-05-03 00:20:39 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.6095, stu_CELoss: 1.0607 DKDLoss: 0.5487 
2022-05-03 00:21:12 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.5375, stu_CELoss: 0.8850 DKDLoss: 0.6525 
2022-05-03 00:21:46 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.7427, stu_CELoss: 1.0282 DKDLoss: 0.7145 
2022-05-03 00:22:19 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.4697, stu_CELoss: 0.8455 DKDLoss: 0.6242 
2022-05-03 00:22:52 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.6024, stu_CELoss: 1.0028 DKDLoss: 0.5997 
2022-05-03 00:23:26 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.5632, stu_CELoss: 0.9733 DKDLoss: 0.5899 
2022-05-03 00:23:59 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.4907, stu_CELoss: 0.9439 DKDLoss: 0.5467 
2022-05-03 00:24:33 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.7078, stu_CELoss: 1.1097 DKDLoss: 0.5981 
2022-05-03 00:25:06 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.5359, stu_CELoss: 0.9886 DKDLoss: 0.5473 
2022-05-03 00:25:39 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.8598, stu_CELoss: 1.2270 DKDLoss: 0.6329 
2022-05-03 00:26:13 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.7108, stu_CELoss: 1.1091 DKDLoss: 0.6017 
2022-05-03 00:26:47 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.9628, stu_CELoss: 1.2809 DKDLoss: 0.6819 
2022-05-03 00:27:21 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.6772, stu_CELoss: 1.0629 DKDLoss: 0.6143 
2022-05-03 00:27:54 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.9062, stu_CELoss: 1.2772 DKDLoss: 0.6289 
2022-05-03 00:28:29 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.4575, stu_CELoss: 0.8848 DKDLoss: 0.5727 
2022-05-03 00:29:02 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.5127, stu_CELoss: 1.0292 DKDLoss: 0.4836 
2022-05-03 00:29:36 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.3808, stu_CELoss: 0.8353 DKDLoss: 0.5455 
2022-05-03 00:30:09 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.5737, stu_CELoss: 0.8950 DKDLoss: 0.6787 
2022-05-03 00:30:43 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.5771, stu_CELoss: 0.9596 DKDLoss: 0.6175 
2022-05-03 00:31:17 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.7452, stu_CELoss: 1.1901 DKDLoss: 0.5551 
2022-05-03 00:31:51 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.4969, stu_CELoss: 0.9718 DKDLoss: 0.5251 
2022-05-03 00:32:25 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4515, stu_CELoss: 0.9267 DKDLoss: 0.5248 
2022-05-03 00:32:59 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.5315, stu_CELoss: 0.9490 DKDLoss: 0.5826 
2022-05-03 00:33:33 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.6123, stu_CELoss: 1.0132 DKDLoss: 0.5990 
2022-05-03 00:34:07 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.6717, stu_CELoss: 0.9826 DKDLoss: 0.6891 
2022-05-03 00:34:40 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.5161, stu_CELoss: 0.9465 DKDLoss: 0.5696 
2022-05-03 00:35:14 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.4116, stu_CELoss: 0.9059 DKDLoss: 0.5058 
2022-05-03 00:35:48 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.8253, stu_CELoss: 1.1222 DKDLoss: 0.7032 
2022-05-03 00:36:22 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.6258, stu_CELoss: 1.1130 DKDLoss: 0.5127 
2022-05-03 00:36:56 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.4822, stu_CELoss: 0.8953 DKDLoss: 0.5868 
2022-05-03 00:37:30 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.7395, stu_CELoss: 1.1385 DKDLoss: 0.6010 
2022-05-03 00:38:04 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.6823, stu_CELoss: 1.1060 DKDLoss: 0.5763 
2022-05-03 00:38:38 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.6371, stu_CELoss: 1.1289 DKDLoss: 0.5082 
2022-05-03 00:39:12 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.7464, stu_CELoss: 1.1377 DKDLoss: 0.6087 
2022-05-03 00:39:46 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.5535, stu_CELoss: 1.0052 DKDLoss: 0.5483 
2022-05-03 00:40:20 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.5088, stu_CELoss: 0.9410 DKDLoss: 0.5678 
2022-05-03 00:40:53 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.6321, stu_CELoss: 0.9942 DKDLoss: 0.6379 
2022-05-03 00:41:28 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.5964, stu_CELoss: 1.0115 DKDLoss: 0.5849 
2022-05-03 00:42:02 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.4896, stu_CELoss: 0.9227 DKDLoss: 0.5669 
2022-05-03 00:42:36 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.5990, stu_CELoss: 1.0407 DKDLoss: 0.5583 
2022-05-03 00:43:10 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.6195, stu_CELoss: 1.0555 DKDLoss: 0.5640 
2022-05-03 00:43:44 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.5622, stu_CELoss: 1.0316 DKDLoss: 0.5305 
2022-05-03 00:44:17 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.7331, stu_CELoss: 1.1168 DKDLoss: 0.6163 
2022-05-03 00:44:19 - train: epoch 061, train_loss: 1.6275
2022-05-03 00:46:50 - eval: epoch: 061, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.014%, stu_acc5: 92.874%, stu_test_loss: 0.9447
2022-05-03 00:46:50 - until epoch: 061, tea_best_acc1: 78.330%, stu_best_acc1: 76.014%
2022-05-03 00:46:50 - epoch 062 lr: 0.0010000000000000002
2022-05-03 00:47:31 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.7572, stu_CELoss: 1.2151 DKDLoss: 0.5421 
2022-05-03 00:48:04 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.8227, stu_CELoss: 1.2771 DKDLoss: 0.5456 
2022-05-03 00:48:37 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.4795, stu_CELoss: 0.8645 DKDLoss: 0.6149 
2022-05-03 00:49:11 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.5421, stu_CELoss: 1.0008 DKDLoss: 0.5412 
2022-05-03 00:49:44 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.5351, stu_CELoss: 0.9400 DKDLoss: 0.5951 
2022-05-03 00:50:18 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.4135, stu_CELoss: 0.8887 DKDLoss: 0.5249 
2022-05-03 00:50:51 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.6160, stu_CELoss: 1.0387 DKDLoss: 0.5773 
2022-05-03 00:51:25 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.4435, stu_CELoss: 0.9133 DKDLoss: 0.5301 
2022-05-03 00:51:58 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.7118, stu_CELoss: 1.0949 DKDLoss: 0.6169 
2022-05-03 00:52:32 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.6879, stu_CELoss: 1.1211 DKDLoss: 0.5668 
2022-05-03 00:53:05 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.4173, stu_CELoss: 0.8969 DKDLoss: 0.5205 
2022-05-03 00:53:39 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.6592, stu_CELoss: 1.0783 DKDLoss: 0.5809 
2022-05-03 00:54:13 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.4560, stu_CELoss: 0.9146 DKDLoss: 0.5414 
2022-05-03 00:54:46 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.4698, stu_CELoss: 0.9707 DKDLoss: 0.4991 
2022-05-03 00:55:20 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.5814, stu_CELoss: 1.0125 DKDLoss: 0.5688 
2022-05-03 00:55:54 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.7166, stu_CELoss: 1.1284 DKDLoss: 0.5882 
2022-05-03 00:56:27 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.3558, stu_CELoss: 0.8451 DKDLoss: 0.5107 
2022-05-03 00:57:01 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.3510, stu_CELoss: 0.8105 DKDLoss: 0.5405 
2022-05-03 00:57:35 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.2778, stu_CELoss: 0.7885 DKDLoss: 0.4893 
2022-05-03 00:58:09 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.5390, stu_CELoss: 1.0242 DKDLoss: 0.5149 
2022-05-03 00:58:43 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.7988, stu_CELoss: 1.1822 DKDLoss: 0.6167 
2022-05-03 00:59:17 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.4933, stu_CELoss: 0.9464 DKDLoss: 0.5469 
2022-05-03 00:59:50 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.4670, stu_CELoss: 0.9740 DKDLoss: 0.4930 
2022-05-03 01:00:24 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.6207, stu_CELoss: 1.0181 DKDLoss: 0.6026 
2022-05-03 01:00:58 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.6379, stu_CELoss: 1.0992 DKDLoss: 0.5388 
2022-05-03 01:01:32 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.5489, stu_CELoss: 1.0103 DKDLoss: 0.5386 
2022-05-03 01:02:06 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.4521, stu_CELoss: 0.9141 DKDLoss: 0.5380 
2022-05-03 01:02:39 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.6839, stu_CELoss: 1.1199 DKDLoss: 0.5640 
2022-05-03 01:03:13 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.4488, stu_CELoss: 0.9457 DKDLoss: 0.5031 
2022-05-03 01:03:46 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.5874, stu_CELoss: 1.0323 DKDLoss: 0.5551 
2022-05-03 01:04:20 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.7060, stu_CELoss: 1.1878 DKDLoss: 0.5183 
2022-05-03 01:04:54 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.3654, stu_CELoss: 0.8983 DKDLoss: 0.4671 
2022-05-03 01:05:27 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.6927, stu_CELoss: 1.0807 DKDLoss: 0.6120 
2022-05-03 01:06:01 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.5410, stu_CELoss: 1.0117 DKDLoss: 0.5293 
2022-05-03 01:06:35 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.5110, stu_CELoss: 0.9773 DKDLoss: 0.5337 
2022-05-03 01:07:08 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.5457, stu_CELoss: 1.0434 DKDLoss: 0.5023 
2022-05-03 01:07:42 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.6531, stu_CELoss: 1.0971 DKDLoss: 0.5560 
2022-05-03 01:08:16 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.6029, stu_CELoss: 0.9950 DKDLoss: 0.6079 
2022-05-03 01:08:49 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.6145, stu_CELoss: 1.0557 DKDLoss: 0.5588 
2022-05-03 01:09:23 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.3350, stu_CELoss: 0.7880 DKDLoss: 0.5470 
2022-05-03 01:09:57 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.3756, stu_CELoss: 0.9285 DKDLoss: 0.4471 
2022-05-03 01:10:30 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.5491, stu_CELoss: 0.9991 DKDLoss: 0.5500 
2022-05-03 01:11:04 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.4024, stu_CELoss: 0.9190 DKDLoss: 0.4834 
2022-05-03 01:11:37 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.3913, stu_CELoss: 0.8566 DKDLoss: 0.5347 
2022-05-03 01:12:11 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.4377, stu_CELoss: 0.9019 DKDLoss: 0.5358 
2022-05-03 01:12:45 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.4118, stu_CELoss: 0.9263 DKDLoss: 0.4855 
2022-05-03 01:13:19 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.4976, stu_CELoss: 1.0180 DKDLoss: 0.4796 
2022-05-03 01:13:52 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.5547, stu_CELoss: 1.0256 DKDLoss: 0.5291 
2022-05-03 01:14:26 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.5434, stu_CELoss: 1.0485 DKDLoss: 0.4948 
2022-05-03 01:15:00 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.7127, stu_CELoss: 1.1797 DKDLoss: 0.5330 
2022-05-03 01:15:01 - train: epoch 062, train_loss: 1.5302
2022-05-03 01:17:33 - eval: epoch: 062, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.132%, stu_acc5: 93.114%, stu_test_loss: 0.9314
2022-05-03 01:17:34 - until epoch: 062, tea_best_acc1: 78.330%, stu_best_acc1: 76.132%
2022-05-03 01:17:34 - epoch 063 lr: 0.0010000000000000002
2022-05-03 01:18:13 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.4874, stu_CELoss: 0.9636 DKDLoss: 0.5238 
2022-05-03 01:18:46 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.3509, stu_CELoss: 0.8364 DKDLoss: 0.5145 
2022-05-03 01:19:19 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.6279, stu_CELoss: 1.0539 DKDLoss: 0.5740 
2022-05-03 01:19:52 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.5905, stu_CELoss: 1.0862 DKDLoss: 0.5043 
2022-05-03 01:20:25 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.3782, stu_CELoss: 0.8556 DKDLoss: 0.5226 
2022-05-03 01:20:58 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.3512, stu_CELoss: 0.8487 DKDLoss: 0.5025 
2022-05-03 01:21:31 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.4158, stu_CELoss: 0.9041 DKDLoss: 0.5116 
2022-05-03 01:22:05 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.4033, stu_CELoss: 0.8999 DKDLoss: 0.5034 
2022-05-03 01:22:38 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.5635, stu_CELoss: 0.9933 DKDLoss: 0.5702 
2022-05-03 01:23:12 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.5568, stu_CELoss: 1.0570 DKDLoss: 0.4999 
2022-05-03 01:23:45 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.5812, stu_CELoss: 1.0489 DKDLoss: 0.5323 
2022-05-03 01:24:19 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.5703, stu_CELoss: 1.0246 DKDLoss: 0.5457 
2022-05-03 01:24:52 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.4321, stu_CELoss: 0.9332 DKDLoss: 0.4989 
2022-05-03 01:25:26 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.7229, stu_CELoss: 1.1903 DKDLoss: 0.5327 
2022-05-03 01:26:00 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.3580, stu_CELoss: 0.8365 DKDLoss: 0.5215 
2022-05-03 01:26:33 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.5155, stu_CELoss: 0.9491 DKDLoss: 0.5664 
2022-05-03 01:27:07 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.3356, stu_CELoss: 0.8570 DKDLoss: 0.4786 
2022-05-03 01:27:41 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.5493, stu_CELoss: 0.9894 DKDLoss: 0.5598 
2022-05-03 01:28:15 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.5060, stu_CELoss: 1.0051 DKDLoss: 0.5009 
2022-05-03 01:28:48 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.4226, stu_CELoss: 0.9101 DKDLoss: 0.5125 
2022-05-03 01:29:22 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.3609, stu_CELoss: 0.8587 DKDLoss: 0.5022 
2022-05-03 01:29:56 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.5526, stu_CELoss: 1.0252 DKDLoss: 0.5274 
2022-05-03 01:30:30 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.5601, stu_CELoss: 1.0283 DKDLoss: 0.5318 
2022-05-03 01:31:04 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.4762, stu_CELoss: 0.9415 DKDLoss: 0.5347 
2022-05-03 01:31:38 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.4589, stu_CELoss: 0.9679 DKDLoss: 0.4910 
2022-05-03 01:32:12 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.5423, stu_CELoss: 1.0644 DKDLoss: 0.4779 
2022-05-03 01:32:46 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.7538, stu_CELoss: 1.1714 DKDLoss: 0.5824 
2022-05-03 01:33:20 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.6597, stu_CELoss: 1.1266 DKDLoss: 0.5331 
2022-05-03 01:33:53 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.8418, stu_CELoss: 1.2209 DKDLoss: 0.6209 
2022-05-03 01:34:27 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.5220, stu_CELoss: 0.9404 DKDLoss: 0.5816 
2022-05-03 01:35:01 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.4459, stu_CELoss: 0.9210 DKDLoss: 0.5250 
2022-05-03 01:35:34 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.7200, stu_CELoss: 1.2124 DKDLoss: 0.5075 
2022-05-03 01:36:08 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.4044, stu_CELoss: 0.9213 DKDLoss: 0.4831 
2022-05-03 01:36:42 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.4035, stu_CELoss: 0.8836 DKDLoss: 0.5199 
2022-05-03 01:37:15 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.6170, stu_CELoss: 1.0650 DKDLoss: 0.5520 
2022-05-03 01:37:49 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.4691, stu_CELoss: 0.9699 DKDLoss: 0.4992 
2022-05-03 01:38:23 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.4916, stu_CELoss: 0.9720 DKDLoss: 0.5195 
2022-05-03 01:38:57 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.4313, stu_CELoss: 0.9062 DKDLoss: 0.5251 
2022-05-03 01:39:31 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.1991, stu_CELoss: 0.7942 DKDLoss: 0.4049 
2022-05-03 01:40:05 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.3927, stu_CELoss: 0.9269 DKDLoss: 0.4658 
2022-05-03 01:40:39 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.5486, stu_CELoss: 1.0216 DKDLoss: 0.5270 
2022-05-03 01:41:13 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.5459, stu_CELoss: 1.0307 DKDLoss: 0.5152 
2022-05-03 01:41:47 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.5730, stu_CELoss: 1.0495 DKDLoss: 0.5235 
2022-05-03 01:42:21 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.6034, stu_CELoss: 1.0877 DKDLoss: 0.5156 
2022-05-03 01:42:55 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.7235, stu_CELoss: 1.2038 DKDLoss: 0.5197 
2022-05-03 01:43:29 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.5128, stu_CELoss: 0.9438 DKDLoss: 0.5691 
2022-05-03 01:44:03 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.3788, stu_CELoss: 0.9038 DKDLoss: 0.4749 
2022-05-03 01:44:38 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.4149, stu_CELoss: 0.9350 DKDLoss: 0.4798 
2022-05-03 01:45:12 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.4770, stu_CELoss: 0.9852 DKDLoss: 0.4918 
2022-05-03 01:45:45 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.5450, stu_CELoss: 1.0413 DKDLoss: 0.5036 
2022-05-03 01:45:47 - train: epoch 063, train_loss: 1.4920
2022-05-03 01:48:18 - eval: epoch: 063, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.452%, stu_acc5: 93.262%, stu_test_loss: 0.9228
2022-05-03 01:48:19 - until epoch: 063, tea_best_acc1: 78.330%, stu_best_acc1: 76.452%
2022-05-03 01:48:19 - epoch 064 lr: 0.0010000000000000002
2022-05-03 01:48:59 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.2754, stu_CELoss: 0.7707 DKDLoss: 0.5047 
2022-05-03 01:49:32 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3972, stu_CELoss: 0.8826 DKDLoss: 0.5146 
2022-05-03 01:50:06 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.5824, stu_CELoss: 1.0138 DKDLoss: 0.5686 
2022-05-03 01:50:39 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.3335, stu_CELoss: 0.9172 DKDLoss: 0.4163 
2022-05-03 01:51:13 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.2944, stu_CELoss: 0.8330 DKDLoss: 0.4614 
2022-05-03 01:51:47 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.4631, stu_CELoss: 0.9642 DKDLoss: 0.4989 
2022-05-03 01:52:20 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.6150, stu_CELoss: 1.1146 DKDLoss: 0.5004 
2022-05-03 01:52:54 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.4263, stu_CELoss: 0.9230 DKDLoss: 0.5034 
2022-05-03 01:53:28 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.3523, stu_CELoss: 0.7841 DKDLoss: 0.5681 
2022-05-03 01:54:02 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.3155, stu_CELoss: 0.8218 DKDLoss: 0.4937 
2022-05-03 01:54:35 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.4732, stu_CELoss: 0.9098 DKDLoss: 0.5634 
2022-05-03 01:55:09 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.3138, stu_CELoss: 0.8648 DKDLoss: 0.4491 
2022-05-03 01:55:43 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.3573, stu_CELoss: 0.8657 DKDLoss: 0.4916 
2022-05-03 01:56:17 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.8157, stu_CELoss: 1.2648 DKDLoss: 0.5509 
2022-05-03 01:56:51 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.5293, stu_CELoss: 1.0250 DKDLoss: 0.5043 
2022-05-03 01:57:24 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.3575, stu_CELoss: 0.8535 DKDLoss: 0.5040 
2022-05-03 01:57:58 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.5290, stu_CELoss: 1.0271 DKDLoss: 0.5019 
2022-05-03 01:58:32 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.2995, stu_CELoss: 0.7643 DKDLoss: 0.5352 
2022-05-03 01:59:06 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.4171, stu_CELoss: 0.8883 DKDLoss: 0.5287 
2022-05-03 01:59:40 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.3020, stu_CELoss: 0.8260 DKDLoss: 0.4760 
2022-05-03 02:00:14 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.3778, stu_CELoss: 0.9107 DKDLoss: 0.4672 
2022-05-03 02:00:47 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.6986, stu_CELoss: 1.1726 DKDLoss: 0.5260 
2022-05-03 02:01:21 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.5510, stu_CELoss: 1.0452 DKDLoss: 0.5058 
2022-05-03 02:01:55 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.2942, stu_CELoss: 0.8045 DKDLoss: 0.4898 
2022-05-03 02:02:29 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.2545, stu_CELoss: 0.7711 DKDLoss: 0.4834 
2022-05-03 02:03:03 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.5254, stu_CELoss: 0.9997 DKDLoss: 0.5257 
2022-05-03 02:03:37 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.5675, stu_CELoss: 1.0486 DKDLoss: 0.5189 
2022-05-03 02:04:11 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.5363, stu_CELoss: 1.0159 DKDLoss: 0.5204 
2022-05-03 02:04:45 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.5117, stu_CELoss: 1.0278 DKDLoss: 0.4839 
2022-05-03 02:05:19 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.4283, stu_CELoss: 0.9529 DKDLoss: 0.4754 
2022-05-03 02:05:53 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.6221, stu_CELoss: 1.0895 DKDLoss: 0.5326 
2022-05-03 02:06:27 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.5099, stu_CELoss: 1.0136 DKDLoss: 0.4963 
2022-05-03 02:07:01 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.4582, stu_CELoss: 0.9789 DKDLoss: 0.4793 
2022-05-03 02:07:35 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.4736, stu_CELoss: 0.9291 DKDLoss: 0.5445 
2022-05-03 02:08:08 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.3717, stu_CELoss: 0.8761 DKDLoss: 0.4957 
2022-05-03 02:08:42 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.3292, stu_CELoss: 0.7788 DKDLoss: 0.5503 
2022-05-03 02:09:17 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.2590, stu_CELoss: 0.7829 DKDLoss: 0.4761 
2022-05-03 02:09:51 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.4043, stu_CELoss: 0.8906 DKDLoss: 0.5137 
2022-05-03 02:10:24 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.4346, stu_CELoss: 0.9519 DKDLoss: 0.4827 
2022-05-03 02:10:58 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.6139, stu_CELoss: 1.1012 DKDLoss: 0.5127 
2022-05-03 02:11:32 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.5321, stu_CELoss: 0.9525 DKDLoss: 0.5796 
2022-05-03 02:12:06 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.3842, stu_CELoss: 0.9312 DKDLoss: 0.4530 
2022-05-03 02:12:40 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.6204, stu_CELoss: 1.0684 DKDLoss: 0.5520 
2022-05-03 02:13:14 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.3487, stu_CELoss: 0.8854 DKDLoss: 0.4633 
2022-05-03 02:13:48 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.4474, stu_CELoss: 0.9185 DKDLoss: 0.5288 
2022-05-03 02:14:22 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.6867, stu_CELoss: 1.1446 DKDLoss: 0.5422 
2022-05-03 02:14:56 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.8487, stu_CELoss: 1.2691 DKDLoss: 0.5795 
2022-05-03 02:15:30 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.5056, stu_CELoss: 0.9491 DKDLoss: 0.5565 
2022-05-03 02:16:04 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.6758, stu_CELoss: 1.1389 DKDLoss: 0.5369 
2022-05-03 02:16:37 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.4083, stu_CELoss: 0.8819 DKDLoss: 0.5263 
2022-05-03 02:16:39 - train: epoch 064, train_loss: 1.4668
2022-05-03 02:19:10 - eval: epoch: 064, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.608%, stu_acc5: 93.272%, stu_test_loss: 0.9177
2022-05-03 02:19:11 - until epoch: 064, tea_best_acc1: 78.330%, stu_best_acc1: 76.608%
2022-05-03 02:19:11 - epoch 065 lr: 0.0010000000000000002
2022-05-03 02:19:51 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.5528, stu_CELoss: 1.0868 DKDLoss: 0.4660 
2022-05-03 02:20:24 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.5178, stu_CELoss: 0.9332 DKDLoss: 0.5846 
2022-05-03 02:20:57 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.4241, stu_CELoss: 0.9252 DKDLoss: 0.4989 
2022-05-03 02:21:30 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.3487, stu_CELoss: 0.8706 DKDLoss: 0.4781 
2022-05-03 02:22:04 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.5082, stu_CELoss: 0.9783 DKDLoss: 0.5300 
2022-05-03 02:22:38 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.4980, stu_CELoss: 1.0483 DKDLoss: 0.4496 
2022-05-03 02:23:11 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.7038, stu_CELoss: 1.1004 DKDLoss: 0.6033 
2022-05-03 02:23:45 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.4282, stu_CELoss: 0.9081 DKDLoss: 0.5201 
2022-05-03 02:24:18 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.6440, stu_CELoss: 1.1260 DKDLoss: 0.5180 
2022-05-03 02:24:52 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.5070, stu_CELoss: 1.0341 DKDLoss: 0.4729 
2022-05-03 02:25:25 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.4435, stu_CELoss: 0.9001 DKDLoss: 0.5434 
2022-05-03 02:25:59 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.4519, stu_CELoss: 0.9227 DKDLoss: 0.5292 
2022-05-03 02:26:32 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.3967, stu_CELoss: 0.9050 DKDLoss: 0.4917 
2022-05-03 02:27:06 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.3429, stu_CELoss: 0.8112 DKDLoss: 0.5317 
2022-05-03 02:27:39 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.5555, stu_CELoss: 1.0669 DKDLoss: 0.4886 
2022-05-03 02:28:12 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.4597, stu_CELoss: 0.8610 DKDLoss: 0.5987 
2022-05-03 02:28:46 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.4461, stu_CELoss: 0.9458 DKDLoss: 0.5003 
2022-05-03 02:29:19 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.3851, stu_CELoss: 0.9017 DKDLoss: 0.4834 
2022-05-03 02:29:53 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.2790, stu_CELoss: 0.8134 DKDLoss: 0.4656 
2022-05-03 02:30:27 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.5879, stu_CELoss: 1.0645 DKDLoss: 0.5234 
2022-05-03 02:31:01 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.4389, stu_CELoss: 0.9572 DKDLoss: 0.4817 
2022-05-03 02:31:35 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.3721, stu_CELoss: 0.9268 DKDLoss: 0.4453 
2022-05-03 02:32:08 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.3828, stu_CELoss: 0.9190 DKDLoss: 0.4638 
2022-05-03 02:32:42 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.4610, stu_CELoss: 0.9638 DKDLoss: 0.4972 
2022-05-03 02:33:16 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.4924, stu_CELoss: 1.0577 DKDLoss: 0.4348 
2022-05-03 02:33:50 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.5071, stu_CELoss: 0.9735 DKDLoss: 0.5336 
2022-05-03 02:34:24 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.4340, stu_CELoss: 0.9225 DKDLoss: 0.5115 
2022-05-03 02:34:57 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.5288, stu_CELoss: 1.0260 DKDLoss: 0.5028 
2022-05-03 02:35:31 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.4841, stu_CELoss: 0.9901 DKDLoss: 0.4940 
2022-05-03 02:36:05 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.3683, stu_CELoss: 0.8647 DKDLoss: 0.5036 
2022-05-03 02:36:39 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.4282, stu_CELoss: 0.9247 DKDLoss: 0.5035 
2022-05-03 02:37:13 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.4009, stu_CELoss: 0.9247 DKDLoss: 0.4762 
2022-05-03 02:37:47 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.3992, stu_CELoss: 0.9032 DKDLoss: 0.4960 
2022-05-03 02:38:21 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.2866, stu_CELoss: 0.8291 DKDLoss: 0.4575 
2022-05-03 02:38:54 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.7102, stu_CELoss: 1.2267 DKDLoss: 0.4835 
2022-05-03 02:39:28 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.5798, stu_CELoss: 1.1111 DKDLoss: 0.4687 
2022-05-03 02:40:02 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.4186, stu_CELoss: 0.9147 DKDLoss: 0.5039 
2022-05-03 02:40:36 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.4056, stu_CELoss: 0.8654 DKDLoss: 0.5402 
2022-05-03 02:41:10 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.5624, stu_CELoss: 1.0529 DKDLoss: 0.5094 
2022-05-03 02:41:44 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.3781, stu_CELoss: 0.9054 DKDLoss: 0.4727 
2022-05-03 02:42:18 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.6181, stu_CELoss: 1.0937 DKDLoss: 0.5244 
2022-05-03 02:42:52 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.4159, stu_CELoss: 0.9256 DKDLoss: 0.4902 
2022-05-03 02:43:26 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.2815, stu_CELoss: 0.8017 DKDLoss: 0.4798 
2022-05-03 02:44:00 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.4066, stu_CELoss: 0.9086 DKDLoss: 0.4980 
2022-05-03 02:44:34 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.4320, stu_CELoss: 0.9102 DKDLoss: 0.5218 
2022-05-03 02:45:08 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.3638, stu_CELoss: 0.8713 DKDLoss: 0.4926 
2022-05-03 02:45:42 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.5277, stu_CELoss: 1.0134 DKDLoss: 0.5143 
2022-05-03 02:46:16 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.3259, stu_CELoss: 0.8167 DKDLoss: 0.5091 
2022-05-03 02:46:50 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.4451, stu_CELoss: 0.9333 DKDLoss: 0.5118 
2022-05-03 02:47:24 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.3250, stu_CELoss: 0.8292 DKDLoss: 0.4958 
2022-05-03 02:47:25 - train: epoch 065, train_loss: 1.4469
2022-05-03 02:49:57 - eval: epoch: 065, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.680%, stu_acc5: 93.356%, stu_test_loss: 0.9157
2022-05-03 02:49:57 - until epoch: 065, tea_best_acc1: 78.330%, stu_best_acc1: 76.680%
2022-05-03 02:49:57 - epoch 066 lr: 0.0010000000000000002
2022-05-03 02:50:38 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.2305, stu_CELoss: 0.7044 DKDLoss: 0.5261 
2022-05-03 02:51:11 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.5256, stu_CELoss: 0.9943 DKDLoss: 0.5313 
2022-05-03 02:51:45 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.2825, stu_CELoss: 0.7723 DKDLoss: 0.5102 
2022-05-03 02:52:18 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.3071, stu_CELoss: 0.8407 DKDLoss: 0.4664 
2022-05-03 02:52:51 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.5369, stu_CELoss: 1.0099 DKDLoss: 0.5270 
2022-05-03 02:53:25 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.4692, stu_CELoss: 0.9996 DKDLoss: 0.4697 
2022-05-03 02:53:58 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.4166, stu_CELoss: 0.8614 DKDLoss: 0.5552 
2022-05-03 02:54:32 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.6790, stu_CELoss: 1.1265 DKDLoss: 0.5524 
2022-05-03 02:55:06 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.3007, stu_CELoss: 0.8000 DKDLoss: 0.5007 
2022-05-03 02:55:39 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.3738, stu_CELoss: 0.8739 DKDLoss: 0.4999 
2022-05-03 02:56:13 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.4631, stu_CELoss: 0.9697 DKDLoss: 0.4935 
2022-05-03 02:56:47 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.3546, stu_CELoss: 0.9399 DKDLoss: 0.4147 
2022-05-03 02:57:21 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.5162, stu_CELoss: 0.9945 DKDLoss: 0.5217 
2022-05-03 02:57:55 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.4224, stu_CELoss: 0.9288 DKDLoss: 0.4936 
2022-05-03 02:58:29 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.4942, stu_CELoss: 1.0279 DKDLoss: 0.4664 
2022-05-03 02:59:02 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.6646, stu_CELoss: 1.1016 DKDLoss: 0.5630 
2022-05-03 02:59:37 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.4667, stu_CELoss: 0.9539 DKDLoss: 0.5128 
2022-05-03 03:00:10 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.3334, stu_CELoss: 0.8525 DKDLoss: 0.4809 
2022-05-03 03:00:44 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.6022, stu_CELoss: 1.0520 DKDLoss: 0.5502 
2022-05-03 03:01:18 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.2116, stu_CELoss: 0.7357 DKDLoss: 0.4759 
2022-05-03 03:01:51 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.4515, stu_CELoss: 0.9283 DKDLoss: 0.5233 
2022-05-03 03:02:25 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.3086, stu_CELoss: 0.7780 DKDLoss: 0.5305 
2022-05-03 03:02:59 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.5137, stu_CELoss: 1.0102 DKDLoss: 0.5035 
2022-05-03 03:03:33 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.3083, stu_CELoss: 0.8390 DKDLoss: 0.4693 
2022-05-03 03:04:07 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.5029, stu_CELoss: 0.9737 DKDLoss: 0.5292 
2022-05-03 03:04:41 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.2811, stu_CELoss: 0.8297 DKDLoss: 0.4514 
2022-05-03 03:05:15 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.4497, stu_CELoss: 0.9389 DKDLoss: 0.5108 
2022-05-03 03:05:48 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.4676, stu_CELoss: 0.9671 DKDLoss: 0.5005 
2022-05-03 03:06:22 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.4411, stu_CELoss: 0.8867 DKDLoss: 0.5544 
2022-05-03 03:06:56 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.5318, stu_CELoss: 1.0210 DKDLoss: 0.5108 
2022-05-03 03:07:30 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.5250, stu_CELoss: 0.9665 DKDLoss: 0.5585 
2022-05-03 03:08:04 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.4546, stu_CELoss: 0.9298 DKDLoss: 0.5248 
2022-05-03 03:08:38 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.4438, stu_CELoss: 0.9065 DKDLoss: 0.5373 
2022-05-03 03:09:11 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.5729, stu_CELoss: 1.0665 DKDLoss: 0.5064 
2022-05-03 03:09:45 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.4640, stu_CELoss: 0.9198 DKDLoss: 0.5442 
2022-05-03 03:10:19 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.3361, stu_CELoss: 0.8431 DKDLoss: 0.4930 
2022-05-03 03:10:53 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.4661, stu_CELoss: 0.9781 DKDLoss: 0.4881 
2022-05-03 03:11:27 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.2971, stu_CELoss: 0.8443 DKDLoss: 0.4528 
2022-05-03 03:12:01 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.4775, stu_CELoss: 0.9714 DKDLoss: 0.5061 
2022-05-03 03:12:35 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.2926, stu_CELoss: 0.8588 DKDLoss: 0.4338 
2022-05-03 03:13:08 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.3386, stu_CELoss: 0.8309 DKDLoss: 0.5077 
2022-05-03 03:13:42 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.2991, stu_CELoss: 0.8429 DKDLoss: 0.4562 
2022-05-03 03:14:16 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.3142, stu_CELoss: 0.8309 DKDLoss: 0.4833 
2022-05-03 03:14:50 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.4117, stu_CELoss: 0.9118 DKDLoss: 0.4999 
2022-05-03 03:15:24 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.6151, stu_CELoss: 1.0222 DKDLoss: 0.5929 
2022-05-03 03:15:58 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.6418, stu_CELoss: 1.1034 DKDLoss: 0.5383 
2022-05-03 03:16:31 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.1961, stu_CELoss: 0.7173 DKDLoss: 0.4787 
2022-05-03 03:17:05 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.4892, stu_CELoss: 0.9800 DKDLoss: 0.5092 
2022-05-03 03:17:39 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.3556, stu_CELoss: 0.8485 DKDLoss: 0.5071 
2022-05-03 03:18:13 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.4460, stu_CELoss: 0.9283 DKDLoss: 0.5176 
2022-05-03 03:18:15 - train: epoch 066, train_loss: 1.4284
2022-05-03 03:20:46 - eval: epoch: 066, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.686%, stu_acc5: 93.392%, stu_test_loss: 0.9139
2022-05-03 03:20:47 - until epoch: 066, tea_best_acc1: 78.330%, stu_best_acc1: 76.686%
2022-05-03 03:20:47 - epoch 067 lr: 0.0010000000000000002
2022-05-03 03:21:27 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.3503, stu_CELoss: 0.8306 DKDLoss: 0.5197 
2022-05-03 03:22:00 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.4790, stu_CELoss: 1.0133 DKDLoss: 0.4658 
2022-05-03 03:22:34 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.5057, stu_CELoss: 0.9503 DKDLoss: 0.5554 
2022-05-03 03:23:07 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.3300, stu_CELoss: 0.8599 DKDLoss: 0.4701 
2022-05-03 03:23:41 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.4591, stu_CELoss: 0.9576 DKDLoss: 0.5014 
2022-05-03 03:24:14 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.3071, stu_CELoss: 0.8372 DKDLoss: 0.4699 
2022-05-03 03:24:47 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.6118, stu_CELoss: 1.1146 DKDLoss: 0.4971 
2022-05-03 03:25:21 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.3885, stu_CELoss: 0.9290 DKDLoss: 0.4595 
2022-05-03 03:25:54 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.7032, stu_CELoss: 1.1667 DKDLoss: 0.5364 
2022-05-03 03:26:27 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.3583, stu_CELoss: 0.8271 DKDLoss: 0.5312 
2022-05-03 03:27:01 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.4595, stu_CELoss: 0.9806 DKDLoss: 0.4789 
2022-05-03 03:27:34 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.4324, stu_CELoss: 0.9648 DKDLoss: 0.4676 
2022-05-03 03:28:07 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.4590, stu_CELoss: 0.9723 DKDLoss: 0.4867 
2022-05-03 03:28:41 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.5208, stu_CELoss: 1.0167 DKDLoss: 0.5041 
2022-05-03 03:29:15 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.5559, stu_CELoss: 0.9994 DKDLoss: 0.5565 
2022-05-03 03:29:48 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.5120, stu_CELoss: 1.0463 DKDLoss: 0.4658 
2022-05-03 03:30:22 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.1700, stu_CELoss: 0.7357 DKDLoss: 0.4343 
2022-05-03 03:30:56 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.6089, stu_CELoss: 1.1292 DKDLoss: 0.4797 
2022-05-03 03:31:29 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.4166, stu_CELoss: 0.9703 DKDLoss: 0.4463 
2022-05-03 03:32:03 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.5709, stu_CELoss: 1.0670 DKDLoss: 0.5039 
2022-05-03 03:32:37 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.2141, stu_CELoss: 0.7765 DKDLoss: 0.4376 
2022-05-03 03:33:11 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.4395, stu_CELoss: 0.9253 DKDLoss: 0.5143 
2022-05-03 03:33:45 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.5115, stu_CELoss: 0.9634 DKDLoss: 0.5481 
2022-05-03 03:34:19 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.2908, stu_CELoss: 0.7926 DKDLoss: 0.4982 
2022-05-03 03:34:52 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.3647, stu_CELoss: 0.8554 DKDLoss: 0.5093 
2022-05-03 03:35:26 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.5473, stu_CELoss: 1.0990 DKDLoss: 0.4484 
2022-05-03 03:36:00 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.2472, stu_CELoss: 0.8367 DKDLoss: 0.4105 
2022-05-03 03:36:34 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.7075, stu_CELoss: 1.1893 DKDLoss: 0.5183 
2022-05-03 03:37:07 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.2666, stu_CELoss: 0.7975 DKDLoss: 0.4691 
2022-05-03 03:37:41 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.4755, stu_CELoss: 0.9445 DKDLoss: 0.5309 
2022-05-03 03:38:15 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.3793, stu_CELoss: 0.8726 DKDLoss: 0.5067 
2022-05-03 03:38:48 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.3898, stu_CELoss: 0.9315 DKDLoss: 0.4583 
2022-05-03 03:39:22 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.2674, stu_CELoss: 0.7804 DKDLoss: 0.4870 
2022-05-03 03:39:56 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.3186, stu_CELoss: 0.8698 DKDLoss: 0.4488 
2022-05-03 03:40:30 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.3621, stu_CELoss: 0.8628 DKDLoss: 0.4993 
2022-05-03 03:41:04 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.2793, stu_CELoss: 0.8064 DKDLoss: 0.4729 
2022-05-03 03:41:38 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.2869, stu_CELoss: 0.8528 DKDLoss: 0.4341 
2022-05-03 03:42:12 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.4939, stu_CELoss: 0.9585 DKDLoss: 0.5355 
2022-05-03 03:42:46 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.5287, stu_CELoss: 1.0256 DKDLoss: 0.5031 
2022-05-03 03:43:19 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.6354, stu_CELoss: 1.1585 DKDLoss: 0.4770 
2022-05-03 03:43:53 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.4937, stu_CELoss: 0.9978 DKDLoss: 0.4959 
2022-05-03 03:44:27 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.5156, stu_CELoss: 1.0823 DKDLoss: 0.4333 
2022-05-03 03:45:01 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.1496, stu_CELoss: 0.6939 DKDLoss: 0.4557 
2022-05-03 03:45:35 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.4035, stu_CELoss: 0.9205 DKDLoss: 0.4831 
2022-05-03 03:46:09 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.6189, stu_CELoss: 1.1621 DKDLoss: 0.4568 
2022-05-03 03:46:43 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.3167, stu_CELoss: 0.8442 DKDLoss: 0.4725 
2022-05-03 03:47:17 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.3836, stu_CELoss: 0.9098 DKDLoss: 0.4738 
2022-05-03 03:47:51 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.2379, stu_CELoss: 0.7265 DKDLoss: 0.5114 
2022-05-03 03:48:25 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.3977, stu_CELoss: 0.9260 DKDLoss: 0.4717 
2022-05-03 03:48:58 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.3983, stu_CELoss: 0.9593 DKDLoss: 0.4390 
2022-05-03 03:49:00 - train: epoch 067, train_loss: 1.4150
2022-05-03 03:51:31 - eval: epoch: 067, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.660%, stu_acc5: 93.376%, stu_test_loss: 0.9128
2022-05-03 03:51:32 - until epoch: 067, tea_best_acc1: 78.330%, stu_best_acc1: 76.686%
2022-05-03 03:51:32 - epoch 068 lr: 0.0010000000000000002
2022-05-03 03:52:13 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.5051, stu_CELoss: 0.9527 DKDLoss: 0.5524 
2022-05-03 03:52:46 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.3246, stu_CELoss: 0.8848 DKDLoss: 0.4398 
2022-05-03 03:53:19 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.4224, stu_CELoss: 0.9178 DKDLoss: 0.5046 
2022-05-03 03:53:52 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.5347, stu_CELoss: 1.0325 DKDLoss: 0.5022 
2022-05-03 03:54:26 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.4003, stu_CELoss: 0.9226 DKDLoss: 0.4776 
2022-05-03 03:54:59 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.5540, stu_CELoss: 1.0608 DKDLoss: 0.4932 
2022-05-03 03:55:33 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.5648, stu_CELoss: 1.0976 DKDLoss: 0.4672 
2022-05-03 03:56:06 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.3191, stu_CELoss: 0.8612 DKDLoss: 0.4579 
2022-05-03 03:56:40 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.3034, stu_CELoss: 0.7590 DKDLoss: 0.5444 
2022-05-03 03:57:13 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.3550, stu_CELoss: 0.8942 DKDLoss: 0.4608 
2022-05-03 03:57:47 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.5010, stu_CELoss: 1.0110 DKDLoss: 0.4900 
2022-05-03 03:58:20 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.4694, stu_CELoss: 0.9569 DKDLoss: 0.5124 
2022-05-03 03:58:54 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.1970, stu_CELoss: 0.7588 DKDLoss: 0.4383 
2022-05-03 03:59:28 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.2111, stu_CELoss: 0.7713 DKDLoss: 0.4399 
2022-05-03 04:00:02 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.5500, stu_CELoss: 1.0171 DKDLoss: 0.5329 
2022-05-03 04:00:36 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.6598, stu_CELoss: 1.1728 DKDLoss: 0.4870 
2022-05-03 04:01:10 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.3104, stu_CELoss: 0.8909 DKDLoss: 0.4195 
2022-05-03 04:01:43 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.5431, stu_CELoss: 1.0961 DKDLoss: 0.4470 
2022-05-03 04:02:17 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.3207, stu_CELoss: 0.7893 DKDLoss: 0.5315 
2022-05-03 04:02:51 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.4995, stu_CELoss: 0.9963 DKDLoss: 0.5032 
2022-05-03 04:03:25 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.4006, stu_CELoss: 0.9600 DKDLoss: 0.4406 
2022-05-03 04:03:59 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.2367, stu_CELoss: 0.7870 DKDLoss: 0.4496 
2022-05-03 04:04:33 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.4167, stu_CELoss: 0.9209 DKDLoss: 0.4958 
2022-05-03 04:05:07 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.4264, stu_CELoss: 0.9590 DKDLoss: 0.4674 
2022-05-03 04:05:41 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.4520, stu_CELoss: 0.9544 DKDLoss: 0.4976 
2022-05-03 04:06:15 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.4204, stu_CELoss: 0.8790 DKDLoss: 0.5414 
2022-05-03 04:06:49 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.1838, stu_CELoss: 0.7229 DKDLoss: 0.4608 
2022-05-03 04:07:23 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.5960, stu_CELoss: 1.1019 DKDLoss: 0.4941 
2022-05-03 04:07:56 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.5288, stu_CELoss: 1.0602 DKDLoss: 0.4686 
2022-05-03 04:08:30 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.4102, stu_CELoss: 0.9069 DKDLoss: 0.5033 
2022-05-03 04:09:04 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.2180, stu_CELoss: 0.7951 DKDLoss: 0.4228 
2022-05-03 04:09:38 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.4716, stu_CELoss: 0.9921 DKDLoss: 0.4795 
2022-05-03 04:10:12 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.3307, stu_CELoss: 0.8954 DKDLoss: 0.4352 
2022-05-03 04:10:46 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.3100, stu_CELoss: 0.8743 DKDLoss: 0.4357 
2022-05-03 04:11:20 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.2687, stu_CELoss: 0.7784 DKDLoss: 0.4903 
2022-05-03 04:11:54 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.4148, stu_CELoss: 0.9254 DKDLoss: 0.4894 
2022-05-03 04:12:28 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.4320, stu_CELoss: 0.9446 DKDLoss: 0.4873 
2022-05-03 04:13:02 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.3493, stu_CELoss: 0.9421 DKDLoss: 0.4072 
2022-05-03 04:13:36 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.4290, stu_CELoss: 0.8920 DKDLoss: 0.5370 
2022-05-03 04:14:10 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.3810, stu_CELoss: 0.9552 DKDLoss: 0.4258 
2022-05-03 04:14:44 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.3049, stu_CELoss: 0.7807 DKDLoss: 0.5242 
2022-05-03 04:15:18 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.4193, stu_CELoss: 0.8831 DKDLoss: 0.5362 
2022-05-03 04:15:52 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.4485, stu_CELoss: 0.9282 DKDLoss: 0.5203 
2022-05-03 04:16:26 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.3899, stu_CELoss: 0.9027 DKDLoss: 0.4871 
2022-05-03 04:17:00 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.2574, stu_CELoss: 0.8373 DKDLoss: 0.4201 
2022-05-03 04:17:34 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.3558, stu_CELoss: 0.8835 DKDLoss: 0.4723 
2022-05-03 04:18:08 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.5175, stu_CELoss: 0.9903 DKDLoss: 0.5272 
2022-05-03 04:18:42 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.4947, stu_CELoss: 0.9949 DKDLoss: 0.4998 
2022-05-03 04:19:16 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.4237, stu_CELoss: 0.9167 DKDLoss: 0.5070 
2022-05-03 04:19:50 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.3739, stu_CELoss: 0.8447 DKDLoss: 0.5291 
2022-05-03 04:19:51 - train: epoch 068, train_loss: 1.4030
2022-05-03 04:22:23 - eval: epoch: 068, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.898%, stu_acc5: 93.444%, stu_test_loss: 0.9079
2022-05-03 04:22:24 - until epoch: 068, tea_best_acc1: 78.330%, stu_best_acc1: 76.898%
2022-05-03 04:22:24 - epoch 069 lr: 0.0010000000000000002
2022-05-03 04:23:04 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.5214, stu_CELoss: 1.0028 DKDLoss: 0.5186 
2022-05-03 04:23:38 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.3266, stu_CELoss: 0.8453 DKDLoss: 0.4813 
2022-05-03 04:24:11 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.4382, stu_CELoss: 0.9286 DKDLoss: 0.5096 
2022-05-03 04:24:44 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.2583, stu_CELoss: 0.8129 DKDLoss: 0.4454 
2022-05-03 04:25:18 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.2867, stu_CELoss: 0.8522 DKDLoss: 0.4346 
2022-05-03 04:25:52 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.1992, stu_CELoss: 0.7523 DKDLoss: 0.4470 
2022-05-03 04:26:25 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.5343, stu_CELoss: 0.9842 DKDLoss: 0.5501 
2022-05-03 04:26:59 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.4536, stu_CELoss: 0.9795 DKDLoss: 0.4741 
2022-05-03 04:27:33 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.3187, stu_CELoss: 0.8657 DKDLoss: 0.4530 
2022-05-03 04:28:07 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.5016, stu_CELoss: 1.0109 DKDLoss: 0.4907 
2022-05-03 04:28:41 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.6701, stu_CELoss: 1.2002 DKDLoss: 0.4699 
2022-05-03 04:29:15 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.3245, stu_CELoss: 0.9050 DKDLoss: 0.4196 
2022-05-03 04:29:48 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.3732, stu_CELoss: 0.8942 DKDLoss: 0.4789 
2022-05-03 04:30:22 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.3251, stu_CELoss: 0.8832 DKDLoss: 0.4419 
2022-05-03 04:30:56 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.5188, stu_CELoss: 1.0254 DKDLoss: 0.4934 
2022-05-03 04:31:30 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.3718, stu_CELoss: 0.9564 DKDLoss: 0.4154 
2022-05-03 04:32:04 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.3175, stu_CELoss: 0.8761 DKDLoss: 0.4413 
2022-05-03 04:32:37 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.3365, stu_CELoss: 0.8954 DKDLoss: 0.4411 
2022-05-03 04:33:11 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.3377, stu_CELoss: 0.8767 DKDLoss: 0.4610 
2022-05-03 04:33:45 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.3080, stu_CELoss: 0.8312 DKDLoss: 0.4767 
2022-05-03 04:34:19 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.5094, stu_CELoss: 1.0064 DKDLoss: 0.5031 
2022-05-03 04:34:52 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.4845, stu_CELoss: 0.9789 DKDLoss: 0.5055 
2022-05-03 04:35:26 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.3026, stu_CELoss: 0.7850 DKDLoss: 0.5176 
2022-05-03 04:35:59 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.4559, stu_CELoss: 0.9877 DKDLoss: 0.4682 
2022-05-03 04:36:33 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.2422, stu_CELoss: 0.8236 DKDLoss: 0.4186 
2022-05-03 04:37:08 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.4117, stu_CELoss: 0.9383 DKDLoss: 0.4733 
2022-05-03 04:37:42 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.6730, stu_CELoss: 1.1903 DKDLoss: 0.4827 
2022-05-03 04:38:15 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.4072, stu_CELoss: 0.9452 DKDLoss: 0.4620 
2022-05-03 04:38:49 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.4379, stu_CELoss: 0.9276 DKDLoss: 0.5103 
2022-05-03 04:39:23 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.5361, stu_CELoss: 1.0679 DKDLoss: 0.4682 
2022-05-03 04:39:57 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.6092, stu_CELoss: 1.1058 DKDLoss: 0.5034 
2022-05-03 04:40:31 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.1865, stu_CELoss: 0.7005 DKDLoss: 0.4859 
2022-05-03 04:41:05 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.4848, stu_CELoss: 0.9961 DKDLoss: 0.4887 
2022-05-03 04:41:39 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.3003, stu_CELoss: 0.7859 DKDLoss: 0.5144 
2022-05-03 04:42:12 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.1816, stu_CELoss: 0.7819 DKDLoss: 0.3997 
2022-05-03 04:42:46 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.2286, stu_CELoss: 0.7716 DKDLoss: 0.4570 
2022-05-03 04:43:20 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.4965, stu_CELoss: 1.0076 DKDLoss: 0.4888 
2022-05-03 04:43:54 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.4081, stu_CELoss: 0.9532 DKDLoss: 0.4549 
2022-05-03 04:44:28 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.3748, stu_CELoss: 0.9289 DKDLoss: 0.4459 
2022-05-03 04:45:01 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.5876, stu_CELoss: 1.0883 DKDLoss: 0.4993 
2022-05-03 04:45:35 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.4258, stu_CELoss: 0.9315 DKDLoss: 0.4943 
2022-05-03 04:46:09 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.3289, stu_CELoss: 0.9010 DKDLoss: 0.4279 
2022-05-03 04:46:43 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.3680, stu_CELoss: 0.8618 DKDLoss: 0.5062 
2022-05-03 04:47:17 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.3407, stu_CELoss: 0.8442 DKDLoss: 0.4965 
2022-05-03 04:47:50 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.4261, stu_CELoss: 0.8962 DKDLoss: 0.5299 
2022-05-03 04:48:24 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.4191, stu_CELoss: 0.9678 DKDLoss: 0.4513 
2022-05-03 04:48:58 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.3183, stu_CELoss: 0.8303 DKDLoss: 0.4880 
2022-05-03 04:49:32 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.4963, stu_CELoss: 1.0072 DKDLoss: 0.4891 
2022-05-03 04:50:06 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.2896, stu_CELoss: 0.8556 DKDLoss: 0.4341 
2022-05-03 04:50:39 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.3732, stu_CELoss: 0.9014 DKDLoss: 0.4718 
2022-05-03 04:50:41 - train: epoch 069, train_loss: 1.3918
2022-05-03 04:53:11 - eval: epoch: 069, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.740%, stu_acc5: 93.414%, stu_test_loss: 0.9089
2022-05-03 04:53:12 - until epoch: 069, tea_best_acc1: 78.330%, stu_best_acc1: 76.898%
2022-05-03 04:53:12 - epoch 070 lr: 0.0010000000000000002
2022-05-03 04:53:52 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.3031, stu_CELoss: 0.8415 DKDLoss: 0.4615 
2022-05-03 04:54:25 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.3428, stu_CELoss: 0.8868 DKDLoss: 0.4559 
2022-05-03 04:54:59 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.5071, stu_CELoss: 1.0134 DKDLoss: 0.4937 
2022-05-03 04:55:32 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.2387, stu_CELoss: 0.7946 DKDLoss: 0.4441 
2022-05-03 04:56:06 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.3321, stu_CELoss: 0.8624 DKDLoss: 0.4697 
2022-05-03 04:56:39 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.3456, stu_CELoss: 0.9152 DKDLoss: 0.4303 
2022-05-03 04:57:12 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.3426, stu_CELoss: 0.8670 DKDLoss: 0.4755 
2022-05-03 04:57:46 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.4150, stu_CELoss: 0.9419 DKDLoss: 0.4731 
2022-05-03 04:58:20 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.2558, stu_CELoss: 0.7742 DKDLoss: 0.4816 
2022-05-03 04:58:53 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.4706, stu_CELoss: 0.9902 DKDLoss: 0.4804 
2022-05-03 04:59:27 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.6778, stu_CELoss: 1.1880 DKDLoss: 0.4898 
2022-05-03 05:00:00 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.2194, stu_CELoss: 0.7553 DKDLoss: 0.4641 
2022-05-03 05:00:34 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.3404, stu_CELoss: 0.8841 DKDLoss: 0.4563 
2022-05-03 05:01:07 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.3001, stu_CELoss: 0.9088 DKDLoss: 0.3913 
2022-05-03 05:01:41 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.2270, stu_CELoss: 0.7959 DKDLoss: 0.4310 
2022-05-03 05:02:14 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.4139, stu_CELoss: 0.9159 DKDLoss: 0.4980 
2022-05-03 05:02:48 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.2907, stu_CELoss: 0.8026 DKDLoss: 0.4881 
2022-05-03 05:03:22 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.3106, stu_CELoss: 0.8556 DKDLoss: 0.4551 
2022-05-03 05:03:55 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.4242, stu_CELoss: 0.9762 DKDLoss: 0.4480 
2022-05-03 05:04:29 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.5125, stu_CELoss: 1.0340 DKDLoss: 0.4785 
2022-05-03 05:05:03 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.2944, stu_CELoss: 0.8414 DKDLoss: 0.4530 
2022-05-03 05:05:37 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.3656, stu_CELoss: 0.8848 DKDLoss: 0.4808 
2022-05-03 05:06:10 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.4646, stu_CELoss: 1.0086 DKDLoss: 0.4559 
2022-05-03 05:06:44 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.2921, stu_CELoss: 0.8289 DKDLoss: 0.4632 
2022-05-03 05:07:17 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.3493, stu_CELoss: 0.8655 DKDLoss: 0.4837 
2022-05-03 05:07:51 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.3553, stu_CELoss: 0.8447 DKDLoss: 0.5106 
2022-05-03 05:08:24 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.4213, stu_CELoss: 0.9194 DKDLoss: 0.5019 
2022-05-03 05:08:58 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.5573, stu_CELoss: 1.0716 DKDLoss: 0.4857 
2022-05-03 05:09:32 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.4737, stu_CELoss: 1.0096 DKDLoss: 0.4640 
2022-05-03 05:10:06 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.4981, stu_CELoss: 0.9684 DKDLoss: 0.5297 
2022-05-03 05:10:39 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.3386, stu_CELoss: 0.8565 DKDLoss: 0.4820 
2022-05-03 05:11:13 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.7487, stu_CELoss: 1.1857 DKDLoss: 0.5631 
2022-05-03 05:11:47 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.2346, stu_CELoss: 0.7448 DKDLoss: 0.4899 
2022-05-03 05:12:21 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.4532, stu_CELoss: 1.0281 DKDLoss: 0.4250 
2022-05-03 05:12:54 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.3700, stu_CELoss: 0.8311 DKDLoss: 0.5389 
2022-05-03 05:13:28 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.3446, stu_CELoss: 0.8929 DKDLoss: 0.4517 
2022-05-03 05:14:02 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.4437, stu_CELoss: 0.9208 DKDLoss: 0.5229 
2022-05-03 05:14:36 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.5559, stu_CELoss: 1.0550 DKDLoss: 0.5010 
2022-05-03 05:15:09 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.4259, stu_CELoss: 0.9177 DKDLoss: 0.5082 
2022-05-03 05:15:43 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.4488, stu_CELoss: 0.9272 DKDLoss: 0.5216 
2022-05-03 05:16:17 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.3328, stu_CELoss: 0.7677 DKDLoss: 0.5650 
2022-05-03 05:16:51 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.3970, stu_CELoss: 0.8579 DKDLoss: 0.5391 
2022-05-03 05:17:25 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.5218, stu_CELoss: 1.0024 DKDLoss: 0.5194 
2022-05-03 05:17:59 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.2262, stu_CELoss: 0.7776 DKDLoss: 0.4486 
2022-05-03 05:18:33 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.3524, stu_CELoss: 0.8824 DKDLoss: 0.4700 
2022-05-03 05:19:06 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.3973, stu_CELoss: 0.9555 DKDLoss: 0.4418 
2022-05-03 05:19:40 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.4580, stu_CELoss: 0.9579 DKDLoss: 0.5001 
2022-05-03 05:20:14 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.4071, stu_CELoss: 0.8959 DKDLoss: 0.5112 
2022-05-03 05:20:48 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.4907, stu_CELoss: 1.0002 DKDLoss: 0.4905 
2022-05-03 05:21:22 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.3695, stu_CELoss: 0.8965 DKDLoss: 0.4730 
2022-05-03 05:21:23 - train: epoch 070, train_loss: 1.3825
2022-05-03 05:23:56 - eval: epoch: 070, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.958%, stu_acc5: 93.426%, stu_test_loss: 0.9062
2022-05-03 05:23:56 - until epoch: 070, tea_best_acc1: 78.330%, stu_best_acc1: 76.958%
2022-05-03 05:23:56 - epoch 071 lr: 0.0010000000000000002
2022-05-03 05:24:36 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.1110, stu_CELoss: 0.6890 DKDLoss: 0.4220 
2022-05-03 05:25:10 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.3331, stu_CELoss: 0.8981 DKDLoss: 0.4349 
2022-05-03 05:25:43 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.5127, stu_CELoss: 1.0512 DKDLoss: 0.4615 
2022-05-03 05:26:16 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.5605, stu_CELoss: 1.0223 DKDLoss: 0.5382 
2022-05-03 05:26:49 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.3945, stu_CELoss: 0.8887 DKDLoss: 0.5058 
2022-05-03 05:27:23 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.4533, stu_CELoss: 0.9168 DKDLoss: 0.5365 
2022-05-03 05:27:56 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.5027, stu_CELoss: 0.9990 DKDLoss: 0.5038 
2022-05-03 05:28:30 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.4582, stu_CELoss: 0.9514 DKDLoss: 0.5069 
2022-05-03 05:29:03 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.5339, stu_CELoss: 1.0314 DKDLoss: 0.5025 
2022-05-03 05:29:36 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.5160, stu_CELoss: 1.0120 DKDLoss: 0.5039 
2022-05-03 05:30:10 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.5894, stu_CELoss: 1.1004 DKDLoss: 0.4890 
2022-05-03 05:30:44 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.3434, stu_CELoss: 0.8819 DKDLoss: 0.4615 
2022-05-03 05:31:17 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.2072, stu_CELoss: 0.7937 DKDLoss: 0.4134 
2022-05-03 05:31:51 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.2443, stu_CELoss: 0.7745 DKDLoss: 0.4697 
2022-05-03 05:32:25 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.2592, stu_CELoss: 0.7924 DKDLoss: 0.4669 
2022-05-03 05:32:58 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.3922, stu_CELoss: 0.9614 DKDLoss: 0.4309 
2022-05-03 05:33:32 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.3912, stu_CELoss: 0.9387 DKDLoss: 0.4525 
2022-05-03 05:34:06 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.4201, stu_CELoss: 0.9748 DKDLoss: 0.4453 
2022-05-03 05:34:39 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.2995, stu_CELoss: 0.8451 DKDLoss: 0.4545 
2022-05-03 05:35:13 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.3313, stu_CELoss: 0.8633 DKDLoss: 0.4679 
2022-05-03 05:35:47 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.2127, stu_CELoss: 0.7401 DKDLoss: 0.4726 
2022-05-03 05:36:21 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.1078, stu_CELoss: 0.6823 DKDLoss: 0.4256 
2022-05-03 05:36:54 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.2338, stu_CELoss: 0.7683 DKDLoss: 0.4655 
2022-05-03 05:37:28 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.2840, stu_CELoss: 0.8448 DKDLoss: 0.4392 
2022-05-03 05:38:02 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.5188, stu_CELoss: 1.0450 DKDLoss: 0.4738 
2022-05-03 05:38:36 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.3054, stu_CELoss: 0.8603 DKDLoss: 0.4451 
2022-05-03 05:39:10 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.4349, stu_CELoss: 0.9351 DKDLoss: 0.4998 
2022-05-03 05:39:43 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.5289, stu_CELoss: 1.0546 DKDLoss: 0.4743 
2022-05-03 05:40:17 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.3194, stu_CELoss: 0.8114 DKDLoss: 0.5080 
2022-05-03 05:40:51 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.3637, stu_CELoss: 0.8547 DKDLoss: 0.5091 
2022-05-03 05:41:25 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.4782, stu_CELoss: 0.9604 DKDLoss: 0.5178 
2022-05-03 05:41:59 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.4023, stu_CELoss: 0.9304 DKDLoss: 0.4718 
2022-05-03 05:42:33 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.3456, stu_CELoss: 0.8805 DKDLoss: 0.4651 
2022-05-03 05:43:07 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.2658, stu_CELoss: 0.8347 DKDLoss: 0.4311 
2022-05-03 05:43:40 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.4402, stu_CELoss: 0.9570 DKDLoss: 0.4832 
2022-05-03 05:44:14 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3308, stu_CELoss: 0.9089 DKDLoss: 0.4219 
2022-05-03 05:44:48 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.4944, stu_CELoss: 1.0250 DKDLoss: 0.4694 
2022-05-03 05:45:22 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.3242, stu_CELoss: 0.8530 DKDLoss: 0.4712 
2022-05-03 05:45:56 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.3590, stu_CELoss: 0.8993 DKDLoss: 0.4597 
2022-05-03 05:46:30 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.2746, stu_CELoss: 0.7857 DKDLoss: 0.4888 
2022-05-03 05:47:04 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.3322, stu_CELoss: 0.9409 DKDLoss: 0.3913 
2022-05-03 05:47:38 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.4638, stu_CELoss: 1.0145 DKDLoss: 0.4492 
2022-05-03 05:48:11 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.2358, stu_CELoss: 0.7876 DKDLoss: 0.4482 
2022-05-03 05:48:45 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.3500, stu_CELoss: 0.8625 DKDLoss: 0.4875 
2022-05-03 05:49:19 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.3234, stu_CELoss: 0.8579 DKDLoss: 0.4655 
2022-05-03 05:49:53 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.2786, stu_CELoss: 0.7650 DKDLoss: 0.5136 
2022-05-03 05:50:27 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.2350, stu_CELoss: 0.7692 DKDLoss: 0.4658 
2022-05-03 05:51:01 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.2839, stu_CELoss: 0.7928 DKDLoss: 0.4912 
2022-05-03 05:51:35 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.2480, stu_CELoss: 0.8146 DKDLoss: 0.4334 
2022-05-03 05:52:08 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.4339, stu_CELoss: 0.9637 DKDLoss: 0.4702 
2022-05-03 05:52:10 - train: epoch 071, train_loss: 1.3720
2022-05-03 05:54:41 - eval: epoch: 071, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.912%, stu_acc5: 93.506%, stu_test_loss: 0.9045
2022-05-03 05:54:42 - until epoch: 071, tea_best_acc1: 78.330%, stu_best_acc1: 76.958%
2022-05-03 05:54:42 - epoch 072 lr: 0.0010000000000000002
2022-05-03 05:55:22 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.5881, stu_CELoss: 1.0857 DKDLoss: 0.5023 
2022-05-03 05:55:55 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.2894, stu_CELoss: 0.8804 DKDLoss: 0.4090 
2022-05-03 05:56:29 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.2747, stu_CELoss: 0.8165 DKDLoss: 0.4582 
2022-05-03 05:57:02 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.4644, stu_CELoss: 0.9708 DKDLoss: 0.4936 
2022-05-03 05:57:35 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.1863, stu_CELoss: 0.7502 DKDLoss: 0.4361 
2022-05-03 05:58:09 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.3891, stu_CELoss: 0.9579 DKDLoss: 0.4312 
2022-05-03 05:58:42 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.4894, stu_CELoss: 1.0181 DKDLoss: 0.4714 
2022-05-03 05:59:16 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.5323, stu_CELoss: 1.0300 DKDLoss: 0.5022 
2022-05-03 05:59:49 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.2989, stu_CELoss: 0.7664 DKDLoss: 0.5324 
2022-05-03 06:00:23 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.2380, stu_CELoss: 0.8082 DKDLoss: 0.4299 
2022-05-03 06:00:57 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.5671, stu_CELoss: 1.0489 DKDLoss: 0.5181 
2022-05-03 06:01:31 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.2762, stu_CELoss: 0.8423 DKDLoss: 0.4340 
2022-05-03 06:02:05 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.3537, stu_CELoss: 0.8901 DKDLoss: 0.4636 
2022-05-03 06:02:38 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.3043, stu_CELoss: 0.8964 DKDLoss: 0.4079 
2022-05-03 06:03:12 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.4436, stu_CELoss: 0.9691 DKDLoss: 0.4746 
2022-05-03 06:03:46 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.3169, stu_CELoss: 0.8518 DKDLoss: 0.4651 
2022-05-03 06:04:20 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.3463, stu_CELoss: 0.9148 DKDLoss: 0.4315 
2022-05-03 06:04:54 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.4484, stu_CELoss: 0.9756 DKDLoss: 0.4729 
2022-05-03 06:05:27 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.3208, stu_CELoss: 0.8616 DKDLoss: 0.4592 
2022-05-03 06:06:01 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.3600, stu_CELoss: 0.8679 DKDLoss: 0.4921 
2022-05-03 06:06:35 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.4092, stu_CELoss: 0.9428 DKDLoss: 0.4663 
2022-05-03 06:07:09 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.4050, stu_CELoss: 0.9451 DKDLoss: 0.4599 
2022-05-03 06:07:43 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.6171, stu_CELoss: 1.0612 DKDLoss: 0.5558 
2022-05-03 06:08:16 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.3739, stu_CELoss: 0.8762 DKDLoss: 0.4976 
2022-05-03 06:08:50 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.3962, stu_CELoss: 0.9473 DKDLoss: 0.4489 
2022-05-03 06:09:24 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.2146, stu_CELoss: 0.8096 DKDLoss: 0.4050 
2022-05-03 06:09:58 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.2961, stu_CELoss: 0.8246 DKDLoss: 0.4715 
2022-05-03 06:10:32 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.5515, stu_CELoss: 1.0245 DKDLoss: 0.5270 
2022-05-03 06:11:06 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.1989, stu_CELoss: 0.7409 DKDLoss: 0.4580 
2022-05-03 06:11:40 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.3333, stu_CELoss: 0.8699 DKDLoss: 0.4634 
2022-05-03 06:12:14 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.4761, stu_CELoss: 1.0085 DKDLoss: 0.4676 
2022-05-03 06:12:47 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.4548, stu_CELoss: 1.0069 DKDLoss: 0.4479 
2022-05-03 06:13:22 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.3493, stu_CELoss: 0.8836 DKDLoss: 0.4658 
2022-05-03 06:13:55 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.3696, stu_CELoss: 0.8287 DKDLoss: 0.5409 
2022-05-03 06:14:29 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.4002, stu_CELoss: 0.9254 DKDLoss: 0.4748 
2022-05-03 06:15:03 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.4185, stu_CELoss: 0.9310 DKDLoss: 0.4876 
2022-05-03 06:15:37 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.7049, stu_CELoss: 1.0951 DKDLoss: 0.6098 
2022-05-03 06:16:11 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.2868, stu_CELoss: 0.8846 DKDLoss: 0.4022 
2022-05-03 06:16:45 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.4038, stu_CELoss: 0.9039 DKDLoss: 0.4998 
2022-05-03 06:17:19 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.2522, stu_CELoss: 0.7676 DKDLoss: 0.4846 
2022-05-03 06:17:53 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.4984, stu_CELoss: 1.0319 DKDLoss: 0.4665 
2022-05-03 06:18:27 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.2849, stu_CELoss: 0.8085 DKDLoss: 0.4764 
2022-05-03 06:19:01 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.2700, stu_CELoss: 0.8155 DKDLoss: 0.4545 
2022-05-03 06:19:34 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.5198, stu_CELoss: 0.9947 DKDLoss: 0.5251 
2022-05-03 06:20:08 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.5279, stu_CELoss: 1.0750 DKDLoss: 0.4530 
2022-05-03 06:20:42 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.2580, stu_CELoss: 0.7673 DKDLoss: 0.4906 
2022-05-03 06:21:16 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.3959, stu_CELoss: 0.9179 DKDLoss: 0.4779 
2022-05-03 06:21:49 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.4138, stu_CELoss: 0.9122 DKDLoss: 0.5016 
2022-05-03 06:22:23 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.4296, stu_CELoss: 0.9845 DKDLoss: 0.4451 
2022-05-03 06:22:57 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.3170, stu_CELoss: 0.8273 DKDLoss: 0.4897 
2022-05-03 06:22:58 - train: epoch 072, train_loss: 1.3646
2022-05-03 06:25:30 - eval: epoch: 072, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.884%, stu_acc5: 93.516%, stu_test_loss: 0.9027
2022-05-03 06:25:31 - until epoch: 072, tea_best_acc1: 78.330%, stu_best_acc1: 76.958%
2022-05-03 06:25:31 - epoch 073 lr: 0.0010000000000000002
2022-05-03 06:26:11 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.5565, stu_CELoss: 1.0841 DKDLoss: 0.4725 
2022-05-03 06:26:45 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.4173, stu_CELoss: 0.9492 DKDLoss: 0.4681 
2022-05-03 06:27:18 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.3282, stu_CELoss: 0.8642 DKDLoss: 0.4640 
2022-05-03 06:27:51 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.2697, stu_CELoss: 0.8389 DKDLoss: 0.4308 
2022-05-03 06:28:25 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.2801, stu_CELoss: 0.8621 DKDLoss: 0.4180 
2022-05-03 06:28:58 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.4080, stu_CELoss: 0.8763 DKDLoss: 0.5317 
2022-05-03 06:29:32 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.1923, stu_CELoss: 0.7235 DKDLoss: 0.4688 
2022-05-03 06:30:05 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.4439, stu_CELoss: 0.9569 DKDLoss: 0.4870 
2022-05-03 06:30:39 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.2825, stu_CELoss: 0.8280 DKDLoss: 0.4545 
2022-05-03 06:31:12 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.3904, stu_CELoss: 0.9150 DKDLoss: 0.4755 
2022-05-03 06:31:46 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.5418, stu_CELoss: 1.0724 DKDLoss: 0.4694 
2022-05-03 06:32:20 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.3798, stu_CELoss: 0.8831 DKDLoss: 0.4967 
2022-05-03 06:32:54 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.4609, stu_CELoss: 0.9622 DKDLoss: 0.4987 
2022-05-03 06:33:28 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.2855, stu_CELoss: 0.8600 DKDLoss: 0.4255 
2022-05-03 06:34:01 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.4542, stu_CELoss: 0.9556 DKDLoss: 0.4986 
2022-05-03 06:34:35 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.2163, stu_CELoss: 0.7509 DKDLoss: 0.4654 
2022-05-03 06:35:09 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.5625, stu_CELoss: 0.9940 DKDLoss: 0.5685 
2022-05-03 06:35:43 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.2924, stu_CELoss: 0.8545 DKDLoss: 0.4379 
2022-05-03 06:36:17 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3565, stu_CELoss: 0.8489 DKDLoss: 0.5076 
2022-05-03 06:36:51 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.2658, stu_CELoss: 0.8544 DKDLoss: 0.4115 
2022-05-03 06:37:25 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.4962, stu_CELoss: 1.0412 DKDLoss: 0.4549 
2022-05-03 06:37:59 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.4174, stu_CELoss: 0.9716 DKDLoss: 0.4458 
2022-05-03 06:38:33 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.1879, stu_CELoss: 0.7773 DKDLoss: 0.4106 
2022-05-03 06:39:06 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.4602, stu_CELoss: 1.0000 DKDLoss: 0.4602 
2022-05-03 06:39:40 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.3931, stu_CELoss: 0.9254 DKDLoss: 0.4677 
2022-05-03 06:40:14 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.2681, stu_CELoss: 0.8219 DKDLoss: 0.4462 
2022-05-03 06:40:48 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.2878, stu_CELoss: 0.7759 DKDLoss: 0.5119 
2022-05-03 06:41:22 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.3773, stu_CELoss: 0.9177 DKDLoss: 0.4596 
2022-05-03 06:41:56 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.4891, stu_CELoss: 1.0157 DKDLoss: 0.4735 
2022-05-03 06:42:30 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.1811, stu_CELoss: 0.7564 DKDLoss: 0.4247 
2022-05-03 06:43:04 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.2629, stu_CELoss: 0.7923 DKDLoss: 0.4706 
2022-05-03 06:43:38 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.2266, stu_CELoss: 0.7866 DKDLoss: 0.4400 
2022-05-03 06:44:11 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.2599, stu_CELoss: 0.8174 DKDLoss: 0.4426 
2022-05-03 06:44:45 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.3796, stu_CELoss: 0.9056 DKDLoss: 0.4739 
2022-05-03 06:45:19 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.2313, stu_CELoss: 0.7837 DKDLoss: 0.4477 
2022-05-03 06:45:53 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.3055, stu_CELoss: 0.7904 DKDLoss: 0.5151 
2022-05-03 06:46:27 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.2410, stu_CELoss: 0.7826 DKDLoss: 0.4584 
2022-05-03 06:47:01 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.5150, stu_CELoss: 1.0230 DKDLoss: 0.4920 
2022-05-03 06:47:35 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.2823, stu_CELoss: 0.8101 DKDLoss: 0.4722 
2022-05-03 06:48:09 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.5312, stu_CELoss: 1.0863 DKDLoss: 0.4449 
2022-05-03 06:48:43 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.4808, stu_CELoss: 1.0048 DKDLoss: 0.4760 
2022-05-03 06:49:16 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.4672, stu_CELoss: 1.0138 DKDLoss: 0.4534 
2022-05-03 06:49:50 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.3839, stu_CELoss: 0.9026 DKDLoss: 0.4813 
2022-05-03 06:50:24 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.4156, stu_CELoss: 0.9061 DKDLoss: 0.5095 
2022-05-03 06:50:58 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.1168, stu_CELoss: 0.6888 DKDLoss: 0.4279 
2022-05-03 06:51:32 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.4532, stu_CELoss: 0.9911 DKDLoss: 0.4621 
2022-05-03 06:52:06 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.5486, stu_CELoss: 1.0478 DKDLoss: 0.5008 
2022-05-03 06:52:40 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.4477, stu_CELoss: 0.9694 DKDLoss: 0.4783 
2022-05-03 06:53:14 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.2992, stu_CELoss: 0.8219 DKDLoss: 0.4772 
2022-05-03 06:53:47 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.3788, stu_CELoss: 0.9274 DKDLoss: 0.4514 
2022-05-03 06:53:49 - train: epoch 073, train_loss: 1.3531
2022-05-03 06:56:20 - eval: epoch: 073, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.956%, stu_acc5: 93.576%, stu_test_loss: 0.9008
2022-05-03 06:56:21 - until epoch: 073, tea_best_acc1: 78.330%, stu_best_acc1: 76.958%
2022-05-03 06:56:21 - epoch 074 lr: 0.0010000000000000002
2022-05-03 06:57:01 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.3017, stu_CELoss: 0.8218 DKDLoss: 0.4799 
2022-05-03 06:57:34 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.3006, stu_CELoss: 0.8587 DKDLoss: 0.4420 
2022-05-03 06:58:07 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.3785, stu_CELoss: 0.9293 DKDLoss: 0.4492 
2022-05-03 06:58:41 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.5371, stu_CELoss: 1.0784 DKDLoss: 0.4587 
2022-05-03 06:59:14 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.4584, stu_CELoss: 0.9745 DKDLoss: 0.4839 
2022-05-03 06:59:48 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.2967, stu_CELoss: 0.8017 DKDLoss: 0.4950 
2022-05-03 07:00:21 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.0940, stu_CELoss: 0.6696 DKDLoss: 0.4244 
2022-05-03 07:00:54 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.3978, stu_CELoss: 0.9304 DKDLoss: 0.4674 
2022-05-03 07:01:27 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.3313, stu_CELoss: 0.9193 DKDLoss: 0.4121 
2022-05-03 07:02:01 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.3600, stu_CELoss: 0.8442 DKDLoss: 0.5158 
2022-05-03 07:02:34 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.2603, stu_CELoss: 0.7832 DKDLoss: 0.4771 
2022-05-03 07:03:07 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.2366, stu_CELoss: 0.7963 DKDLoss: 0.4404 
2022-05-03 07:03:41 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.2887, stu_CELoss: 0.8941 DKDLoss: 0.3946 
2022-05-03 07:04:14 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.2710, stu_CELoss: 0.8256 DKDLoss: 0.4453 
2022-05-03 07:04:48 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.3526, stu_CELoss: 0.9070 DKDLoss: 0.4455 
2022-05-03 07:05:21 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.1570, stu_CELoss: 0.7258 DKDLoss: 0.4312 
2022-05-03 07:05:55 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.4436, stu_CELoss: 0.9485 DKDLoss: 0.4951 
2022-05-03 07:06:29 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.5063, stu_CELoss: 1.0574 DKDLoss: 0.4489 
2022-05-03 07:07:02 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.4635, stu_CELoss: 0.9794 DKDLoss: 0.4841 
2022-05-03 07:07:36 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.2580, stu_CELoss: 0.7967 DKDLoss: 0.4613 
2022-05-03 07:08:10 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.2888, stu_CELoss: 0.8014 DKDLoss: 0.4873 
2022-05-03 07:08:44 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.5949, stu_CELoss: 1.0713 DKDLoss: 0.5236 
2022-05-03 07:09:17 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.4864, stu_CELoss: 1.0132 DKDLoss: 0.4732 
2022-05-03 07:09:51 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.3236, stu_CELoss: 0.8688 DKDLoss: 0.4548 
2022-05-03 07:10:25 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.1341, stu_CELoss: 0.7412 DKDLoss: 0.3928 
2022-05-03 07:10:58 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.1808, stu_CELoss: 0.8006 DKDLoss: 0.3802 
2022-05-03 07:11:32 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.3227, stu_CELoss: 0.8279 DKDLoss: 0.4948 
2022-05-03 07:12:05 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.5795, stu_CELoss: 1.0954 DKDLoss: 0.4841 
2022-05-03 07:12:39 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.2659, stu_CELoss: 0.7549 DKDLoss: 0.5111 
2022-05-03 07:13:12 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.4177, stu_CELoss: 0.9852 DKDLoss: 0.4325 
2022-05-03 07:13:46 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.2926, stu_CELoss: 0.8490 DKDLoss: 0.4436 
2022-05-03 07:14:20 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.2042, stu_CELoss: 0.7821 DKDLoss: 0.4221 
2022-05-03 07:14:54 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.4828, stu_CELoss: 0.9982 DKDLoss: 0.4846 
2022-05-03 07:15:28 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.4014, stu_CELoss: 0.9414 DKDLoss: 0.4601 
2022-05-03 07:16:02 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.2873, stu_CELoss: 0.8377 DKDLoss: 0.4495 
2022-05-03 07:16:36 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3835, stu_CELoss: 0.9085 DKDLoss: 0.4750 
2022-05-03 07:17:10 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.3360, stu_CELoss: 0.8783 DKDLoss: 0.4578 
2022-05-03 07:17:44 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.3006, stu_CELoss: 0.8170 DKDLoss: 0.4835 
2022-05-03 07:18:18 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.1804, stu_CELoss: 0.7390 DKDLoss: 0.4414 
2022-05-03 07:18:52 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.2762, stu_CELoss: 0.8783 DKDLoss: 0.3979 
2022-05-03 07:19:26 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.3318, stu_CELoss: 0.8876 DKDLoss: 0.4442 
2022-05-03 07:20:01 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.3127, stu_CELoss: 0.8407 DKDLoss: 0.4721 
2022-05-03 07:20:35 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.1132, stu_CELoss: 0.6556 DKDLoss: 0.4576 
2022-05-03 07:21:09 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.2760, stu_CELoss: 0.8143 DKDLoss: 0.4617 
2022-05-03 07:21:43 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.1598, stu_CELoss: 0.7331 DKDLoss: 0.4267 
2022-05-03 07:22:17 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.4793, stu_CELoss: 0.9536 DKDLoss: 0.5257 
2022-05-03 07:22:51 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.2475, stu_CELoss: 0.8428 DKDLoss: 0.4047 
2022-05-03 07:23:25 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.3662, stu_CELoss: 0.8699 DKDLoss: 0.4962 
2022-05-03 07:23:59 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.6618, stu_CELoss: 1.1199 DKDLoss: 0.5419 
2022-05-03 07:24:33 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.5093, stu_CELoss: 1.0002 DKDLoss: 0.5092 
2022-05-03 07:24:35 - train: epoch 074, train_loss: 1.3475
2022-05-03 07:27:06 - eval: epoch: 074, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.982%, stu_acc5: 93.490%, stu_test_loss: 0.9031
2022-05-03 07:27:07 - until epoch: 074, tea_best_acc1: 78.330%, stu_best_acc1: 76.982%
2022-05-03 07:27:07 - epoch 075 lr: 0.0010000000000000002
2022-05-03 07:27:47 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.4776, stu_CELoss: 1.0229 DKDLoss: 0.4547 
2022-05-03 07:28:21 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.2802, stu_CELoss: 0.8108 DKDLoss: 0.4694 
2022-05-03 07:28:54 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.5631, stu_CELoss: 1.0609 DKDLoss: 0.5022 
2022-05-03 07:29:28 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.2400, stu_CELoss: 0.8162 DKDLoss: 0.4238 
2022-05-03 07:30:01 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.3959, stu_CELoss: 0.9088 DKDLoss: 0.4871 
2022-05-03 07:30:35 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.3376, stu_CELoss: 0.8834 DKDLoss: 0.4541 
2022-05-03 07:31:08 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.5300, stu_CELoss: 1.0477 DKDLoss: 0.4823 
2022-05-03 07:31:42 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.3266, stu_CELoss: 0.8604 DKDLoss: 0.4662 
2022-05-03 07:32:15 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.3778, stu_CELoss: 0.8999 DKDLoss: 0.4778 
2022-05-03 07:32:48 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.1663, stu_CELoss: 0.7299 DKDLoss: 0.4364 
2022-05-03 07:33:22 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.2768, stu_CELoss: 0.8478 DKDLoss: 0.4290 
2022-05-03 07:33:55 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.1765, stu_CELoss: 0.7514 DKDLoss: 0.4251 
2022-05-03 07:34:28 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.3394, stu_CELoss: 0.9062 DKDLoss: 0.4332 
2022-05-03 07:35:02 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.3644, stu_CELoss: 0.9145 DKDLoss: 0.4499 
2022-05-03 07:35:36 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.4040, stu_CELoss: 0.9583 DKDLoss: 0.4457 
2022-05-03 07:36:09 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.4171, stu_CELoss: 0.9276 DKDLoss: 0.4895 
2022-05-03 07:36:43 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.3807, stu_CELoss: 0.8785 DKDLoss: 0.5022 
2022-05-03 07:37:16 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.5577, stu_CELoss: 1.0363 DKDLoss: 0.5213 
2022-05-03 07:37:49 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.2535, stu_CELoss: 0.7798 DKDLoss: 0.4737 
2022-05-03 07:38:23 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.2798, stu_CELoss: 0.8334 DKDLoss: 0.4464 
2022-05-03 07:38:57 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.2836, stu_CELoss: 0.8782 DKDLoss: 0.4053 
2022-05-03 07:39:31 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.3867, stu_CELoss: 0.9058 DKDLoss: 0.4809 
2022-05-03 07:40:05 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.3428, stu_CELoss: 0.9158 DKDLoss: 0.4270 
2022-05-03 07:40:39 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.3864, stu_CELoss: 0.9047 DKDLoss: 0.4817 
2022-05-03 07:41:12 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3853, stu_CELoss: 0.9062 DKDLoss: 0.4791 
2022-05-03 07:41:46 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.2073, stu_CELoss: 0.7333 DKDLoss: 0.4740 
2022-05-03 07:42:20 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.2459, stu_CELoss: 0.7740 DKDLoss: 0.4718 
2022-05-03 07:42:54 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.2464, stu_CELoss: 0.7534 DKDLoss: 0.4930 
2022-05-03 07:43:27 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.3931, stu_CELoss: 0.9242 DKDLoss: 0.4689 
2022-05-03 07:44:01 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.5282, stu_CELoss: 1.0196 DKDLoss: 0.5086 
2022-05-03 07:44:35 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.4472, stu_CELoss: 0.9705 DKDLoss: 0.4767 
2022-05-03 07:45:09 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.3496, stu_CELoss: 0.8248 DKDLoss: 0.5248 
2022-05-03 07:45:43 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.3095, stu_CELoss: 0.8236 DKDLoss: 0.4858 
2022-05-03 07:46:17 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.1672, stu_CELoss: 0.7729 DKDLoss: 0.3943 
2022-05-03 07:46:51 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.3390, stu_CELoss: 0.9062 DKDLoss: 0.4328 
2022-05-03 07:47:25 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.4848, stu_CELoss: 1.0283 DKDLoss: 0.4565 
2022-05-03 07:47:59 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.3158, stu_CELoss: 0.8543 DKDLoss: 0.4615 
2022-05-03 07:48:33 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.2553, stu_CELoss: 0.7472 DKDLoss: 0.5081 
2022-05-03 07:49:07 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.2259, stu_CELoss: 0.7782 DKDLoss: 0.4476 
2022-05-03 07:49:41 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.3064, stu_CELoss: 0.8531 DKDLoss: 0.4533 
2022-05-03 07:50:15 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.2517, stu_CELoss: 0.8025 DKDLoss: 0.4493 
2022-05-03 07:50:48 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.5041, stu_CELoss: 0.9953 DKDLoss: 0.5088 
2022-05-03 07:51:22 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.3514, stu_CELoss: 0.8983 DKDLoss: 0.4531 
2022-05-03 07:51:56 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.3221, stu_CELoss: 0.8870 DKDLoss: 0.4351 
2022-05-03 07:52:30 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.4375, stu_CELoss: 0.9634 DKDLoss: 0.4741 
2022-05-03 07:53:05 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.2171, stu_CELoss: 0.8111 DKDLoss: 0.4061 
2022-05-03 07:53:39 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.4269, stu_CELoss: 0.9190 DKDLoss: 0.5079 
2022-05-03 07:54:13 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.2951, stu_CELoss: 0.8322 DKDLoss: 0.4629 
2022-05-03 07:54:47 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.2745, stu_CELoss: 0.8647 DKDLoss: 0.4099 
2022-05-03 07:55:20 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.2749, stu_CELoss: 0.8486 DKDLoss: 0.4263 
2022-05-03 07:55:22 - train: epoch 075, train_loss: 1.3423
2022-05-03 07:57:53 - eval: epoch: 075, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 76.958%, stu_acc5: 93.526%, stu_test_loss: 0.8996
2022-05-03 07:57:54 - until epoch: 075, tea_best_acc1: 78.330%, stu_best_acc1: 76.982%
2022-05-03 07:57:54 - epoch 076 lr: 0.0010000000000000002
2022-05-03 07:58:34 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.2822, stu_CELoss: 0.8162 DKDLoss: 0.4661 
2022-05-03 07:59:07 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.1757, stu_CELoss: 0.7202 DKDLoss: 0.4555 
2022-05-03 07:59:40 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.4658, stu_CELoss: 0.9750 DKDLoss: 0.4908 
2022-05-03 08:00:14 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.5270, stu_CELoss: 1.0045 DKDLoss: 0.5226 
2022-05-03 08:00:47 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.2891, stu_CELoss: 0.8337 DKDLoss: 0.4555 
2022-05-03 08:01:20 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.2623, stu_CELoss: 0.8295 DKDLoss: 0.4328 
2022-05-03 08:01:53 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.2200, stu_CELoss: 0.7983 DKDLoss: 0.4217 
2022-05-03 08:02:27 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.3865, stu_CELoss: 0.9284 DKDLoss: 0.4580 
2022-05-03 08:03:00 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.2564, stu_CELoss: 0.8088 DKDLoss: 0.4476 
2022-05-03 08:03:34 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.4028, stu_CELoss: 0.9108 DKDLoss: 0.4919 
2022-05-03 08:04:08 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.2438, stu_CELoss: 0.8264 DKDLoss: 0.4174 
2022-05-03 08:04:42 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.3477, stu_CELoss: 0.8667 DKDLoss: 0.4810 
2022-05-03 08:05:15 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.3795, stu_CELoss: 0.9120 DKDLoss: 0.4675 
2022-05-03 08:05:49 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.3026, stu_CELoss: 0.9045 DKDLoss: 0.3982 
2022-05-03 08:06:23 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.2054, stu_CELoss: 0.7745 DKDLoss: 0.4309 
2022-05-03 08:06:57 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.3191, stu_CELoss: 0.8400 DKDLoss: 0.4791 
2022-05-03 08:07:30 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.2523, stu_CELoss: 0.7932 DKDLoss: 0.4591 
2022-05-03 08:08:04 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.3163, stu_CELoss: 0.8785 DKDLoss: 0.4378 
2022-05-03 08:08:38 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.2221, stu_CELoss: 0.8159 DKDLoss: 0.4062 
2022-05-03 08:09:12 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.3997, stu_CELoss: 0.9208 DKDLoss: 0.4789 
2022-05-03 08:09:45 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.4958, stu_CELoss: 1.0479 DKDLoss: 0.4479 
2022-05-03 08:10:19 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.2982, stu_CELoss: 0.8550 DKDLoss: 0.4432 
2022-05-03 08:10:53 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.3394, stu_CELoss: 0.8800 DKDLoss: 0.4594 
2022-05-03 08:11:27 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.3719, stu_CELoss: 0.9036 DKDLoss: 0.4683 
2022-05-03 08:12:00 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.3721, stu_CELoss: 0.9307 DKDLoss: 0.4414 
2022-05-03 08:12:34 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.5961, stu_CELoss: 1.1399 DKDLoss: 0.4563 
2022-05-03 08:13:08 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.2601, stu_CELoss: 0.8285 DKDLoss: 0.4315 
2022-05-03 08:13:43 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1535, stu_CELoss: 0.7246 DKDLoss: 0.4289 
2022-05-03 08:14:17 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.3283, stu_CELoss: 0.8353 DKDLoss: 0.4930 
2022-05-03 08:14:51 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.2580, stu_CELoss: 0.8272 DKDLoss: 0.4308 
2022-05-03 08:15:25 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.2336, stu_CELoss: 0.8626 DKDLoss: 0.3710 
2022-05-03 08:15:58 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.4349, stu_CELoss: 0.9593 DKDLoss: 0.4756 
2022-05-03 08:16:32 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.3035, stu_CELoss: 0.8562 DKDLoss: 0.4473 
2022-05-03 08:17:06 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.3201, stu_CELoss: 0.8755 DKDLoss: 0.4446 
2022-05-03 08:17:40 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.2705, stu_CELoss: 0.8176 DKDLoss: 0.4529 
2022-05-03 08:18:15 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.3785, stu_CELoss: 0.9771 DKDLoss: 0.4014 
2022-05-03 08:18:49 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.2655, stu_CELoss: 0.8622 DKDLoss: 0.4033 
2022-05-03 08:19:23 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.2480, stu_CELoss: 0.8189 DKDLoss: 0.4290 
2022-05-03 08:19:57 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.2927, stu_CELoss: 0.8217 DKDLoss: 0.4710 
2022-05-03 08:20:31 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.2965, stu_CELoss: 0.8780 DKDLoss: 0.4184 
2022-05-03 08:21:05 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.4461, stu_CELoss: 0.9762 DKDLoss: 0.4699 
2022-05-03 08:21:39 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.5442, stu_CELoss: 1.0751 DKDLoss: 0.4691 
2022-05-03 08:22:13 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.4823, stu_CELoss: 0.9636 DKDLoss: 0.5187 
2022-05-03 08:22:47 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.5080, stu_CELoss: 1.0915 DKDLoss: 0.4165 
2022-05-03 08:23:21 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.4709, stu_CELoss: 0.9477 DKDLoss: 0.5232 
2022-05-03 08:23:55 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.3410, stu_CELoss: 0.8373 DKDLoss: 0.5037 
2022-05-03 08:24:28 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.3794, stu_CELoss: 0.8962 DKDLoss: 0.4833 
2022-05-03 08:25:02 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.1814, stu_CELoss: 0.6978 DKDLoss: 0.4836 
2022-05-03 08:25:37 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.4140, stu_CELoss: 0.9136 DKDLoss: 0.5005 
2022-05-03 08:26:10 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.4423, stu_CELoss: 1.0249 DKDLoss: 0.4174 
2022-05-03 08:26:12 - train: epoch 076, train_loss: 1.3356
2022-05-03 08:28:43 - eval: epoch: 076, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 77.052%, stu_acc5: 93.500%, stu_test_loss: 0.8992
2022-05-03 08:28:44 - until epoch: 076, tea_best_acc1: 78.330%, stu_best_acc1: 77.052%
2022-05-03 08:28:44 - epoch 077 lr: 0.0010000000000000002
2022-05-03 08:29:24 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.4973, stu_CELoss: 1.0185 DKDLoss: 0.4788 
2022-05-03 08:29:57 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.4465, stu_CELoss: 0.9919 DKDLoss: 0.4547 
2022-05-03 08:30:30 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.4730, stu_CELoss: 1.0660 DKDLoss: 0.4070 
2022-05-03 08:31:04 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.2344, stu_CELoss: 0.8379 DKDLoss: 0.3966 
2022-05-03 08:31:37 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1190, stu_CELoss: 0.7103 DKDLoss: 0.4087 
2022-05-03 08:32:11 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.2087, stu_CELoss: 0.7392 DKDLoss: 0.4695 
2022-05-03 08:32:44 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.3812, stu_CELoss: 0.9159 DKDLoss: 0.4653 
2022-05-03 08:33:18 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.3759, stu_CELoss: 0.8750 DKDLoss: 0.5009 
2022-05-03 08:33:52 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.3624, stu_CELoss: 0.8710 DKDLoss: 0.4915 
2022-05-03 08:34:25 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.1821, stu_CELoss: 0.7691 DKDLoss: 0.4130 
2022-05-03 08:34:59 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.2548, stu_CELoss: 0.7913 DKDLoss: 0.4635 
2022-05-03 08:35:32 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.3098, stu_CELoss: 0.8745 DKDLoss: 0.4354 
2022-05-03 08:36:05 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.1915, stu_CELoss: 0.7684 DKDLoss: 0.4231 
2022-05-03 08:36:39 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.2737, stu_CELoss: 0.8620 DKDLoss: 0.4117 
2022-05-03 08:37:12 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.2623, stu_CELoss: 0.8620 DKDLoss: 0.4003 
2022-05-03 08:37:46 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.3580, stu_CELoss: 0.8913 DKDLoss: 0.4666 
2022-05-03 08:38:19 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.3459, stu_CELoss: 0.8721 DKDLoss: 0.4738 
2022-05-03 08:38:53 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.3879, stu_CELoss: 0.9053 DKDLoss: 0.4826 
2022-05-03 08:39:26 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.3977, stu_CELoss: 0.9720 DKDLoss: 0.4257 
2022-05-03 08:40:00 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.2296, stu_CELoss: 0.7867 DKDLoss: 0.4429 
2022-05-03 08:40:34 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.4665, stu_CELoss: 0.9589 DKDLoss: 0.5076 
2022-05-03 08:41:07 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.2524, stu_CELoss: 0.8418 DKDLoss: 0.4106 
2022-05-03 08:41:41 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.3974, stu_CELoss: 0.9448 DKDLoss: 0.4525 
2022-05-03 08:42:15 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.2371, stu_CELoss: 0.7450 DKDLoss: 0.4921 
2022-05-03 08:42:49 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.4897, stu_CELoss: 0.9872 DKDLoss: 0.5025 
2022-05-03 08:43:23 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.1629, stu_CELoss: 0.7831 DKDLoss: 0.3798 
2022-05-03 08:43:56 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.2716, stu_CELoss: 0.7933 DKDLoss: 0.4783 
2022-05-03 08:44:30 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.3329, stu_CELoss: 0.8770 DKDLoss: 0.4559 
2022-05-03 08:45:04 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.5233, stu_CELoss: 1.0000 DKDLoss: 0.5233 
2022-05-03 08:45:38 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1895, stu_CELoss: 0.7555 DKDLoss: 0.4339 
2022-05-03 08:46:12 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.3547, stu_CELoss: 0.9521 DKDLoss: 0.4026 
2022-05-03 08:46:45 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.3435, stu_CELoss: 0.8820 DKDLoss: 0.4616 
2022-05-03 08:47:19 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.4159, stu_CELoss: 0.9383 DKDLoss: 0.4776 
2022-05-03 08:47:53 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.3720, stu_CELoss: 0.8939 DKDLoss: 0.4781 
2022-05-03 08:48:27 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.1609, stu_CELoss: 0.7453 DKDLoss: 0.4157 
2022-05-03 08:49:00 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.2807, stu_CELoss: 0.8526 DKDLoss: 0.4281 
2022-05-03 08:49:34 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.3038, stu_CELoss: 0.8697 DKDLoss: 0.4342 
2022-05-03 08:50:08 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.2006, stu_CELoss: 0.8203 DKDLoss: 0.3803 
2022-05-03 08:50:42 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.2199, stu_CELoss: 0.8183 DKDLoss: 0.4017 
2022-05-03 08:51:16 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.4812, stu_CELoss: 1.0082 DKDLoss: 0.4730 
2022-05-03 08:51:50 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.3333, stu_CELoss: 0.9149 DKDLoss: 0.4184 
2022-05-03 08:52:24 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.3158, stu_CELoss: 0.9012 DKDLoss: 0.4145 
2022-05-03 08:52:58 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.2857, stu_CELoss: 0.8723 DKDLoss: 0.4135 
2022-05-03 08:53:32 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.2884, stu_CELoss: 0.8853 DKDLoss: 0.4031 
2022-05-03 08:54:06 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.4378, stu_CELoss: 0.9513 DKDLoss: 0.4865 
2022-05-03 08:54:40 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.2321, stu_CELoss: 0.7745 DKDLoss: 0.4576 
2022-05-03 08:55:14 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.3224, stu_CELoss: 0.8167 DKDLoss: 0.5056 
2022-05-03 08:55:48 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.1838, stu_CELoss: 0.7587 DKDLoss: 0.4250 
2022-05-03 08:56:22 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.4620, stu_CELoss: 1.0255 DKDLoss: 0.4364 
2022-05-03 08:56:55 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.2038, stu_CELoss: 0.7672 DKDLoss: 0.4366 
2022-05-03 08:56:57 - train: epoch 077, train_loss: 1.3298
2022-05-03 08:59:29 - eval: epoch: 077, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 77.108%, stu_acc5: 93.508%, stu_test_loss: 0.8988
2022-05-03 08:59:30 - until epoch: 077, tea_best_acc1: 78.330%, stu_best_acc1: 77.108%
2022-05-03 08:59:30 - epoch 078 lr: 0.0010000000000000002
2022-05-03 09:00:10 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.3639, stu_CELoss: 0.9029 DKDLoss: 0.4610 
2022-05-03 09:00:43 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.3460, stu_CELoss: 0.9705 DKDLoss: 0.3755 
2022-05-03 09:01:17 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.2277, stu_CELoss: 0.7866 DKDLoss: 0.4411 
2022-05-03 09:01:50 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.2238, stu_CELoss: 0.8119 DKDLoss: 0.4119 
2022-05-03 09:02:24 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.2378, stu_CELoss: 0.8124 DKDLoss: 0.4253 
2022-05-03 09:02:57 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.2166, stu_CELoss: 0.8151 DKDLoss: 0.4015 
2022-05-03 09:03:31 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.3552, stu_CELoss: 0.9333 DKDLoss: 0.4219 
2022-05-03 09:04:05 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.4192, stu_CELoss: 0.9621 DKDLoss: 0.4571 
2022-05-03 09:04:38 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.2238, stu_CELoss: 0.8610 DKDLoss: 0.3628 
2022-05-03 09:05:11 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.1744, stu_CELoss: 0.7436 DKDLoss: 0.4308 
2022-05-03 09:05:45 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.3295, stu_CELoss: 0.8533 DKDLoss: 0.4762 
2022-05-03 09:06:18 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.2765, stu_CELoss: 0.8259 DKDLoss: 0.4505 
2022-05-03 09:06:51 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.1555, stu_CELoss: 0.7610 DKDLoss: 0.3945 
2022-05-03 09:07:25 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.1694, stu_CELoss: 0.7388 DKDLoss: 0.4306 
2022-05-03 09:07:58 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.3491, stu_CELoss: 0.8437 DKDLoss: 0.5054 
2022-05-03 09:08:32 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.3211, stu_CELoss: 0.9235 DKDLoss: 0.3976 
2022-05-03 09:09:05 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.3252, stu_CELoss: 0.8456 DKDLoss: 0.4797 
2022-05-03 09:09:39 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.4607, stu_CELoss: 0.9831 DKDLoss: 0.4776 
2022-05-03 09:10:13 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.3555, stu_CELoss: 0.9218 DKDLoss: 0.4337 
2022-05-03 09:10:47 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.4174, stu_CELoss: 0.9468 DKDLoss: 0.4705 
2022-05-03 09:11:20 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.2839, stu_CELoss: 0.8273 DKDLoss: 0.4566 
2022-05-03 09:11:54 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.2777, stu_CELoss: 0.8367 DKDLoss: 0.4410 
2022-05-03 09:12:28 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.2968, stu_CELoss: 0.8323 DKDLoss: 0.4644 
2022-05-03 09:13:02 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.3699, stu_CELoss: 0.9660 DKDLoss: 0.4039 
2022-05-03 09:13:36 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.3028, stu_CELoss: 0.8488 DKDLoss: 0.4540 
2022-05-03 09:14:10 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.2344, stu_CELoss: 0.7873 DKDLoss: 0.4471 
2022-05-03 09:14:43 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.3086, stu_CELoss: 0.8658 DKDLoss: 0.4428 
2022-05-03 09:15:17 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.1712, stu_CELoss: 0.7452 DKDLoss: 0.4260 
2022-05-03 09:15:51 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.3208, stu_CELoss: 0.8879 DKDLoss: 0.4329 
2022-05-03 09:16:25 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.4034, stu_CELoss: 0.9028 DKDLoss: 0.5006 
2022-05-03 09:16:58 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.3162, stu_CELoss: 0.8730 DKDLoss: 0.4432 
2022-05-03 09:17:32 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.3610, stu_CELoss: 0.8649 DKDLoss: 0.4961 
2022-05-03 09:18:06 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.8369, stu_CELoss: 1.3379 DKDLoss: 0.4990 
2022-05-03 09:18:40 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.5456, stu_CELoss: 1.1006 DKDLoss: 0.4450 
2022-05-03 09:19:13 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.3589, stu_CELoss: 0.9406 DKDLoss: 0.4183 
2022-05-03 09:19:47 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.5400, stu_CELoss: 1.0738 DKDLoss: 0.4662 
2022-05-03 09:20:21 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.2526, stu_CELoss: 0.8059 DKDLoss: 0.4467 
2022-05-03 09:20:55 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.2377, stu_CELoss: 0.8264 DKDLoss: 0.4113 
2022-05-03 09:21:29 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.3692, stu_CELoss: 0.9405 DKDLoss: 0.4287 
2022-05-03 09:22:03 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.3834, stu_CELoss: 0.8874 DKDLoss: 0.4960 
2022-05-03 09:22:37 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.4072, stu_CELoss: 0.9156 DKDLoss: 0.4915 
2022-05-03 09:23:11 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.2476, stu_CELoss: 0.7975 DKDLoss: 0.4501 
2022-05-03 09:23:45 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.2914, stu_CELoss: 0.8587 DKDLoss: 0.4327 
2022-05-03 09:24:18 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.2310, stu_CELoss: 0.8052 DKDLoss: 0.4258 
2022-05-03 09:24:52 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.4753, stu_CELoss: 1.0525 DKDLoss: 0.4229 
2022-05-03 09:25:26 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.2170, stu_CELoss: 0.7890 DKDLoss: 0.4280 
2022-05-03 09:26:00 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.3775, stu_CELoss: 0.9280 DKDLoss: 0.4495 
2022-05-03 09:26:34 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.6149, stu_CELoss: 1.1251 DKDLoss: 0.4898 
2022-05-03 09:27:08 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.2442, stu_CELoss: 0.7911 DKDLoss: 0.4531 
2022-05-03 09:27:41 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.2243, stu_CELoss: 0.8282 DKDLoss: 0.3961 
2022-05-03 09:27:43 - train: epoch 078, train_loss: 1.3244
2022-05-03 09:30:15 - eval: epoch: 078, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 77.140%, stu_acc5: 93.462%, stu_test_loss: 0.9002
2022-05-03 09:30:16 - until epoch: 078, tea_best_acc1: 78.330%, stu_best_acc1: 77.140%
2022-05-03 09:30:16 - epoch 079 lr: 0.0010000000000000002
2022-05-03 09:30:56 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.1998, stu_CELoss: 0.7808 DKDLoss: 0.4189 
2022-05-03 09:31:29 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.3185, stu_CELoss: 0.8921 DKDLoss: 0.4264 
2022-05-03 09:32:02 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.2716, stu_CELoss: 0.8327 DKDLoss: 0.4389 
2022-05-03 09:32:36 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.3829, stu_CELoss: 0.8562 DKDLoss: 0.5267 
2022-05-03 09:33:09 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.1428, stu_CELoss: 0.7315 DKDLoss: 0.4113 
2022-05-03 09:33:43 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.0763, stu_CELoss: 0.7052 DKDLoss: 0.3711 
2022-05-03 09:34:17 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.3501, stu_CELoss: 0.9243 DKDLoss: 0.4258 
2022-05-03 09:34:50 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.1838, stu_CELoss: 0.7950 DKDLoss: 0.3888 
2022-05-03 09:35:24 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.4097, stu_CELoss: 0.9589 DKDLoss: 0.4508 
2022-05-03 09:35:57 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.2682, stu_CELoss: 0.8566 DKDLoss: 0.4116 
2022-05-03 09:36:31 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.4520, stu_CELoss: 0.9265 DKDLoss: 0.5255 
2022-05-03 09:37:05 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.4885, stu_CELoss: 1.0587 DKDLoss: 0.4299 
2022-05-03 09:37:39 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.2157, stu_CELoss: 0.8018 DKDLoss: 0.4139 
2022-05-03 09:38:12 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.4194, stu_CELoss: 1.0502 DKDLoss: 0.3693 
2022-05-03 09:38:46 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.2764, stu_CELoss: 0.8216 DKDLoss: 0.4549 
2022-05-03 09:39:20 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.2253, stu_CELoss: 0.7769 DKDLoss: 0.4484 
2022-05-03 09:39:54 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.2966, stu_CELoss: 0.8692 DKDLoss: 0.4274 
2022-05-03 09:40:28 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.3153, stu_CELoss: 0.8495 DKDLoss: 0.4658 
2022-05-03 09:41:03 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.2972, stu_CELoss: 0.8460 DKDLoss: 0.4512 
2022-05-03 09:41:36 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.4875, stu_CELoss: 1.0491 DKDLoss: 0.4384 
2022-05-03 09:42:10 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.1973, stu_CELoss: 0.7488 DKDLoss: 0.4484 
2022-05-03 09:42:44 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.3591, stu_CELoss: 0.9287 DKDLoss: 0.4305 
2022-05-03 09:43:18 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.3549, stu_CELoss: 0.8844 DKDLoss: 0.4705 
2022-05-03 09:43:52 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.3272, stu_CELoss: 0.9354 DKDLoss: 0.3918 
2022-05-03 09:44:25 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.3763, stu_CELoss: 0.9083 DKDLoss: 0.4679 
2022-05-03 09:44:59 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.3631, stu_CELoss: 0.9606 DKDLoss: 0.4025 
2022-05-03 09:45:33 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.2265, stu_CELoss: 0.8166 DKDLoss: 0.4099 
2022-05-03 09:46:06 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.1939, stu_CELoss: 0.7271 DKDLoss: 0.4668 
2022-05-03 09:46:40 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.2522, stu_CELoss: 0.8156 DKDLoss: 0.4366 
2022-05-03 09:47:14 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.5035, stu_CELoss: 1.0361 DKDLoss: 0.4674 
2022-05-03 09:47:48 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.2598, stu_CELoss: 0.8431 DKDLoss: 0.4166 
2022-05-03 09:48:22 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.4321, stu_CELoss: 0.9596 DKDLoss: 0.4725 
2022-05-03 09:48:56 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.3557, stu_CELoss: 0.8765 DKDLoss: 0.4792 
2022-05-03 09:49:30 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.3734, stu_CELoss: 0.8993 DKDLoss: 0.4740 
2022-05-03 09:50:04 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.1773, stu_CELoss: 0.7016 DKDLoss: 0.4758 
2022-05-03 09:50:38 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.0008, stu_CELoss: 0.6207 DKDLoss: 0.3801 
2022-05-03 09:51:12 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.2964, stu_CELoss: 0.8860 DKDLoss: 0.4104 
2022-05-03 09:51:46 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.2849, stu_CELoss: 0.8098 DKDLoss: 0.4751 
2022-05-03 09:52:20 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.3727, stu_CELoss: 0.8911 DKDLoss: 0.4816 
2022-05-03 09:52:53 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.0839, stu_CELoss: 0.6617 DKDLoss: 0.4223 
2022-05-03 09:53:27 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.5960, stu_CELoss: 1.1102 DKDLoss: 0.4858 
2022-05-03 09:54:01 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.2916, stu_CELoss: 0.8557 DKDLoss: 0.4358 
2022-05-03 09:54:35 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.1422, stu_CELoss: 0.6720 DKDLoss: 0.4702 
2022-05-03 09:55:09 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.2491, stu_CELoss: 0.8047 DKDLoss: 0.4443 
2022-05-03 09:55:43 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.4744, stu_CELoss: 1.0083 DKDLoss: 0.4661 
2022-05-03 09:56:17 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.1911, stu_CELoss: 0.7944 DKDLoss: 0.3966 
2022-05-03 09:56:51 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.4718, stu_CELoss: 0.9609 DKDLoss: 0.5109 
2022-05-03 09:57:25 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.3539, stu_CELoss: 0.9161 DKDLoss: 0.4378 
2022-05-03 09:57:59 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.3424, stu_CELoss: 0.9139 DKDLoss: 0.4286 
2022-05-03 09:58:33 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.1879, stu_CELoss: 0.8054 DKDLoss: 0.3824 
2022-05-03 09:58:34 - train: epoch 079, train_loss: 1.3181
2022-05-03 10:01:07 - eval: epoch: 079, tea_acc1: 78.330%, tea_acc5: 94.064%, tea_test_loss: 0.8657, stu_acc1: 77.192%, stu_acc5: 93.626%, stu_test_loss: 0.8965
2022-05-03 10:01:07 - until epoch: 079, tea_best_acc1: 78.330%, stu_best_acc1: 77.192%
2022-05-03 10:01:07 - epoch 080 lr: 0.0010000000000000002
2022-05-03 10:01:48 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.3000, stu_CELoss: 0.8718 DKDLoss: 0.4282 
2022-05-03 10:02:21 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.3065, stu_CELoss: 0.9400 DKDLoss: 0.3665 
2022-05-03 10:02:54 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.3308, stu_CELoss: 0.9272 DKDLoss: 0.4036 
2022-05-03 10:03:27 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.1141, stu_CELoss: 0.7057 DKDLoss: 0.4084 
2022-05-03 10:04:01 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.1905, stu_CELoss: 0.7627 DKDLoss: 0.4278 
2022-05-03 10:04:34 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.2610, stu_CELoss: 0.8388 DKDLoss: 0.4222 
2022-05-03 10:05:07 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.4123, stu_CELoss: 0.9630 DKDLoss: 0.4492 
2022-05-03 10:05:41 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.1324, stu_CELoss: 0.7047 DKDLoss: 0.4277 
2022-05-03 10:06:14 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.2344, stu_CELoss: 0.8335 DKDLoss: 0.4009 
2022-05-03 10:06:48 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.1994, stu_CELoss: 0.7285 DKDLoss: 0.4709 
2022-05-03 10:07:21 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.3627, stu_CELoss: 0.8868 DKDLoss: 0.4759 
2022-05-03 10:07:55 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.2194, stu_CELoss: 0.8076 DKDLoss: 0.4118 
2022-05-03 10:08:29 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.1540, stu_CELoss: 0.7674 DKDLoss: 0.3866 
2022-05-03 10:09:03 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.2860, stu_CELoss: 0.8266 DKDLoss: 0.4593 
